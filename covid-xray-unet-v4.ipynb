{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800d70c2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:27.407059Z",
     "iopub.status.busy": "2022-08-11T08:13:27.405895Z",
     "iopub.status.idle": "2022-08-11T08:13:27.417809Z",
     "shell.execute_reply": "2022-08-11T08:13:27.416936Z"
    },
    "papermill": {
     "duration": 0.019295,
     "end_time": "2022-08-11T08:13:27.419950",
     "exception": false,
     "start_time": "2022-08-11T08:13:27.400655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781186b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:27.426380Z",
     "iopub.status.busy": "2022-08-11T08:13:27.426107Z",
     "iopub.status.idle": "2022-08-11T08:13:29.606045Z",
     "shell.execute_reply": "2022-08-11T08:13:29.605034Z"
    },
    "papermill": {
     "duration": 2.18605,
     "end_time": "2022-08-11T08:13:29.608629",
     "exception": false,
     "start_time": "2022-08-11T08:13:27.422579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "import torchvision.transforms.functional as Func\n",
    "\n",
    "\n",
    "class ToPILImage(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_pil_image(image)\n",
    "        if target is not None:\n",
    "            target = Func.to_pil_image(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        seed = np.random.randint(65536)\n",
    "        torch.manual_seed(seed)\n",
    "        crop = torchvision.transforms.RandomCrop(self.size)\n",
    "        image = crop(image)\n",
    "        if target is not None:\n",
    "            target = crop(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        super(Resize, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.resize(image, self.size)\n",
    "        if target is not None:\n",
    "            target = Func.resize(target, self.size, interpolation=Func.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = Func.hflip(image)\n",
    "            if target is not None:\n",
    "                target = Func.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __call__(self, image, target):\n",
    "        color_jitter = torchvision.transforms.ColorJitter()\n",
    "        image = color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class GrayScale(object):\n",
    "    def __call__(self, image, target):\n",
    "        gray_scale = torchvision.transforms.Grayscale()\n",
    "        image = gray_scale(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, degrees):\n",
    "        super(RandomRotation, self).__init__()\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        degree = random.randint(self.degrees[0], self.degrees[1])\n",
    "        image = Func.rotate(image, degree)\n",
    "        if target is not None:\n",
    "            target = Func.rotate(target, degree)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = Func.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_tensor(image)\n",
    "        if target is not None:\n",
    "            target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889a58a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:29.616193Z",
     "iopub.status.busy": "2022-08-11T08:13:29.615088Z",
     "iopub.status.idle": "2022-08-11T08:13:29.624486Z",
     "shell.execute_reply": "2022-08-11T08:13:29.623225Z"
    },
    "papermill": {
     "duration": 0.015204,
     "end_time": "2022-08-11T08:13:29.626808",
     "exception": false,
     "start_time": "2022-08-11T08:13:29.611604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, target_path, transforms=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data_paths = data_path\n",
    "        self.target_paths = target_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        image = plt.imread(self.data_paths[item])\n",
    "        target = plt.imread(self.target_paths[item])\n",
    "\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # image = torch.from_numpy(image)\n",
    "        # target = torch.from_numpy(target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "\n",
    "def load_data(data_path, target_path, batch_size, drop_last=False, transforms=None):\n",
    "    datas = CustomDataset(data_path=data_path, target_path=target_path, transforms=transforms)\n",
    "    data_loader = DataLoader(datas, shuffle=True, batch_size=batch_size, drop_last=drop_last)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186ed3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:29.633068Z",
     "iopub.status.busy": "2022-08-11T08:13:29.632786Z",
     "iopub.status.idle": "2022-08-11T08:13:29.657345Z",
     "shell.execute_reply": "2022-08-11T08:13:29.656436Z"
    },
    "papermill": {
     "duration": 0.030252,
     "end_time": "2022-08-11T08:13:29.659423",
     "exception": false,
     "start_time": "2022-08-11T08:13:29.629171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net模型的pytorch实现。\n",
    "    论文地址：https://arxiv.org/abs/1505.04597\n",
    "    模型的总体结构: 编码器 -> 一个ConvBlock -> 解码器 -> 一个Conv 1 * 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.eb1 = EncoderBlock(1, 64, 64, kernel_size=2)\n",
    "        self.eb2 = EncoderBlock(64, 128, 128, kernel_size=2)\n",
    "        self.eb3 = EncoderBlock(128, 256, 256, kernel_size=2)\n",
    "        self.eb4 = EncoderBlock(256, 512, 512, kernel_size=2)\n",
    "        # 编码器与解码器之间有一个ConvBlock\n",
    "        self.cb = ConvBlock(512, 1024, 1024)\n",
    "        # 解码器部分\n",
    "        self.db1 = DecoderBlock(1024, 512, 512)\n",
    "        self.db2 = DecoderBlock(512, 512, 256)\n",
    "        self.db3 = DecoderBlock(256, 128, 128)\n",
    "        self.db4 = DecoderBlock(128, 64, 64)\n",
    "        # 一个Conv 1 * 1, 二分类，结果为两个通道\n",
    "        self.conv1x1 = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ex1, skip_x1 = self.eb1(x)\n",
    "        ex2, skip_x2 = self.eb2(ex1)\n",
    "        ex3, skip_x3 = self.eb3(ex2)\n",
    "        ex4, skip_x4 = self.eb4(ex3)\n",
    "        cbx = self.cb(ex4)\n",
    "        dx1 = self.db1(cbx, skip_x4)\n",
    "        dx2 = self.db2(dx1, skip_x3)\n",
    "        dx3 = self.db3(dx2, skip_x2)\n",
    "        dx4 = self.db4(dx3, skip_x1)\n",
    "        crop = transforms.CenterCrop(size=(x.shape[-1], x.shape[-2]))\n",
    "        # normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        return self.sigmoid(self.conv1x1(crop(dx4)))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"一个Conv2d卷积后跟一个Relu激活函数，卷积核大小为3 * 3\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_relu_list = [nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(mid_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(out_channels),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "        self.conv_relu = nn.Sequential(*conv_relu_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_relu(x)\n",
    "\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    \"\"\"下采样，使用max pool方法执行，核大小为 2 * 2，用在编码器的ConvBlock后面\n",
    "\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    \"\"\"上采样，用在解码器的ConvBlock前面，使用转置卷积，同时通道数减半，\n",
    "\n",
    "    C_out = out_channels\n",
    "    H_out = (H_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    W_out = (W_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "\n",
    "    :param in_channels: 转置卷积的输入通道数\n",
    "    :param out_channels: 转置卷积的输出通道数\n",
    "    :param kernel_size: 转置卷积的卷积核大小，默认为2\n",
    "    :param stride: 转置卷积的步幅，默认为2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, dilation=1, padding=0, output_padding=1):\n",
    "        super(UpSampling, self).__init__()\n",
    "        # self.up_sample = nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "        # stride=2, kernel_size=2相当于宽高翻倍\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            dilation=dilation, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_sample(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"编码器中的一个层次块\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "        self.down_sample = DownSampling(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block(x)\n",
    "        return self.down_sample(x1), x1\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    \"\"\"跳跃连接，在通道维上连接\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 将从编码器传过来的特征图裁剪到与输入相同尺寸\n",
    "        x1 = functional.center_crop(skip_x, [x.shape[-2], x.shape[-1]])\n",
    "        if x1.shape != x.shape:\n",
    "            raise Exception('要连接的两个特征图尺寸不一致，skip_x.shape={}，x.shape={}'.format(skip_x.shape, x.shape))\n",
    "        # 通道维连接\n",
    "        return torch.cat([x, x1], dim=1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的层次块，每个层次块都是UpSampling -> Concat -> ConvBlock\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up_sample = UpSampling(in_channels, out_channels)\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x1 = self.up_sample(x)\n",
    "        concat = ConcatLayer()\n",
    "        x2 = concat(x1, skip_x)\n",
    "        return self.conv_block(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d84256b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:29.665978Z",
     "iopub.status.busy": "2022-08-11T08:13:29.665596Z",
     "iopub.status.idle": "2022-08-11T08:13:29.682624Z",
     "shell.execute_reply": "2022-08-11T08:13:29.681592Z"
    },
    "papermill": {
     "duration": 0.023003,
     "end_time": "2022-08-11T08:13:29.684975",
     "exception": false,
     "start_time": "2022-08-11T08:13:29.661972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames,SpellCheckingInspection\n",
    "def train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch=0, num_classes=2,\n",
    "          device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    loss_change_list = []\n",
    "    valid_loss_change_list = []\n",
    "    saved_last = {}\n",
    "    saved_best = {}\n",
    "    loss_change = {}\n",
    "\n",
    "    search_best = SearchBest()\n",
    "\n",
    "    for i in range(current_epoch, total_epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_onehot = convert_to_one_hot(y, num_classes=num_classes)\n",
    "            predict = model(x)\n",
    "\n",
    "            loss_value = criterion(predict, y_onehot)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "            print('Epoch {}: Batch {}/{} loss: {:.4f}'.format(i + 1, index + 1, len(train_loader), loss_value.item()))\n",
    "\n",
    "        loss_change_list.append(total_loss / len(train_loader))\n",
    "        valid_avg_loss = valid(model, criterion, valid_loader, num_classes, device)\n",
    "        valid_loss_change_list.append(valid_avg_loss)\n",
    "        print('Epoch {} train loss: {:.4f} valid loss: {:.4f}'.format(i + 1, total_loss / len(train_loader),\n",
    "                                                                      valid_avg_loss))\n",
    "        search_best(valid_avg_loss)\n",
    "        if search_best.counter == 0:\n",
    "            # save the relevant params of the best model state in the current time.\n",
    "            saved_best['best_model_state_dict'] = model.state_dict()\n",
    "            saved_best['best_optimizer_state_dict'] = optimizer.state_dict()\n",
    "            saved_best['epoch'] = i + 1\n",
    "    loss_change['train_loss_change_history'] = loss_change_list\n",
    "    loss_change['valid_loss_change_history'] = valid_loss_change_list\n",
    "    saved_last['last_model_state_dict'] = model.state_dict()\n",
    "    saved_last['last_optimizer_state_dict'] = optimizer.state_dict()\n",
    "    saved_last['epoch'] = total_epoch\n",
    "    torch.save(saved_best, './best_model.pth')\n",
    "    torch.save(saved_last, './last_model.pth')\n",
    "    torch.save(loss_change, './loss_change.pth')\n",
    "\n",
    "\n",
    "def convert_to_one_hot(data, num_classes):\n",
    "    if type(data) is not torch.Tensor:\n",
    "        raise RuntimeError('data must be a torch.Tensor')\n",
    "    if data.dtype is not torch.int64:\n",
    "        data = data.to(torch.int64)\n",
    "    data = F.one_hot(data, num_classes=num_classes).permute((0, -1, 1, 2))\n",
    "    return data.to(torch.float32)\n",
    "\n",
    "\n",
    "class SearchBest(object):\n",
    "    def __init__(self, min_delta=0, verbose=True):\n",
    "        super(SearchBest, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, valid_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = valid_loss\n",
    "        elif self.best_score - valid_loss >= self.min_delta:\n",
    "            self.best_score = valid_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print('performance reducing: {}'.format(self.counter))\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def valid(model, criterion, valid_loader, num_classes, device):\n",
    "    \"\"\"\n",
    "    :return: validate loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    valid_total_loss = 0.0\n",
    "    for index, (x, y) in enumerate(valid_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_onehot = convert_to_one_hot(y, num_classes)\n",
    "        with torch.no_grad():\n",
    "            predict = model(x)\n",
    "            valid_loss = criterion(predict, y_onehot)\n",
    "            valid_total_loss += valid_loss.item()\n",
    "    return valid_total_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9eeee09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T08:13:29.691321Z",
     "iopub.status.busy": "2022-08-11T08:13:29.691052Z",
     "iopub.status.idle": "2022-08-11T10:50:32.466616Z",
     "shell.execute_reply": "2022-08-11T10:50:32.465625Z"
    },
    "papermill": {
     "duration": 9422.781536,
     "end_time": "2022-08-11T10:50:32.469174",
     "exception": false,
     "start_time": "2022-08-11T08:13:29.687638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151: Batch 1/116 loss: 0.0381\n",
      "Epoch 151: Batch 2/116 loss: 0.0270\n",
      "Epoch 151: Batch 3/116 loss: 0.0415\n",
      "Epoch 151: Batch 4/116 loss: 0.0374\n",
      "Epoch 151: Batch 5/116 loss: 0.0569\n",
      "Epoch 151: Batch 6/116 loss: 0.0790\n",
      "Epoch 151: Batch 7/116 loss: 0.0439\n",
      "Epoch 151: Batch 8/116 loss: 0.0345\n",
      "Epoch 151: Batch 9/116 loss: 0.0461\n",
      "Epoch 151: Batch 10/116 loss: 0.0298\n",
      "Epoch 151: Batch 11/116 loss: 0.0450\n",
      "Epoch 151: Batch 12/116 loss: 0.0440\n",
      "Epoch 151: Batch 13/116 loss: 0.0365\n",
      "Epoch 151: Batch 14/116 loss: 0.0682\n",
      "Epoch 151: Batch 15/116 loss: 0.0309\n",
      "Epoch 151: Batch 16/116 loss: 0.0457\n",
      "Epoch 151: Batch 17/116 loss: 0.0660\n",
      "Epoch 151: Batch 18/116 loss: 0.0434\n",
      "Epoch 151: Batch 19/116 loss: 0.0496\n",
      "Epoch 151: Batch 20/116 loss: 0.0494\n",
      "Epoch 151: Batch 21/116 loss: 0.0479\n",
      "Epoch 151: Batch 22/116 loss: 0.0582\n",
      "Epoch 151: Batch 23/116 loss: 0.0531\n",
      "Epoch 151: Batch 24/116 loss: 0.0916\n",
      "Epoch 151: Batch 25/116 loss: 0.0536\n",
      "Epoch 151: Batch 26/116 loss: 0.0217\n",
      "Epoch 151: Batch 27/116 loss: 0.0635\n",
      "Epoch 151: Batch 28/116 loss: 0.0338\n",
      "Epoch 151: Batch 29/116 loss: 0.0412\n",
      "Epoch 151: Batch 30/116 loss: 0.0423\n",
      "Epoch 151: Batch 31/116 loss: 0.0341\n",
      "Epoch 151: Batch 32/116 loss: 0.0275\n",
      "Epoch 151: Batch 33/116 loss: 0.0389\n",
      "Epoch 151: Batch 34/116 loss: 0.0306\n",
      "Epoch 151: Batch 35/116 loss: 0.0604\n",
      "Epoch 151: Batch 36/116 loss: 0.0560\n",
      "Epoch 151: Batch 37/116 loss: 0.0613\n",
      "Epoch 151: Batch 38/116 loss: 0.0375\n",
      "Epoch 151: Batch 39/116 loss: 0.0533\n",
      "Epoch 151: Batch 40/116 loss: 0.0434\n",
      "Epoch 151: Batch 41/116 loss: 0.0463\n",
      "Epoch 151: Batch 42/116 loss: 0.0570\n",
      "Epoch 151: Batch 43/116 loss: 0.0441\n",
      "Epoch 151: Batch 44/116 loss: 0.0322\n",
      "Epoch 151: Batch 45/116 loss: 0.0642\n",
      "Epoch 151: Batch 46/116 loss: 0.0548\n",
      "Epoch 151: Batch 47/116 loss: 0.0626\n",
      "Epoch 151: Batch 48/116 loss: 0.0585\n",
      "Epoch 151: Batch 49/116 loss: 0.0372\n",
      "Epoch 151: Batch 50/116 loss: 0.0521\n",
      "Epoch 151: Batch 51/116 loss: 0.0347\n",
      "Epoch 151: Batch 52/116 loss: 0.0389\n",
      "Epoch 151: Batch 53/116 loss: 0.0561\n",
      "Epoch 151: Batch 54/116 loss: 0.0431\n",
      "Epoch 151: Batch 55/116 loss: 0.0595\n",
      "Epoch 151: Batch 56/116 loss: 0.0384\n",
      "Epoch 151: Batch 57/116 loss: 0.0363\n",
      "Epoch 151: Batch 58/116 loss: 0.0406\n",
      "Epoch 151: Batch 59/116 loss: 0.0359\n",
      "Epoch 151: Batch 60/116 loss: 0.0366\n",
      "Epoch 151: Batch 61/116 loss: 0.0411\n",
      "Epoch 151: Batch 62/116 loss: 0.0665\n",
      "Epoch 151: Batch 63/116 loss: 0.0477\n",
      "Epoch 151: Batch 64/116 loss: 0.0441\n",
      "Epoch 151: Batch 65/116 loss: 0.0363\n",
      "Epoch 151: Batch 66/116 loss: 0.0447\n",
      "Epoch 151: Batch 67/116 loss: 0.0689\n",
      "Epoch 151: Batch 68/116 loss: 0.0554\n",
      "Epoch 151: Batch 69/116 loss: 0.0444\n",
      "Epoch 151: Batch 70/116 loss: 0.0372\n",
      "Epoch 151: Batch 71/116 loss: 0.0382\n",
      "Epoch 151: Batch 72/116 loss: 0.0584\n",
      "Epoch 151: Batch 73/116 loss: 0.0289\n",
      "Epoch 151: Batch 74/116 loss: 0.0373\n",
      "Epoch 151: Batch 75/116 loss: 0.0350\n",
      "Epoch 151: Batch 76/116 loss: 0.0547\n",
      "Epoch 151: Batch 77/116 loss: 0.0307\n",
      "Epoch 151: Batch 78/116 loss: 0.0636\n",
      "Epoch 151: Batch 79/116 loss: 0.0420\n",
      "Epoch 151: Batch 80/116 loss: 0.0484\n",
      "Epoch 151: Batch 81/116 loss: 0.0402\n",
      "Epoch 151: Batch 82/116 loss: 0.0367\n",
      "Epoch 151: Batch 83/116 loss: 0.0475\n",
      "Epoch 151: Batch 84/116 loss: 0.0393\n",
      "Epoch 151: Batch 85/116 loss: 0.0426\n",
      "Epoch 151: Batch 86/116 loss: 0.0453\n",
      "Epoch 151: Batch 87/116 loss: 0.0526\n",
      "Epoch 151: Batch 88/116 loss: 0.0348\n",
      "Epoch 151: Batch 89/116 loss: 0.0402\n",
      "Epoch 151: Batch 90/116 loss: 0.0531\n",
      "Epoch 151: Batch 91/116 loss: 0.0459\n",
      "Epoch 151: Batch 92/116 loss: 0.0447\n",
      "Epoch 151: Batch 93/116 loss: 0.0595\n",
      "Epoch 151: Batch 94/116 loss: 0.0477\n",
      "Epoch 151: Batch 95/116 loss: 0.0565\n",
      "Epoch 151: Batch 96/116 loss: 0.0391\n",
      "Epoch 151: Batch 97/116 loss: 0.0316\n",
      "Epoch 151: Batch 98/116 loss: 0.0502\n",
      "Epoch 151: Batch 99/116 loss: 0.0567\n",
      "Epoch 151: Batch 100/116 loss: 0.0393\n",
      "Epoch 151: Batch 101/116 loss: 0.0314\n",
      "Epoch 151: Batch 102/116 loss: 0.0500\n",
      "Epoch 151: Batch 103/116 loss: 0.0367\n",
      "Epoch 151: Batch 104/116 loss: 0.0420\n",
      "Epoch 151: Batch 105/116 loss: 0.0400\n",
      "Epoch 151: Batch 106/116 loss: 0.0421\n",
      "Epoch 151: Batch 107/116 loss: 0.0544\n",
      "Epoch 151: Batch 108/116 loss: 0.0314\n",
      "Epoch 151: Batch 109/116 loss: 0.0558\n",
      "Epoch 151: Batch 110/116 loss: 0.0632\n",
      "Epoch 151: Batch 111/116 loss: 0.0391\n",
      "Epoch 151: Batch 112/116 loss: 0.0710\n",
      "Epoch 151: Batch 113/116 loss: 0.0343\n",
      "Epoch 151: Batch 114/116 loss: 0.0466\n",
      "Epoch 151: Batch 115/116 loss: 0.0554\n",
      "Epoch 151: Batch 116/116 loss: 0.0482\n",
      "Epoch 151 train loss: 0.0462 valid loss: 0.0495\n",
      "Epoch 152: Batch 1/116 loss: 0.0385\n",
      "Epoch 152: Batch 2/116 loss: 0.0383\n",
      "Epoch 152: Batch 3/116 loss: 0.0406\n",
      "Epoch 152: Batch 4/116 loss: 0.0283\n",
      "Epoch 152: Batch 5/116 loss: 0.0424\n",
      "Epoch 152: Batch 6/116 loss: 0.0361\n",
      "Epoch 152: Batch 7/116 loss: 0.0390\n",
      "Epoch 152: Batch 8/116 loss: 0.0454\n",
      "Epoch 152: Batch 9/116 loss: 0.0564\n",
      "Epoch 152: Batch 10/116 loss: 0.0256\n",
      "Epoch 152: Batch 11/116 loss: 0.0377\n",
      "Epoch 152: Batch 12/116 loss: 0.0356\n",
      "Epoch 152: Batch 13/116 loss: 0.0463\n",
      "Epoch 152: Batch 14/116 loss: 0.0367\n",
      "Epoch 152: Batch 15/116 loss: 0.0411\n",
      "Epoch 152: Batch 16/116 loss: 0.0435\n",
      "Epoch 152: Batch 17/116 loss: 0.0402\n",
      "Epoch 152: Batch 18/116 loss: 0.0523\n",
      "Epoch 152: Batch 19/116 loss: 0.0461\n",
      "Epoch 152: Batch 20/116 loss: 0.0514\n",
      "Epoch 152: Batch 21/116 loss: 0.0358\n",
      "Epoch 152: Batch 22/116 loss: 0.0360\n",
      "Epoch 152: Batch 23/116 loss: 0.0546\n",
      "Epoch 152: Batch 24/116 loss: 0.0434\n",
      "Epoch 152: Batch 25/116 loss: 0.0485\n",
      "Epoch 152: Batch 26/116 loss: 0.0385\n",
      "Epoch 152: Batch 27/116 loss: 0.0306\n",
      "Epoch 152: Batch 28/116 loss: 0.0489\n",
      "Epoch 152: Batch 29/116 loss: 0.0349\n",
      "Epoch 152: Batch 30/116 loss: 0.0422\n",
      "Epoch 152: Batch 31/116 loss: 0.0295\n",
      "Epoch 152: Batch 32/116 loss: 0.0564\n",
      "Epoch 152: Batch 33/116 loss: 0.0436\n",
      "Epoch 152: Batch 34/116 loss: 0.0585\n",
      "Epoch 152: Batch 35/116 loss: 0.0453\n",
      "Epoch 152: Batch 36/116 loss: 0.0380\n",
      "Epoch 152: Batch 37/116 loss: 0.0384\n",
      "Epoch 152: Batch 38/116 loss: 0.0407\n",
      "Epoch 152: Batch 39/116 loss: 0.0471\n",
      "Epoch 152: Batch 40/116 loss: 0.0630\n",
      "Epoch 152: Batch 41/116 loss: 0.0477\n",
      "Epoch 152: Batch 42/116 loss: 0.0411\n",
      "Epoch 152: Batch 43/116 loss: 0.0353\n",
      "Epoch 152: Batch 44/116 loss: 0.0510\n",
      "Epoch 152: Batch 45/116 loss: 0.0592\n",
      "Epoch 152: Batch 46/116 loss: 0.0661\n",
      "Epoch 152: Batch 47/116 loss: 0.0419\n",
      "Epoch 152: Batch 48/116 loss: 0.0476\n",
      "Epoch 152: Batch 49/116 loss: 0.0627\n",
      "Epoch 152: Batch 50/116 loss: 0.0399\n",
      "Epoch 152: Batch 51/116 loss: 0.0248\n",
      "Epoch 152: Batch 52/116 loss: 0.0432\n",
      "Epoch 152: Batch 53/116 loss: 0.0512\n",
      "Epoch 152: Batch 54/116 loss: 0.0411\n",
      "Epoch 152: Batch 55/116 loss: 0.0444\n",
      "Epoch 152: Batch 56/116 loss: 0.0636\n",
      "Epoch 152: Batch 57/116 loss: 0.0564\n",
      "Epoch 152: Batch 58/116 loss: 0.0544\n",
      "Epoch 152: Batch 59/116 loss: 0.0521\n",
      "Epoch 152: Batch 60/116 loss: 0.0402\n",
      "Epoch 152: Batch 61/116 loss: 0.0306\n",
      "Epoch 152: Batch 62/116 loss: 0.0388\n",
      "Epoch 152: Batch 63/116 loss: 0.0526\n",
      "Epoch 152: Batch 64/116 loss: 0.0366\n",
      "Epoch 152: Batch 65/116 loss: 0.0375\n",
      "Epoch 152: Batch 66/116 loss: 0.0532\n",
      "Epoch 152: Batch 67/116 loss: 0.0554\n",
      "Epoch 152: Batch 68/116 loss: 0.0433\n",
      "Epoch 152: Batch 69/116 loss: 0.0456\n",
      "Epoch 152: Batch 70/116 loss: 0.0570\n",
      "Epoch 152: Batch 71/116 loss: 0.0485\n",
      "Epoch 152: Batch 72/116 loss: 0.0471\n",
      "Epoch 152: Batch 73/116 loss: 0.0376\n",
      "Epoch 152: Batch 74/116 loss: 0.0425\n",
      "Epoch 152: Batch 75/116 loss: 0.0413\n",
      "Epoch 152: Batch 76/116 loss: 0.0358\n",
      "Epoch 152: Batch 77/116 loss: 0.0450\n",
      "Epoch 152: Batch 78/116 loss: 0.0311\n",
      "Epoch 152: Batch 79/116 loss: 0.0378\n",
      "Epoch 152: Batch 80/116 loss: 0.0449\n",
      "Epoch 152: Batch 81/116 loss: 0.0422\n",
      "Epoch 152: Batch 82/116 loss: 0.0351\n",
      "Epoch 152: Batch 83/116 loss: 0.0369\n",
      "Epoch 152: Batch 84/116 loss: 0.0462\n",
      "Epoch 152: Batch 85/116 loss: 0.0603\n",
      "Epoch 152: Batch 86/116 loss: 0.0385\n",
      "Epoch 152: Batch 87/116 loss: 0.0738\n",
      "Epoch 152: Batch 88/116 loss: 0.0413\n",
      "Epoch 152: Batch 89/116 loss: 0.0646\n",
      "Epoch 152: Batch 90/116 loss: 0.0488\n",
      "Epoch 152: Batch 91/116 loss: 0.0463\n",
      "Epoch 152: Batch 92/116 loss: 0.0394\n",
      "Epoch 152: Batch 93/116 loss: 0.0402\n",
      "Epoch 152: Batch 94/116 loss: 0.0573\n",
      "Epoch 152: Batch 95/116 loss: 0.0413\n",
      "Epoch 152: Batch 96/116 loss: 0.0460\n",
      "Epoch 152: Batch 97/116 loss: 0.0535\n",
      "Epoch 152: Batch 98/116 loss: 0.0387\n",
      "Epoch 152: Batch 99/116 loss: 0.0575\n",
      "Epoch 152: Batch 100/116 loss: 0.0517\n",
      "Epoch 152: Batch 101/116 loss: 0.0614\n",
      "Epoch 152: Batch 102/116 loss: 0.0337\n",
      "Epoch 152: Batch 103/116 loss: 0.0452\n",
      "Epoch 152: Batch 104/116 loss: 0.0309\n",
      "Epoch 152: Batch 105/116 loss: 0.0399\n",
      "Epoch 152: Batch 106/116 loss: 0.0334\n",
      "Epoch 152: Batch 107/116 loss: 0.0526\n",
      "Epoch 152: Batch 108/116 loss: 0.0284\n",
      "Epoch 152: Batch 109/116 loss: 0.0363\n",
      "Epoch 152: Batch 110/116 loss: 0.0532\n",
      "Epoch 152: Batch 111/116 loss: 0.0444\n",
      "Epoch 152: Batch 112/116 loss: 0.0466\n",
      "Epoch 152: Batch 113/116 loss: 0.0341\n",
      "Epoch 152: Batch 114/116 loss: 0.0373\n",
      "Epoch 152: Batch 115/116 loss: 0.0494\n",
      "Epoch 152: Batch 116/116 loss: 0.0662\n",
      "Epoch 152 train loss: 0.0446 valid loss: 0.0596\n",
      "performance reducing: 1\n",
      "Epoch 153: Batch 1/116 loss: 0.0279\n",
      "Epoch 153: Batch 2/116 loss: 0.0621\n",
      "Epoch 153: Batch 3/116 loss: 0.0521\n",
      "Epoch 153: Batch 4/116 loss: 0.0487\n",
      "Epoch 153: Batch 5/116 loss: 0.0485\n",
      "Epoch 153: Batch 6/116 loss: 0.0307\n",
      "Epoch 153: Batch 7/116 loss: 0.0392\n",
      "Epoch 153: Batch 8/116 loss: 0.0362\n",
      "Epoch 153: Batch 9/116 loss: 0.0316\n",
      "Epoch 153: Batch 10/116 loss: 0.0486\n",
      "Epoch 153: Batch 11/116 loss: 0.0308\n",
      "Epoch 153: Batch 12/116 loss: 0.0336\n",
      "Epoch 153: Batch 13/116 loss: 0.0426\n",
      "Epoch 153: Batch 14/116 loss: 0.0798\n",
      "Epoch 153: Batch 15/116 loss: 0.0604\n",
      "Epoch 153: Batch 16/116 loss: 0.0467\n",
      "Epoch 153: Batch 17/116 loss: 0.0393\n",
      "Epoch 153: Batch 18/116 loss: 0.0335\n",
      "Epoch 153: Batch 19/116 loss: 0.0599\n",
      "Epoch 153: Batch 20/116 loss: 0.0385\n",
      "Epoch 153: Batch 21/116 loss: 0.0326\n",
      "Epoch 153: Batch 22/116 loss: 0.0459\n",
      "Epoch 153: Batch 23/116 loss: 0.0268\n",
      "Epoch 153: Batch 24/116 loss: 0.0633\n",
      "Epoch 153: Batch 25/116 loss: 0.0402\n",
      "Epoch 153: Batch 26/116 loss: 0.0526\n",
      "Epoch 153: Batch 27/116 loss: 0.0414\n",
      "Epoch 153: Batch 28/116 loss: 0.0468\n",
      "Epoch 153: Batch 29/116 loss: 0.0380\n",
      "Epoch 153: Batch 30/116 loss: 0.0362\n",
      "Epoch 153: Batch 31/116 loss: 0.0329\n",
      "Epoch 153: Batch 32/116 loss: 0.0400\n",
      "Epoch 153: Batch 33/116 loss: 0.0436\n",
      "Epoch 153: Batch 34/116 loss: 0.0342\n",
      "Epoch 153: Batch 35/116 loss: 0.0246\n",
      "Epoch 153: Batch 36/116 loss: 0.0568\n",
      "Epoch 153: Batch 37/116 loss: 0.0305\n",
      "Epoch 153: Batch 38/116 loss: 0.0457\n",
      "Epoch 153: Batch 39/116 loss: 0.0408\n",
      "Epoch 153: Batch 40/116 loss: 0.0424\n",
      "Epoch 153: Batch 41/116 loss: 0.0359\n",
      "Epoch 153: Batch 42/116 loss: 0.0474\n",
      "Epoch 153: Batch 43/116 loss: 0.0406\n",
      "Epoch 153: Batch 44/116 loss: 0.0639\n",
      "Epoch 153: Batch 45/116 loss: 0.0599\n",
      "Epoch 153: Batch 46/116 loss: 0.0516\n",
      "Epoch 153: Batch 47/116 loss: 0.0572\n",
      "Epoch 153: Batch 48/116 loss: 0.0819\n",
      "Epoch 153: Batch 49/116 loss: 0.0497\n",
      "Epoch 153: Batch 50/116 loss: 0.0503\n",
      "Epoch 153: Batch 51/116 loss: 0.0362\n",
      "Epoch 153: Batch 52/116 loss: 0.0823\n",
      "Epoch 153: Batch 53/116 loss: 0.0568\n",
      "Epoch 153: Batch 54/116 loss: 0.0457\n",
      "Epoch 153: Batch 55/116 loss: 0.0385\n",
      "Epoch 153: Batch 56/116 loss: 0.0393\n",
      "Epoch 153: Batch 57/116 loss: 0.0631\n",
      "Epoch 153: Batch 58/116 loss: 0.0337\n",
      "Epoch 153: Batch 59/116 loss: 0.0542\n",
      "Epoch 153: Batch 60/116 loss: 0.0438\n",
      "Epoch 153: Batch 61/116 loss: 0.0456\n",
      "Epoch 153: Batch 62/116 loss: 0.0423\n",
      "Epoch 153: Batch 63/116 loss: 0.0446\n",
      "Epoch 153: Batch 64/116 loss: 0.0300\n",
      "Epoch 153: Batch 65/116 loss: 0.0607\n",
      "Epoch 153: Batch 66/116 loss: 0.0440\n",
      "Epoch 153: Batch 67/116 loss: 0.0284\n",
      "Epoch 153: Batch 68/116 loss: 0.0527\n",
      "Epoch 153: Batch 69/116 loss: 0.0444\n",
      "Epoch 153: Batch 70/116 loss: 0.0377\n",
      "Epoch 153: Batch 71/116 loss: 0.0470\n",
      "Epoch 153: Batch 72/116 loss: 0.0582\n",
      "Epoch 153: Batch 73/116 loss: 0.0263\n",
      "Epoch 153: Batch 74/116 loss: 0.0459\n",
      "Epoch 153: Batch 75/116 loss: 0.0320\n",
      "Epoch 153: Batch 76/116 loss: 0.0395\n",
      "Epoch 153: Batch 77/116 loss: 0.0382\n",
      "Epoch 153: Batch 78/116 loss: 0.0447\n",
      "Epoch 153: Batch 79/116 loss: 0.0438\n",
      "Epoch 153: Batch 80/116 loss: 0.0297\n",
      "Epoch 153: Batch 81/116 loss: 0.0433\n",
      "Epoch 153: Batch 82/116 loss: 0.0344\n",
      "Epoch 153: Batch 83/116 loss: 0.0288\n",
      "Epoch 153: Batch 84/116 loss: 0.0322\n",
      "Epoch 153: Batch 85/116 loss: 0.0385\n",
      "Epoch 153: Batch 86/116 loss: 0.0374\n",
      "Epoch 153: Batch 87/116 loss: 0.0488\n",
      "Epoch 153: Batch 88/116 loss: 0.0453\n",
      "Epoch 153: Batch 89/116 loss: 0.0363\n",
      "Epoch 153: Batch 90/116 loss: 0.0494\n",
      "Epoch 153: Batch 91/116 loss: 0.0430\n",
      "Epoch 153: Batch 92/116 loss: 0.0412\n",
      "Epoch 153: Batch 93/116 loss: 0.0600\n",
      "Epoch 153: Batch 94/116 loss: 0.0531\n",
      "Epoch 153: Batch 95/116 loss: 0.0582\n",
      "Epoch 153: Batch 96/116 loss: 0.0588\n",
      "Epoch 153: Batch 97/116 loss: 0.0346\n",
      "Epoch 153: Batch 98/116 loss: 0.0332\n",
      "Epoch 153: Batch 99/116 loss: 0.0642\n",
      "Epoch 153: Batch 100/116 loss: 0.0362\n",
      "Epoch 153: Batch 101/116 loss: 0.0592\n",
      "Epoch 153: Batch 102/116 loss: 0.0398\n",
      "Epoch 153: Batch 103/116 loss: 0.0400\n",
      "Epoch 153: Batch 104/116 loss: 0.0512\n",
      "Epoch 153: Batch 105/116 loss: 0.0347\n",
      "Epoch 153: Batch 106/116 loss: 0.0456\n",
      "Epoch 153: Batch 107/116 loss: 0.0466\n",
      "Epoch 153: Batch 108/116 loss: 0.0509\n",
      "Epoch 153: Batch 109/116 loss: 0.0348\n",
      "Epoch 153: Batch 110/116 loss: 0.0567\n",
      "Epoch 153: Batch 111/116 loss: 0.0244\n",
      "Epoch 153: Batch 112/116 loss: 0.0535\n",
      "Epoch 153: Batch 113/116 loss: 0.0458\n",
      "Epoch 153: Batch 114/116 loss: 0.0527\n",
      "Epoch 153: Batch 115/116 loss: 0.0381\n",
      "Epoch 153: Batch 116/116 loss: 0.0407\n",
      "Epoch 153 train loss: 0.0445 valid loss: 0.0543\n",
      "performance reducing: 2\n",
      "Epoch 154: Batch 1/116 loss: 0.0451\n",
      "Epoch 154: Batch 2/116 loss: 0.0569\n",
      "Epoch 154: Batch 3/116 loss: 0.0511\n",
      "Epoch 154: Batch 4/116 loss: 0.0532\n",
      "Epoch 154: Batch 5/116 loss: 0.0520\n",
      "Epoch 154: Batch 6/116 loss: 0.0592\n",
      "Epoch 154: Batch 7/116 loss: 0.0443\n",
      "Epoch 154: Batch 8/116 loss: 0.0563\n",
      "Epoch 154: Batch 9/116 loss: 0.0463\n",
      "Epoch 154: Batch 10/116 loss: 0.0389\n",
      "Epoch 154: Batch 11/116 loss: 0.0530\n",
      "Epoch 154: Batch 12/116 loss: 0.0374\n",
      "Epoch 154: Batch 13/116 loss: 0.0476\n",
      "Epoch 154: Batch 14/116 loss: 0.0472\n",
      "Epoch 154: Batch 15/116 loss: 0.0438\n",
      "Epoch 154: Batch 16/116 loss: 0.0338\n",
      "Epoch 154: Batch 17/116 loss: 0.0404\n",
      "Epoch 154: Batch 18/116 loss: 0.0729\n",
      "Epoch 154: Batch 19/116 loss: 0.0396\n",
      "Epoch 154: Batch 20/116 loss: 0.0360\n",
      "Epoch 154: Batch 21/116 loss: 0.0428\n",
      "Epoch 154: Batch 22/116 loss: 0.0425\n",
      "Epoch 154: Batch 23/116 loss: 0.0495\n",
      "Epoch 154: Batch 24/116 loss: 0.0535\n",
      "Epoch 154: Batch 25/116 loss: 0.0393\n",
      "Epoch 154: Batch 26/116 loss: 0.0472\n",
      "Epoch 154: Batch 27/116 loss: 0.0675\n",
      "Epoch 154: Batch 28/116 loss: 0.0326\n",
      "Epoch 154: Batch 29/116 loss: 0.0393\n",
      "Epoch 154: Batch 30/116 loss: 0.0332\n",
      "Epoch 154: Batch 31/116 loss: 0.0340\n",
      "Epoch 154: Batch 32/116 loss: 0.0544\n",
      "Epoch 154: Batch 33/116 loss: 0.0404\n",
      "Epoch 154: Batch 34/116 loss: 0.0524\n",
      "Epoch 154: Batch 35/116 loss: 0.0555\n",
      "Epoch 154: Batch 36/116 loss: 0.0419\n",
      "Epoch 154: Batch 37/116 loss: 0.0454\n",
      "Epoch 154: Batch 38/116 loss: 0.0374\n",
      "Epoch 154: Batch 39/116 loss: 0.0364\n",
      "Epoch 154: Batch 40/116 loss: 0.0265\n",
      "Epoch 154: Batch 41/116 loss: 0.0418\n",
      "Epoch 154: Batch 42/116 loss: 0.0381\n",
      "Epoch 154: Batch 43/116 loss: 0.0326\n",
      "Epoch 154: Batch 44/116 loss: 0.0392\n",
      "Epoch 154: Batch 45/116 loss: 0.0515\n",
      "Epoch 154: Batch 46/116 loss: 0.0352\n",
      "Epoch 154: Batch 47/116 loss: 0.0447\n",
      "Epoch 154: Batch 48/116 loss: 0.0492\n",
      "Epoch 154: Batch 49/116 loss: 0.0534\n",
      "Epoch 154: Batch 50/116 loss: 0.0434\n",
      "Epoch 154: Batch 51/116 loss: 0.0375\n",
      "Epoch 154: Batch 52/116 loss: 0.0551\n",
      "Epoch 154: Batch 53/116 loss: 0.0355\n",
      "Epoch 154: Batch 54/116 loss: 0.0799\n",
      "Epoch 154: Batch 55/116 loss: 0.0581\n",
      "Epoch 154: Batch 56/116 loss: 0.0567\n",
      "Epoch 154: Batch 57/116 loss: 0.0385\n",
      "Epoch 154: Batch 58/116 loss: 0.0671\n",
      "Epoch 154: Batch 59/116 loss: 0.0429\n",
      "Epoch 154: Batch 60/116 loss: 0.0450\n",
      "Epoch 154: Batch 61/116 loss: 0.0423\n",
      "Epoch 154: Batch 62/116 loss: 0.0330\n",
      "Epoch 154: Batch 63/116 loss: 0.0425\n",
      "Epoch 154: Batch 64/116 loss: 0.0343\n",
      "Epoch 154: Batch 65/116 loss: 0.0488\n",
      "Epoch 154: Batch 66/116 loss: 0.0492\n",
      "Epoch 154: Batch 67/116 loss: 0.0469\n",
      "Epoch 154: Batch 68/116 loss: 0.0480\n",
      "Epoch 154: Batch 69/116 loss: 0.0275\n",
      "Epoch 154: Batch 70/116 loss: 0.0261\n",
      "Epoch 154: Batch 71/116 loss: 0.0340\n",
      "Epoch 154: Batch 72/116 loss: 0.0442\n",
      "Epoch 154: Batch 73/116 loss: 0.0416\n",
      "Epoch 154: Batch 74/116 loss: 0.0414\n",
      "Epoch 154: Batch 75/116 loss: 0.0565\n",
      "Epoch 154: Batch 76/116 loss: 0.0546\n",
      "Epoch 154: Batch 77/116 loss: 0.0455\n",
      "Epoch 154: Batch 78/116 loss: 0.0291\n",
      "Epoch 154: Batch 79/116 loss: 0.0659\n",
      "Epoch 154: Batch 80/116 loss: 0.0450\n",
      "Epoch 154: Batch 81/116 loss: 0.0498\n",
      "Epoch 154: Batch 82/116 loss: 0.0430\n",
      "Epoch 154: Batch 83/116 loss: 0.0310\n",
      "Epoch 154: Batch 84/116 loss: 0.0381\n",
      "Epoch 154: Batch 85/116 loss: 0.0683\n",
      "Epoch 154: Batch 86/116 loss: 0.0680\n",
      "Epoch 154: Batch 87/116 loss: 0.0399\n",
      "Epoch 154: Batch 88/116 loss: 0.0443\n",
      "Epoch 154: Batch 89/116 loss: 0.0410\n",
      "Epoch 154: Batch 90/116 loss: 0.0431\n",
      "Epoch 154: Batch 91/116 loss: 0.0388\n",
      "Epoch 154: Batch 92/116 loss: 0.0380\n",
      "Epoch 154: Batch 93/116 loss: 0.0520\n",
      "Epoch 154: Batch 94/116 loss: 0.0464\n",
      "Epoch 154: Batch 95/116 loss: 0.0508\n",
      "Epoch 154: Batch 96/116 loss: 0.0411\n",
      "Epoch 154: Batch 97/116 loss: 0.0499\n",
      "Epoch 154: Batch 98/116 loss: 0.0332\n",
      "Epoch 154: Batch 99/116 loss: 0.0364\n",
      "Epoch 154: Batch 100/116 loss: 0.0429\n",
      "Epoch 154: Batch 101/116 loss: 0.0511\n",
      "Epoch 154: Batch 102/116 loss: 0.0565\n",
      "Epoch 154: Batch 103/116 loss: 0.0421\n",
      "Epoch 154: Batch 104/116 loss: 0.0450\n",
      "Epoch 154: Batch 105/116 loss: 0.0370\n",
      "Epoch 154: Batch 106/116 loss: 0.0523\n",
      "Epoch 154: Batch 107/116 loss: 0.0421\n",
      "Epoch 154: Batch 108/116 loss: 0.0446\n",
      "Epoch 154: Batch 109/116 loss: 0.0308\n",
      "Epoch 154: Batch 110/116 loss: 0.0353\n",
      "Epoch 154: Batch 111/116 loss: 0.0337\n",
      "Epoch 154: Batch 112/116 loss: 0.0387\n",
      "Epoch 154: Batch 113/116 loss: 0.0347\n",
      "Epoch 154: Batch 114/116 loss: 0.0423\n",
      "Epoch 154: Batch 115/116 loss: 0.0566\n",
      "Epoch 154: Batch 116/116 loss: 0.0237\n",
      "Epoch 154 train loss: 0.0448 valid loss: 0.0550\n",
      "performance reducing: 3\n",
      "Epoch 155: Batch 1/116 loss: 0.0525\n",
      "Epoch 155: Batch 2/116 loss: 0.0326\n",
      "Epoch 155: Batch 3/116 loss: 0.0520\n",
      "Epoch 155: Batch 4/116 loss: 0.0419\n",
      "Epoch 155: Batch 5/116 loss: 0.0210\n",
      "Epoch 155: Batch 6/116 loss: 0.0395\n",
      "Epoch 155: Batch 7/116 loss: 0.0386\n",
      "Epoch 155: Batch 8/116 loss: 0.0595\n",
      "Epoch 155: Batch 9/116 loss: 0.0279\n",
      "Epoch 155: Batch 10/116 loss: 0.0479\n",
      "Epoch 155: Batch 11/116 loss: 0.0416\n",
      "Epoch 155: Batch 12/116 loss: 0.0450\n",
      "Epoch 155: Batch 13/116 loss: 0.0498\n",
      "Epoch 155: Batch 14/116 loss: 0.0274\n",
      "Epoch 155: Batch 15/116 loss: 0.0431\n",
      "Epoch 155: Batch 16/116 loss: 0.0642\n",
      "Epoch 155: Batch 17/116 loss: 0.0522\n",
      "Epoch 155: Batch 18/116 loss: 0.0516\n",
      "Epoch 155: Batch 19/116 loss: 0.0421\n",
      "Epoch 155: Batch 20/116 loss: 0.0362\n",
      "Epoch 155: Batch 21/116 loss: 0.0441\n",
      "Epoch 155: Batch 22/116 loss: 0.0325\n",
      "Epoch 155: Batch 23/116 loss: 0.0337\n",
      "Epoch 155: Batch 24/116 loss: 0.0307\n",
      "Epoch 155: Batch 25/116 loss: 0.0424\n",
      "Epoch 155: Batch 26/116 loss: 0.0514\n",
      "Epoch 155: Batch 27/116 loss: 0.0592\n",
      "Epoch 155: Batch 28/116 loss: 0.0475\n",
      "Epoch 155: Batch 29/116 loss: 0.0467\n",
      "Epoch 155: Batch 30/116 loss: 0.0338\n",
      "Epoch 155: Batch 31/116 loss: 0.0385\n",
      "Epoch 155: Batch 32/116 loss: 0.0504\n",
      "Epoch 155: Batch 33/116 loss: 0.0690\n",
      "Epoch 155: Batch 34/116 loss: 0.0403\n",
      "Epoch 155: Batch 35/116 loss: 0.0568\n",
      "Epoch 155: Batch 36/116 loss: 0.0654\n",
      "Epoch 155: Batch 37/116 loss: 0.0476\n",
      "Epoch 155: Batch 38/116 loss: 0.0568\n",
      "Epoch 155: Batch 39/116 loss: 0.0456\n",
      "Epoch 155: Batch 40/116 loss: 0.0521\n",
      "Epoch 155: Batch 41/116 loss: 0.0376\n",
      "Epoch 155: Batch 42/116 loss: 0.0601\n",
      "Epoch 155: Batch 43/116 loss: 0.0385\n",
      "Epoch 155: Batch 44/116 loss: 0.0402\n",
      "Epoch 155: Batch 45/116 loss: 0.0685\n",
      "Epoch 155: Batch 46/116 loss: 0.0479\n",
      "Epoch 155: Batch 47/116 loss: 0.0394\n",
      "Epoch 155: Batch 48/116 loss: 0.0268\n",
      "Epoch 155: Batch 49/116 loss: 0.0385\n",
      "Epoch 155: Batch 50/116 loss: 0.0404\n",
      "Epoch 155: Batch 51/116 loss: 0.0398\n",
      "Epoch 155: Batch 52/116 loss: 0.0332\n",
      "Epoch 155: Batch 53/116 loss: 0.0425\n",
      "Epoch 155: Batch 54/116 loss: 0.0441\n",
      "Epoch 155: Batch 55/116 loss: 0.0245\n",
      "Epoch 155: Batch 56/116 loss: 0.0328\n",
      "Epoch 155: Batch 57/116 loss: 0.0317\n",
      "Epoch 155: Batch 58/116 loss: 0.0464\n",
      "Epoch 155: Batch 59/116 loss: 0.0423\n",
      "Epoch 155: Batch 60/116 loss: 0.0479\n",
      "Epoch 155: Batch 61/116 loss: 0.0468\n",
      "Epoch 155: Batch 62/116 loss: 0.0332\n",
      "Epoch 155: Batch 63/116 loss: 0.0696\n",
      "Epoch 155: Batch 64/116 loss: 0.0434\n",
      "Epoch 155: Batch 65/116 loss: 0.0502\n",
      "Epoch 155: Batch 66/116 loss: 0.0370\n",
      "Epoch 155: Batch 67/116 loss: 0.0496\n",
      "Epoch 155: Batch 68/116 loss: 0.0365\n",
      "Epoch 155: Batch 69/116 loss: 0.0384\n",
      "Epoch 155: Batch 70/116 loss: 0.0373\n",
      "Epoch 155: Batch 71/116 loss: 0.0431\n",
      "Epoch 155: Batch 72/116 loss: 0.0611\n",
      "Epoch 155: Batch 73/116 loss: 0.0501\n",
      "Epoch 155: Batch 74/116 loss: 0.0391\n",
      "Epoch 155: Batch 75/116 loss: 0.0434\n",
      "Epoch 155: Batch 76/116 loss: 0.0532\n",
      "Epoch 155: Batch 77/116 loss: 0.0331\n",
      "Epoch 155: Batch 78/116 loss: 0.0456\n",
      "Epoch 155: Batch 79/116 loss: 0.0509\n",
      "Epoch 155: Batch 80/116 loss: 0.0426\n",
      "Epoch 155: Batch 81/116 loss: 0.0601\n",
      "Epoch 155: Batch 82/116 loss: 0.0350\n",
      "Epoch 155: Batch 83/116 loss: 0.0440\n",
      "Epoch 155: Batch 84/116 loss: 0.0347\n",
      "Epoch 155: Batch 85/116 loss: 0.0449\n",
      "Epoch 155: Batch 86/116 loss: 0.0261\n",
      "Epoch 155: Batch 87/116 loss: 0.0823\n",
      "Epoch 155: Batch 88/116 loss: 0.0418\n",
      "Epoch 155: Batch 89/116 loss: 0.0596\n",
      "Epoch 155: Batch 90/116 loss: 0.0540\n",
      "Epoch 155: Batch 91/116 loss: 0.0443\n",
      "Epoch 155: Batch 92/116 loss: 0.0480\n",
      "Epoch 155: Batch 93/116 loss: 0.0431\n",
      "Epoch 155: Batch 94/116 loss: 0.0495\n",
      "Epoch 155: Batch 95/116 loss: 0.0462\n",
      "Epoch 155: Batch 96/116 loss: 0.0599\n",
      "Epoch 155: Batch 97/116 loss: 0.0304\n",
      "Epoch 155: Batch 98/116 loss: 0.0512\n",
      "Epoch 155: Batch 99/116 loss: 0.0300\n",
      "Epoch 155: Batch 100/116 loss: 0.0792\n",
      "Epoch 155: Batch 101/116 loss: 0.0553\n",
      "Epoch 155: Batch 102/116 loss: 0.0437\n",
      "Epoch 155: Batch 103/116 loss: 0.0304\n",
      "Epoch 155: Batch 104/116 loss: 0.0503\n",
      "Epoch 155: Batch 105/116 loss: 0.0476\n",
      "Epoch 155: Batch 106/116 loss: 0.0362\n",
      "Epoch 155: Batch 107/116 loss: 0.0428\n",
      "Epoch 155: Batch 108/116 loss: 0.0570\n",
      "Epoch 155: Batch 109/116 loss: 0.0345\n",
      "Epoch 155: Batch 110/116 loss: 0.0333\n",
      "Epoch 155: Batch 111/116 loss: 0.0352\n",
      "Epoch 155: Batch 112/116 loss: 0.0469\n",
      "Epoch 155: Batch 113/116 loss: 0.0394\n",
      "Epoch 155: Batch 114/116 loss: 0.0337\n",
      "Epoch 155: Batch 115/116 loss: 0.0464\n",
      "Epoch 155: Batch 116/116 loss: 0.0332\n",
      "Epoch 155 train loss: 0.0446 valid loss: 0.0502\n",
      "performance reducing: 4\n",
      "Epoch 156: Batch 1/116 loss: 0.0445\n",
      "Epoch 156: Batch 2/116 loss: 0.0365\n",
      "Epoch 156: Batch 3/116 loss: 0.0366\n",
      "Epoch 156: Batch 4/116 loss: 0.0501\n",
      "Epoch 156: Batch 5/116 loss: 0.0500\n",
      "Epoch 156: Batch 6/116 loss: 0.0401\n",
      "Epoch 156: Batch 7/116 loss: 0.0317\n",
      "Epoch 156: Batch 8/116 loss: 0.0345\n",
      "Epoch 156: Batch 9/116 loss: 0.0519\n",
      "Epoch 156: Batch 10/116 loss: 0.0514\n",
      "Epoch 156: Batch 11/116 loss: 0.0362\n",
      "Epoch 156: Batch 12/116 loss: 0.0409\n",
      "Epoch 156: Batch 13/116 loss: 0.0289\n",
      "Epoch 156: Batch 14/116 loss: 0.0604\n",
      "Epoch 156: Batch 15/116 loss: 0.0305\n",
      "Epoch 156: Batch 16/116 loss: 0.0511\n",
      "Epoch 156: Batch 17/116 loss: 0.0344\n",
      "Epoch 156: Batch 18/116 loss: 0.0649\n",
      "Epoch 156: Batch 19/116 loss: 0.0437\n",
      "Epoch 156: Batch 20/116 loss: 0.0334\n",
      "Epoch 156: Batch 21/116 loss: 0.0558\n",
      "Epoch 156: Batch 22/116 loss: 0.0408\n",
      "Epoch 156: Batch 23/116 loss: 0.0619\n",
      "Epoch 156: Batch 24/116 loss: 0.0531\n",
      "Epoch 156: Batch 25/116 loss: 0.0434\n",
      "Epoch 156: Batch 26/116 loss: 0.0405\n",
      "Epoch 156: Batch 27/116 loss: 0.0414\n",
      "Epoch 156: Batch 28/116 loss: 0.0445\n",
      "Epoch 156: Batch 29/116 loss: 0.0632\n",
      "Epoch 156: Batch 30/116 loss: 0.0474\n",
      "Epoch 156: Batch 31/116 loss: 0.0408\n",
      "Epoch 156: Batch 32/116 loss: 0.0332\n",
      "Epoch 156: Batch 33/116 loss: 0.0486\n",
      "Epoch 156: Batch 34/116 loss: 0.0412\n",
      "Epoch 156: Batch 35/116 loss: 0.0498\n",
      "Epoch 156: Batch 36/116 loss: 0.0434\n",
      "Epoch 156: Batch 37/116 loss: 0.0369\n",
      "Epoch 156: Batch 38/116 loss: 0.0435\n",
      "Epoch 156: Batch 39/116 loss: 0.0371\n",
      "Epoch 156: Batch 40/116 loss: 0.0473\n",
      "Epoch 156: Batch 41/116 loss: 0.0465\n",
      "Epoch 156: Batch 42/116 loss: 0.0432\n",
      "Epoch 156: Batch 43/116 loss: 0.0489\n",
      "Epoch 156: Batch 44/116 loss: 0.0392\n",
      "Epoch 156: Batch 45/116 loss: 0.0371\n",
      "Epoch 156: Batch 46/116 loss: 0.0326\n",
      "Epoch 156: Batch 47/116 loss: 0.0356\n",
      "Epoch 156: Batch 48/116 loss: 0.0335\n",
      "Epoch 156: Batch 49/116 loss: 0.0466\n",
      "Epoch 156: Batch 50/116 loss: 0.0286\n",
      "Epoch 156: Batch 51/116 loss: 0.0201\n",
      "Epoch 156: Batch 52/116 loss: 0.0477\n",
      "Epoch 156: Batch 53/116 loss: 0.0280\n",
      "Epoch 156: Batch 54/116 loss: 0.0405\n",
      "Epoch 156: Batch 55/116 loss: 0.0376\n",
      "Epoch 156: Batch 56/116 loss: 0.0638\n",
      "Epoch 156: Batch 57/116 loss: 0.0266\n",
      "Epoch 156: Batch 58/116 loss: 0.0412\n",
      "Epoch 156: Batch 59/116 loss: 0.0330\n",
      "Epoch 156: Batch 60/116 loss: 0.0812\n",
      "Epoch 156: Batch 61/116 loss: 0.0338\n",
      "Epoch 156: Batch 62/116 loss: 0.0329\n",
      "Epoch 156: Batch 63/116 loss: 0.0674\n",
      "Epoch 156: Batch 64/116 loss: 0.0269\n",
      "Epoch 156: Batch 65/116 loss: 0.0514\n",
      "Epoch 156: Batch 66/116 loss: 0.0292\n",
      "Epoch 156: Batch 67/116 loss: 0.0465\n",
      "Epoch 156: Batch 68/116 loss: 0.0428\n",
      "Epoch 156: Batch 69/116 loss: 0.0553\n",
      "Epoch 156: Batch 70/116 loss: 0.0662\n",
      "Epoch 156: Batch 71/116 loss: 0.0407\n",
      "Epoch 156: Batch 72/116 loss: 0.0564\n",
      "Epoch 156: Batch 73/116 loss: 0.0315\n",
      "Epoch 156: Batch 74/116 loss: 0.0431\n",
      "Epoch 156: Batch 75/116 loss: 0.0490\n",
      "Epoch 156: Batch 76/116 loss: 0.0387\n",
      "Epoch 156: Batch 77/116 loss: 0.0667\n",
      "Epoch 156: Batch 78/116 loss: 0.0590\n",
      "Epoch 156: Batch 79/116 loss: 0.0585\n",
      "Epoch 156: Batch 80/116 loss: 0.0504\n",
      "Epoch 156: Batch 81/116 loss: 0.0449\n",
      "Epoch 156: Batch 82/116 loss: 0.0467\n",
      "Epoch 156: Batch 83/116 loss: 0.0487\n",
      "Epoch 156: Batch 84/116 loss: 0.0451\n",
      "Epoch 156: Batch 85/116 loss: 0.0503\n",
      "Epoch 156: Batch 86/116 loss: 0.0400\n",
      "Epoch 156: Batch 87/116 loss: 0.0449\n",
      "Epoch 156: Batch 88/116 loss: 0.0484\n",
      "Epoch 156: Batch 89/116 loss: 0.0676\n",
      "Epoch 156: Batch 90/116 loss: 0.0290\n",
      "Epoch 156: Batch 91/116 loss: 0.0361\n",
      "Epoch 156: Batch 92/116 loss: 0.0317\n",
      "Epoch 156: Batch 93/116 loss: 0.0261\n",
      "Epoch 156: Batch 94/116 loss: 0.0422\n",
      "Epoch 156: Batch 95/116 loss: 0.0387\n",
      "Epoch 156: Batch 96/116 loss: 0.0645\n",
      "Epoch 156: Batch 97/116 loss: 0.0416\n",
      "Epoch 156: Batch 98/116 loss: 0.0432\n",
      "Epoch 156: Batch 99/116 loss: 0.0406\n",
      "Epoch 156: Batch 100/116 loss: 0.0391\n",
      "Epoch 156: Batch 101/116 loss: 0.0491\n",
      "Epoch 156: Batch 102/116 loss: 0.0467\n",
      "Epoch 156: Batch 103/116 loss: 0.0379\n",
      "Epoch 156: Batch 104/116 loss: 0.0383\n",
      "Epoch 156: Batch 105/116 loss: 0.0311\n",
      "Epoch 156: Batch 106/116 loss: 0.0507\n",
      "Epoch 156: Batch 107/116 loss: 0.0448\n",
      "Epoch 156: Batch 108/116 loss: 0.0412\n",
      "Epoch 156: Batch 109/116 loss: 0.0363\n",
      "Epoch 156: Batch 110/116 loss: 0.0451\n",
      "Epoch 156: Batch 111/116 loss: 0.0305\n",
      "Epoch 156: Batch 112/116 loss: 0.0552\n",
      "Epoch 156: Batch 113/116 loss: 0.0530\n",
      "Epoch 156: Batch 114/116 loss: 0.0513\n",
      "Epoch 156: Batch 115/116 loss: 0.0418\n",
      "Epoch 156: Batch 116/116 loss: 0.0248\n",
      "Epoch 156 train loss: 0.0437 valid loss: 0.0531\n",
      "performance reducing: 5\n",
      "Epoch 157: Batch 1/116 loss: 0.0407\n",
      "Epoch 157: Batch 2/116 loss: 0.0548\n",
      "Epoch 157: Batch 3/116 loss: 0.0243\n",
      "Epoch 157: Batch 4/116 loss: 0.0595\n",
      "Epoch 157: Batch 5/116 loss: 0.0550\n",
      "Epoch 157: Batch 6/116 loss: 0.0631\n",
      "Epoch 157: Batch 7/116 loss: 0.0380\n",
      "Epoch 157: Batch 8/116 loss: 0.0429\n",
      "Epoch 157: Batch 9/116 loss: 0.0378\n",
      "Epoch 157: Batch 10/116 loss: 0.0487\n",
      "Epoch 157: Batch 11/116 loss: 0.0575\n",
      "Epoch 157: Batch 12/116 loss: 0.0341\n",
      "Epoch 157: Batch 13/116 loss: 0.0478\n",
      "Epoch 157: Batch 14/116 loss: 0.0409\n",
      "Epoch 157: Batch 15/116 loss: 0.0488\n",
      "Epoch 157: Batch 16/116 loss: 0.0301\n",
      "Epoch 157: Batch 17/116 loss: 0.0398\n",
      "Epoch 157: Batch 18/116 loss: 0.0603\n",
      "Epoch 157: Batch 19/116 loss: 0.0423\n",
      "Epoch 157: Batch 20/116 loss: 0.0431\n",
      "Epoch 157: Batch 21/116 loss: 0.0337\n",
      "Epoch 157: Batch 22/116 loss: 0.0309\n",
      "Epoch 157: Batch 23/116 loss: 0.0495\n",
      "Epoch 157: Batch 24/116 loss: 0.0307\n",
      "Epoch 157: Batch 25/116 loss: 0.0410\n",
      "Epoch 157: Batch 26/116 loss: 0.0365\n",
      "Epoch 157: Batch 27/116 loss: 0.0363\n",
      "Epoch 157: Batch 28/116 loss: 0.0434\n",
      "Epoch 157: Batch 29/116 loss: 0.0421\n",
      "Epoch 157: Batch 30/116 loss: 0.0442\n",
      "Epoch 157: Batch 31/116 loss: 0.0460\n",
      "Epoch 157: Batch 32/116 loss: 0.0345\n",
      "Epoch 157: Batch 33/116 loss: 0.0575\n",
      "Epoch 157: Batch 34/116 loss: 0.0322\n",
      "Epoch 157: Batch 35/116 loss: 0.0446\n",
      "Epoch 157: Batch 36/116 loss: 0.0455\n",
      "Epoch 157: Batch 37/116 loss: 0.0488\n",
      "Epoch 157: Batch 38/116 loss: 0.0330\n",
      "Epoch 157: Batch 39/116 loss: 0.0391\n",
      "Epoch 157: Batch 40/116 loss: 0.0542\n",
      "Epoch 157: Batch 41/116 loss: 0.0370\n",
      "Epoch 157: Batch 42/116 loss: 0.0282\n",
      "Epoch 157: Batch 43/116 loss: 0.0394\n",
      "Epoch 157: Batch 44/116 loss: 0.0387\n",
      "Epoch 157: Batch 45/116 loss: 0.0251\n",
      "Epoch 157: Batch 46/116 loss: 0.0375\n",
      "Epoch 157: Batch 47/116 loss: 0.0406\n",
      "Epoch 157: Batch 48/116 loss: 0.0292\n",
      "Epoch 157: Batch 49/116 loss: 0.0352\n",
      "Epoch 157: Batch 50/116 loss: 0.0423\n",
      "Epoch 157: Batch 51/116 loss: 0.0567\n",
      "Epoch 157: Batch 52/116 loss: 0.0473\n",
      "Epoch 157: Batch 53/116 loss: 0.0403\n",
      "Epoch 157: Batch 54/116 loss: 0.0442\n",
      "Epoch 157: Batch 55/116 loss: 0.0626\n",
      "Epoch 157: Batch 56/116 loss: 0.0399\n",
      "Epoch 157: Batch 57/116 loss: 0.0443\n",
      "Epoch 157: Batch 58/116 loss: 0.0285\n",
      "Epoch 157: Batch 59/116 loss: 0.0438\n",
      "Epoch 157: Batch 60/116 loss: 0.0514\n",
      "Epoch 157: Batch 61/116 loss: 0.0290\n",
      "Epoch 157: Batch 62/116 loss: 0.0470\n",
      "Epoch 157: Batch 63/116 loss: 0.0636\n",
      "Epoch 157: Batch 64/116 loss: 0.0435\n",
      "Epoch 157: Batch 65/116 loss: 0.0477\n",
      "Epoch 157: Batch 66/116 loss: 0.0372\n",
      "Epoch 157: Batch 67/116 loss: 0.0380\n",
      "Epoch 157: Batch 68/116 loss: 0.0542\n",
      "Epoch 157: Batch 69/116 loss: 0.0282\n",
      "Epoch 157: Batch 70/116 loss: 0.0443\n",
      "Epoch 157: Batch 71/116 loss: 0.0368\n",
      "Epoch 157: Batch 72/116 loss: 0.0433\n",
      "Epoch 157: Batch 73/116 loss: 0.0511\n",
      "Epoch 157: Batch 74/116 loss: 0.0291\n",
      "Epoch 157: Batch 75/116 loss: 0.0536\n",
      "Epoch 157: Batch 76/116 loss: 0.0351\n",
      "Epoch 157: Batch 77/116 loss: 0.0433\n",
      "Epoch 157: Batch 78/116 loss: 0.0436\n",
      "Epoch 157: Batch 79/116 loss: 0.0457\n",
      "Epoch 157: Batch 80/116 loss: 0.0340\n",
      "Epoch 157: Batch 81/116 loss: 0.0392\n",
      "Epoch 157: Batch 82/116 loss: 0.0516\n",
      "Epoch 157: Batch 83/116 loss: 0.0361\n",
      "Epoch 157: Batch 84/116 loss: 0.0360\n",
      "Epoch 157: Batch 85/116 loss: 0.0619\n",
      "Epoch 157: Batch 86/116 loss: 0.0553\n",
      "Epoch 157: Batch 87/116 loss: 0.0513\n",
      "Epoch 157: Batch 88/116 loss: 0.0518\n",
      "Epoch 157: Batch 89/116 loss: 0.0486\n",
      "Epoch 157: Batch 90/116 loss: 0.0560\n",
      "Epoch 157: Batch 91/116 loss: 0.0293\n",
      "Epoch 157: Batch 92/116 loss: 0.0509\n",
      "Epoch 157: Batch 93/116 loss: 0.0539\n",
      "Epoch 157: Batch 94/116 loss: 0.0257\n",
      "Epoch 157: Batch 95/116 loss: 0.0684\n",
      "Epoch 157: Batch 96/116 loss: 0.0627\n",
      "Epoch 157: Batch 97/116 loss: 0.0442\n",
      "Epoch 157: Batch 98/116 loss: 0.0508\n",
      "Epoch 157: Batch 99/116 loss: 0.0296\n",
      "Epoch 157: Batch 100/116 loss: 0.0393\n",
      "Epoch 157: Batch 101/116 loss: 0.0430\n",
      "Epoch 157: Batch 102/116 loss: 0.0387\n",
      "Epoch 157: Batch 103/116 loss: 0.0199\n",
      "Epoch 157: Batch 104/116 loss: 0.0492\n",
      "Epoch 157: Batch 105/116 loss: 0.0318\n",
      "Epoch 157: Batch 106/116 loss: 0.0653\n",
      "Epoch 157: Batch 107/116 loss: 0.0359\n",
      "Epoch 157: Batch 108/116 loss: 0.0547\n",
      "Epoch 157: Batch 109/116 loss: 0.0513\n",
      "Epoch 157: Batch 110/116 loss: 0.0569\n",
      "Epoch 157: Batch 111/116 loss: 0.0443\n",
      "Epoch 157: Batch 112/116 loss: 0.0563\n",
      "Epoch 157: Batch 113/116 loss: 0.0357\n",
      "Epoch 157: Batch 114/116 loss: 0.0460\n",
      "Epoch 157: Batch 115/116 loss: 0.0467\n",
      "Epoch 157: Batch 116/116 loss: 0.0467\n",
      "Epoch 157 train loss: 0.0436 valid loss: 0.0543\n",
      "performance reducing: 6\n",
      "Epoch 158: Batch 1/116 loss: 0.0427\n",
      "Epoch 158: Batch 2/116 loss: 0.0392\n",
      "Epoch 158: Batch 3/116 loss: 0.0345\n",
      "Epoch 158: Batch 4/116 loss: 0.0488\n",
      "Epoch 158: Batch 5/116 loss: 0.0291\n",
      "Epoch 158: Batch 6/116 loss: 0.0436\n",
      "Epoch 158: Batch 7/116 loss: 0.0459\n",
      "Epoch 158: Batch 8/116 loss: 0.0363\n",
      "Epoch 158: Batch 9/116 loss: 0.0472\n",
      "Epoch 158: Batch 10/116 loss: 0.0389\n",
      "Epoch 158: Batch 11/116 loss: 0.0226\n",
      "Epoch 158: Batch 12/116 loss: 0.0188\n",
      "Epoch 158: Batch 13/116 loss: 0.0394\n",
      "Epoch 158: Batch 14/116 loss: 0.0379\n",
      "Epoch 158: Batch 15/116 loss: 0.0394\n",
      "Epoch 158: Batch 16/116 loss: 0.0344\n",
      "Epoch 158: Batch 17/116 loss: 0.0639\n",
      "Epoch 158: Batch 18/116 loss: 0.0277\n",
      "Epoch 158: Batch 19/116 loss: 0.0704\n",
      "Epoch 158: Batch 20/116 loss: 0.0379\n",
      "Epoch 158: Batch 21/116 loss: 0.0877\n",
      "Epoch 158: Batch 22/116 loss: 0.0424\n",
      "Epoch 158: Batch 23/116 loss: 0.0482\n",
      "Epoch 158: Batch 24/116 loss: 0.0476\n",
      "Epoch 158: Batch 25/116 loss: 0.0519\n",
      "Epoch 158: Batch 26/116 loss: 0.0478\n",
      "Epoch 158: Batch 27/116 loss: 0.0378\n",
      "Epoch 158: Batch 28/116 loss: 0.0467\n",
      "Epoch 158: Batch 29/116 loss: 0.0479\n",
      "Epoch 158: Batch 30/116 loss: 0.0538\n",
      "Epoch 158: Batch 31/116 loss: 0.0448\n",
      "Epoch 158: Batch 32/116 loss: 0.0604\n",
      "Epoch 158: Batch 33/116 loss: 0.0334\n",
      "Epoch 158: Batch 34/116 loss: 0.0269\n",
      "Epoch 158: Batch 35/116 loss: 0.0423\n",
      "Epoch 158: Batch 36/116 loss: 0.0450\n",
      "Epoch 158: Batch 37/116 loss: 0.0631\n",
      "Epoch 158: Batch 38/116 loss: 0.0369\n",
      "Epoch 158: Batch 39/116 loss: 0.0467\n",
      "Epoch 158: Batch 40/116 loss: 0.0456\n",
      "Epoch 158: Batch 41/116 loss: 0.0461\n",
      "Epoch 158: Batch 42/116 loss: 0.0437\n",
      "Epoch 158: Batch 43/116 loss: 0.0351\n",
      "Epoch 158: Batch 44/116 loss: 0.0378\n",
      "Epoch 158: Batch 45/116 loss: 0.0439\n",
      "Epoch 158: Batch 46/116 loss: 0.0480\n",
      "Epoch 158: Batch 47/116 loss: 0.0469\n",
      "Epoch 158: Batch 48/116 loss: 0.0258\n",
      "Epoch 158: Batch 49/116 loss: 0.0559\n",
      "Epoch 158: Batch 50/116 loss: 0.0408\n",
      "Epoch 158: Batch 51/116 loss: 0.0320\n",
      "Epoch 158: Batch 52/116 loss: 0.0456\n",
      "Epoch 158: Batch 53/116 loss: 0.0417\n",
      "Epoch 158: Batch 54/116 loss: 0.0339\n",
      "Epoch 158: Batch 55/116 loss: 0.0377\n",
      "Epoch 158: Batch 56/116 loss: 0.0383\n",
      "Epoch 158: Batch 57/116 loss: 0.0296\n",
      "Epoch 158: Batch 58/116 loss: 0.0360\n",
      "Epoch 158: Batch 59/116 loss: 0.0437\n",
      "Epoch 158: Batch 60/116 loss: 0.0469\n",
      "Epoch 158: Batch 61/116 loss: 0.0450\n",
      "Epoch 158: Batch 62/116 loss: 0.0466\n",
      "Epoch 158: Batch 63/116 loss: 0.0341\n",
      "Epoch 158: Batch 64/116 loss: 0.0397\n",
      "Epoch 158: Batch 65/116 loss: 0.0335\n",
      "Epoch 158: Batch 66/116 loss: 0.0436\n",
      "Epoch 158: Batch 67/116 loss: 0.0476\n",
      "Epoch 158: Batch 68/116 loss: 0.0394\n",
      "Epoch 158: Batch 69/116 loss: 0.0506\n",
      "Epoch 158: Batch 70/116 loss: 0.0410\n",
      "Epoch 158: Batch 71/116 loss: 0.0502\n",
      "Epoch 158: Batch 72/116 loss: 0.0410\n",
      "Epoch 158: Batch 73/116 loss: 0.0573\n",
      "Epoch 158: Batch 74/116 loss: 0.0456\n",
      "Epoch 158: Batch 75/116 loss: 0.0437\n",
      "Epoch 158: Batch 76/116 loss: 0.0433\n",
      "Epoch 158: Batch 77/116 loss: 0.0468\n",
      "Epoch 158: Batch 78/116 loss: 0.0393\n",
      "Epoch 158: Batch 79/116 loss: 0.0415\n",
      "Epoch 158: Batch 80/116 loss: 0.0500\n",
      "Epoch 158: Batch 81/116 loss: 0.0288\n",
      "Epoch 158: Batch 82/116 loss: 0.0285\n",
      "Epoch 158: Batch 83/116 loss: 0.0534\n",
      "Epoch 158: Batch 84/116 loss: 0.0337\n",
      "Epoch 158: Batch 85/116 loss: 0.0477\n",
      "Epoch 158: Batch 86/116 loss: 0.0507\n",
      "Epoch 158: Batch 87/116 loss: 0.0447\n",
      "Epoch 158: Batch 88/116 loss: 0.0368\n",
      "Epoch 158: Batch 89/116 loss: 0.0395\n",
      "Epoch 158: Batch 90/116 loss: 0.0363\n",
      "Epoch 158: Batch 91/116 loss: 0.0335\n",
      "Epoch 158: Batch 92/116 loss: 0.0447\n",
      "Epoch 158: Batch 93/116 loss: 0.0337\n",
      "Epoch 158: Batch 94/116 loss: 0.0382\n",
      "Epoch 158: Batch 95/116 loss: 0.0577\n",
      "Epoch 158: Batch 96/116 loss: 0.0346\n",
      "Epoch 158: Batch 97/116 loss: 0.0532\n",
      "Epoch 158: Batch 98/116 loss: 0.0432\n",
      "Epoch 158: Batch 99/116 loss: 0.0312\n",
      "Epoch 158: Batch 100/116 loss: 0.0413\n",
      "Epoch 158: Batch 101/116 loss: 0.0299\n",
      "Epoch 158: Batch 102/116 loss: 0.0515\n",
      "Epoch 158: Batch 103/116 loss: 0.0479\n",
      "Epoch 158: Batch 104/116 loss: 0.0452\n",
      "Epoch 158: Batch 105/116 loss: 0.0501\n",
      "Epoch 158: Batch 106/116 loss: 0.0511\n",
      "Epoch 158: Batch 107/116 loss: 0.0665\n",
      "Epoch 158: Batch 108/116 loss: 0.0427\n",
      "Epoch 158: Batch 109/116 loss: 0.0490\n",
      "Epoch 158: Batch 110/116 loss: 0.0438\n",
      "Epoch 158: Batch 111/116 loss: 0.0273\n",
      "Epoch 158: Batch 112/116 loss: 0.0424\n",
      "Epoch 158: Batch 113/116 loss: 0.0411\n",
      "Epoch 158: Batch 114/116 loss: 0.0554\n",
      "Epoch 158: Batch 115/116 loss: 0.0505\n",
      "Epoch 158: Batch 116/116 loss: 0.0444\n",
      "Epoch 158 train loss: 0.0431 valid loss: 0.0570\n",
      "performance reducing: 7\n",
      "Epoch 159: Batch 1/116 loss: 0.0456\n",
      "Epoch 159: Batch 2/116 loss: 0.0450\n",
      "Epoch 159: Batch 3/116 loss: 0.0466\n",
      "Epoch 159: Batch 4/116 loss: 0.0348\n",
      "Epoch 159: Batch 5/116 loss: 0.0355\n",
      "Epoch 159: Batch 6/116 loss: 0.0283\n",
      "Epoch 159: Batch 7/116 loss: 0.0408\n",
      "Epoch 159: Batch 8/116 loss: 0.0381\n",
      "Epoch 159: Batch 9/116 loss: 0.0705\n",
      "Epoch 159: Batch 10/116 loss: 0.0333\n",
      "Epoch 159: Batch 11/116 loss: 0.0311\n",
      "Epoch 159: Batch 12/116 loss: 0.0238\n",
      "Epoch 159: Batch 13/116 loss: 0.0518\n",
      "Epoch 159: Batch 14/116 loss: 0.0485\n",
      "Epoch 159: Batch 15/116 loss: 0.0393\n",
      "Epoch 159: Batch 16/116 loss: 0.0393\n",
      "Epoch 159: Batch 17/116 loss: 0.0445\n",
      "Epoch 159: Batch 18/116 loss: 0.0451\n",
      "Epoch 159: Batch 19/116 loss: 0.0509\n",
      "Epoch 159: Batch 20/116 loss: 0.0417\n",
      "Epoch 159: Batch 21/116 loss: 0.0437\n",
      "Epoch 159: Batch 22/116 loss: 0.0371\n",
      "Epoch 159: Batch 23/116 loss: 0.0505\n",
      "Epoch 159: Batch 24/116 loss: 0.0484\n",
      "Epoch 159: Batch 25/116 loss: 0.0523\n",
      "Epoch 159: Batch 26/116 loss: 0.0463\n",
      "Epoch 159: Batch 27/116 loss: 0.0289\n",
      "Epoch 159: Batch 28/116 loss: 0.0645\n",
      "Epoch 159: Batch 29/116 loss: 0.0430\n",
      "Epoch 159: Batch 30/116 loss: 0.0398\n",
      "Epoch 159: Batch 31/116 loss: 0.0441\n",
      "Epoch 159: Batch 32/116 loss: 0.0324\n",
      "Epoch 159: Batch 33/116 loss: 0.0329\n",
      "Epoch 159: Batch 34/116 loss: 0.0195\n",
      "Epoch 159: Batch 35/116 loss: 0.0600\n",
      "Epoch 159: Batch 36/116 loss: 0.0295\n",
      "Epoch 159: Batch 37/116 loss: 0.0469\n",
      "Epoch 159: Batch 38/116 loss: 0.0512\n",
      "Epoch 159: Batch 39/116 loss: 0.0358\n",
      "Epoch 159: Batch 40/116 loss: 0.0467\n",
      "Epoch 159: Batch 41/116 loss: 0.0489\n",
      "Epoch 159: Batch 42/116 loss: 0.0435\n",
      "Epoch 159: Batch 43/116 loss: 0.0366\n",
      "Epoch 159: Batch 44/116 loss: 0.0544\n",
      "Epoch 159: Batch 45/116 loss: 0.0424\n",
      "Epoch 159: Batch 46/116 loss: 0.0423\n",
      "Epoch 159: Batch 47/116 loss: 0.0357\n",
      "Epoch 159: Batch 48/116 loss: 0.0358\n",
      "Epoch 159: Batch 49/116 loss: 0.0379\n",
      "Epoch 159: Batch 50/116 loss: 0.0436\n",
      "Epoch 159: Batch 51/116 loss: 0.0371\n",
      "Epoch 159: Batch 52/116 loss: 0.0385\n",
      "Epoch 159: Batch 53/116 loss: 0.0330\n",
      "Epoch 159: Batch 54/116 loss: 0.0288\n",
      "Epoch 159: Batch 55/116 loss: 0.0436\n",
      "Epoch 159: Batch 56/116 loss: 0.0453\n",
      "Epoch 159: Batch 57/116 loss: 0.0285\n",
      "Epoch 159: Batch 58/116 loss: 0.0421\n",
      "Epoch 159: Batch 59/116 loss: 0.0226\n",
      "Epoch 159: Batch 60/116 loss: 0.0397\n",
      "Epoch 159: Batch 61/116 loss: 0.0377\n",
      "Epoch 159: Batch 62/116 loss: 0.0517\n",
      "Epoch 159: Batch 63/116 loss: 0.0427\n",
      "Epoch 159: Batch 64/116 loss: 0.0361\n",
      "Epoch 159: Batch 65/116 loss: 0.0393\n",
      "Epoch 159: Batch 66/116 loss: 0.0475\n",
      "Epoch 159: Batch 67/116 loss: 0.0446\n",
      "Epoch 159: Batch 68/116 loss: 0.0409\n",
      "Epoch 159: Batch 69/116 loss: 0.0278\n",
      "Epoch 159: Batch 70/116 loss: 0.0564\n",
      "Epoch 159: Batch 71/116 loss: 0.0298\n",
      "Epoch 159: Batch 72/116 loss: 0.0328\n",
      "Epoch 159: Batch 73/116 loss: 0.0713\n",
      "Epoch 159: Batch 74/116 loss: 0.0437\n",
      "Epoch 159: Batch 75/116 loss: 0.0486\n",
      "Epoch 159: Batch 76/116 loss: 0.0438\n",
      "Epoch 159: Batch 77/116 loss: 0.0518\n",
      "Epoch 159: Batch 78/116 loss: 0.0459\n",
      "Epoch 159: Batch 79/116 loss: 0.0518\n",
      "Epoch 159: Batch 80/116 loss: 0.0318\n",
      "Epoch 159: Batch 81/116 loss: 0.0404\n",
      "Epoch 159: Batch 82/116 loss: 0.0324\n",
      "Epoch 159: Batch 83/116 loss: 0.0321\n",
      "Epoch 159: Batch 84/116 loss: 0.0189\n",
      "Epoch 159: Batch 85/116 loss: 0.0265\n",
      "Epoch 159: Batch 86/116 loss: 0.0343\n",
      "Epoch 159: Batch 87/116 loss: 0.0352\n",
      "Epoch 159: Batch 88/116 loss: 0.0328\n",
      "Epoch 159: Batch 89/116 loss: 0.0547\n",
      "Epoch 159: Batch 90/116 loss: 0.0351\n",
      "Epoch 159: Batch 91/116 loss: 0.0539\n",
      "Epoch 159: Batch 92/116 loss: 0.0379\n",
      "Epoch 159: Batch 93/116 loss: 0.0446\n",
      "Epoch 159: Batch 94/116 loss: 0.0387\n",
      "Epoch 159: Batch 95/116 loss: 0.0564\n",
      "Epoch 159: Batch 96/116 loss: 0.0507\n",
      "Epoch 159: Batch 97/116 loss: 0.0605\n",
      "Epoch 159: Batch 98/116 loss: 0.0577\n",
      "Epoch 159: Batch 99/116 loss: 0.0295\n",
      "Epoch 159: Batch 100/116 loss: 0.0595\n",
      "Epoch 159: Batch 101/116 loss: 0.0435\n",
      "Epoch 159: Batch 102/116 loss: 0.0530\n",
      "Epoch 159: Batch 103/116 loss: 0.0412\n",
      "Epoch 159: Batch 104/116 loss: 0.0454\n",
      "Epoch 159: Batch 105/116 loss: 0.0577\n",
      "Epoch 159: Batch 106/116 loss: 0.0403\n",
      "Epoch 159: Batch 107/116 loss: 0.0567\n",
      "Epoch 159: Batch 108/116 loss: 0.0384\n",
      "Epoch 159: Batch 109/116 loss: 0.0443\n",
      "Epoch 159: Batch 110/116 loss: 0.0333\n",
      "Epoch 159: Batch 111/116 loss: 0.0554\n",
      "Epoch 159: Batch 112/116 loss: 0.0512\n",
      "Epoch 159: Batch 113/116 loss: 0.0361\n",
      "Epoch 159: Batch 114/116 loss: 0.0619\n",
      "Epoch 159: Batch 115/116 loss: 0.0488\n",
      "Epoch 159: Batch 116/116 loss: 0.0422\n",
      "Epoch 159 train loss: 0.0425 valid loss: 0.0535\n",
      "performance reducing: 8\n",
      "Epoch 160: Batch 1/116 loss: 0.0471\n",
      "Epoch 160: Batch 2/116 loss: 0.0460\n",
      "Epoch 160: Batch 3/116 loss: 0.0359\n",
      "Epoch 160: Batch 4/116 loss: 0.0373\n",
      "Epoch 160: Batch 5/116 loss: 0.0527\n",
      "Epoch 160: Batch 6/116 loss: 0.0404\n",
      "Epoch 160: Batch 7/116 loss: 0.0489\n",
      "Epoch 160: Batch 8/116 loss: 0.0496\n",
      "Epoch 160: Batch 9/116 loss: 0.0708\n",
      "Epoch 160: Batch 10/116 loss: 0.0319\n",
      "Epoch 160: Batch 11/116 loss: 0.0306\n",
      "Epoch 160: Batch 12/116 loss: 0.0513\n",
      "Epoch 160: Batch 13/116 loss: 0.0380\n",
      "Epoch 160: Batch 14/116 loss: 0.0440\n",
      "Epoch 160: Batch 15/116 loss: 0.0389\n",
      "Epoch 160: Batch 16/116 loss: 0.0417\n",
      "Epoch 160: Batch 17/116 loss: 0.0455\n",
      "Epoch 160: Batch 18/116 loss: 0.0286\n",
      "Epoch 160: Batch 19/116 loss: 0.0357\n",
      "Epoch 160: Batch 20/116 loss: 0.0421\n",
      "Epoch 160: Batch 21/116 loss: 0.0222\n",
      "Epoch 160: Batch 22/116 loss: 0.0469\n",
      "Epoch 160: Batch 23/116 loss: 0.0623\n",
      "Epoch 160: Batch 24/116 loss: 0.0341\n",
      "Epoch 160: Batch 25/116 loss: 0.0362\n",
      "Epoch 160: Batch 26/116 loss: 0.0473\n",
      "Epoch 160: Batch 27/116 loss: 0.0408\n",
      "Epoch 160: Batch 28/116 loss: 0.0489\n",
      "Epoch 160: Batch 29/116 loss: 0.0443\n",
      "Epoch 160: Batch 30/116 loss: 0.0437\n",
      "Epoch 160: Batch 31/116 loss: 0.0335\n",
      "Epoch 160: Batch 32/116 loss: 0.0446\n",
      "Epoch 160: Batch 33/116 loss: 0.0385\n",
      "Epoch 160: Batch 34/116 loss: 0.0410\n",
      "Epoch 160: Batch 35/116 loss: 0.0511\n",
      "Epoch 160: Batch 36/116 loss: 0.0527\n",
      "Epoch 160: Batch 37/116 loss: 0.0442\n",
      "Epoch 160: Batch 38/116 loss: 0.0578\n",
      "Epoch 160: Batch 39/116 loss: 0.0509\n",
      "Epoch 160: Batch 40/116 loss: 0.0256\n",
      "Epoch 160: Batch 41/116 loss: 0.0435\n",
      "Epoch 160: Batch 42/116 loss: 0.0526\n",
      "Epoch 160: Batch 43/116 loss: 0.0559\n",
      "Epoch 160: Batch 44/116 loss: 0.0462\n",
      "Epoch 160: Batch 45/116 loss: 0.0400\n",
      "Epoch 160: Batch 46/116 loss: 0.0266\n",
      "Epoch 160: Batch 47/116 loss: 0.0332\n",
      "Epoch 160: Batch 48/116 loss: 0.0294\n",
      "Epoch 160: Batch 49/116 loss: 0.0456\n",
      "Epoch 160: Batch 50/116 loss: 0.0320\n",
      "Epoch 160: Batch 51/116 loss: 0.0251\n",
      "Epoch 160: Batch 52/116 loss: 0.0573\n",
      "Epoch 160: Batch 53/116 loss: 0.0549\n",
      "Epoch 160: Batch 54/116 loss: 0.0461\n",
      "Epoch 160: Batch 55/116 loss: 0.0412\n",
      "Epoch 160: Batch 56/116 loss: 0.0554\n",
      "Epoch 160: Batch 57/116 loss: 0.0266\n",
      "Epoch 160: Batch 58/116 loss: 0.0545\n",
      "Epoch 160: Batch 59/116 loss: 0.0366\n",
      "Epoch 160: Batch 60/116 loss: 0.0537\n",
      "Epoch 160: Batch 61/116 loss: 0.0355\n",
      "Epoch 160: Batch 62/116 loss: 0.0311\n",
      "Epoch 160: Batch 63/116 loss: 0.0301\n",
      "Epoch 160: Batch 64/116 loss: 0.0612\n",
      "Epoch 160: Batch 65/116 loss: 0.0543\n",
      "Epoch 160: Batch 66/116 loss: 0.0530\n",
      "Epoch 160: Batch 67/116 loss: 0.0361\n",
      "Epoch 160: Batch 68/116 loss: 0.0276\n",
      "Epoch 160: Batch 69/116 loss: 0.0332\n",
      "Epoch 160: Batch 70/116 loss: 0.0333\n",
      "Epoch 160: Batch 71/116 loss: 0.0497\n",
      "Epoch 160: Batch 72/116 loss: 0.0394\n",
      "Epoch 160: Batch 73/116 loss: 0.0512\n",
      "Epoch 160: Batch 74/116 loss: 0.0499\n",
      "Epoch 160: Batch 75/116 loss: 0.0400\n",
      "Epoch 160: Batch 76/116 loss: 0.0499\n",
      "Epoch 160: Batch 77/116 loss: 0.0477\n",
      "Epoch 160: Batch 78/116 loss: 0.0397\n",
      "Epoch 160: Batch 79/116 loss: 0.0470\n",
      "Epoch 160: Batch 80/116 loss: 0.0583\n",
      "Epoch 160: Batch 81/116 loss: 0.0371\n",
      "Epoch 160: Batch 82/116 loss: 0.0464\n",
      "Epoch 160: Batch 83/116 loss: 0.0450\n",
      "Epoch 160: Batch 84/116 loss: 0.0470\n",
      "Epoch 160: Batch 85/116 loss: 0.0238\n",
      "Epoch 160: Batch 86/116 loss: 0.0343\n",
      "Epoch 160: Batch 87/116 loss: 0.0416\n",
      "Epoch 160: Batch 88/116 loss: 0.0664\n",
      "Epoch 160: Batch 89/116 loss: 0.0553\n",
      "Epoch 160: Batch 90/116 loss: 0.0633\n",
      "Epoch 160: Batch 91/116 loss: 0.0527\n",
      "Epoch 160: Batch 92/116 loss: 0.0653\n",
      "Epoch 160: Batch 93/116 loss: 0.0723\n",
      "Epoch 160: Batch 94/116 loss: 0.0441\n",
      "Epoch 160: Batch 95/116 loss: 0.0456\n",
      "Epoch 160: Batch 96/116 loss: 0.0834\n",
      "Epoch 160: Batch 97/116 loss: 0.0316\n",
      "Epoch 160: Batch 98/116 loss: 0.0277\n",
      "Epoch 160: Batch 99/116 loss: 0.0654\n",
      "Epoch 160: Batch 100/116 loss: 0.0610\n",
      "Epoch 160: Batch 101/116 loss: 0.0593\n",
      "Epoch 160: Batch 102/116 loss: 0.0436\n",
      "Epoch 160: Batch 103/116 loss: 0.0414\n",
      "Epoch 160: Batch 104/116 loss: 0.0508\n",
      "Epoch 160: Batch 105/116 loss: 0.0472\n",
      "Epoch 160: Batch 106/116 loss: 0.0385\n",
      "Epoch 160: Batch 107/116 loss: 0.0545\n",
      "Epoch 160: Batch 108/116 loss: 0.0368\n",
      "Epoch 160: Batch 109/116 loss: 0.0370\n",
      "Epoch 160: Batch 110/116 loss: 0.0368\n",
      "Epoch 160: Batch 111/116 loss: 0.0652\n",
      "Epoch 160: Batch 112/116 loss: 0.0453\n",
      "Epoch 160: Batch 113/116 loss: 0.0399\n",
      "Epoch 160: Batch 114/116 loss: 0.0403\n",
      "Epoch 160: Batch 115/116 loss: 0.0441\n",
      "Epoch 160: Batch 116/116 loss: 0.0537\n",
      "Epoch 160 train loss: 0.0448 valid loss: 0.0758\n",
      "performance reducing: 9\n",
      "Epoch 161: Batch 1/116 loss: 0.0598\n",
      "Epoch 161: Batch 2/116 loss: 0.0640\n",
      "Epoch 161: Batch 3/116 loss: 0.0310\n",
      "Epoch 161: Batch 4/116 loss: 0.0363\n",
      "Epoch 161: Batch 5/116 loss: 0.0427\n",
      "Epoch 161: Batch 6/116 loss: 0.0437\n",
      "Epoch 161: Batch 7/116 loss: 0.0395\n",
      "Epoch 161: Batch 8/116 loss: 0.0385\n",
      "Epoch 161: Batch 9/116 loss: 0.0338\n",
      "Epoch 161: Batch 10/116 loss: 0.0339\n",
      "Epoch 161: Batch 11/116 loss: 0.0378\n",
      "Epoch 161: Batch 12/116 loss: 0.0529\n",
      "Epoch 161: Batch 13/116 loss: 0.0496\n",
      "Epoch 161: Batch 14/116 loss: 0.0496\n",
      "Epoch 161: Batch 15/116 loss: 0.0649\n",
      "Epoch 161: Batch 16/116 loss: 0.0330\n",
      "Epoch 161: Batch 17/116 loss: 0.0461\n",
      "Epoch 161: Batch 18/116 loss: 0.0401\n",
      "Epoch 161: Batch 19/116 loss: 0.0550\n",
      "Epoch 161: Batch 20/116 loss: 0.0323\n",
      "Epoch 161: Batch 21/116 loss: 0.0303\n",
      "Epoch 161: Batch 22/116 loss: 0.0627\n",
      "Epoch 161: Batch 23/116 loss: 0.0427\n",
      "Epoch 161: Batch 24/116 loss: 0.0265\n",
      "Epoch 161: Batch 25/116 loss: 0.0414\n",
      "Epoch 161: Batch 26/116 loss: 0.0429\n",
      "Epoch 161: Batch 27/116 loss: 0.0541\n",
      "Epoch 161: Batch 28/116 loss: 0.0315\n",
      "Epoch 161: Batch 29/116 loss: 0.0558\n",
      "Epoch 161: Batch 30/116 loss: 0.0402\n",
      "Epoch 161: Batch 31/116 loss: 0.0416\n",
      "Epoch 161: Batch 32/116 loss: 0.0515\n",
      "Epoch 161: Batch 33/116 loss: 0.0360\n",
      "Epoch 161: Batch 34/116 loss: 0.0423\n",
      "Epoch 161: Batch 35/116 loss: 0.0261\n",
      "Epoch 161: Batch 36/116 loss: 0.0471\n",
      "Epoch 161: Batch 37/116 loss: 0.0389\n",
      "Epoch 161: Batch 38/116 loss: 0.0351\n",
      "Epoch 161: Batch 39/116 loss: 0.0447\n",
      "Epoch 161: Batch 40/116 loss: 0.0405\n",
      "Epoch 161: Batch 41/116 loss: 0.0400\n",
      "Epoch 161: Batch 42/116 loss: 0.0589\n",
      "Epoch 161: Batch 43/116 loss: 0.0363\n",
      "Epoch 161: Batch 44/116 loss: 0.0373\n",
      "Epoch 161: Batch 45/116 loss: 0.0389\n",
      "Epoch 161: Batch 46/116 loss: 0.0442\n",
      "Epoch 161: Batch 47/116 loss: 0.0711\n",
      "Epoch 161: Batch 48/116 loss: 0.0477\n",
      "Epoch 161: Batch 49/116 loss: 0.0286\n",
      "Epoch 161: Batch 50/116 loss: 0.0439\n",
      "Epoch 161: Batch 51/116 loss: 0.0272\n",
      "Epoch 161: Batch 52/116 loss: 0.0338\n",
      "Epoch 161: Batch 53/116 loss: 0.0504\n",
      "Epoch 161: Batch 54/116 loss: 0.0586\n",
      "Epoch 161: Batch 55/116 loss: 0.0470\n",
      "Epoch 161: Batch 56/116 loss: 0.0492\n",
      "Epoch 161: Batch 57/116 loss: 0.0548\n",
      "Epoch 161: Batch 58/116 loss: 0.0329\n",
      "Epoch 161: Batch 59/116 loss: 0.0721\n",
      "Epoch 161: Batch 60/116 loss: 0.0313\n",
      "Epoch 161: Batch 61/116 loss: 0.0467\n",
      "Epoch 161: Batch 62/116 loss: 0.0400\n",
      "Epoch 161: Batch 63/116 loss: 0.0431\n",
      "Epoch 161: Batch 64/116 loss: 0.0268\n",
      "Epoch 161: Batch 65/116 loss: 0.0275\n",
      "Epoch 161: Batch 66/116 loss: 0.0507\n",
      "Epoch 161: Batch 67/116 loss: 0.0383\n",
      "Epoch 161: Batch 68/116 loss: 0.0339\n",
      "Epoch 161: Batch 69/116 loss: 0.0491\n",
      "Epoch 161: Batch 70/116 loss: 0.0525\n",
      "Epoch 161: Batch 71/116 loss: 0.0431\n",
      "Epoch 161: Batch 72/116 loss: 0.0425\n",
      "Epoch 161: Batch 73/116 loss: 0.0319\n",
      "Epoch 161: Batch 74/116 loss: 0.0484\n",
      "Epoch 161: Batch 75/116 loss: 0.0948\n",
      "Epoch 161: Batch 76/116 loss: 0.0551\n",
      "Epoch 161: Batch 77/116 loss: 0.0610\n",
      "Epoch 161: Batch 78/116 loss: 0.0446\n",
      "Epoch 161: Batch 79/116 loss: 0.0745\n",
      "Epoch 161: Batch 80/116 loss: 0.0604\n",
      "Epoch 161: Batch 81/116 loss: 0.0436\n",
      "Epoch 161: Batch 82/116 loss: 0.0452\n",
      "Epoch 161: Batch 83/116 loss: 0.0460\n",
      "Epoch 161: Batch 84/116 loss: 0.0555\n",
      "Epoch 161: Batch 85/116 loss: 0.0528\n",
      "Epoch 161: Batch 86/116 loss: 0.0404\n",
      "Epoch 161: Batch 87/116 loss: 0.0412\n",
      "Epoch 161: Batch 88/116 loss: 0.0443\n",
      "Epoch 161: Batch 89/116 loss: 0.0439\n",
      "Epoch 161: Batch 90/116 loss: 0.0556\n",
      "Epoch 161: Batch 91/116 loss: 0.0474\n",
      "Epoch 161: Batch 92/116 loss: 0.0414\n",
      "Epoch 161: Batch 93/116 loss: 0.0513\n",
      "Epoch 161: Batch 94/116 loss: 0.0541\n",
      "Epoch 161: Batch 95/116 loss: 0.0407\n",
      "Epoch 161: Batch 96/116 loss: 0.0645\n",
      "Epoch 161: Batch 97/116 loss: 0.0467\n",
      "Epoch 161: Batch 98/116 loss: 0.0551\n",
      "Epoch 161: Batch 99/116 loss: 0.0473\n",
      "Epoch 161: Batch 100/116 loss: 0.0430\n",
      "Epoch 161: Batch 101/116 loss: 0.0447\n",
      "Epoch 161: Batch 102/116 loss: 0.0487\n",
      "Epoch 161: Batch 103/116 loss: 0.0445\n",
      "Epoch 161: Batch 104/116 loss: 0.0355\n",
      "Epoch 161: Batch 105/116 loss: 0.0497\n",
      "Epoch 161: Batch 106/116 loss: 0.0583\n",
      "Epoch 161: Batch 107/116 loss: 0.0409\n",
      "Epoch 161: Batch 108/116 loss: 0.0301\n",
      "Epoch 161: Batch 109/116 loss: 0.0417\n",
      "Epoch 161: Batch 110/116 loss: 0.0602\n",
      "Epoch 161: Batch 111/116 loss: 0.0380\n",
      "Epoch 161: Batch 112/116 loss: 0.0260\n",
      "Epoch 161: Batch 113/116 loss: 0.0378\n",
      "Epoch 161: Batch 114/116 loss: 0.0517\n",
      "Epoch 161: Batch 115/116 loss: 0.0634\n",
      "Epoch 161: Batch 116/116 loss: 0.0314\n",
      "Epoch 161 train loss: 0.0452 valid loss: 0.0528\n",
      "performance reducing: 10\n",
      "Epoch 162: Batch 1/116 loss: 0.0490\n",
      "Epoch 162: Batch 2/116 loss: 0.0409\n",
      "Epoch 162: Batch 3/116 loss: 0.0467\n",
      "Epoch 162: Batch 4/116 loss: 0.0532\n",
      "Epoch 162: Batch 5/116 loss: 0.0327\n",
      "Epoch 162: Batch 6/116 loss: 0.0529\n",
      "Epoch 162: Batch 7/116 loss: 0.0385\n",
      "Epoch 162: Batch 8/116 loss: 0.0707\n",
      "Epoch 162: Batch 9/116 loss: 0.0534\n",
      "Epoch 162: Batch 10/116 loss: 0.0523\n",
      "Epoch 162: Batch 11/116 loss: 0.0394\n",
      "Epoch 162: Batch 12/116 loss: 0.0421\n",
      "Epoch 162: Batch 13/116 loss: 0.0230\n",
      "Epoch 162: Batch 14/116 loss: 0.0342\n",
      "Epoch 162: Batch 15/116 loss: 0.0352\n",
      "Epoch 162: Batch 16/116 loss: 0.0549\n",
      "Epoch 162: Batch 17/116 loss: 0.0298\n",
      "Epoch 162: Batch 18/116 loss: 0.0384\n",
      "Epoch 162: Batch 19/116 loss: 0.0494\n",
      "Epoch 162: Batch 20/116 loss: 0.0312\n",
      "Epoch 162: Batch 21/116 loss: 0.0404\n",
      "Epoch 162: Batch 22/116 loss: 0.0325\n",
      "Epoch 162: Batch 23/116 loss: 0.0321\n",
      "Epoch 162: Batch 24/116 loss: 0.0514\n",
      "Epoch 162: Batch 25/116 loss: 0.0398\n",
      "Epoch 162: Batch 26/116 loss: 0.0445\n",
      "Epoch 162: Batch 27/116 loss: 0.0474\n",
      "Epoch 162: Batch 28/116 loss: 0.0229\n",
      "Epoch 162: Batch 29/116 loss: 0.0349\n",
      "Epoch 162: Batch 30/116 loss: 0.0289\n",
      "Epoch 162: Batch 31/116 loss: 0.0342\n",
      "Epoch 162: Batch 32/116 loss: 0.0555\n",
      "Epoch 162: Batch 33/116 loss: 0.0403\n",
      "Epoch 162: Batch 34/116 loss: 0.0514\n",
      "Epoch 162: Batch 35/116 loss: 0.0456\n",
      "Epoch 162: Batch 36/116 loss: 0.0324\n",
      "Epoch 162: Batch 37/116 loss: 0.0406\n",
      "Epoch 162: Batch 38/116 loss: 0.0573\n",
      "Epoch 162: Batch 39/116 loss: 0.0649\n",
      "Epoch 162: Batch 40/116 loss: 0.0578\n",
      "Epoch 162: Batch 41/116 loss: 0.0414\n",
      "Epoch 162: Batch 42/116 loss: 0.0395\n",
      "Epoch 162: Batch 43/116 loss: 0.0589\n",
      "Epoch 162: Batch 44/116 loss: 0.0467\n",
      "Epoch 162: Batch 45/116 loss: 0.0343\n",
      "Epoch 162: Batch 46/116 loss: 0.0512\n",
      "Epoch 162: Batch 47/116 loss: 0.0546\n",
      "Epoch 162: Batch 48/116 loss: 0.0368\n",
      "Epoch 162: Batch 49/116 loss: 0.0336\n",
      "Epoch 162: Batch 50/116 loss: 0.0415\n",
      "Epoch 162: Batch 51/116 loss: 0.0432\n",
      "Epoch 162: Batch 52/116 loss: 0.0417\n",
      "Epoch 162: Batch 53/116 loss: 0.0337\n",
      "Epoch 162: Batch 54/116 loss: 0.0591\n",
      "Epoch 162: Batch 55/116 loss: 0.0499\n",
      "Epoch 162: Batch 56/116 loss: 0.0538\n",
      "Epoch 162: Batch 57/116 loss: 0.0289\n",
      "Epoch 162: Batch 58/116 loss: 0.0457\n",
      "Epoch 162: Batch 59/116 loss: 0.0470\n",
      "Epoch 162: Batch 60/116 loss: 0.0458\n",
      "Epoch 162: Batch 61/116 loss: 0.0573\n",
      "Epoch 162: Batch 62/116 loss: 0.0271\n",
      "Epoch 162: Batch 63/116 loss: 0.0458\n",
      "Epoch 162: Batch 64/116 loss: 0.0464\n",
      "Epoch 162: Batch 65/116 loss: 0.0299\n",
      "Epoch 162: Batch 66/116 loss: 0.0417\n",
      "Epoch 162: Batch 67/116 loss: 0.0307\n",
      "Epoch 162: Batch 68/116 loss: 0.0431\n",
      "Epoch 162: Batch 69/116 loss: 0.0332\n",
      "Epoch 162: Batch 70/116 loss: 0.0363\n",
      "Epoch 162: Batch 71/116 loss: 0.0325\n",
      "Epoch 162: Batch 72/116 loss: 0.0431\n",
      "Epoch 162: Batch 73/116 loss: 0.0384\n",
      "Epoch 162: Batch 74/116 loss: 0.0487\n",
      "Epoch 162: Batch 75/116 loss: 0.0380\n",
      "Epoch 162: Batch 76/116 loss: 0.0340\n",
      "Epoch 162: Batch 77/116 loss: 0.0330\n",
      "Epoch 162: Batch 78/116 loss: 0.0421\n",
      "Epoch 162: Batch 79/116 loss: 0.0450\n",
      "Epoch 162: Batch 80/116 loss: 0.0361\n",
      "Epoch 162: Batch 81/116 loss: 0.0669\n",
      "Epoch 162: Batch 82/116 loss: 0.0681\n",
      "Epoch 162: Batch 83/116 loss: 0.0390\n",
      "Epoch 162: Batch 84/116 loss: 0.0520\n",
      "Epoch 162: Batch 85/116 loss: 0.0481\n",
      "Epoch 162: Batch 86/116 loss: 0.0416\n",
      "Epoch 162: Batch 87/116 loss: 0.0520\n",
      "Epoch 162: Batch 88/116 loss: 0.0414\n",
      "Epoch 162: Batch 89/116 loss: 0.0503\n",
      "Epoch 162: Batch 90/116 loss: 0.0433\n",
      "Epoch 162: Batch 91/116 loss: 0.0375\n",
      "Epoch 162: Batch 92/116 loss: 0.0396\n",
      "Epoch 162: Batch 93/116 loss: 0.0248\n",
      "Epoch 162: Batch 94/116 loss: 0.0290\n",
      "Epoch 162: Batch 95/116 loss: 0.0588\n",
      "Epoch 162: Batch 96/116 loss: 0.0363\n",
      "Epoch 162: Batch 97/116 loss: 0.0296\n",
      "Epoch 162: Batch 98/116 loss: 0.0566\n",
      "Epoch 162: Batch 99/116 loss: 0.0604\n",
      "Epoch 162: Batch 100/116 loss: 0.0502\n",
      "Epoch 162: Batch 101/116 loss: 0.0479\n",
      "Epoch 162: Batch 102/116 loss: 0.0577\n",
      "Epoch 162: Batch 103/116 loss: 0.0512\n",
      "Epoch 162: Batch 104/116 loss: 0.0362\n",
      "Epoch 162: Batch 105/116 loss: 0.0447\n",
      "Epoch 162: Batch 106/116 loss: 0.0380\n",
      "Epoch 162: Batch 107/116 loss: 0.0468\n",
      "Epoch 162: Batch 108/116 loss: 0.0499\n",
      "Epoch 162: Batch 109/116 loss: 0.0553\n",
      "Epoch 162: Batch 110/116 loss: 0.0566\n",
      "Epoch 162: Batch 111/116 loss: 0.0522\n",
      "Epoch 162: Batch 112/116 loss: 0.0413\n",
      "Epoch 162: Batch 113/116 loss: 0.0372\n",
      "Epoch 162: Batch 114/116 loss: 0.0452\n",
      "Epoch 162: Batch 115/116 loss: 0.0547\n",
      "Epoch 162: Batch 116/116 loss: 0.0486\n",
      "Epoch 162 train loss: 0.0438 valid loss: 0.0540\n",
      "performance reducing: 11\n",
      "Epoch 163: Batch 1/116 loss: 0.0537\n",
      "Epoch 163: Batch 2/116 loss: 0.0484\n",
      "Epoch 163: Batch 3/116 loss: 0.0644\n",
      "Epoch 163: Batch 4/116 loss: 0.0383\n",
      "Epoch 163: Batch 5/116 loss: 0.0506\n",
      "Epoch 163: Batch 6/116 loss: 0.0337\n",
      "Epoch 163: Batch 7/116 loss: 0.0364\n",
      "Epoch 163: Batch 8/116 loss: 0.0541\n",
      "Epoch 163: Batch 9/116 loss: 0.0381\n",
      "Epoch 163: Batch 10/116 loss: 0.0449\n",
      "Epoch 163: Batch 11/116 loss: 0.0276\n",
      "Epoch 163: Batch 12/116 loss: 0.0399\n",
      "Epoch 163: Batch 13/116 loss: 0.0459\n",
      "Epoch 163: Batch 14/116 loss: 0.0291\n",
      "Epoch 163: Batch 15/116 loss: 0.0358\n",
      "Epoch 163: Batch 16/116 loss: 0.0408\n",
      "Epoch 163: Batch 17/116 loss: 0.0507\n",
      "Epoch 163: Batch 18/116 loss: 0.0386\n",
      "Epoch 163: Batch 19/116 loss: 0.0450\n",
      "Epoch 163: Batch 20/116 loss: 0.0383\n",
      "Epoch 163: Batch 21/116 loss: 0.0459\n",
      "Epoch 163: Batch 22/116 loss: 0.0480\n",
      "Epoch 163: Batch 23/116 loss: 0.0456\n",
      "Epoch 163: Batch 24/116 loss: 0.0410\n",
      "Epoch 163: Batch 25/116 loss: 0.0414\n",
      "Epoch 163: Batch 26/116 loss: 0.0459\n",
      "Epoch 163: Batch 27/116 loss: 0.0589\n",
      "Epoch 163: Batch 28/116 loss: 0.0428\n",
      "Epoch 163: Batch 29/116 loss: 0.0485\n",
      "Epoch 163: Batch 30/116 loss: 0.0318\n",
      "Epoch 163: Batch 31/116 loss: 0.0466\n",
      "Epoch 163: Batch 32/116 loss: 0.0614\n",
      "Epoch 163: Batch 33/116 loss: 0.0540\n",
      "Epoch 163: Batch 34/116 loss: 0.0542\n",
      "Epoch 163: Batch 35/116 loss: 0.0263\n",
      "Epoch 163: Batch 36/116 loss: 0.0576\n",
      "Epoch 163: Batch 37/116 loss: 0.0521\n",
      "Epoch 163: Batch 38/116 loss: 0.0340\n",
      "Epoch 163: Batch 39/116 loss: 0.0496\n",
      "Epoch 163: Batch 40/116 loss: 0.0505\n",
      "Epoch 163: Batch 41/116 loss: 0.0347\n",
      "Epoch 163: Batch 42/116 loss: 0.0341\n",
      "Epoch 163: Batch 43/116 loss: 0.0608\n",
      "Epoch 163: Batch 44/116 loss: 0.0345\n",
      "Epoch 163: Batch 45/116 loss: 0.0365\n",
      "Epoch 163: Batch 46/116 loss: 0.0390\n",
      "Epoch 163: Batch 47/116 loss: 0.0420\n",
      "Epoch 163: Batch 48/116 loss: 0.0302\n",
      "Epoch 163: Batch 49/116 loss: 0.0378\n",
      "Epoch 163: Batch 50/116 loss: 0.0322\n",
      "Epoch 163: Batch 51/116 loss: 0.0444\n",
      "Epoch 163: Batch 52/116 loss: 0.0405\n",
      "Epoch 163: Batch 53/116 loss: 0.0298\n",
      "Epoch 163: Batch 54/116 loss: 0.0376\n",
      "Epoch 163: Batch 55/116 loss: 0.0532\n",
      "Epoch 163: Batch 56/116 loss: 0.0347\n",
      "Epoch 163: Batch 57/116 loss: 0.0275\n",
      "Epoch 163: Batch 58/116 loss: 0.0430\n",
      "Epoch 163: Batch 59/116 loss: 0.0294\n",
      "Epoch 163: Batch 60/116 loss: 0.0402\n",
      "Epoch 163: Batch 61/116 loss: 0.0485\n",
      "Epoch 163: Batch 62/116 loss: 0.0217\n",
      "Epoch 163: Batch 63/116 loss: 0.0754\n",
      "Epoch 163: Batch 64/116 loss: 0.0376\n",
      "Epoch 163: Batch 65/116 loss: 0.0396\n",
      "Epoch 163: Batch 66/116 loss: 0.0427\n",
      "Epoch 163: Batch 67/116 loss: 0.0202\n",
      "Epoch 163: Batch 68/116 loss: 0.0425\n",
      "Epoch 163: Batch 69/116 loss: 0.0550\n",
      "Epoch 163: Batch 70/116 loss: 0.0370\n",
      "Epoch 163: Batch 71/116 loss: 0.0501\n",
      "Epoch 163: Batch 72/116 loss: 0.0490\n",
      "Epoch 163: Batch 73/116 loss: 0.0482\n",
      "Epoch 163: Batch 74/116 loss: 0.0615\n",
      "Epoch 163: Batch 75/116 loss: 0.0405\n",
      "Epoch 163: Batch 76/116 loss: 0.0528\n",
      "Epoch 163: Batch 77/116 loss: 0.0424\n",
      "Epoch 163: Batch 78/116 loss: 0.0357\n",
      "Epoch 163: Batch 79/116 loss: 0.0400\n",
      "Epoch 163: Batch 80/116 loss: 0.0365\n",
      "Epoch 163: Batch 81/116 loss: 0.0547\n",
      "Epoch 163: Batch 82/116 loss: 0.0389\n",
      "Epoch 163: Batch 83/116 loss: 0.0324\n",
      "Epoch 163: Batch 84/116 loss: 0.0414\n",
      "Epoch 163: Batch 85/116 loss: 0.0248\n",
      "Epoch 163: Batch 86/116 loss: 0.0306\n",
      "Epoch 163: Batch 87/116 loss: 0.0406\n",
      "Epoch 163: Batch 88/116 loss: 0.0395\n",
      "Epoch 163: Batch 89/116 loss: 0.0368\n",
      "Epoch 163: Batch 90/116 loss: 0.0565\n",
      "Epoch 163: Batch 91/116 loss: 0.0366\n",
      "Epoch 163: Batch 92/116 loss: 0.0394\n",
      "Epoch 163: Batch 93/116 loss: 0.0304\n",
      "Epoch 163: Batch 94/116 loss: 0.1119\n",
      "Epoch 163: Batch 95/116 loss: 0.0523\n",
      "Epoch 163: Batch 96/116 loss: 0.0376\n",
      "Epoch 163: Batch 97/116 loss: 0.0510\n",
      "Epoch 163: Batch 98/116 loss: 0.0480\n",
      "Epoch 163: Batch 99/116 loss: 0.0374\n",
      "Epoch 163: Batch 100/116 loss: 0.0348\n",
      "Epoch 163: Batch 101/116 loss: 0.0925\n",
      "Epoch 163: Batch 102/116 loss: 0.0481\n",
      "Epoch 163: Batch 103/116 loss: 0.0368\n",
      "Epoch 163: Batch 104/116 loss: 0.0546\n",
      "Epoch 163: Batch 105/116 loss: 0.0560\n",
      "Epoch 163: Batch 106/116 loss: 0.0555\n",
      "Epoch 163: Batch 107/116 loss: 0.0560\n",
      "Epoch 163: Batch 108/116 loss: 0.0534\n",
      "Epoch 163: Batch 109/116 loss: 0.0490\n",
      "Epoch 163: Batch 110/116 loss: 0.0462\n",
      "Epoch 163: Batch 111/116 loss: 0.0430\n",
      "Epoch 163: Batch 112/116 loss: 0.0700\n",
      "Epoch 163: Batch 113/116 loss: 0.0415\n",
      "Epoch 163: Batch 114/116 loss: 0.0421\n",
      "Epoch 163: Batch 115/116 loss: 0.0424\n",
      "Epoch 163: Batch 116/116 loss: 0.0519\n",
      "Epoch 163 train loss: 0.0443 valid loss: 0.0535\n",
      "performance reducing: 12\n",
      "Epoch 164: Batch 1/116 loss: 0.0327\n",
      "Epoch 164: Batch 2/116 loss: 0.0411\n",
      "Epoch 164: Batch 3/116 loss: 0.0466\n",
      "Epoch 164: Batch 4/116 loss: 0.0557\n",
      "Epoch 164: Batch 5/116 loss: 0.0663\n",
      "Epoch 164: Batch 6/116 loss: 0.0425\n",
      "Epoch 164: Batch 7/116 loss: 0.0361\n",
      "Epoch 164: Batch 8/116 loss: 0.0408\n",
      "Epoch 164: Batch 9/116 loss: 0.0531\n",
      "Epoch 164: Batch 10/116 loss: 0.0430\n",
      "Epoch 164: Batch 11/116 loss: 0.0414\n",
      "Epoch 164: Batch 12/116 loss: 0.0570\n",
      "Epoch 164: Batch 13/116 loss: 0.0375\n",
      "Epoch 164: Batch 14/116 loss: 0.0333\n",
      "Epoch 164: Batch 15/116 loss: 0.0431\n",
      "Epoch 164: Batch 16/116 loss: 0.0303\n",
      "Epoch 164: Batch 17/116 loss: 0.0447\n",
      "Epoch 164: Batch 18/116 loss: 0.0472\n",
      "Epoch 164: Batch 19/116 loss: 0.0412\n",
      "Epoch 164: Batch 20/116 loss: 0.0356\n",
      "Epoch 164: Batch 21/116 loss: 0.0524\n",
      "Epoch 164: Batch 22/116 loss: 0.0498\n",
      "Epoch 164: Batch 23/116 loss: 0.0443\n",
      "Epoch 164: Batch 24/116 loss: 0.0443\n",
      "Epoch 164: Batch 25/116 loss: 0.0356\n",
      "Epoch 164: Batch 26/116 loss: 0.0335\n",
      "Epoch 164: Batch 27/116 loss: 0.0349\n",
      "Epoch 164: Batch 28/116 loss: 0.0423\n",
      "Epoch 164: Batch 29/116 loss: 0.0426\n",
      "Epoch 164: Batch 30/116 loss: 0.0477\n",
      "Epoch 164: Batch 31/116 loss: 0.0473\n",
      "Epoch 164: Batch 32/116 loss: 0.0423\n",
      "Epoch 164: Batch 33/116 loss: 0.0534\n",
      "Epoch 164: Batch 34/116 loss: 0.0288\n",
      "Epoch 164: Batch 35/116 loss: 0.0447\n",
      "Epoch 164: Batch 36/116 loss: 0.0277\n",
      "Epoch 164: Batch 37/116 loss: 0.0371\n",
      "Epoch 164: Batch 38/116 loss: 0.0355\n",
      "Epoch 164: Batch 39/116 loss: 0.0309\n",
      "Epoch 164: Batch 40/116 loss: 0.0360\n",
      "Epoch 164: Batch 41/116 loss: 0.0374\n",
      "Epoch 164: Batch 42/116 loss: 0.0505\n",
      "Epoch 164: Batch 43/116 loss: 0.0397\n",
      "Epoch 164: Batch 44/116 loss: 0.0517\n",
      "Epoch 164: Batch 45/116 loss: 0.0360\n",
      "Epoch 164: Batch 46/116 loss: 0.0315\n",
      "Epoch 164: Batch 47/116 loss: 0.0543\n",
      "Epoch 164: Batch 48/116 loss: 0.0360\n",
      "Epoch 164: Batch 49/116 loss: 0.0314\n",
      "Epoch 164: Batch 50/116 loss: 0.0294\n",
      "Epoch 164: Batch 51/116 loss: 0.0346\n",
      "Epoch 164: Batch 52/116 loss: 0.0588\n",
      "Epoch 164: Batch 53/116 loss: 0.0367\n",
      "Epoch 164: Batch 54/116 loss: 0.0566\n",
      "Epoch 164: Batch 55/116 loss: 0.0383\n",
      "Epoch 164: Batch 56/116 loss: 0.0405\n",
      "Epoch 164: Batch 57/116 loss: 0.0557\n",
      "Epoch 164: Batch 58/116 loss: 0.0477\n",
      "Epoch 164: Batch 59/116 loss: 0.0244\n",
      "Epoch 164: Batch 60/116 loss: 0.0452\n",
      "Epoch 164: Batch 61/116 loss: 0.0520\n",
      "Epoch 164: Batch 62/116 loss: 0.0387\n",
      "Epoch 164: Batch 63/116 loss: 0.0283\n",
      "Epoch 164: Batch 64/116 loss: 0.0424\n",
      "Epoch 164: Batch 65/116 loss: 0.0377\n",
      "Epoch 164: Batch 66/116 loss: 0.0506\n",
      "Epoch 164: Batch 67/116 loss: 0.0386\n",
      "Epoch 164: Batch 68/116 loss: 0.0613\n",
      "Epoch 164: Batch 69/116 loss: 0.0441\n",
      "Epoch 164: Batch 70/116 loss: 0.0556\n",
      "Epoch 164: Batch 71/116 loss: 0.0240\n",
      "Epoch 164: Batch 72/116 loss: 0.0481\n",
      "Epoch 164: Batch 73/116 loss: 0.0382\n",
      "Epoch 164: Batch 74/116 loss: 0.0390\n",
      "Epoch 164: Batch 75/116 loss: 0.0537\n",
      "Epoch 164: Batch 76/116 loss: 0.0378\n",
      "Epoch 164: Batch 77/116 loss: 0.0524\n",
      "Epoch 164: Batch 78/116 loss: 0.0327\n",
      "Epoch 164: Batch 79/116 loss: 0.0483\n",
      "Epoch 164: Batch 80/116 loss: 0.0265\n",
      "Epoch 164: Batch 81/116 loss: 0.0380\n",
      "Epoch 164: Batch 82/116 loss: 0.0391\n",
      "Epoch 164: Batch 83/116 loss: 0.0480\n",
      "Epoch 164: Batch 84/116 loss: 0.0443\n",
      "Epoch 164: Batch 85/116 loss: 0.0397\n",
      "Epoch 164: Batch 86/116 loss: 0.0351\n",
      "Epoch 164: Batch 87/116 loss: 0.0507\n",
      "Epoch 164: Batch 88/116 loss: 0.0583\n",
      "Epoch 164: Batch 89/116 loss: 0.0298\n",
      "Epoch 164: Batch 90/116 loss: 0.0442\n",
      "Epoch 164: Batch 91/116 loss: 0.0505\n",
      "Epoch 164: Batch 92/116 loss: 0.0403\n",
      "Epoch 164: Batch 93/116 loss: 0.0316\n",
      "Epoch 164: Batch 94/116 loss: 0.0371\n",
      "Epoch 164: Batch 95/116 loss: 0.0562\n",
      "Epoch 164: Batch 96/116 loss: 0.0387\n",
      "Epoch 164: Batch 97/116 loss: 0.0599\n",
      "Epoch 164: Batch 98/116 loss: 0.0478\n",
      "Epoch 164: Batch 99/116 loss: 0.0437\n",
      "Epoch 164: Batch 100/116 loss: 0.0466\n",
      "Epoch 164: Batch 101/116 loss: 0.0397\n",
      "Epoch 164: Batch 102/116 loss: 0.0408\n",
      "Epoch 164: Batch 103/116 loss: 0.0487\n",
      "Epoch 164: Batch 104/116 loss: 0.0427\n",
      "Epoch 164: Batch 105/116 loss: 0.0334\n",
      "Epoch 164: Batch 106/116 loss: 0.0419\n",
      "Epoch 164: Batch 107/116 loss: 0.0476\n",
      "Epoch 164: Batch 108/116 loss: 0.0309\n",
      "Epoch 164: Batch 109/116 loss: 0.0545\n",
      "Epoch 164: Batch 110/116 loss: 0.0619\n",
      "Epoch 164: Batch 111/116 loss: 0.0416\n",
      "Epoch 164: Batch 112/116 loss: 0.0405\n",
      "Epoch 164: Batch 113/116 loss: 0.0381\n",
      "Epoch 164: Batch 114/116 loss: 0.0379\n",
      "Epoch 164: Batch 115/116 loss: 0.0400\n",
      "Epoch 164: Batch 116/116 loss: 0.0322\n",
      "Epoch 164 train loss: 0.0424 valid loss: 0.0513\n",
      "performance reducing: 13\n",
      "Epoch 165: Batch 1/116 loss: 0.0377\n",
      "Epoch 165: Batch 2/116 loss: 0.0375\n",
      "Epoch 165: Batch 3/116 loss: 0.0565\n",
      "Epoch 165: Batch 4/116 loss: 0.0592\n",
      "Epoch 165: Batch 5/116 loss: 0.0288\n",
      "Epoch 165: Batch 6/116 loss: 0.0368\n",
      "Epoch 165: Batch 7/116 loss: 0.0303\n",
      "Epoch 165: Batch 8/116 loss: 0.0440\n",
      "Epoch 165: Batch 9/116 loss: 0.0335\n",
      "Epoch 165: Batch 10/116 loss: 0.0617\n",
      "Epoch 165: Batch 11/116 loss: 0.0328\n",
      "Epoch 165: Batch 12/116 loss: 0.0300\n",
      "Epoch 165: Batch 13/116 loss: 0.0409\n",
      "Epoch 165: Batch 14/116 loss: 0.0277\n",
      "Epoch 165: Batch 15/116 loss: 0.0278\n",
      "Epoch 165: Batch 16/116 loss: 0.0368\n",
      "Epoch 165: Batch 17/116 loss: 0.0360\n",
      "Epoch 165: Batch 18/116 loss: 0.0316\n",
      "Epoch 165: Batch 19/116 loss: 0.0439\n",
      "Epoch 165: Batch 20/116 loss: 0.0294\n",
      "Epoch 165: Batch 21/116 loss: 0.0334\n",
      "Epoch 165: Batch 22/116 loss: 0.0383\n",
      "Epoch 165: Batch 23/116 loss: 0.0456\n",
      "Epoch 165: Batch 24/116 loss: 0.0427\n",
      "Epoch 165: Batch 25/116 loss: 0.0345\n",
      "Epoch 165: Batch 26/116 loss: 0.0455\n",
      "Epoch 165: Batch 27/116 loss: 0.0424\n",
      "Epoch 165: Batch 28/116 loss: 0.0477\n",
      "Epoch 165: Batch 29/116 loss: 0.0484\n",
      "Epoch 165: Batch 30/116 loss: 0.0467\n",
      "Epoch 165: Batch 31/116 loss: 0.0472\n",
      "Epoch 165: Batch 32/116 loss: 0.0449\n",
      "Epoch 165: Batch 33/116 loss: 0.0500\n",
      "Epoch 165: Batch 34/116 loss: 0.0444\n",
      "Epoch 165: Batch 35/116 loss: 0.0489\n",
      "Epoch 165: Batch 36/116 loss: 0.0527\n",
      "Epoch 165: Batch 37/116 loss: 0.0544\n",
      "Epoch 165: Batch 38/116 loss: 0.0391\n",
      "Epoch 165: Batch 39/116 loss: 0.0377\n",
      "Epoch 165: Batch 40/116 loss: 0.0500\n",
      "Epoch 165: Batch 41/116 loss: 0.0433\n",
      "Epoch 165: Batch 42/116 loss: 0.0543\n",
      "Epoch 165: Batch 43/116 loss: 0.0592\n",
      "Epoch 165: Batch 44/116 loss: 0.0441\n",
      "Epoch 165: Batch 45/116 loss: 0.0388\n",
      "Epoch 165: Batch 46/116 loss: 0.0546\n",
      "Epoch 165: Batch 47/116 loss: 0.0332\n",
      "Epoch 165: Batch 48/116 loss: 0.0430\n",
      "Epoch 165: Batch 49/116 loss: 0.0414\n",
      "Epoch 165: Batch 50/116 loss: 0.0384\n",
      "Epoch 165: Batch 51/116 loss: 0.0509\n",
      "Epoch 165: Batch 52/116 loss: 0.0748\n",
      "Epoch 165: Batch 53/116 loss: 0.0352\n",
      "Epoch 165: Batch 54/116 loss: 0.0355\n",
      "Epoch 165: Batch 55/116 loss: 0.0493\n",
      "Epoch 165: Batch 56/116 loss: 0.0349\n",
      "Epoch 165: Batch 57/116 loss: 0.0575\n",
      "Epoch 165: Batch 58/116 loss: 0.0277\n",
      "Epoch 165: Batch 59/116 loss: 0.0511\n",
      "Epoch 165: Batch 60/116 loss: 0.0559\n",
      "Epoch 165: Batch 61/116 loss: 0.0333\n",
      "Epoch 165: Batch 62/116 loss: 0.0425\n",
      "Epoch 165: Batch 63/116 loss: 0.0295\n",
      "Epoch 165: Batch 64/116 loss: 0.0389\n",
      "Epoch 165: Batch 65/116 loss: 0.0328\n",
      "Epoch 165: Batch 66/116 loss: 0.0524\n",
      "Epoch 165: Batch 67/116 loss: 0.0451\n",
      "Epoch 165: Batch 68/116 loss: 0.0323\n",
      "Epoch 165: Batch 69/116 loss: 0.0318\n",
      "Epoch 165: Batch 70/116 loss: 0.0482\n",
      "Epoch 165: Batch 71/116 loss: 0.0459\n",
      "Epoch 165: Batch 72/116 loss: 0.0636\n",
      "Epoch 165: Batch 73/116 loss: 0.0681\n",
      "Epoch 165: Batch 74/116 loss: 0.0488\n",
      "Epoch 165: Batch 75/116 loss: 0.0402\n",
      "Epoch 165: Batch 76/116 loss: 0.0506\n",
      "Epoch 165: Batch 77/116 loss: 0.0398\n",
      "Epoch 165: Batch 78/116 loss: 0.0364\n",
      "Epoch 165: Batch 79/116 loss: 0.0638\n",
      "Epoch 165: Batch 80/116 loss: 0.0298\n",
      "Epoch 165: Batch 81/116 loss: 0.0471\n",
      "Epoch 165: Batch 82/116 loss: 0.0459\n",
      "Epoch 165: Batch 83/116 loss: 0.0511\n",
      "Epoch 165: Batch 84/116 loss: 0.0427\n",
      "Epoch 165: Batch 85/116 loss: 0.0407\n",
      "Epoch 165: Batch 86/116 loss: 0.0409\n",
      "Epoch 165: Batch 87/116 loss: 0.0315\n",
      "Epoch 165: Batch 88/116 loss: 0.0386\n",
      "Epoch 165: Batch 89/116 loss: 0.0624\n",
      "Epoch 165: Batch 90/116 loss: 0.0313\n",
      "Epoch 165: Batch 91/116 loss: 0.0490\n",
      "Epoch 165: Batch 92/116 loss: 0.0463\n",
      "Epoch 165: Batch 93/116 loss: 0.0605\n",
      "Epoch 165: Batch 94/116 loss: 0.0620\n",
      "Epoch 165: Batch 95/116 loss: 0.0465\n",
      "Epoch 165: Batch 96/116 loss: 0.0613\n",
      "Epoch 165: Batch 97/116 loss: 0.0591\n",
      "Epoch 165: Batch 98/116 loss: 0.0457\n",
      "Epoch 165: Batch 99/116 loss: 0.0525\n",
      "Epoch 165: Batch 100/116 loss: 0.0541\n",
      "Epoch 165: Batch 101/116 loss: 0.0331\n",
      "Epoch 165: Batch 102/116 loss: 0.0355\n",
      "Epoch 165: Batch 103/116 loss: 0.0337\n",
      "Epoch 165: Batch 104/116 loss: 0.0413\n",
      "Epoch 165: Batch 105/116 loss: 0.0405\n",
      "Epoch 165: Batch 106/116 loss: 0.0421\n",
      "Epoch 165: Batch 107/116 loss: 0.0398\n",
      "Epoch 165: Batch 108/116 loss: 0.0397\n",
      "Epoch 165: Batch 109/116 loss: 0.0550\n",
      "Epoch 165: Batch 110/116 loss: 0.0447\n",
      "Epoch 165: Batch 111/116 loss: 0.0327\n",
      "Epoch 165: Batch 112/116 loss: 0.0328\n",
      "Epoch 165: Batch 113/116 loss: 0.0402\n",
      "Epoch 165: Batch 114/116 loss: 0.0315\n",
      "Epoch 165: Batch 115/116 loss: 0.0390\n",
      "Epoch 165: Batch 116/116 loss: 0.0420\n",
      "Epoch 165 train loss: 0.0435 valid loss: 0.0504\n",
      "performance reducing: 14\n",
      "Epoch 166: Batch 1/116 loss: 0.0420\n",
      "Epoch 166: Batch 2/116 loss: 0.0473\n",
      "Epoch 166: Batch 3/116 loss: 0.0453\n",
      "Epoch 166: Batch 4/116 loss: 0.0471\n",
      "Epoch 166: Batch 5/116 loss: 0.0463\n",
      "Epoch 166: Batch 6/116 loss: 0.0309\n",
      "Epoch 166: Batch 7/116 loss: 0.0498\n",
      "Epoch 166: Batch 8/116 loss: 0.0385\n",
      "Epoch 166: Batch 9/116 loss: 0.0391\n",
      "Epoch 166: Batch 10/116 loss: 0.0399\n",
      "Epoch 166: Batch 11/116 loss: 0.0228\n",
      "Epoch 166: Batch 12/116 loss: 0.0494\n",
      "Epoch 166: Batch 13/116 loss: 0.0485\n",
      "Epoch 166: Batch 14/116 loss: 0.0355\n",
      "Epoch 166: Batch 15/116 loss: 0.0379\n",
      "Epoch 166: Batch 16/116 loss: 0.0520\n",
      "Epoch 166: Batch 17/116 loss: 0.0308\n",
      "Epoch 166: Batch 18/116 loss: 0.0431\n",
      "Epoch 166: Batch 19/116 loss: 0.0317\n",
      "Epoch 166: Batch 20/116 loss: 0.0340\n",
      "Epoch 166: Batch 21/116 loss: 0.0282\n",
      "Epoch 166: Batch 22/116 loss: 0.0465\n",
      "Epoch 166: Batch 23/116 loss: 0.0259\n",
      "Epoch 166: Batch 24/116 loss: 0.0520\n",
      "Epoch 166: Batch 25/116 loss: 0.0297\n",
      "Epoch 166: Batch 26/116 loss: 0.0600\n",
      "Epoch 166: Batch 27/116 loss: 0.0245\n",
      "Epoch 166: Batch 28/116 loss: 0.0634\n",
      "Epoch 166: Batch 29/116 loss: 0.0347\n",
      "Epoch 166: Batch 30/116 loss: 0.0418\n",
      "Epoch 166: Batch 31/116 loss: 0.0335\n",
      "Epoch 166: Batch 32/116 loss: 0.0432\n",
      "Epoch 166: Batch 33/116 loss: 0.0368\n",
      "Epoch 166: Batch 34/116 loss: 0.0471\n",
      "Epoch 166: Batch 35/116 loss: 0.0569\n",
      "Epoch 166: Batch 36/116 loss: 0.0429\n",
      "Epoch 166: Batch 37/116 loss: 0.0388\n",
      "Epoch 166: Batch 38/116 loss: 0.0311\n",
      "Epoch 166: Batch 39/116 loss: 0.0456\n",
      "Epoch 166: Batch 40/116 loss: 0.0442\n",
      "Epoch 166: Batch 41/116 loss: 0.0649\n",
      "Epoch 166: Batch 42/116 loss: 0.0370\n",
      "Epoch 166: Batch 43/116 loss: 0.0314\n",
      "Epoch 166: Batch 44/116 loss: 0.0420\n",
      "Epoch 166: Batch 45/116 loss: 0.0538\n",
      "Epoch 166: Batch 46/116 loss: 0.0362\n",
      "Epoch 166: Batch 47/116 loss: 0.0423\n",
      "Epoch 166: Batch 48/116 loss: 0.0415\n",
      "Epoch 166: Batch 49/116 loss: 0.0546\n",
      "Epoch 166: Batch 50/116 loss: 0.0273\n",
      "Epoch 166: Batch 51/116 loss: 0.0380\n",
      "Epoch 166: Batch 52/116 loss: 0.0290\n",
      "Epoch 166: Batch 53/116 loss: 0.0445\n",
      "Epoch 166: Batch 54/116 loss: 0.0382\n",
      "Epoch 166: Batch 55/116 loss: 0.0348\n",
      "Epoch 166: Batch 56/116 loss: 0.0452\n",
      "Epoch 166: Batch 57/116 loss: 0.0340\n",
      "Epoch 166: Batch 58/116 loss: 0.0445\n",
      "Epoch 166: Batch 59/116 loss: 0.0419\n",
      "Epoch 166: Batch 60/116 loss: 0.0494\n",
      "Epoch 166: Batch 61/116 loss: 0.0412\n",
      "Epoch 166: Batch 62/116 loss: 0.0608\n",
      "Epoch 166: Batch 63/116 loss: 0.0345\n",
      "Epoch 166: Batch 64/116 loss: 0.0646\n",
      "Epoch 166: Batch 65/116 loss: 0.0550\n",
      "Epoch 166: Batch 66/116 loss: 0.0450\n",
      "Epoch 166: Batch 67/116 loss: 0.0355\n",
      "Epoch 166: Batch 68/116 loss: 0.0363\n",
      "Epoch 166: Batch 69/116 loss: 0.0872\n",
      "Epoch 166: Batch 70/116 loss: 0.0386\n",
      "Epoch 166: Batch 71/116 loss: 0.0575\n",
      "Epoch 166: Batch 72/116 loss: 0.0456\n",
      "Epoch 166: Batch 73/116 loss: 0.0547\n",
      "Epoch 166: Batch 74/116 loss: 0.0452\n",
      "Epoch 166: Batch 75/116 loss: 0.0341\n",
      "Epoch 166: Batch 76/116 loss: 0.0379\n",
      "Epoch 166: Batch 77/116 loss: 0.0458\n",
      "Epoch 166: Batch 78/116 loss: 0.0279\n",
      "Epoch 166: Batch 79/116 loss: 0.0458\n",
      "Epoch 166: Batch 80/116 loss: 0.0481\n",
      "Epoch 166: Batch 81/116 loss: 0.0348\n",
      "Epoch 166: Batch 82/116 loss: 0.0550\n",
      "Epoch 166: Batch 83/116 loss: 0.0335\n",
      "Epoch 166: Batch 84/116 loss: 0.0459\n",
      "Epoch 166: Batch 85/116 loss: 0.0390\n",
      "Epoch 166: Batch 86/116 loss: 0.0335\n",
      "Epoch 166: Batch 87/116 loss: 0.0282\n",
      "Epoch 166: Batch 88/116 loss: 0.0316\n",
      "Epoch 166: Batch 89/116 loss: 0.0622\n",
      "Epoch 166: Batch 90/116 loss: 0.0406\n",
      "Epoch 166: Batch 91/116 loss: 0.0347\n",
      "Epoch 166: Batch 92/116 loss: 0.0357\n",
      "Epoch 166: Batch 93/116 loss: 0.0478\n",
      "Epoch 166: Batch 94/116 loss: 0.0373\n",
      "Epoch 166: Batch 95/116 loss: 0.0309\n",
      "Epoch 166: Batch 96/116 loss: 0.0435\n",
      "Epoch 166: Batch 97/116 loss: 0.0456\n",
      "Epoch 166: Batch 98/116 loss: 0.0512\n",
      "Epoch 166: Batch 99/116 loss: 0.0296\n",
      "Epoch 166: Batch 100/116 loss: 0.0290\n",
      "Epoch 166: Batch 101/116 loss: 0.0451\n",
      "Epoch 166: Batch 102/116 loss: 0.0508\n",
      "Epoch 166: Batch 103/116 loss: 0.0456\n",
      "Epoch 166: Batch 104/116 loss: 0.0470\n",
      "Epoch 166: Batch 105/116 loss: 0.0335\n",
      "Epoch 166: Batch 106/116 loss: 0.0478\n",
      "Epoch 166: Batch 107/116 loss: 0.0291\n",
      "Epoch 166: Batch 108/116 loss: 0.0338\n",
      "Epoch 166: Batch 109/116 loss: 0.0311\n",
      "Epoch 166: Batch 110/116 loss: 0.0349\n",
      "Epoch 166: Batch 111/116 loss: 0.0378\n",
      "Epoch 166: Batch 112/116 loss: 0.0374\n",
      "Epoch 166: Batch 113/116 loss: 0.0523\n",
      "Epoch 166: Batch 114/116 loss: 0.0306\n",
      "Epoch 166: Batch 115/116 loss: 0.0294\n",
      "Epoch 166: Batch 116/116 loss: 0.0481\n",
      "Epoch 166 train loss: 0.0417 valid loss: 0.0518\n",
      "performance reducing: 15\n",
      "Epoch 167: Batch 1/116 loss: 0.0515\n",
      "Epoch 167: Batch 2/116 loss: 0.0354\n",
      "Epoch 167: Batch 3/116 loss: 0.0605\n",
      "Epoch 167: Batch 4/116 loss: 0.0281\n",
      "Epoch 167: Batch 5/116 loss: 0.0357\n",
      "Epoch 167: Batch 6/116 loss: 0.0479\n",
      "Epoch 167: Batch 7/116 loss: 0.0431\n",
      "Epoch 167: Batch 8/116 loss: 0.0478\n",
      "Epoch 167: Batch 9/116 loss: 0.0530\n",
      "Epoch 167: Batch 10/116 loss: 0.0336\n",
      "Epoch 167: Batch 11/116 loss: 0.0641\n",
      "Epoch 167: Batch 12/116 loss: 0.0313\n",
      "Epoch 167: Batch 13/116 loss: 0.0380\n",
      "Epoch 167: Batch 14/116 loss: 0.0643\n",
      "Epoch 167: Batch 15/116 loss: 0.0426\n",
      "Epoch 167: Batch 16/116 loss: 0.0376\n",
      "Epoch 167: Batch 17/116 loss: 0.0402\n",
      "Epoch 167: Batch 18/116 loss: 0.0378\n",
      "Epoch 167: Batch 19/116 loss: 0.0335\n",
      "Epoch 167: Batch 20/116 loss: 0.0440\n",
      "Epoch 167: Batch 21/116 loss: 0.0481\n",
      "Epoch 167: Batch 22/116 loss: 0.0294\n",
      "Epoch 167: Batch 23/116 loss: 0.0382\n",
      "Epoch 167: Batch 24/116 loss: 0.0482\n",
      "Epoch 167: Batch 25/116 loss: 0.0351\n",
      "Epoch 167: Batch 26/116 loss: 0.0333\n",
      "Epoch 167: Batch 27/116 loss: 0.0459\n",
      "Epoch 167: Batch 28/116 loss: 0.0571\n",
      "Epoch 167: Batch 29/116 loss: 0.0427\n",
      "Epoch 167: Batch 30/116 loss: 0.0335\n",
      "Epoch 167: Batch 31/116 loss: 0.0308\n",
      "Epoch 167: Batch 32/116 loss: 0.0380\n",
      "Epoch 167: Batch 33/116 loss: 0.0421\n",
      "Epoch 167: Batch 34/116 loss: 0.0292\n",
      "Epoch 167: Batch 35/116 loss: 0.0357\n",
      "Epoch 167: Batch 36/116 loss: 0.0566\n",
      "Epoch 167: Batch 37/116 loss: 0.0358\n",
      "Epoch 167: Batch 38/116 loss: 0.0438\n",
      "Epoch 167: Batch 39/116 loss: 0.0449\n",
      "Epoch 167: Batch 40/116 loss: 0.0253\n",
      "Epoch 167: Batch 41/116 loss: 0.0283\n",
      "Epoch 167: Batch 42/116 loss: 0.0626\n",
      "Epoch 167: Batch 43/116 loss: 0.0332\n",
      "Epoch 167: Batch 44/116 loss: 0.0300\n",
      "Epoch 167: Batch 45/116 loss: 0.0289\n",
      "Epoch 167: Batch 46/116 loss: 0.0543\n",
      "Epoch 167: Batch 47/116 loss: 0.0501\n",
      "Epoch 167: Batch 48/116 loss: 0.0357\n",
      "Epoch 167: Batch 49/116 loss: 0.0655\n",
      "Epoch 167: Batch 50/116 loss: 0.0437\n",
      "Epoch 167: Batch 51/116 loss: 0.0347\n",
      "Epoch 167: Batch 52/116 loss: 0.0390\n",
      "Epoch 167: Batch 53/116 loss: 0.0560\n",
      "Epoch 167: Batch 54/116 loss: 0.0576\n",
      "Epoch 167: Batch 55/116 loss: 0.0406\n",
      "Epoch 167: Batch 56/116 loss: 0.0355\n",
      "Epoch 167: Batch 57/116 loss: 0.0503\n",
      "Epoch 167: Batch 58/116 loss: 0.0536\n",
      "Epoch 167: Batch 59/116 loss: 0.0570\n",
      "Epoch 167: Batch 60/116 loss: 0.0318\n",
      "Epoch 167: Batch 61/116 loss: 0.0435\n",
      "Epoch 167: Batch 62/116 loss: 0.0300\n",
      "Epoch 167: Batch 63/116 loss: 0.0405\n",
      "Epoch 167: Batch 64/116 loss: 0.0574\n",
      "Epoch 167: Batch 65/116 loss: 0.0598\n",
      "Epoch 167: Batch 66/116 loss: 0.0551\n",
      "Epoch 167: Batch 67/116 loss: 0.0355\n",
      "Epoch 167: Batch 68/116 loss: 0.0489\n",
      "Epoch 167: Batch 69/116 loss: 0.0349\n",
      "Epoch 167: Batch 70/116 loss: 0.0442\n",
      "Epoch 167: Batch 71/116 loss: 0.0492\n",
      "Epoch 167: Batch 72/116 loss: 0.0382\n",
      "Epoch 167: Batch 73/116 loss: 0.0420\n",
      "Epoch 167: Batch 74/116 loss: 0.0381\n",
      "Epoch 167: Batch 75/116 loss: 0.0481\n",
      "Epoch 167: Batch 76/116 loss: 0.0339\n",
      "Epoch 167: Batch 77/116 loss: 0.0544\n",
      "Epoch 167: Batch 78/116 loss: 0.0324\n",
      "Epoch 167: Batch 79/116 loss: 0.0340\n",
      "Epoch 167: Batch 80/116 loss: 0.0368\n",
      "Epoch 167: Batch 81/116 loss: 0.0511\n",
      "Epoch 167: Batch 82/116 loss: 0.0361\n",
      "Epoch 167: Batch 83/116 loss: 0.0334\n",
      "Epoch 167: Batch 84/116 loss: 0.0520\n",
      "Epoch 167: Batch 85/116 loss: 0.0563\n",
      "Epoch 167: Batch 86/116 loss: 0.0502\n",
      "Epoch 167: Batch 87/116 loss: 0.0445\n",
      "Epoch 167: Batch 88/116 loss: 0.0605\n",
      "Epoch 167: Batch 89/116 loss: 0.0411\n",
      "Epoch 167: Batch 90/116 loss: 0.0508\n",
      "Epoch 167: Batch 91/116 loss: 0.0397\n",
      "Epoch 167: Batch 92/116 loss: 0.0370\n",
      "Epoch 167: Batch 93/116 loss: 0.0379\n",
      "Epoch 167: Batch 94/116 loss: 0.0472\n",
      "Epoch 167: Batch 95/116 loss: 0.0225\n",
      "Epoch 167: Batch 96/116 loss: 0.0531\n",
      "Epoch 167: Batch 97/116 loss: 0.0388\n",
      "Epoch 167: Batch 98/116 loss: 0.0341\n",
      "Epoch 167: Batch 99/116 loss: 0.0295\n",
      "Epoch 167: Batch 100/116 loss: 0.0394\n",
      "Epoch 167: Batch 101/116 loss: 0.0179\n",
      "Epoch 167: Batch 102/116 loss: 0.0488\n",
      "Epoch 167: Batch 103/116 loss: 0.0274\n",
      "Epoch 167: Batch 104/116 loss: 0.0576\n",
      "Epoch 167: Batch 105/116 loss: 0.0380\n",
      "Epoch 167: Batch 106/116 loss: 0.0429\n",
      "Epoch 167: Batch 107/116 loss: 0.0409\n",
      "Epoch 167: Batch 108/116 loss: 0.0648\n",
      "Epoch 167: Batch 109/116 loss: 0.0655\n",
      "Epoch 167: Batch 110/116 loss: 0.0408\n",
      "Epoch 167: Batch 111/116 loss: 0.0396\n",
      "Epoch 167: Batch 112/116 loss: 0.0332\n",
      "Epoch 167: Batch 113/116 loss: 0.0473\n",
      "Epoch 167: Batch 114/116 loss: 0.0444\n",
      "Epoch 167: Batch 115/116 loss: 0.0286\n",
      "Epoch 167: Batch 116/116 loss: 0.0502\n",
      "Epoch 167 train loss: 0.0427 valid loss: 0.0524\n",
      "performance reducing: 16\n",
      "Epoch 168: Batch 1/116 loss: 0.0293\n",
      "Epoch 168: Batch 2/116 loss: 0.0353\n",
      "Epoch 168: Batch 3/116 loss: 0.0525\n",
      "Epoch 168: Batch 4/116 loss: 0.0379\n",
      "Epoch 168: Batch 5/116 loss: 0.0421\n",
      "Epoch 168: Batch 6/116 loss: 0.0559\n",
      "Epoch 168: Batch 7/116 loss: 0.0461\n",
      "Epoch 168: Batch 8/116 loss: 0.0405\n",
      "Epoch 168: Batch 9/116 loss: 0.0685\n",
      "Epoch 168: Batch 10/116 loss: 0.0367\n",
      "Epoch 168: Batch 11/116 loss: 0.0467\n",
      "Epoch 168: Batch 12/116 loss: 0.0610\n",
      "Epoch 168: Batch 13/116 loss: 0.0506\n",
      "Epoch 168: Batch 14/116 loss: 0.0433\n",
      "Epoch 168: Batch 15/116 loss: 0.0444\n",
      "Epoch 168: Batch 16/116 loss: 0.0489\n",
      "Epoch 168: Batch 17/116 loss: 0.0220\n",
      "Epoch 168: Batch 18/116 loss: 0.0454\n",
      "Epoch 168: Batch 19/116 loss: 0.0528\n",
      "Epoch 168: Batch 20/116 loss: 0.0341\n",
      "Epoch 168: Batch 21/116 loss: 0.0356\n",
      "Epoch 168: Batch 22/116 loss: 0.0627\n",
      "Epoch 168: Batch 23/116 loss: 0.0311\n",
      "Epoch 168: Batch 24/116 loss: 0.0485\n",
      "Epoch 168: Batch 25/116 loss: 0.0452\n",
      "Epoch 168: Batch 26/116 loss: 0.0469\n",
      "Epoch 168: Batch 27/116 loss: 0.0323\n",
      "Epoch 168: Batch 28/116 loss: 0.0327\n",
      "Epoch 168: Batch 29/116 loss: 0.0555\n",
      "Epoch 168: Batch 30/116 loss: 0.0281\n",
      "Epoch 168: Batch 31/116 loss: 0.0607\n",
      "Epoch 168: Batch 32/116 loss: 0.0396\n",
      "Epoch 168: Batch 33/116 loss: 0.0247\n",
      "Epoch 168: Batch 34/116 loss: 0.0580\n",
      "Epoch 168: Batch 35/116 loss: 0.0322\n",
      "Epoch 168: Batch 36/116 loss: 0.0456\n",
      "Epoch 168: Batch 37/116 loss: 0.0441\n",
      "Epoch 168: Batch 38/116 loss: 0.0347\n",
      "Epoch 168: Batch 39/116 loss: 0.0444\n",
      "Epoch 168: Batch 40/116 loss: 0.0486\n",
      "Epoch 168: Batch 41/116 loss: 0.0365\n",
      "Epoch 168: Batch 42/116 loss: 0.0484\n",
      "Epoch 168: Batch 43/116 loss: 0.0578\n",
      "Epoch 168: Batch 44/116 loss: 0.0430\n",
      "Epoch 168: Batch 45/116 loss: 0.0346\n",
      "Epoch 168: Batch 46/116 loss: 0.0312\n",
      "Epoch 168: Batch 47/116 loss: 0.0344\n",
      "Epoch 168: Batch 48/116 loss: 0.0417\n",
      "Epoch 168: Batch 49/116 loss: 0.0423\n",
      "Epoch 168: Batch 50/116 loss: 0.0657\n",
      "Epoch 168: Batch 51/116 loss: 0.0535\n",
      "Epoch 168: Batch 52/116 loss: 0.0410\n",
      "Epoch 168: Batch 53/116 loss: 0.0477\n",
      "Epoch 168: Batch 54/116 loss: 0.0271\n",
      "Epoch 168: Batch 55/116 loss: 0.0405\n",
      "Epoch 168: Batch 56/116 loss: 0.0340\n",
      "Epoch 168: Batch 57/116 loss: 0.0452\n",
      "Epoch 168: Batch 58/116 loss: 0.0719\n",
      "Epoch 168: Batch 59/116 loss: 0.0416\n",
      "Epoch 168: Batch 60/116 loss: 0.0399\n",
      "Epoch 168: Batch 61/116 loss: 0.0375\n",
      "Epoch 168: Batch 62/116 loss: 0.0405\n",
      "Epoch 168: Batch 63/116 loss: 0.0545\n",
      "Epoch 168: Batch 64/116 loss: 0.0497\n",
      "Epoch 168: Batch 65/116 loss: 0.0244\n",
      "Epoch 168: Batch 66/116 loss: 0.0535\n",
      "Epoch 168: Batch 67/116 loss: 0.0480\n",
      "Epoch 168: Batch 68/116 loss: 0.0363\n",
      "Epoch 168: Batch 69/116 loss: 0.0360\n",
      "Epoch 168: Batch 70/116 loss: 0.0498\n",
      "Epoch 168: Batch 71/116 loss: 0.0374\n",
      "Epoch 168: Batch 72/116 loss: 0.0358\n",
      "Epoch 168: Batch 73/116 loss: 0.0487\n",
      "Epoch 168: Batch 74/116 loss: 0.0558\n",
      "Epoch 168: Batch 75/116 loss: 0.0480\n",
      "Epoch 168: Batch 76/116 loss: 0.0447\n",
      "Epoch 168: Batch 77/116 loss: 0.0307\n",
      "Epoch 168: Batch 78/116 loss: 0.0417\n",
      "Epoch 168: Batch 79/116 loss: 0.0347\n",
      "Epoch 168: Batch 80/116 loss: 0.0429\n",
      "Epoch 168: Batch 81/116 loss: 0.0406\n",
      "Epoch 168: Batch 82/116 loss: 0.0353\n",
      "Epoch 168: Batch 83/116 loss: 0.0383\n",
      "Epoch 168: Batch 84/116 loss: 0.0277\n",
      "Epoch 168: Batch 85/116 loss: 0.0343\n",
      "Epoch 168: Batch 86/116 loss: 0.0502\n",
      "Epoch 168: Batch 87/116 loss: 0.0256\n",
      "Epoch 168: Batch 88/116 loss: 0.0341\n",
      "Epoch 168: Batch 89/116 loss: 0.0380\n",
      "Epoch 168: Batch 90/116 loss: 0.0651\n",
      "Epoch 168: Batch 91/116 loss: 0.0496\n",
      "Epoch 168: Batch 92/116 loss: 0.0386\n",
      "Epoch 168: Batch 93/116 loss: 0.0469\n",
      "Epoch 168: Batch 94/116 loss: 0.0491\n",
      "Epoch 168: Batch 95/116 loss: 0.0440\n",
      "Epoch 168: Batch 96/116 loss: 0.0355\n",
      "Epoch 168: Batch 97/116 loss: 0.0298\n",
      "Epoch 168: Batch 98/116 loss: 0.0479\n",
      "Epoch 168: Batch 99/116 loss: 0.0294\n",
      "Epoch 168: Batch 100/116 loss: 0.0503\n",
      "Epoch 168: Batch 101/116 loss: 0.0454\n",
      "Epoch 168: Batch 102/116 loss: 0.0559\n",
      "Epoch 168: Batch 103/116 loss: 0.0372\n",
      "Epoch 168: Batch 104/116 loss: 0.0752\n",
      "Epoch 168: Batch 105/116 loss: 0.0452\n",
      "Epoch 168: Batch 106/116 loss: 0.0444\n",
      "Epoch 168: Batch 107/116 loss: 0.0568\n",
      "Epoch 168: Batch 108/116 loss: 0.0411\n",
      "Epoch 168: Batch 109/116 loss: 0.0334\n",
      "Epoch 168: Batch 110/116 loss: 0.0390\n",
      "Epoch 168: Batch 111/116 loss: 0.0428\n",
      "Epoch 168: Batch 112/116 loss: 0.0382\n",
      "Epoch 168: Batch 113/116 loss: 0.0308\n",
      "Epoch 168: Batch 114/116 loss: 0.0278\n",
      "Epoch 168: Batch 115/116 loss: 0.0384\n",
      "Epoch 168: Batch 116/116 loss: 0.0316\n",
      "Epoch 168 train loss: 0.0428 valid loss: 0.0604\n",
      "performance reducing: 17\n",
      "Epoch 169: Batch 1/116 loss: 0.0509\n",
      "Epoch 169: Batch 2/116 loss: 0.0540\n",
      "Epoch 169: Batch 3/116 loss: 0.0547\n",
      "Epoch 169: Batch 4/116 loss: 0.0693\n",
      "Epoch 169: Batch 5/116 loss: 0.0398\n",
      "Epoch 169: Batch 6/116 loss: 0.0610\n",
      "Epoch 169: Batch 7/116 loss: 0.0615\n",
      "Epoch 169: Batch 8/116 loss: 0.0464\n",
      "Epoch 169: Batch 9/116 loss: 0.0499\n",
      "Epoch 169: Batch 10/116 loss: 0.0326\n",
      "Epoch 169: Batch 11/116 loss: 0.0542\n",
      "Epoch 169: Batch 12/116 loss: 0.0488\n",
      "Epoch 169: Batch 13/116 loss: 0.0386\n",
      "Epoch 169: Batch 14/116 loss: 0.0317\n",
      "Epoch 169: Batch 15/116 loss: 0.0486\n",
      "Epoch 169: Batch 16/116 loss: 0.0582\n",
      "Epoch 169: Batch 17/116 loss: 0.0415\n",
      "Epoch 169: Batch 18/116 loss: 0.0670\n",
      "Epoch 169: Batch 19/116 loss: 0.0383\n",
      "Epoch 169: Batch 20/116 loss: 0.0370\n",
      "Epoch 169: Batch 21/116 loss: 0.0348\n",
      "Epoch 169: Batch 22/116 loss: 0.0511\n",
      "Epoch 169: Batch 23/116 loss: 0.0425\n",
      "Epoch 169: Batch 24/116 loss: 0.0403\n",
      "Epoch 169: Batch 25/116 loss: 0.0462\n",
      "Epoch 169: Batch 26/116 loss: 0.0396\n",
      "Epoch 169: Batch 27/116 loss: 0.0487\n",
      "Epoch 169: Batch 28/116 loss: 0.0490\n",
      "Epoch 169: Batch 29/116 loss: 0.0407\n",
      "Epoch 169: Batch 30/116 loss: 0.0427\n",
      "Epoch 169: Batch 31/116 loss: 0.0249\n",
      "Epoch 169: Batch 32/116 loss: 0.0389\n",
      "Epoch 169: Batch 33/116 loss: 0.0378\n",
      "Epoch 169: Batch 34/116 loss: 0.0413\n",
      "Epoch 169: Batch 35/116 loss: 0.0351\n",
      "Epoch 169: Batch 36/116 loss: 0.0411\n",
      "Epoch 169: Batch 37/116 loss: 0.0330\n",
      "Epoch 169: Batch 38/116 loss: 0.0394\n",
      "Epoch 169: Batch 39/116 loss: 0.0546\n",
      "Epoch 169: Batch 40/116 loss: 0.0509\n",
      "Epoch 169: Batch 41/116 loss: 0.0559\n",
      "Epoch 169: Batch 42/116 loss: 0.0503\n",
      "Epoch 169: Batch 43/116 loss: 0.0533\n",
      "Epoch 169: Batch 44/116 loss: 0.0358\n",
      "Epoch 169: Batch 45/116 loss: 0.0250\n",
      "Epoch 169: Batch 46/116 loss: 0.0465\n",
      "Epoch 169: Batch 47/116 loss: 0.0705\n",
      "Epoch 169: Batch 48/116 loss: 0.0539\n",
      "Epoch 169: Batch 49/116 loss: 0.0313\n",
      "Epoch 169: Batch 50/116 loss: 0.0405\n",
      "Epoch 169: Batch 51/116 loss: 0.0410\n",
      "Epoch 169: Batch 52/116 loss: 0.0427\n",
      "Epoch 169: Batch 53/116 loss: 0.0542\n",
      "Epoch 169: Batch 54/116 loss: 0.0445\n",
      "Epoch 169: Batch 55/116 loss: 0.0538\n",
      "Epoch 169: Batch 56/116 loss: 0.0586\n",
      "Epoch 169: Batch 57/116 loss: 0.0419\n",
      "Epoch 169: Batch 58/116 loss: 0.0452\n",
      "Epoch 169: Batch 59/116 loss: 0.0575\n",
      "Epoch 169: Batch 60/116 loss: 0.0316\n",
      "Epoch 169: Batch 61/116 loss: 0.0215\n",
      "Epoch 169: Batch 62/116 loss: 0.0624\n",
      "Epoch 169: Batch 63/116 loss: 0.0520\n",
      "Epoch 169: Batch 64/116 loss: 0.0477\n",
      "Epoch 169: Batch 65/116 loss: 0.0559\n",
      "Epoch 169: Batch 66/116 loss: 0.0512\n",
      "Epoch 169: Batch 67/116 loss: 0.0386\n",
      "Epoch 169: Batch 68/116 loss: 0.0538\n",
      "Epoch 169: Batch 69/116 loss: 0.0409\n",
      "Epoch 169: Batch 70/116 loss: 0.0406\n",
      "Epoch 169: Batch 71/116 loss: 0.0313\n",
      "Epoch 169: Batch 72/116 loss: 0.0455\n",
      "Epoch 169: Batch 73/116 loss: 0.0508\n",
      "Epoch 169: Batch 74/116 loss: 0.0379\n",
      "Epoch 169: Batch 75/116 loss: 0.0434\n",
      "Epoch 169: Batch 76/116 loss: 0.0337\n",
      "Epoch 169: Batch 77/116 loss: 0.0288\n",
      "Epoch 169: Batch 78/116 loss: 0.0366\n",
      "Epoch 169: Batch 79/116 loss: 0.0296\n",
      "Epoch 169: Batch 80/116 loss: 0.0393\n",
      "Epoch 169: Batch 81/116 loss: 0.0518\n",
      "Epoch 169: Batch 82/116 loss: 0.0448\n",
      "Epoch 169: Batch 83/116 loss: 0.0402\n",
      "Epoch 169: Batch 84/116 loss: 0.0681\n",
      "Epoch 169: Batch 85/116 loss: 0.0415\n",
      "Epoch 169: Batch 86/116 loss: 0.0406\n",
      "Epoch 169: Batch 87/116 loss: 0.0516\n",
      "Epoch 169: Batch 88/116 loss: 0.0458\n",
      "Epoch 169: Batch 89/116 loss: 0.0450\n",
      "Epoch 169: Batch 90/116 loss: 0.0359\n",
      "Epoch 169: Batch 91/116 loss: 0.0438\n",
      "Epoch 169: Batch 92/116 loss: 0.0513\n",
      "Epoch 169: Batch 93/116 loss: 0.0440\n",
      "Epoch 169: Batch 94/116 loss: 0.0302\n",
      "Epoch 169: Batch 95/116 loss: 0.0268\n",
      "Epoch 169: Batch 96/116 loss: 0.0283\n",
      "Epoch 169: Batch 97/116 loss: 0.0530\n",
      "Epoch 169: Batch 98/116 loss: 0.0281\n",
      "Epoch 169: Batch 99/116 loss: 0.0369\n",
      "Epoch 169: Batch 100/116 loss: 0.0537\n",
      "Epoch 169: Batch 101/116 loss: 0.0387\n",
      "Epoch 169: Batch 102/116 loss: 0.0284\n",
      "Epoch 169: Batch 103/116 loss: 0.0612\n",
      "Epoch 169: Batch 104/116 loss: 0.0404\n",
      "Epoch 169: Batch 105/116 loss: 0.0815\n",
      "Epoch 169: Batch 106/116 loss: 0.0462\n",
      "Epoch 169: Batch 107/116 loss: 0.0524\n",
      "Epoch 169: Batch 108/116 loss: 0.0289\n",
      "Epoch 169: Batch 109/116 loss: 0.0455\n",
      "Epoch 169: Batch 110/116 loss: 0.0522\n",
      "Epoch 169: Batch 111/116 loss: 0.0719\n",
      "Epoch 169: Batch 112/116 loss: 0.0442\n",
      "Epoch 169: Batch 113/116 loss: 0.0421\n",
      "Epoch 169: Batch 114/116 loss: 0.0429\n",
      "Epoch 169: Batch 115/116 loss: 0.0395\n",
      "Epoch 169: Batch 116/116 loss: 0.0466\n",
      "Epoch 169 train loss: 0.0450 valid loss: 0.0628\n",
      "performance reducing: 18\n",
      "Epoch 170: Batch 1/116 loss: 0.0702\n",
      "Epoch 170: Batch 2/116 loss: 0.0390\n",
      "Epoch 170: Batch 3/116 loss: 0.0580\n",
      "Epoch 170: Batch 4/116 loss: 0.0434\n",
      "Epoch 170: Batch 5/116 loss: 0.0506\n",
      "Epoch 170: Batch 6/116 loss: 0.0457\n",
      "Epoch 170: Batch 7/116 loss: 0.0447\n",
      "Epoch 170: Batch 8/116 loss: 0.0448\n",
      "Epoch 170: Batch 9/116 loss: 0.0551\n",
      "Epoch 170: Batch 10/116 loss: 0.0245\n",
      "Epoch 170: Batch 11/116 loss: 0.0503\n",
      "Epoch 170: Batch 12/116 loss: 0.0453\n",
      "Epoch 170: Batch 13/116 loss: 0.0275\n",
      "Epoch 170: Batch 14/116 loss: 0.0280\n",
      "Epoch 170: Batch 15/116 loss: 0.0541\n",
      "Epoch 170: Batch 16/116 loss: 0.0474\n",
      "Epoch 170: Batch 17/116 loss: 0.0348\n",
      "Epoch 170: Batch 18/116 loss: 0.0452\n",
      "Epoch 170: Batch 19/116 loss: 0.0465\n",
      "Epoch 170: Batch 20/116 loss: 0.0459\n",
      "Epoch 170: Batch 21/116 loss: 0.0428\n",
      "Epoch 170: Batch 22/116 loss: 0.0314\n",
      "Epoch 170: Batch 23/116 loss: 0.0362\n",
      "Epoch 170: Batch 24/116 loss: 0.0490\n",
      "Epoch 170: Batch 25/116 loss: 0.0337\n",
      "Epoch 170: Batch 26/116 loss: 0.0338\n",
      "Epoch 170: Batch 27/116 loss: 0.0212\n",
      "Epoch 170: Batch 28/116 loss: 0.0418\n",
      "Epoch 170: Batch 29/116 loss: 0.0335\n",
      "Epoch 170: Batch 30/116 loss: 0.0396\n",
      "Epoch 170: Batch 31/116 loss: 0.0422\n",
      "Epoch 170: Batch 32/116 loss: 0.0549\n",
      "Epoch 170: Batch 33/116 loss: 0.0451\n",
      "Epoch 170: Batch 34/116 loss: 0.0327\n",
      "Epoch 170: Batch 35/116 loss: 0.0346\n",
      "Epoch 170: Batch 36/116 loss: 0.0377\n",
      "Epoch 170: Batch 37/116 loss: 0.0481\n",
      "Epoch 170: Batch 38/116 loss: 0.0414\n",
      "Epoch 170: Batch 39/116 loss: 0.0396\n",
      "Epoch 170: Batch 40/116 loss: 0.0408\n",
      "Epoch 170: Batch 41/116 loss: 0.0455\n",
      "Epoch 170: Batch 42/116 loss: 0.0494\n",
      "Epoch 170: Batch 43/116 loss: 0.0381\n",
      "Epoch 170: Batch 44/116 loss: 0.0453\n",
      "Epoch 170: Batch 45/116 loss: 0.0358\n",
      "Epoch 170: Batch 46/116 loss: 0.0455\n",
      "Epoch 170: Batch 47/116 loss: 0.0344\n",
      "Epoch 170: Batch 48/116 loss: 0.0273\n",
      "Epoch 170: Batch 49/116 loss: 0.0390\n",
      "Epoch 170: Batch 50/116 loss: 0.0566\n",
      "Epoch 170: Batch 51/116 loss: 0.0496\n",
      "Epoch 170: Batch 52/116 loss: 0.0427\n",
      "Epoch 170: Batch 53/116 loss: 0.0309\n",
      "Epoch 170: Batch 54/116 loss: 0.0366\n",
      "Epoch 170: Batch 55/116 loss: 0.0522\n",
      "Epoch 170: Batch 56/116 loss: 0.0486\n",
      "Epoch 170: Batch 57/116 loss: 0.0387\n",
      "Epoch 170: Batch 58/116 loss: 0.0250\n",
      "Epoch 170: Batch 59/116 loss: 0.0624\n",
      "Epoch 170: Batch 60/116 loss: 0.0339\n",
      "Epoch 170: Batch 61/116 loss: 0.0485\n",
      "Epoch 170: Batch 62/116 loss: 0.0384\n",
      "Epoch 170: Batch 63/116 loss: 0.0567\n",
      "Epoch 170: Batch 64/116 loss: 0.0492\n",
      "Epoch 170: Batch 65/116 loss: 0.0712\n",
      "Epoch 170: Batch 66/116 loss: 0.0576\n",
      "Epoch 170: Batch 67/116 loss: 0.0435\n",
      "Epoch 170: Batch 68/116 loss: 0.0302\n",
      "Epoch 170: Batch 69/116 loss: 0.0434\n",
      "Epoch 170: Batch 70/116 loss: 0.0345\n",
      "Epoch 170: Batch 71/116 loss: 0.0418\n",
      "Epoch 170: Batch 72/116 loss: 0.0706\n",
      "Epoch 170: Batch 73/116 loss: 0.0501\n",
      "Epoch 170: Batch 74/116 loss: 0.0482\n",
      "Epoch 170: Batch 75/116 loss: 0.0558\n",
      "Epoch 170: Batch 76/116 loss: 0.0388\n",
      "Epoch 170: Batch 77/116 loss: 0.0422\n",
      "Epoch 170: Batch 78/116 loss: 0.0400\n",
      "Epoch 170: Batch 79/116 loss: 0.0323\n",
      "Epoch 170: Batch 80/116 loss: 0.0388\n",
      "Epoch 170: Batch 81/116 loss: 0.0391\n",
      "Epoch 170: Batch 82/116 loss: 0.0401\n",
      "Epoch 170: Batch 83/116 loss: 0.0322\n",
      "Epoch 170: Batch 84/116 loss: 0.0558\n",
      "Epoch 170: Batch 85/116 loss: 0.0325\n",
      "Epoch 170: Batch 86/116 loss: 0.0448\n",
      "Epoch 170: Batch 87/116 loss: 0.0334\n",
      "Epoch 170: Batch 88/116 loss: 0.0390\n",
      "Epoch 170: Batch 89/116 loss: 0.0401\n",
      "Epoch 170: Batch 90/116 loss: 0.0453\n",
      "Epoch 170: Batch 91/116 loss: 0.0828\n",
      "Epoch 170: Batch 92/116 loss: 0.0568\n",
      "Epoch 170: Batch 93/116 loss: 0.0344\n",
      "Epoch 170: Batch 94/116 loss: 0.0470\n",
      "Epoch 170: Batch 95/116 loss: 0.0409\n",
      "Epoch 170: Batch 96/116 loss: 0.0287\n",
      "Epoch 170: Batch 97/116 loss: 0.0403\n",
      "Epoch 170: Batch 98/116 loss: 0.0431\n",
      "Epoch 170: Batch 99/116 loss: 0.0333\n",
      "Epoch 170: Batch 100/116 loss: 0.0434\n",
      "Epoch 170: Batch 101/116 loss: 0.0274\n",
      "Epoch 170: Batch 102/116 loss: 0.0280\n",
      "Epoch 170: Batch 103/116 loss: 0.0612\n",
      "Epoch 170: Batch 104/116 loss: 0.0623\n",
      "Epoch 170: Batch 105/116 loss: 0.0359\n",
      "Epoch 170: Batch 106/116 loss: 0.0394\n",
      "Epoch 170: Batch 107/116 loss: 0.0391\n",
      "Epoch 170: Batch 108/116 loss: 0.0531\n",
      "Epoch 170: Batch 109/116 loss: 0.0331\n",
      "Epoch 170: Batch 110/116 loss: 0.0331\n",
      "Epoch 170: Batch 111/116 loss: 0.0574\n",
      "Epoch 170: Batch 112/116 loss: 0.0411\n",
      "Epoch 170: Batch 113/116 loss: 0.0315\n",
      "Epoch 170: Batch 114/116 loss: 0.0385\n",
      "Epoch 170: Batch 115/116 loss: 0.0254\n",
      "Epoch 170: Batch 116/116 loss: 0.0372\n",
      "Epoch 170 train loss: 0.0427 valid loss: 0.0514\n",
      "performance reducing: 19\n",
      "Epoch 171: Batch 1/116 loss: 0.0562\n",
      "Epoch 171: Batch 2/116 loss: 0.0537\n",
      "Epoch 171: Batch 3/116 loss: 0.0491\n",
      "Epoch 171: Batch 4/116 loss: 0.0742\n",
      "Epoch 171: Batch 5/116 loss: 0.0362\n",
      "Epoch 171: Batch 6/116 loss: 0.0459\n",
      "Epoch 171: Batch 7/116 loss: 0.0443\n",
      "Epoch 171: Batch 8/116 loss: 0.0533\n",
      "Epoch 171: Batch 9/116 loss: 0.0518\n",
      "Epoch 171: Batch 10/116 loss: 0.0315\n",
      "Epoch 171: Batch 11/116 loss: 0.0436\n",
      "Epoch 171: Batch 12/116 loss: 0.0360\n",
      "Epoch 171: Batch 13/116 loss: 0.0369\n",
      "Epoch 171: Batch 14/116 loss: 0.0486\n",
      "Epoch 171: Batch 15/116 loss: 0.0330\n",
      "Epoch 171: Batch 16/116 loss: 0.0388\n",
      "Epoch 171: Batch 17/116 loss: 0.0412\n",
      "Epoch 171: Batch 18/116 loss: 0.0443\n",
      "Epoch 171: Batch 19/116 loss: 0.0312\n",
      "Epoch 171: Batch 20/116 loss: 0.0424\n",
      "Epoch 171: Batch 21/116 loss: 0.0329\n",
      "Epoch 171: Batch 22/116 loss: 0.0595\n",
      "Epoch 171: Batch 23/116 loss: 0.0301\n",
      "Epoch 171: Batch 24/116 loss: 0.0437\n",
      "Epoch 171: Batch 25/116 loss: 0.0459\n",
      "Epoch 171: Batch 26/116 loss: 0.0490\n",
      "Epoch 171: Batch 27/116 loss: 0.0513\n",
      "Epoch 171: Batch 28/116 loss: 0.0768\n",
      "Epoch 171: Batch 29/116 loss: 0.0362\n",
      "Epoch 171: Batch 30/116 loss: 0.0507\n",
      "Epoch 171: Batch 31/116 loss: 0.0459\n",
      "Epoch 171: Batch 32/116 loss: 0.0484\n",
      "Epoch 171: Batch 33/116 loss: 0.0327\n",
      "Epoch 171: Batch 34/116 loss: 0.0520\n",
      "Epoch 171: Batch 35/116 loss: 0.0450\n",
      "Epoch 171: Batch 36/116 loss: 0.0570\n",
      "Epoch 171: Batch 37/116 loss: 0.0255\n",
      "Epoch 171: Batch 38/116 loss: 0.0451\n",
      "Epoch 171: Batch 39/116 loss: 0.0480\n",
      "Epoch 171: Batch 40/116 loss: 0.0439\n",
      "Epoch 171: Batch 41/116 loss: 0.0298\n",
      "Epoch 171: Batch 42/116 loss: 0.0480\n",
      "Epoch 171: Batch 43/116 loss: 0.0378\n",
      "Epoch 171: Batch 44/116 loss: 0.0362\n",
      "Epoch 171: Batch 45/116 loss: 0.0422\n",
      "Epoch 171: Batch 46/116 loss: 0.0376\n",
      "Epoch 171: Batch 47/116 loss: 0.0432\n",
      "Epoch 171: Batch 48/116 loss: 0.0447\n",
      "Epoch 171: Batch 49/116 loss: 0.0339\n",
      "Epoch 171: Batch 50/116 loss: 0.0402\n",
      "Epoch 171: Batch 51/116 loss: 0.0219\n",
      "Epoch 171: Batch 52/116 loss: 0.0322\n",
      "Epoch 171: Batch 53/116 loss: 0.0266\n",
      "Epoch 171: Batch 54/116 loss: 0.0360\n",
      "Epoch 171: Batch 55/116 loss: 0.0499\n",
      "Epoch 171: Batch 56/116 loss: 0.0328\n",
      "Epoch 171: Batch 57/116 loss: 0.0326\n",
      "Epoch 171: Batch 58/116 loss: 0.0539\n",
      "Epoch 171: Batch 59/116 loss: 0.0534\n",
      "Epoch 171: Batch 60/116 loss: 0.0259\n",
      "Epoch 171: Batch 61/116 loss: 0.0414\n",
      "Epoch 171: Batch 62/116 loss: 0.0320\n",
      "Epoch 171: Batch 63/116 loss: 0.0513\n",
      "Epoch 171: Batch 64/116 loss: 0.0488\n",
      "Epoch 171: Batch 65/116 loss: 0.0428\n",
      "Epoch 171: Batch 66/116 loss: 0.0429\n",
      "Epoch 171: Batch 67/116 loss: 0.0251\n",
      "Epoch 171: Batch 68/116 loss: 0.0250\n",
      "Epoch 171: Batch 69/116 loss: 0.0348\n",
      "Epoch 171: Batch 70/116 loss: 0.0649\n",
      "Epoch 171: Batch 71/116 loss: 0.0468\n",
      "Epoch 171: Batch 72/116 loss: 0.0315\n",
      "Epoch 171: Batch 73/116 loss: 0.0316\n",
      "Epoch 171: Batch 74/116 loss: 0.0723\n",
      "Epoch 171: Batch 75/116 loss: 0.0593\n",
      "Epoch 171: Batch 76/116 loss: 0.0394\n",
      "Epoch 171: Batch 77/116 loss: 0.0393\n",
      "Epoch 171: Batch 78/116 loss: 0.0362\n",
      "Epoch 171: Batch 79/116 loss: 0.0345\n",
      "Epoch 171: Batch 80/116 loss: 0.0253\n",
      "Epoch 171: Batch 81/116 loss: 0.0464\n",
      "Epoch 171: Batch 82/116 loss: 0.0462\n",
      "Epoch 171: Batch 83/116 loss: 0.0262\n",
      "Epoch 171: Batch 84/116 loss: 0.0400\n",
      "Epoch 171: Batch 85/116 loss: 0.0543\n",
      "Epoch 171: Batch 86/116 loss: 0.0306\n",
      "Epoch 171: Batch 87/116 loss: 0.0355\n",
      "Epoch 171: Batch 88/116 loss: 0.0452\n",
      "Epoch 171: Batch 89/116 loss: 0.0524\n",
      "Epoch 171: Batch 90/116 loss: 0.0430\n",
      "Epoch 171: Batch 91/116 loss: 0.0403\n",
      "Epoch 171: Batch 92/116 loss: 0.0409\n",
      "Epoch 171: Batch 93/116 loss: 0.0482\n",
      "Epoch 171: Batch 94/116 loss: 0.0358\n",
      "Epoch 171: Batch 95/116 loss: 0.0432\n",
      "Epoch 171: Batch 96/116 loss: 0.0447\n",
      "Epoch 171: Batch 97/116 loss: 0.0600\n",
      "Epoch 171: Batch 98/116 loss: 0.0507\n",
      "Epoch 171: Batch 99/116 loss: 0.0535\n",
      "Epoch 171: Batch 100/116 loss: 0.0503\n",
      "Epoch 171: Batch 101/116 loss: 0.0458\n",
      "Epoch 171: Batch 102/116 loss: 0.0226\n",
      "Epoch 171: Batch 103/116 loss: 0.0251\n",
      "Epoch 171: Batch 104/116 loss: 0.0423\n",
      "Epoch 171: Batch 105/116 loss: 0.0341\n",
      "Epoch 171: Batch 106/116 loss: 0.0494\n",
      "Epoch 171: Batch 107/116 loss: 0.0319\n",
      "Epoch 171: Batch 108/116 loss: 0.0201\n",
      "Epoch 171: Batch 109/116 loss: 0.0296\n",
      "Epoch 171: Batch 110/116 loss: 0.0347\n",
      "Epoch 171: Batch 111/116 loss: 0.0245\n",
      "Epoch 171: Batch 112/116 loss: 0.0302\n",
      "Epoch 171: Batch 113/116 loss: 0.0332\n",
      "Epoch 171: Batch 114/116 loss: 0.0359\n",
      "Epoch 171: Batch 115/116 loss: 0.0326\n",
      "Epoch 171: Batch 116/116 loss: 0.0427\n",
      "Epoch 171 train loss: 0.0415 valid loss: 0.0530\n",
      "performance reducing: 20\n",
      "Epoch 172: Batch 1/116 loss: 0.0388\n",
      "Epoch 172: Batch 2/116 loss: 0.0377\n",
      "Epoch 172: Batch 3/116 loss: 0.0417\n",
      "Epoch 172: Batch 4/116 loss: 0.0357\n",
      "Epoch 172: Batch 5/116 loss: 0.0352\n",
      "Epoch 172: Batch 6/116 loss: 0.0369\n",
      "Epoch 172: Batch 7/116 loss: 0.0412\n",
      "Epoch 172: Batch 8/116 loss: 0.0520\n",
      "Epoch 172: Batch 9/116 loss: 0.0353\n",
      "Epoch 172: Batch 10/116 loss: 0.0450\n",
      "Epoch 172: Batch 11/116 loss: 0.0380\n",
      "Epoch 172: Batch 12/116 loss: 0.0727\n",
      "Epoch 172: Batch 13/116 loss: 0.0395\n",
      "Epoch 172: Batch 14/116 loss: 0.0373\n",
      "Epoch 172: Batch 15/116 loss: 0.0365\n",
      "Epoch 172: Batch 16/116 loss: 0.0389\n",
      "Epoch 172: Batch 17/116 loss: 0.0344\n",
      "Epoch 172: Batch 18/116 loss: 0.0649\n",
      "Epoch 172: Batch 19/116 loss: 0.0343\n",
      "Epoch 172: Batch 20/116 loss: 0.0527\n",
      "Epoch 172: Batch 21/116 loss: 0.0392\n",
      "Epoch 172: Batch 22/116 loss: 0.0313\n",
      "Epoch 172: Batch 23/116 loss: 0.0364\n",
      "Epoch 172: Batch 24/116 loss: 0.0396\n",
      "Epoch 172: Batch 25/116 loss: 0.0495\n",
      "Epoch 172: Batch 26/116 loss: 0.0312\n",
      "Epoch 172: Batch 27/116 loss: 0.0841\n",
      "Epoch 172: Batch 28/116 loss: 0.0597\n",
      "Epoch 172: Batch 29/116 loss: 0.0326\n",
      "Epoch 172: Batch 30/116 loss: 0.0345\n",
      "Epoch 172: Batch 31/116 loss: 0.0513\n",
      "Epoch 172: Batch 32/116 loss: 0.0616\n",
      "Epoch 172: Batch 33/116 loss: 0.0498\n",
      "Epoch 172: Batch 34/116 loss: 0.0339\n",
      "Epoch 172: Batch 35/116 loss: 0.0266\n",
      "Epoch 172: Batch 36/116 loss: 0.0375\n",
      "Epoch 172: Batch 37/116 loss: 0.0355\n",
      "Epoch 172: Batch 38/116 loss: 0.0568\n",
      "Epoch 172: Batch 39/116 loss: 0.0321\n",
      "Epoch 172: Batch 40/116 loss: 0.0563\n",
      "Epoch 172: Batch 41/116 loss: 0.0355\n",
      "Epoch 172: Batch 42/116 loss: 0.0518\n",
      "Epoch 172: Batch 43/116 loss: 0.0347\n",
      "Epoch 172: Batch 44/116 loss: 0.0345\n",
      "Epoch 172: Batch 45/116 loss: 0.0284\n",
      "Epoch 172: Batch 46/116 loss: 0.0343\n",
      "Epoch 172: Batch 47/116 loss: 0.0435\n",
      "Epoch 172: Batch 48/116 loss: 0.0543\n",
      "Epoch 172: Batch 49/116 loss: 0.0461\n",
      "Epoch 172: Batch 50/116 loss: 0.0495\n",
      "Epoch 172: Batch 51/116 loss: 0.0282\n",
      "Epoch 172: Batch 52/116 loss: 0.0441\n",
      "Epoch 172: Batch 53/116 loss: 0.0337\n",
      "Epoch 172: Batch 54/116 loss: 0.0217\n",
      "Epoch 172: Batch 55/116 loss: 0.0541\n",
      "Epoch 172: Batch 56/116 loss: 0.0519\n",
      "Epoch 172: Batch 57/116 loss: 0.0576\n",
      "Epoch 172: Batch 58/116 loss: 0.0400\n",
      "Epoch 172: Batch 59/116 loss: 0.0397\n",
      "Epoch 172: Batch 60/116 loss: 0.0419\n",
      "Epoch 172: Batch 61/116 loss: 0.0605\n",
      "Epoch 172: Batch 62/116 loss: 0.0296\n",
      "Epoch 172: Batch 63/116 loss: 0.0710\n",
      "Epoch 172: Batch 64/116 loss: 0.0323\n",
      "Epoch 172: Batch 65/116 loss: 0.0362\n",
      "Epoch 172: Batch 66/116 loss: 0.0671\n",
      "Epoch 172: Batch 67/116 loss: 0.0489\n",
      "Epoch 172: Batch 68/116 loss: 0.0460\n",
      "Epoch 172: Batch 69/116 loss: 0.0513\n",
      "Epoch 172: Batch 70/116 loss: 0.0416\n",
      "Epoch 172: Batch 71/116 loss: 0.0234\n",
      "Epoch 172: Batch 72/116 loss: 0.0346\n",
      "Epoch 172: Batch 73/116 loss: 0.0473\n",
      "Epoch 172: Batch 74/116 loss: 0.0451\n",
      "Epoch 172: Batch 75/116 loss: 0.0393\n",
      "Epoch 172: Batch 76/116 loss: 0.0427\n",
      "Epoch 172: Batch 77/116 loss: 0.0484\n",
      "Epoch 172: Batch 78/116 loss: 0.0352\n",
      "Epoch 172: Batch 79/116 loss: 0.0376\n",
      "Epoch 172: Batch 80/116 loss: 0.0720\n",
      "Epoch 172: Batch 81/116 loss: 0.1021\n",
      "Epoch 172: Batch 82/116 loss: 0.0381\n",
      "Epoch 172: Batch 83/116 loss: 0.0511\n",
      "Epoch 172: Batch 84/116 loss: 0.0413\n",
      "Epoch 172: Batch 85/116 loss: 0.0392\n",
      "Epoch 172: Batch 86/116 loss: 0.0511\n",
      "Epoch 172: Batch 87/116 loss: 0.0359\n",
      "Epoch 172: Batch 88/116 loss: 0.0290\n",
      "Epoch 172: Batch 89/116 loss: 0.0366\n",
      "Epoch 172: Batch 90/116 loss: 0.0674\n",
      "Epoch 172: Batch 91/116 loss: 0.0371\n",
      "Epoch 172: Batch 92/116 loss: 0.0379\n",
      "Epoch 172: Batch 93/116 loss: 0.0237\n",
      "Epoch 172: Batch 94/116 loss: 0.0351\n",
      "Epoch 172: Batch 95/116 loss: 0.0688\n",
      "Epoch 172: Batch 96/116 loss: 0.0527\n",
      "Epoch 172: Batch 97/116 loss: 0.0394\n",
      "Epoch 172: Batch 98/116 loss: 0.0248\n",
      "Epoch 172: Batch 99/116 loss: 0.0430\n",
      "Epoch 172: Batch 100/116 loss: 0.0449\n",
      "Epoch 172: Batch 101/116 loss: 0.0359\n",
      "Epoch 172: Batch 102/116 loss: 0.0276\n",
      "Epoch 172: Batch 103/116 loss: 0.0367\n",
      "Epoch 172: Batch 104/116 loss: 0.0298\n",
      "Epoch 172: Batch 105/116 loss: 0.0314\n",
      "Epoch 172: Batch 106/116 loss: 0.0478\n",
      "Epoch 172: Batch 107/116 loss: 0.0426\n",
      "Epoch 172: Batch 108/116 loss: 0.0451\n",
      "Epoch 172: Batch 109/116 loss: 0.0488\n",
      "Epoch 172: Batch 110/116 loss: 0.0430\n",
      "Epoch 172: Batch 111/116 loss: 0.0335\n",
      "Epoch 172: Batch 112/116 loss: 0.0438\n",
      "Epoch 172: Batch 113/116 loss: 0.0435\n",
      "Epoch 172: Batch 114/116 loss: 0.0367\n",
      "Epoch 172: Batch 115/116 loss: 0.0466\n",
      "Epoch 172: Batch 116/116 loss: 0.0404\n",
      "Epoch 172 train loss: 0.0431 valid loss: 0.0517\n",
      "performance reducing: 21\n",
      "Epoch 173: Batch 1/116 loss: 0.0283\n",
      "Epoch 173: Batch 2/116 loss: 0.0337\n",
      "Epoch 173: Batch 3/116 loss: 0.0492\n",
      "Epoch 173: Batch 4/116 loss: 0.0354\n",
      "Epoch 173: Batch 5/116 loss: 0.0428\n",
      "Epoch 173: Batch 6/116 loss: 0.0359\n",
      "Epoch 173: Batch 7/116 loss: 0.0302\n",
      "Epoch 173: Batch 8/116 loss: 0.0293\n",
      "Epoch 173: Batch 9/116 loss: 0.0512\n",
      "Epoch 173: Batch 10/116 loss: 0.0394\n",
      "Epoch 173: Batch 11/116 loss: 0.0533\n",
      "Epoch 173: Batch 12/116 loss: 0.0449\n",
      "Epoch 173: Batch 13/116 loss: 0.0391\n",
      "Epoch 173: Batch 14/116 loss: 0.0442\n",
      "Epoch 173: Batch 15/116 loss: 0.0344\n",
      "Epoch 173: Batch 16/116 loss: 0.0293\n",
      "Epoch 173: Batch 17/116 loss: 0.0415\n",
      "Epoch 173: Batch 18/116 loss: 0.0360\n",
      "Epoch 173: Batch 19/116 loss: 0.0519\n",
      "Epoch 173: Batch 20/116 loss: 0.0384\n",
      "Epoch 173: Batch 21/116 loss: 0.0382\n",
      "Epoch 173: Batch 22/116 loss: 0.0294\n",
      "Epoch 173: Batch 23/116 loss: 0.0291\n",
      "Epoch 173: Batch 24/116 loss: 0.0266\n",
      "Epoch 173: Batch 25/116 loss: 0.0498\n",
      "Epoch 173: Batch 26/116 loss: 0.0453\n",
      "Epoch 173: Batch 27/116 loss: 0.0500\n",
      "Epoch 173: Batch 28/116 loss: 0.0429\n",
      "Epoch 173: Batch 29/116 loss: 0.0305\n",
      "Epoch 173: Batch 30/116 loss: 0.0346\n",
      "Epoch 173: Batch 31/116 loss: 0.0413\n",
      "Epoch 173: Batch 32/116 loss: 0.0310\n",
      "Epoch 173: Batch 33/116 loss: 0.0512\n",
      "Epoch 173: Batch 34/116 loss: 0.0416\n",
      "Epoch 173: Batch 35/116 loss: 0.0365\n",
      "Epoch 173: Batch 36/116 loss: 0.0421\n",
      "Epoch 173: Batch 37/116 loss: 0.0229\n",
      "Epoch 173: Batch 38/116 loss: 0.0360\n",
      "Epoch 173: Batch 39/116 loss: 0.0334\n",
      "Epoch 173: Batch 40/116 loss: 0.0432\n",
      "Epoch 173: Batch 41/116 loss: 0.0564\n",
      "Epoch 173: Batch 42/116 loss: 0.0497\n",
      "Epoch 173: Batch 43/116 loss: 0.0669\n",
      "Epoch 173: Batch 44/116 loss: 0.0495\n",
      "Epoch 173: Batch 45/116 loss: 0.0423\n",
      "Epoch 173: Batch 46/116 loss: 0.0434\n",
      "Epoch 173: Batch 47/116 loss: 0.0531\n",
      "Epoch 173: Batch 48/116 loss: 0.0793\n",
      "Epoch 173: Batch 49/116 loss: 0.0402\n",
      "Epoch 173: Batch 50/116 loss: 0.0402\n",
      "Epoch 173: Batch 51/116 loss: 0.0571\n",
      "Epoch 173: Batch 52/116 loss: 0.0494\n",
      "Epoch 173: Batch 53/116 loss: 0.0631\n",
      "Epoch 173: Batch 54/116 loss: 0.0527\n",
      "Epoch 173: Batch 55/116 loss: 0.0385\n",
      "Epoch 173: Batch 56/116 loss: 0.0494\n",
      "Epoch 173: Batch 57/116 loss: 0.0350\n",
      "Epoch 173: Batch 58/116 loss: 0.0735\n",
      "Epoch 173: Batch 59/116 loss: 0.0406\n",
      "Epoch 173: Batch 60/116 loss: 0.0472\n",
      "Epoch 173: Batch 61/116 loss: 0.0404\n",
      "Epoch 173: Batch 62/116 loss: 0.0238\n",
      "Epoch 173: Batch 63/116 loss: 0.0538\n",
      "Epoch 173: Batch 64/116 loss: 0.0454\n",
      "Epoch 173: Batch 65/116 loss: 0.0388\n",
      "Epoch 173: Batch 66/116 loss: 0.0458\n",
      "Epoch 173: Batch 67/116 loss: 0.0300\n",
      "Epoch 173: Batch 68/116 loss: 0.0475\n",
      "Epoch 173: Batch 69/116 loss: 0.0228\n",
      "Epoch 173: Batch 70/116 loss: 0.0579\n",
      "Epoch 173: Batch 71/116 loss: 0.0326\n",
      "Epoch 173: Batch 72/116 loss: 0.0961\n",
      "Epoch 173: Batch 73/116 loss: 0.0283\n",
      "Epoch 173: Batch 74/116 loss: 0.0279\n",
      "Epoch 173: Batch 75/116 loss: 0.0527\n",
      "Epoch 173: Batch 76/116 loss: 0.0377\n",
      "Epoch 173: Batch 77/116 loss: 0.0370\n",
      "Epoch 173: Batch 78/116 loss: 0.0395\n",
      "Epoch 173: Batch 79/116 loss: 0.0512\n",
      "Epoch 173: Batch 80/116 loss: 0.0420\n",
      "Epoch 173: Batch 81/116 loss: 0.0497\n",
      "Epoch 173: Batch 82/116 loss: 0.0469\n",
      "Epoch 173: Batch 83/116 loss: 0.0426\n",
      "Epoch 173: Batch 84/116 loss: 0.0519\n",
      "Epoch 173: Batch 85/116 loss: 0.0333\n",
      "Epoch 173: Batch 86/116 loss: 0.0285\n",
      "Epoch 173: Batch 87/116 loss: 0.0323\n",
      "Epoch 173: Batch 88/116 loss: 0.0633\n",
      "Epoch 173: Batch 89/116 loss: 0.0531\n",
      "Epoch 173: Batch 90/116 loss: 0.0404\n",
      "Epoch 173: Batch 91/116 loss: 0.0529\n",
      "Epoch 173: Batch 92/116 loss: 0.0346\n",
      "Epoch 173: Batch 93/116 loss: 0.0438\n",
      "Epoch 173: Batch 94/116 loss: 0.0396\n",
      "Epoch 173: Batch 95/116 loss: 0.0553\n",
      "Epoch 173: Batch 96/116 loss: 0.0339\n",
      "Epoch 173: Batch 97/116 loss: 0.0352\n",
      "Epoch 173: Batch 98/116 loss: 0.0274\n",
      "Epoch 173: Batch 99/116 loss: 0.0402\n",
      "Epoch 173: Batch 100/116 loss: 0.0311\n",
      "Epoch 173: Batch 101/116 loss: 0.0281\n",
      "Epoch 173: Batch 102/116 loss: 0.0563\n",
      "Epoch 173: Batch 103/116 loss: 0.0168\n",
      "Epoch 173: Batch 104/116 loss: 0.0402\n",
      "Epoch 173: Batch 105/116 loss: 0.0317\n",
      "Epoch 173: Batch 106/116 loss: 0.0552\n",
      "Epoch 173: Batch 107/116 loss: 0.0519\n",
      "Epoch 173: Batch 108/116 loss: 0.0441\n",
      "Epoch 173: Batch 109/116 loss: 0.0481\n",
      "Epoch 173: Batch 110/116 loss: 0.0393\n",
      "Epoch 173: Batch 111/116 loss: 0.0344\n",
      "Epoch 173: Batch 112/116 loss: 0.0447\n",
      "Epoch 173: Batch 113/116 loss: 0.0354\n",
      "Epoch 173: Batch 114/116 loss: 0.0343\n",
      "Epoch 173: Batch 115/116 loss: 0.0422\n",
      "Epoch 173: Batch 116/116 loss: 0.0561\n",
      "Epoch 173 train loss: 0.0424 valid loss: 0.0506\n",
      "performance reducing: 22\n",
      "Epoch 174: Batch 1/116 loss: 0.0356\n",
      "Epoch 174: Batch 2/116 loss: 0.0339\n",
      "Epoch 174: Batch 3/116 loss: 0.0403\n",
      "Epoch 174: Batch 4/116 loss: 0.0321\n",
      "Epoch 174: Batch 5/116 loss: 0.0501\n",
      "Epoch 174: Batch 6/116 loss: 0.0395\n",
      "Epoch 174: Batch 7/116 loss: 0.0288\n",
      "Epoch 174: Batch 8/116 loss: 0.0437\n",
      "Epoch 174: Batch 9/116 loss: 0.0418\n",
      "Epoch 174: Batch 10/116 loss: 0.0400\n",
      "Epoch 174: Batch 11/116 loss: 0.0422\n",
      "Epoch 174: Batch 12/116 loss: 0.0254\n",
      "Epoch 174: Batch 13/116 loss: 0.0361\n",
      "Epoch 174: Batch 14/116 loss: 0.0389\n",
      "Epoch 174: Batch 15/116 loss: 0.0418\n",
      "Epoch 174: Batch 16/116 loss: 0.0280\n",
      "Epoch 174: Batch 17/116 loss: 0.0303\n",
      "Epoch 174: Batch 18/116 loss: 0.0493\n",
      "Epoch 174: Batch 19/116 loss: 0.0479\n",
      "Epoch 174: Batch 20/116 loss: 0.0227\n",
      "Epoch 174: Batch 21/116 loss: 0.0339\n",
      "Epoch 174: Batch 22/116 loss: 0.0315\n",
      "Epoch 174: Batch 23/116 loss: 0.0508\n",
      "Epoch 174: Batch 24/116 loss: 0.0357\n",
      "Epoch 174: Batch 25/116 loss: 0.0313\n",
      "Epoch 174: Batch 26/116 loss: 0.0558\n",
      "Epoch 174: Batch 27/116 loss: 0.0551\n",
      "Epoch 174: Batch 28/116 loss: 0.0445\n",
      "Epoch 174: Batch 29/116 loss: 0.0416\n",
      "Epoch 174: Batch 30/116 loss: 0.0335\n",
      "Epoch 174: Batch 31/116 loss: 0.0338\n",
      "Epoch 174: Batch 32/116 loss: 0.0417\n",
      "Epoch 174: Batch 33/116 loss: 0.0531\n",
      "Epoch 174: Batch 34/116 loss: 0.0411\n",
      "Epoch 174: Batch 35/116 loss: 0.0404\n",
      "Epoch 174: Batch 36/116 loss: 0.0306\n",
      "Epoch 174: Batch 37/116 loss: 0.0586\n",
      "Epoch 174: Batch 38/116 loss: 0.0315\n",
      "Epoch 174: Batch 39/116 loss: 0.0373\n",
      "Epoch 174: Batch 40/116 loss: 0.0375\n",
      "Epoch 174: Batch 41/116 loss: 0.0386\n",
      "Epoch 174: Batch 42/116 loss: 0.0554\n",
      "Epoch 174: Batch 43/116 loss: 0.0396\n",
      "Epoch 174: Batch 44/116 loss: 0.0260\n",
      "Epoch 174: Batch 45/116 loss: 0.0376\n",
      "Epoch 174: Batch 46/116 loss: 0.0441\n",
      "Epoch 174: Batch 47/116 loss: 0.0510\n",
      "Epoch 174: Batch 48/116 loss: 0.0325\n",
      "Epoch 174: Batch 49/116 loss: 0.0310\n",
      "Epoch 174: Batch 50/116 loss: 0.0405\n",
      "Epoch 174: Batch 51/116 loss: 0.0376\n",
      "Epoch 174: Batch 52/116 loss: 0.0297\n",
      "Epoch 174: Batch 53/116 loss: 0.0295\n",
      "Epoch 174: Batch 54/116 loss: 0.0573\n",
      "Epoch 174: Batch 55/116 loss: 0.0494\n",
      "Epoch 174: Batch 56/116 loss: 0.0688\n",
      "Epoch 174: Batch 57/116 loss: 0.0411\n",
      "Epoch 174: Batch 58/116 loss: 0.0518\n",
      "Epoch 174: Batch 59/116 loss: 0.0257\n",
      "Epoch 174: Batch 60/116 loss: 0.0448\n",
      "Epoch 174: Batch 61/116 loss: 0.0343\n",
      "Epoch 174: Batch 62/116 loss: 0.0430\n",
      "Epoch 174: Batch 63/116 loss: 0.0617\n",
      "Epoch 174: Batch 64/116 loss: 0.0281\n",
      "Epoch 174: Batch 65/116 loss: 0.0389\n",
      "Epoch 174: Batch 66/116 loss: 0.0326\n",
      "Epoch 174: Batch 67/116 loss: 0.0416\n",
      "Epoch 174: Batch 68/116 loss: 0.0394\n",
      "Epoch 174: Batch 69/116 loss: 0.0379\n",
      "Epoch 174: Batch 70/116 loss: 0.0392\n",
      "Epoch 174: Batch 71/116 loss: 0.0397\n",
      "Epoch 174: Batch 72/116 loss: 0.0620\n",
      "Epoch 174: Batch 73/116 loss: 0.0396\n",
      "Epoch 174: Batch 74/116 loss: 0.0363\n",
      "Epoch 174: Batch 75/116 loss: 0.0519\n",
      "Epoch 174: Batch 76/116 loss: 0.0417\n",
      "Epoch 174: Batch 77/116 loss: 0.0320\n",
      "Epoch 174: Batch 78/116 loss: 0.0557\n",
      "Epoch 174: Batch 79/116 loss: 0.0338\n",
      "Epoch 174: Batch 80/116 loss: 0.0480\n",
      "Epoch 174: Batch 81/116 loss: 0.0472\n",
      "Epoch 174: Batch 82/116 loss: 0.0247\n",
      "Epoch 174: Batch 83/116 loss: 0.0390\n",
      "Epoch 174: Batch 84/116 loss: 0.0491\n",
      "Epoch 174: Batch 85/116 loss: 0.0367\n",
      "Epoch 174: Batch 86/116 loss: 0.0379\n",
      "Epoch 174: Batch 87/116 loss: 0.0347\n",
      "Epoch 174: Batch 88/116 loss: 0.0431\n",
      "Epoch 174: Batch 89/116 loss: 0.0549\n",
      "Epoch 174: Batch 90/116 loss: 0.0332\n",
      "Epoch 174: Batch 91/116 loss: 0.0497\n",
      "Epoch 174: Batch 92/116 loss: 0.0392\n",
      "Epoch 174: Batch 93/116 loss: 0.0339\n",
      "Epoch 174: Batch 94/116 loss: 0.0485\n",
      "Epoch 174: Batch 95/116 loss: 0.0374\n",
      "Epoch 174: Batch 96/116 loss: 0.0464\n",
      "Epoch 174: Batch 97/116 loss: 0.0372\n",
      "Epoch 174: Batch 98/116 loss: 0.0348\n",
      "Epoch 174: Batch 99/116 loss: 0.0377\n",
      "Epoch 174: Batch 100/116 loss: 0.0331\n",
      "Epoch 174: Batch 101/116 loss: 0.0500\n",
      "Epoch 174: Batch 102/116 loss: 0.0348\n",
      "Epoch 174: Batch 103/116 loss: 0.0409\n",
      "Epoch 174: Batch 104/116 loss: 0.0602\n",
      "Epoch 174: Batch 105/116 loss: 0.0362\n",
      "Epoch 174: Batch 106/116 loss: 0.0597\n",
      "Epoch 174: Batch 107/116 loss: 0.0500\n",
      "Epoch 174: Batch 108/116 loss: 0.0322\n",
      "Epoch 174: Batch 109/116 loss: 0.0319\n",
      "Epoch 174: Batch 110/116 loss: 0.0286\n",
      "Epoch 174: Batch 111/116 loss: 0.0362\n",
      "Epoch 174: Batch 112/116 loss: 0.0446\n",
      "Epoch 174: Batch 113/116 loss: 0.0424\n",
      "Epoch 174: Batch 114/116 loss: 0.0493\n",
      "Epoch 174: Batch 115/116 loss: 0.0468\n",
      "Epoch 174: Batch 116/116 loss: 0.0435\n",
      "Epoch 174 train loss: 0.0407 valid loss: 0.0504\n",
      "performance reducing: 23\n",
      "Epoch 175: Batch 1/116 loss: 0.0609\n",
      "Epoch 175: Batch 2/116 loss: 0.0465\n",
      "Epoch 175: Batch 3/116 loss: 0.0315\n",
      "Epoch 175: Batch 4/116 loss: 0.0537\n",
      "Epoch 175: Batch 5/116 loss: 0.0409\n",
      "Epoch 175: Batch 6/116 loss: 0.0576\n",
      "Epoch 175: Batch 7/116 loss: 0.0377\n",
      "Epoch 175: Batch 8/116 loss: 0.0317\n",
      "Epoch 175: Batch 9/116 loss: 0.0232\n",
      "Epoch 175: Batch 10/116 loss: 0.0301\n",
      "Epoch 175: Batch 11/116 loss: 0.0367\n",
      "Epoch 175: Batch 12/116 loss: 0.0323\n",
      "Epoch 175: Batch 13/116 loss: 0.0459\n",
      "Epoch 175: Batch 14/116 loss: 0.0370\n",
      "Epoch 175: Batch 15/116 loss: 0.0447\n",
      "Epoch 175: Batch 16/116 loss: 0.0592\n",
      "Epoch 175: Batch 17/116 loss: 0.0247\n",
      "Epoch 175: Batch 18/116 loss: 0.0305\n",
      "Epoch 175: Batch 19/116 loss: 0.0366\n",
      "Epoch 175: Batch 20/116 loss: 0.0605\n",
      "Epoch 175: Batch 21/116 loss: 0.0383\n",
      "Epoch 175: Batch 22/116 loss: 0.0518\n",
      "Epoch 175: Batch 23/116 loss: 0.0387\n",
      "Epoch 175: Batch 24/116 loss: 0.0486\n",
      "Epoch 175: Batch 25/116 loss: 0.0246\n",
      "Epoch 175: Batch 26/116 loss: 0.0303\n",
      "Epoch 175: Batch 27/116 loss: 0.0304\n",
      "Epoch 175: Batch 28/116 loss: 0.0421\n",
      "Epoch 175: Batch 29/116 loss: 0.0472\n",
      "Epoch 175: Batch 30/116 loss: 0.0534\n",
      "Epoch 175: Batch 31/116 loss: 0.0492\n",
      "Epoch 175: Batch 32/116 loss: 0.0404\n",
      "Epoch 175: Batch 33/116 loss: 0.0400\n",
      "Epoch 175: Batch 34/116 loss: 0.0655\n",
      "Epoch 175: Batch 35/116 loss: 0.0273\n",
      "Epoch 175: Batch 36/116 loss: 0.0487\n",
      "Epoch 175: Batch 37/116 loss: 0.0487\n",
      "Epoch 175: Batch 38/116 loss: 0.0445\n",
      "Epoch 175: Batch 39/116 loss: 0.0345\n",
      "Epoch 175: Batch 40/116 loss: 0.0390\n",
      "Epoch 175: Batch 41/116 loss: 0.0402\n",
      "Epoch 175: Batch 42/116 loss: 0.0419\n",
      "Epoch 175: Batch 43/116 loss: 0.0574\n",
      "Epoch 175: Batch 44/116 loss: 0.0481\n",
      "Epoch 175: Batch 45/116 loss: 0.0681\n",
      "Epoch 175: Batch 46/116 loss: 0.0274\n",
      "Epoch 175: Batch 47/116 loss: 0.0270\n",
      "Epoch 175: Batch 48/116 loss: 0.0421\n",
      "Epoch 175: Batch 49/116 loss: 0.0290\n",
      "Epoch 175: Batch 50/116 loss: 0.0516\n",
      "Epoch 175: Batch 51/116 loss: 0.0477\n",
      "Epoch 175: Batch 52/116 loss: 0.0422\n",
      "Epoch 175: Batch 53/116 loss: 0.0482\n",
      "Epoch 175: Batch 54/116 loss: 0.0367\n",
      "Epoch 175: Batch 55/116 loss: 0.0357\n",
      "Epoch 175: Batch 56/116 loss: 0.0343\n",
      "Epoch 175: Batch 57/116 loss: 0.0435\n",
      "Epoch 175: Batch 58/116 loss: 0.0311\n",
      "Epoch 175: Batch 59/116 loss: 0.0797\n",
      "Epoch 175: Batch 60/116 loss: 0.0408\n",
      "Epoch 175: Batch 61/116 loss: 0.0471\n",
      "Epoch 175: Batch 62/116 loss: 0.0461\n",
      "Epoch 175: Batch 63/116 loss: 0.0414\n",
      "Epoch 175: Batch 64/116 loss: 0.0477\n",
      "Epoch 175: Batch 65/116 loss: 0.0229\n",
      "Epoch 175: Batch 66/116 loss: 0.0434\n",
      "Epoch 175: Batch 67/116 loss: 0.0299\n",
      "Epoch 175: Batch 68/116 loss: 0.0340\n",
      "Epoch 175: Batch 69/116 loss: 0.0474\n",
      "Epoch 175: Batch 70/116 loss: 0.0286\n",
      "Epoch 175: Batch 71/116 loss: 0.0252\n",
      "Epoch 175: Batch 72/116 loss: 0.0336\n",
      "Epoch 175: Batch 73/116 loss: 0.0470\n",
      "Epoch 175: Batch 74/116 loss: 0.0284\n",
      "Epoch 175: Batch 75/116 loss: 0.0447\n",
      "Epoch 175: Batch 76/116 loss: 0.0380\n",
      "Epoch 175: Batch 77/116 loss: 0.0352\n",
      "Epoch 175: Batch 78/116 loss: 0.0553\n",
      "Epoch 175: Batch 79/116 loss: 0.0350\n",
      "Epoch 175: Batch 80/116 loss: 0.0297\n",
      "Epoch 175: Batch 81/116 loss: 0.0505\n",
      "Epoch 175: Batch 82/116 loss: 0.0208\n",
      "Epoch 175: Batch 83/116 loss: 0.0436\n",
      "Epoch 175: Batch 84/116 loss: 0.0403\n",
      "Epoch 175: Batch 85/116 loss: 0.0442\n",
      "Epoch 175: Batch 86/116 loss: 0.0345\n",
      "Epoch 175: Batch 87/116 loss: 0.0402\n",
      "Epoch 175: Batch 88/116 loss: 0.0430\n",
      "Epoch 175: Batch 89/116 loss: 0.0282\n",
      "Epoch 175: Batch 90/116 loss: 0.0335\n",
      "Epoch 175: Batch 91/116 loss: 0.0378\n",
      "Epoch 175: Batch 92/116 loss: 0.0368\n",
      "Epoch 175: Batch 93/116 loss: 0.0578\n",
      "Epoch 175: Batch 94/116 loss: 0.0543\n",
      "Epoch 175: Batch 95/116 loss: 0.0453\n",
      "Epoch 175: Batch 96/116 loss: 0.0387\n",
      "Epoch 175: Batch 97/116 loss: 0.0451\n",
      "Epoch 175: Batch 98/116 loss: 0.0438\n",
      "Epoch 175: Batch 99/116 loss: 0.0422\n",
      "Epoch 175: Batch 100/116 loss: 0.0445\n",
      "Epoch 175: Batch 101/116 loss: 0.0487\n",
      "Epoch 175: Batch 102/116 loss: 0.0362\n",
      "Epoch 175: Batch 103/116 loss: 0.0252\n",
      "Epoch 175: Batch 104/116 loss: 0.0303\n",
      "Epoch 175: Batch 105/116 loss: 0.0457\n",
      "Epoch 175: Batch 106/116 loss: 0.0543\n",
      "Epoch 175: Batch 107/116 loss: 0.0373\n",
      "Epoch 175: Batch 108/116 loss: 0.0443\n",
      "Epoch 175: Batch 109/116 loss: 0.0583\n",
      "Epoch 175: Batch 110/116 loss: 0.0342\n",
      "Epoch 175: Batch 111/116 loss: 0.0463\n",
      "Epoch 175: Batch 112/116 loss: 0.0282\n",
      "Epoch 175: Batch 113/116 loss: 0.0400\n",
      "Epoch 175: Batch 114/116 loss: 0.0463\n",
      "Epoch 175: Batch 115/116 loss: 0.0478\n",
      "Epoch 175: Batch 116/116 loss: 0.0511\n",
      "Epoch 175 train loss: 0.0414 valid loss: 0.0507\n",
      "performance reducing: 24\n",
      "Epoch 176: Batch 1/116 loss: 0.0364\n",
      "Epoch 176: Batch 2/116 loss: 0.0350\n",
      "Epoch 176: Batch 3/116 loss: 0.0398\n",
      "Epoch 176: Batch 4/116 loss: 0.0718\n",
      "Epoch 176: Batch 5/116 loss: 0.0306\n",
      "Epoch 176: Batch 6/116 loss: 0.0435\n",
      "Epoch 176: Batch 7/116 loss: 0.0421\n",
      "Epoch 176: Batch 8/116 loss: 0.0350\n",
      "Epoch 176: Batch 9/116 loss: 0.0422\n",
      "Epoch 176: Batch 10/116 loss: 0.0338\n",
      "Epoch 176: Batch 11/116 loss: 0.0327\n",
      "Epoch 176: Batch 12/116 loss: 0.0426\n",
      "Epoch 176: Batch 13/116 loss: 0.0417\n",
      "Epoch 176: Batch 14/116 loss: 0.0395\n",
      "Epoch 176: Batch 15/116 loss: 0.0315\n",
      "Epoch 176: Batch 16/116 loss: 0.0295\n",
      "Epoch 176: Batch 17/116 loss: 0.0295\n",
      "Epoch 176: Batch 18/116 loss: 0.0333\n",
      "Epoch 176: Batch 19/116 loss: 0.0513\n",
      "Epoch 176: Batch 20/116 loss: 0.0772\n",
      "Epoch 176: Batch 21/116 loss: 0.0420\n",
      "Epoch 176: Batch 22/116 loss: 0.0470\n",
      "Epoch 176: Batch 23/116 loss: 0.0459\n",
      "Epoch 176: Batch 24/116 loss: 0.0298\n",
      "Epoch 176: Batch 25/116 loss: 0.0236\n",
      "Epoch 176: Batch 26/116 loss: 0.0494\n",
      "Epoch 176: Batch 27/116 loss: 0.0299\n",
      "Epoch 176: Batch 28/116 loss: 0.0379\n",
      "Epoch 176: Batch 29/116 loss: 0.0348\n",
      "Epoch 176: Batch 30/116 loss: 0.0447\n",
      "Epoch 176: Batch 31/116 loss: 0.0393\n",
      "Epoch 176: Batch 32/116 loss: 0.0348\n",
      "Epoch 176: Batch 33/116 loss: 0.0324\n",
      "Epoch 176: Batch 34/116 loss: 0.0443\n",
      "Epoch 176: Batch 35/116 loss: 0.0284\n",
      "Epoch 176: Batch 36/116 loss: 0.0415\n",
      "Epoch 176: Batch 37/116 loss: 0.0355\n",
      "Epoch 176: Batch 38/116 loss: 0.0408\n",
      "Epoch 176: Batch 39/116 loss: 0.0393\n",
      "Epoch 176: Batch 40/116 loss: 0.0327\n",
      "Epoch 176: Batch 41/116 loss: 0.0316\n",
      "Epoch 176: Batch 42/116 loss: 0.0368\n",
      "Epoch 176: Batch 43/116 loss: 0.0413\n",
      "Epoch 176: Batch 44/116 loss: 0.0484\n",
      "Epoch 176: Batch 45/116 loss: 0.0332\n",
      "Epoch 176: Batch 46/116 loss: 0.0432\n",
      "Epoch 176: Batch 47/116 loss: 0.0423\n",
      "Epoch 176: Batch 48/116 loss: 0.0375\n",
      "Epoch 176: Batch 49/116 loss: 0.0539\n",
      "Epoch 176: Batch 50/116 loss: 0.0339\n",
      "Epoch 176: Batch 51/116 loss: 0.0541\n",
      "Epoch 176: Batch 52/116 loss: 0.0425\n",
      "Epoch 176: Batch 53/116 loss: 0.0627\n",
      "Epoch 176: Batch 54/116 loss: 0.0417\n",
      "Epoch 176: Batch 55/116 loss: 0.0470\n",
      "Epoch 176: Batch 56/116 loss: 0.0467\n",
      "Epoch 176: Batch 57/116 loss: 0.0436\n",
      "Epoch 176: Batch 58/116 loss: 0.0406\n",
      "Epoch 176: Batch 59/116 loss: 0.0484\n",
      "Epoch 176: Batch 60/116 loss: 0.0332\n",
      "Epoch 176: Batch 61/116 loss: 0.0383\n",
      "Epoch 176: Batch 62/116 loss: 0.0417\n",
      "Epoch 176: Batch 63/116 loss: 0.0565\n",
      "Epoch 176: Batch 64/116 loss: 0.0567\n",
      "Epoch 176: Batch 65/116 loss: 0.0477\n",
      "Epoch 176: Batch 66/116 loss: 0.0469\n",
      "Epoch 176: Batch 67/116 loss: 0.0408\n",
      "Epoch 176: Batch 68/116 loss: 0.0606\n",
      "Epoch 176: Batch 69/116 loss: 0.0418\n",
      "Epoch 176: Batch 70/116 loss: 0.0558\n",
      "Epoch 176: Batch 71/116 loss: 0.0293\n",
      "Epoch 176: Batch 72/116 loss: 0.0328\n",
      "Epoch 176: Batch 73/116 loss: 0.0311\n",
      "Epoch 176: Batch 74/116 loss: 0.0379\n",
      "Epoch 176: Batch 75/116 loss: 0.0371\n",
      "Epoch 176: Batch 76/116 loss: 0.0481\n",
      "Epoch 176: Batch 77/116 loss: 0.0362\n",
      "Epoch 176: Batch 78/116 loss: 0.0379\n",
      "Epoch 176: Batch 79/116 loss: 0.0296\n",
      "Epoch 176: Batch 80/116 loss: 0.0543\n",
      "Epoch 176: Batch 81/116 loss: 0.0433\n",
      "Epoch 176: Batch 82/116 loss: 0.0450\n",
      "Epoch 176: Batch 83/116 loss: 0.0301\n",
      "Epoch 176: Batch 84/116 loss: 0.0391\n",
      "Epoch 176: Batch 85/116 loss: 0.0543\n",
      "Epoch 176: Batch 86/116 loss: 0.0257\n",
      "Epoch 176: Batch 87/116 loss: 0.0336\n",
      "Epoch 176: Batch 88/116 loss: 0.0349\n",
      "Epoch 176: Batch 89/116 loss: 0.0366\n",
      "Epoch 176: Batch 90/116 loss: 0.0663\n",
      "Epoch 176: Batch 91/116 loss: 0.0419\n",
      "Epoch 176: Batch 92/116 loss: 0.0329\n",
      "Epoch 176: Batch 93/116 loss: 0.0361\n",
      "Epoch 176: Batch 94/116 loss: 0.0383\n",
      "Epoch 176: Batch 95/116 loss: 0.0428\n",
      "Epoch 176: Batch 96/116 loss: 0.0428\n",
      "Epoch 176: Batch 97/116 loss: 0.0454\n",
      "Epoch 176: Batch 98/116 loss: 0.0518\n",
      "Epoch 176: Batch 99/116 loss: 0.0386\n",
      "Epoch 176: Batch 100/116 loss: 0.0322\n",
      "Epoch 176: Batch 101/116 loss: 0.0394\n",
      "Epoch 176: Batch 102/116 loss: 0.0441\n",
      "Epoch 176: Batch 103/116 loss: 0.0577\n",
      "Epoch 176: Batch 104/116 loss: 0.0424\n",
      "Epoch 176: Batch 105/116 loss: 0.0265\n",
      "Epoch 176: Batch 106/116 loss: 0.0366\n",
      "Epoch 176: Batch 107/116 loss: 0.0555\n",
      "Epoch 176: Batch 108/116 loss: 0.0567\n",
      "Epoch 176: Batch 109/116 loss: 0.0457\n",
      "Epoch 176: Batch 110/116 loss: 0.0351\n",
      "Epoch 176: Batch 111/116 loss: 0.0448\n",
      "Epoch 176: Batch 112/116 loss: 0.0464\n",
      "Epoch 176: Batch 113/116 loss: 0.0472\n",
      "Epoch 176: Batch 114/116 loss: 0.0515\n",
      "Epoch 176: Batch 115/116 loss: 0.0518\n",
      "Epoch 176: Batch 116/116 loss: 0.0282\n",
      "Epoch 176 train loss: 0.0415 valid loss: 0.0554\n",
      "performance reducing: 25\n",
      "Epoch 177: Batch 1/116 loss: 0.0375\n",
      "Epoch 177: Batch 2/116 loss: 0.0313\n",
      "Epoch 177: Batch 3/116 loss: 0.0275\n",
      "Epoch 177: Batch 4/116 loss: 0.0503\n",
      "Epoch 177: Batch 5/116 loss: 0.0463\n",
      "Epoch 177: Batch 6/116 loss: 0.0433\n",
      "Epoch 177: Batch 7/116 loss: 0.0408\n",
      "Epoch 177: Batch 8/116 loss: 0.0721\n",
      "Epoch 177: Batch 9/116 loss: 0.0426\n",
      "Epoch 177: Batch 10/116 loss: 0.0377\n",
      "Epoch 177: Batch 11/116 loss: 0.0411\n",
      "Epoch 177: Batch 12/116 loss: 0.0361\n",
      "Epoch 177: Batch 13/116 loss: 0.0372\n",
      "Epoch 177: Batch 14/116 loss: 0.0503\n",
      "Epoch 177: Batch 15/116 loss: 0.0327\n",
      "Epoch 177: Batch 16/116 loss: 0.0254\n",
      "Epoch 177: Batch 17/116 loss: 0.0589\n",
      "Epoch 177: Batch 18/116 loss: 0.0345\n",
      "Epoch 177: Batch 19/116 loss: 0.0327\n",
      "Epoch 177: Batch 20/116 loss: 0.0463\n",
      "Epoch 177: Batch 21/116 loss: 0.0379\n",
      "Epoch 177: Batch 22/116 loss: 0.0335\n",
      "Epoch 177: Batch 23/116 loss: 0.0308\n",
      "Epoch 177: Batch 24/116 loss: 0.0544\n",
      "Epoch 177: Batch 25/116 loss: 0.0545\n",
      "Epoch 177: Batch 26/116 loss: 0.0653\n",
      "Epoch 177: Batch 27/116 loss: 0.0294\n",
      "Epoch 177: Batch 28/116 loss: 0.0392\n",
      "Epoch 177: Batch 29/116 loss: 0.0437\n",
      "Epoch 177: Batch 30/116 loss: 0.0348\n",
      "Epoch 177: Batch 31/116 loss: 0.0388\n",
      "Epoch 177: Batch 32/116 loss: 0.0433\n",
      "Epoch 177: Batch 33/116 loss: 0.0424\n",
      "Epoch 177: Batch 34/116 loss: 0.0480\n",
      "Epoch 177: Batch 35/116 loss: 0.0448\n",
      "Epoch 177: Batch 36/116 loss: 0.0381\n",
      "Epoch 177: Batch 37/116 loss: 0.0304\n",
      "Epoch 177: Batch 38/116 loss: 0.0401\n",
      "Epoch 177: Batch 39/116 loss: 0.0278\n",
      "Epoch 177: Batch 40/116 loss: 0.0318\n",
      "Epoch 177: Batch 41/116 loss: 0.0388\n",
      "Epoch 177: Batch 42/116 loss: 0.0559\n",
      "Epoch 177: Batch 43/116 loss: 0.0285\n",
      "Epoch 177: Batch 44/116 loss: 0.0450\n",
      "Epoch 177: Batch 45/116 loss: 0.0319\n",
      "Epoch 177: Batch 46/116 loss: 0.0451\n",
      "Epoch 177: Batch 47/116 loss: 0.0449\n",
      "Epoch 177: Batch 48/116 loss: 0.0394\n",
      "Epoch 177: Batch 49/116 loss: 0.0445\n",
      "Epoch 177: Batch 50/116 loss: 0.0301\n",
      "Epoch 177: Batch 51/116 loss: 0.0409\n",
      "Epoch 177: Batch 52/116 loss: 0.0489\n",
      "Epoch 177: Batch 53/116 loss: 0.0420\n",
      "Epoch 177: Batch 54/116 loss: 0.0361\n",
      "Epoch 177: Batch 55/116 loss: 0.0583\n",
      "Epoch 177: Batch 56/116 loss: 0.0367\n",
      "Epoch 177: Batch 57/116 loss: 0.0373\n",
      "Epoch 177: Batch 58/116 loss: 0.0471\n",
      "Epoch 177: Batch 59/116 loss: 0.0362\n",
      "Epoch 177: Batch 60/116 loss: 0.0296\n",
      "Epoch 177: Batch 61/116 loss: 0.0371\n",
      "Epoch 177: Batch 62/116 loss: 0.0282\n",
      "Epoch 177: Batch 63/116 loss: 0.0442\n",
      "Epoch 177: Batch 64/116 loss: 0.0305\n",
      "Epoch 177: Batch 65/116 loss: 0.0351\n",
      "Epoch 177: Batch 66/116 loss: 0.0242\n",
      "Epoch 177: Batch 67/116 loss: 0.0374\n",
      "Epoch 177: Batch 68/116 loss: 0.0487\n",
      "Epoch 177: Batch 69/116 loss: 0.0324\n",
      "Epoch 177: Batch 70/116 loss: 0.0366\n",
      "Epoch 177: Batch 71/116 loss: 0.0346\n",
      "Epoch 177: Batch 72/116 loss: 0.0342\n",
      "Epoch 177: Batch 73/116 loss: 0.0452\n",
      "Epoch 177: Batch 74/116 loss: 0.0407\n",
      "Epoch 177: Batch 75/116 loss: 0.0513\n",
      "Epoch 177: Batch 76/116 loss: 0.0421\n",
      "Epoch 177: Batch 77/116 loss: 0.0298\n",
      "Epoch 177: Batch 78/116 loss: 0.0303\n",
      "Epoch 177: Batch 79/116 loss: 0.0289\n",
      "Epoch 177: Batch 80/116 loss: 0.0351\n",
      "Epoch 177: Batch 81/116 loss: 0.0409\n",
      "Epoch 177: Batch 82/116 loss: 0.0563\n",
      "Epoch 177: Batch 83/116 loss: 0.0384\n",
      "Epoch 177: Batch 84/116 loss: 0.0476\n",
      "Epoch 177: Batch 85/116 loss: 0.0380\n",
      "Epoch 177: Batch 86/116 loss: 0.0703\n",
      "Epoch 177: Batch 87/116 loss: 0.0388\n",
      "Epoch 177: Batch 88/116 loss: 0.0319\n",
      "Epoch 177: Batch 89/116 loss: 0.0280\n",
      "Epoch 177: Batch 90/116 loss: 0.0431\n",
      "Epoch 177: Batch 91/116 loss: 0.0505\n",
      "Epoch 177: Batch 92/116 loss: 0.0394\n",
      "Epoch 177: Batch 93/116 loss: 0.0426\n",
      "Epoch 177: Batch 94/116 loss: 0.0320\n",
      "Epoch 177: Batch 95/116 loss: 0.0304\n",
      "Epoch 177: Batch 96/116 loss: 0.0356\n",
      "Epoch 177: Batch 97/116 loss: 0.0322\n",
      "Epoch 177: Batch 98/116 loss: 0.0342\n",
      "Epoch 177: Batch 99/116 loss: 0.0461\n",
      "Epoch 177: Batch 100/116 loss: 0.0480\n",
      "Epoch 177: Batch 101/116 loss: 0.0490\n",
      "Epoch 177: Batch 102/116 loss: 0.0522\n",
      "Epoch 177: Batch 103/116 loss: 0.0397\n",
      "Epoch 177: Batch 104/116 loss: 0.0385\n",
      "Epoch 177: Batch 105/116 loss: 0.0379\n",
      "Epoch 177: Batch 106/116 loss: 0.0298\n",
      "Epoch 177: Batch 107/116 loss: 0.0566\n",
      "Epoch 177: Batch 108/116 loss: 0.0271\n",
      "Epoch 177: Batch 109/116 loss: 0.0512\n",
      "Epoch 177: Batch 110/116 loss: 0.0596\n",
      "Epoch 177: Batch 111/116 loss: 0.0218\n",
      "Epoch 177: Batch 112/116 loss: 0.0626\n",
      "Epoch 177: Batch 113/116 loss: 0.0487\n",
      "Epoch 177: Batch 114/116 loss: 0.0376\n",
      "Epoch 177: Batch 115/116 loss: 0.0432\n",
      "Epoch 177: Batch 116/116 loss: 0.0425\n",
      "Epoch 177 train loss: 0.0405 valid loss: 0.0515\n",
      "performance reducing: 26\n",
      "Epoch 178: Batch 1/116 loss: 0.0337\n",
      "Epoch 178: Batch 2/116 loss: 0.0357\n",
      "Epoch 178: Batch 3/116 loss: 0.0285\n",
      "Epoch 178: Batch 4/116 loss: 0.0371\n",
      "Epoch 178: Batch 5/116 loss: 0.0433\n",
      "Epoch 178: Batch 6/116 loss: 0.0592\n",
      "Epoch 178: Batch 7/116 loss: 0.0460\n",
      "Epoch 178: Batch 8/116 loss: 0.0371\n",
      "Epoch 178: Batch 9/116 loss: 0.0434\n",
      "Epoch 178: Batch 10/116 loss: 0.0701\n",
      "Epoch 178: Batch 11/116 loss: 0.0420\n",
      "Epoch 178: Batch 12/116 loss: 0.0562\n",
      "Epoch 178: Batch 13/116 loss: 0.0536\n",
      "Epoch 178: Batch 14/116 loss: 0.0571\n",
      "Epoch 178: Batch 15/116 loss: 0.0399\n",
      "Epoch 178: Batch 16/116 loss: 0.0267\n",
      "Epoch 178: Batch 17/116 loss: 0.0299\n",
      "Epoch 178: Batch 18/116 loss: 0.0398\n",
      "Epoch 178: Batch 19/116 loss: 0.0355\n",
      "Epoch 178: Batch 20/116 loss: 0.0426\n",
      "Epoch 178: Batch 21/116 loss: 0.0345\n",
      "Epoch 178: Batch 22/116 loss: 0.0251\n",
      "Epoch 178: Batch 23/116 loss: 0.0385\n",
      "Epoch 178: Batch 24/116 loss: 0.0436\n",
      "Epoch 178: Batch 25/116 loss: 0.0276\n",
      "Epoch 178: Batch 26/116 loss: 0.0359\n",
      "Epoch 178: Batch 27/116 loss: 0.0434\n",
      "Epoch 178: Batch 28/116 loss: 0.0384\n",
      "Epoch 178: Batch 29/116 loss: 0.0372\n",
      "Epoch 178: Batch 30/116 loss: 0.0323\n",
      "Epoch 178: Batch 31/116 loss: 0.0581\n",
      "Epoch 178: Batch 32/116 loss: 0.0446\n",
      "Epoch 178: Batch 33/116 loss: 0.0347\n",
      "Epoch 178: Batch 34/116 loss: 0.0436\n",
      "Epoch 178: Batch 35/116 loss: 0.0612\n",
      "Epoch 178: Batch 36/116 loss: 0.0445\n",
      "Epoch 178: Batch 37/116 loss: 0.0448\n",
      "Epoch 178: Batch 38/116 loss: 0.0460\n",
      "Epoch 178: Batch 39/116 loss: 0.0302\n",
      "Epoch 178: Batch 40/116 loss: 0.0477\n",
      "Epoch 178: Batch 41/116 loss: 0.0319\n",
      "Epoch 178: Batch 42/116 loss: 0.0345\n",
      "Epoch 178: Batch 43/116 loss: 0.0336\n",
      "Epoch 178: Batch 44/116 loss: 0.0514\n",
      "Epoch 178: Batch 45/116 loss: 0.0354\n",
      "Epoch 178: Batch 46/116 loss: 0.0373\n",
      "Epoch 178: Batch 47/116 loss: 0.0427\n",
      "Epoch 178: Batch 48/116 loss: 0.0327\n",
      "Epoch 178: Batch 49/116 loss: 0.0432\n",
      "Epoch 178: Batch 50/116 loss: 0.0346\n",
      "Epoch 178: Batch 51/116 loss: 0.0390\n",
      "Epoch 178: Batch 52/116 loss: 0.0348\n",
      "Epoch 178: Batch 53/116 loss: 0.0300\n",
      "Epoch 178: Batch 54/116 loss: 0.0474\n",
      "Epoch 178: Batch 55/116 loss: 0.0243\n",
      "Epoch 178: Batch 56/116 loss: 0.0571\n",
      "Epoch 178: Batch 57/116 loss: 0.0443\n",
      "Epoch 178: Batch 58/116 loss: 0.0361\n",
      "Epoch 178: Batch 59/116 loss: 0.0421\n",
      "Epoch 178: Batch 60/116 loss: 0.0358\n",
      "Epoch 178: Batch 61/116 loss: 0.0524\n",
      "Epoch 178: Batch 62/116 loss: 0.0469\n",
      "Epoch 178: Batch 63/116 loss: 0.0385\n",
      "Epoch 178: Batch 64/116 loss: 0.0262\n",
      "Epoch 178: Batch 65/116 loss: 0.0398\n",
      "Epoch 178: Batch 66/116 loss: 0.0267\n",
      "Epoch 178: Batch 67/116 loss: 0.0405\n",
      "Epoch 178: Batch 68/116 loss: 0.0367\n",
      "Epoch 178: Batch 69/116 loss: 0.0369\n",
      "Epoch 178: Batch 70/116 loss: 0.0539\n",
      "Epoch 178: Batch 71/116 loss: 0.0379\n",
      "Epoch 178: Batch 72/116 loss: 0.0347\n",
      "Epoch 178: Batch 73/116 loss: 0.0398\n",
      "Epoch 178: Batch 74/116 loss: 0.0360\n",
      "Epoch 178: Batch 75/116 loss: 0.0374\n",
      "Epoch 178: Batch 76/116 loss: 0.0277\n",
      "Epoch 178: Batch 77/116 loss: 0.0450\n",
      "Epoch 178: Batch 78/116 loss: 0.0376\n",
      "Epoch 178: Batch 79/116 loss: 0.0333\n",
      "Epoch 178: Batch 80/116 loss: 0.0295\n",
      "Epoch 178: Batch 81/116 loss: 0.0441\n",
      "Epoch 178: Batch 82/116 loss: 0.0304\n",
      "Epoch 178: Batch 83/116 loss: 0.0345\n",
      "Epoch 178: Batch 84/116 loss: 0.0514\n",
      "Epoch 178: Batch 85/116 loss: 0.0310\n",
      "Epoch 178: Batch 86/116 loss: 0.0451\n",
      "Epoch 178: Batch 87/116 loss: 0.0291\n",
      "Epoch 178: Batch 88/116 loss: 0.0256\n",
      "Epoch 178: Batch 89/116 loss: 0.0264\n",
      "Epoch 178: Batch 90/116 loss: 0.0371\n",
      "Epoch 178: Batch 91/116 loss: 0.0390\n",
      "Epoch 178: Batch 92/116 loss: 0.0386\n",
      "Epoch 178: Batch 93/116 loss: 0.0391\n",
      "Epoch 178: Batch 94/116 loss: 0.0597\n",
      "Epoch 178: Batch 95/116 loss: 0.0446\n",
      "Epoch 178: Batch 96/116 loss: 0.0345\n",
      "Epoch 178: Batch 97/116 loss: 0.0498\n",
      "Epoch 178: Batch 98/116 loss: 0.0467\n",
      "Epoch 178: Batch 99/116 loss: 0.0476\n",
      "Epoch 178: Batch 100/116 loss: 0.0355\n",
      "Epoch 178: Batch 101/116 loss: 0.0564\n",
      "Epoch 178: Batch 102/116 loss: 0.0570\n",
      "Epoch 178: Batch 103/116 loss: 0.0519\n",
      "Epoch 178: Batch 104/116 loss: 0.0378\n",
      "Epoch 178: Batch 105/116 loss: 0.0289\n",
      "Epoch 178: Batch 106/116 loss: 0.0371\n",
      "Epoch 178: Batch 107/116 loss: 0.0196\n",
      "Epoch 178: Batch 108/116 loss: 0.0472\n",
      "Epoch 178: Batch 109/116 loss: 0.0614\n",
      "Epoch 178: Batch 110/116 loss: 0.0307\n",
      "Epoch 178: Batch 111/116 loss: 0.0314\n",
      "Epoch 178: Batch 112/116 loss: 0.0450\n",
      "Epoch 178: Batch 113/116 loss: 0.0524\n",
      "Epoch 178: Batch 114/116 loss: 0.0552\n",
      "Epoch 178: Batch 115/116 loss: 0.0396\n",
      "Epoch 178: Batch 116/116 loss: 0.0355\n",
      "Epoch 178 train loss: 0.0403 valid loss: 0.0572\n",
      "performance reducing: 27\n",
      "Epoch 179: Batch 1/116 loss: 0.0302\n",
      "Epoch 179: Batch 2/116 loss: 0.0402\n",
      "Epoch 179: Batch 3/116 loss: 0.0289\n",
      "Epoch 179: Batch 4/116 loss: 0.0441\n",
      "Epoch 179: Batch 5/116 loss: 0.0593\n",
      "Epoch 179: Batch 6/116 loss: 0.0401\n",
      "Epoch 179: Batch 7/116 loss: 0.0312\n",
      "Epoch 179: Batch 8/116 loss: 0.0560\n",
      "Epoch 179: Batch 9/116 loss: 0.0265\n",
      "Epoch 179: Batch 10/116 loss: 0.0547\n",
      "Epoch 179: Batch 11/116 loss: 0.0303\n",
      "Epoch 179: Batch 12/116 loss: 0.0454\n",
      "Epoch 179: Batch 13/116 loss: 0.0297\n",
      "Epoch 179: Batch 14/116 loss: 0.0348\n",
      "Epoch 179: Batch 15/116 loss: 0.0476\n",
      "Epoch 179: Batch 16/116 loss: 0.0272\n",
      "Epoch 179: Batch 17/116 loss: 0.0497\n",
      "Epoch 179: Batch 18/116 loss: 0.0503\n",
      "Epoch 179: Batch 19/116 loss: 0.0409\n",
      "Epoch 179: Batch 20/116 loss: 0.0418\n",
      "Epoch 179: Batch 21/116 loss: 0.0332\n",
      "Epoch 179: Batch 22/116 loss: 0.0464\n",
      "Epoch 179: Batch 23/116 loss: 0.0336\n",
      "Epoch 179: Batch 24/116 loss: 0.0339\n",
      "Epoch 179: Batch 25/116 loss: 0.0347\n",
      "Epoch 179: Batch 26/116 loss: 0.0419\n",
      "Epoch 179: Batch 27/116 loss: 0.0252\n",
      "Epoch 179: Batch 28/116 loss: 0.0473\n",
      "Epoch 179: Batch 29/116 loss: 0.0369\n",
      "Epoch 179: Batch 30/116 loss: 0.0490\n",
      "Epoch 179: Batch 31/116 loss: 0.0441\n",
      "Epoch 179: Batch 32/116 loss: 0.0386\n",
      "Epoch 179: Batch 33/116 loss: 0.0353\n",
      "Epoch 179: Batch 34/116 loss: 0.0330\n",
      "Epoch 179: Batch 35/116 loss: 0.0261\n",
      "Epoch 179: Batch 36/116 loss: 0.0359\n",
      "Epoch 179: Batch 37/116 loss: 0.0278\n",
      "Epoch 179: Batch 38/116 loss: 0.0341\n",
      "Epoch 179: Batch 39/116 loss: 0.0479\n",
      "Epoch 179: Batch 40/116 loss: 0.0408\n",
      "Epoch 179: Batch 41/116 loss: 0.0296\n",
      "Epoch 179: Batch 42/116 loss: 0.0490\n",
      "Epoch 179: Batch 43/116 loss: 0.0542\n",
      "Epoch 179: Batch 44/116 loss: 0.0526\n",
      "Epoch 179: Batch 45/116 loss: 0.0410\n",
      "Epoch 179: Batch 46/116 loss: 0.0401\n",
      "Epoch 179: Batch 47/116 loss: 0.0558\n",
      "Epoch 179: Batch 48/116 loss: 0.0450\n",
      "Epoch 179: Batch 49/116 loss: 0.0330\n",
      "Epoch 179: Batch 50/116 loss: 0.0364\n",
      "Epoch 179: Batch 51/116 loss: 0.0424\n",
      "Epoch 179: Batch 52/116 loss: 0.0382\n",
      "Epoch 179: Batch 53/116 loss: 0.0294\n",
      "Epoch 179: Batch 54/116 loss: 0.0432\n",
      "Epoch 179: Batch 55/116 loss: 0.0353\n",
      "Epoch 179: Batch 56/116 loss: 0.0411\n",
      "Epoch 179: Batch 57/116 loss: 0.0205\n",
      "Epoch 179: Batch 58/116 loss: 0.0418\n",
      "Epoch 179: Batch 59/116 loss: 0.0502\n",
      "Epoch 179: Batch 60/116 loss: 0.0518\n",
      "Epoch 179: Batch 61/116 loss: 0.0509\n",
      "Epoch 179: Batch 62/116 loss: 0.0449\n",
      "Epoch 179: Batch 63/116 loss: 0.0286\n",
      "Epoch 179: Batch 64/116 loss: 0.0406\n",
      "Epoch 179: Batch 65/116 loss: 0.0376\n",
      "Epoch 179: Batch 66/116 loss: 0.0486\n",
      "Epoch 179: Batch 67/116 loss: 0.0689\n",
      "Epoch 179: Batch 68/116 loss: 0.0490\n",
      "Epoch 179: Batch 69/116 loss: 0.0478\n",
      "Epoch 179: Batch 70/116 loss: 0.0447\n",
      "Epoch 179: Batch 71/116 loss: 0.0368\n",
      "Epoch 179: Batch 72/116 loss: 0.0308\n",
      "Epoch 179: Batch 73/116 loss: 0.0315\n",
      "Epoch 179: Batch 74/116 loss: 0.0421\n",
      "Epoch 179: Batch 75/116 loss: 0.0331\n",
      "Epoch 179: Batch 76/116 loss: 0.0389\n",
      "Epoch 179: Batch 77/116 loss: 0.0326\n",
      "Epoch 179: Batch 78/116 loss: 0.0349\n",
      "Epoch 179: Batch 79/116 loss: 0.0415\n",
      "Epoch 179: Batch 80/116 loss: 0.0370\n",
      "Epoch 179: Batch 81/116 loss: 0.0331\n",
      "Epoch 179: Batch 82/116 loss: 0.0477\n",
      "Epoch 179: Batch 83/116 loss: 0.0513\n",
      "Epoch 179: Batch 84/116 loss: 0.0390\n",
      "Epoch 179: Batch 85/116 loss: 0.0463\n",
      "Epoch 179: Batch 86/116 loss: 0.0281\n",
      "Epoch 179: Batch 87/116 loss: 0.0468\n",
      "Epoch 179: Batch 88/116 loss: 0.0496\n",
      "Epoch 179: Batch 89/116 loss: 0.0340\n",
      "Epoch 179: Batch 90/116 loss: 0.0188\n",
      "Epoch 179: Batch 91/116 loss: 0.0403\n",
      "Epoch 179: Batch 92/116 loss: 0.0670\n",
      "Epoch 179: Batch 93/116 loss: 0.0573\n",
      "Epoch 179: Batch 94/116 loss: 0.0412\n",
      "Epoch 179: Batch 95/116 loss: 0.0456\n",
      "Epoch 179: Batch 96/116 loss: 0.0491\n",
      "Epoch 179: Batch 97/116 loss: 0.0404\n",
      "Epoch 179: Batch 98/116 loss: 0.0364\n",
      "Epoch 179: Batch 99/116 loss: 0.0244\n",
      "Epoch 179: Batch 100/116 loss: 0.0412\n",
      "Epoch 179: Batch 101/116 loss: 0.0410\n",
      "Epoch 179: Batch 102/116 loss: 0.0362\n",
      "Epoch 179: Batch 103/116 loss: 0.0413\n",
      "Epoch 179: Batch 104/116 loss: 0.0446\n",
      "Epoch 179: Batch 105/116 loss: 0.0313\n",
      "Epoch 179: Batch 106/116 loss: 0.0502\n",
      "Epoch 179: Batch 107/116 loss: 0.0596\n",
      "Epoch 179: Batch 108/116 loss: 0.0542\n",
      "Epoch 179: Batch 109/116 loss: 0.0421\n",
      "Epoch 179: Batch 110/116 loss: 0.0600\n",
      "Epoch 179: Batch 111/116 loss: 0.0355\n",
      "Epoch 179: Batch 112/116 loss: 0.0390\n",
      "Epoch 179: Batch 113/116 loss: 0.0419\n",
      "Epoch 179: Batch 114/116 loss: 0.0385\n",
      "Epoch 179: Batch 115/116 loss: 0.0501\n",
      "Epoch 179: Batch 116/116 loss: 0.0401\n",
      "Epoch 179 train loss: 0.0409 valid loss: 0.0588\n",
      "performance reducing: 28\n",
      "Epoch 180: Batch 1/116 loss: 0.0404\n",
      "Epoch 180: Batch 2/116 loss: 0.0421\n",
      "Epoch 180: Batch 3/116 loss: 0.0475\n",
      "Epoch 180: Batch 4/116 loss: 0.0390\n",
      "Epoch 180: Batch 5/116 loss: 0.0592\n",
      "Epoch 180: Batch 6/116 loss: 0.0470\n",
      "Epoch 180: Batch 7/116 loss: 0.0430\n",
      "Epoch 180: Batch 8/116 loss: 0.0314\n",
      "Epoch 180: Batch 9/116 loss: 0.0440\n",
      "Epoch 180: Batch 10/116 loss: 0.0534\n",
      "Epoch 180: Batch 11/116 loss: 0.0277\n",
      "Epoch 180: Batch 12/116 loss: 0.0387\n",
      "Epoch 180: Batch 13/116 loss: 0.0678\n",
      "Epoch 180: Batch 14/116 loss: 0.0404\n",
      "Epoch 180: Batch 15/116 loss: 0.0259\n",
      "Epoch 180: Batch 16/116 loss: 0.0631\n",
      "Epoch 180: Batch 17/116 loss: 0.0434\n",
      "Epoch 180: Batch 18/116 loss: 0.0356\n",
      "Epoch 180: Batch 19/116 loss: 0.0337\n",
      "Epoch 180: Batch 20/116 loss: 0.0631\n",
      "Epoch 180: Batch 21/116 loss: 0.0596\n",
      "Epoch 180: Batch 22/116 loss: 0.0406\n",
      "Epoch 180: Batch 23/116 loss: 0.0565\n",
      "Epoch 180: Batch 24/116 loss: 0.0421\n",
      "Epoch 180: Batch 25/116 loss: 0.0395\n",
      "Epoch 180: Batch 26/116 loss: 0.0201\n",
      "Epoch 180: Batch 27/116 loss: 0.0558\n",
      "Epoch 180: Batch 28/116 loss: 0.0353\n",
      "Epoch 180: Batch 29/116 loss: 0.0331\n",
      "Epoch 180: Batch 30/116 loss: 0.0362\n",
      "Epoch 180: Batch 31/116 loss: 0.0445\n",
      "Epoch 180: Batch 32/116 loss: 0.0337\n",
      "Epoch 180: Batch 33/116 loss: 0.0281\n",
      "Epoch 180: Batch 34/116 loss: 0.0493\n",
      "Epoch 180: Batch 35/116 loss: 0.0234\n",
      "Epoch 180: Batch 36/116 loss: 0.0394\n",
      "Epoch 180: Batch 37/116 loss: 0.0424\n",
      "Epoch 180: Batch 38/116 loss: 0.0290\n",
      "Epoch 180: Batch 39/116 loss: 0.0545\n",
      "Epoch 180: Batch 40/116 loss: 0.0428\n",
      "Epoch 180: Batch 41/116 loss: 0.0284\n",
      "Epoch 180: Batch 42/116 loss: 0.0333\n",
      "Epoch 180: Batch 43/116 loss: 0.0371\n",
      "Epoch 180: Batch 44/116 loss: 0.0419\n",
      "Epoch 180: Batch 45/116 loss: 0.0476\n",
      "Epoch 180: Batch 46/116 loss: 0.0365\n",
      "Epoch 180: Batch 47/116 loss: 0.0462\n",
      "Epoch 180: Batch 48/116 loss: 0.0285\n",
      "Epoch 180: Batch 49/116 loss: 0.0359\n",
      "Epoch 180: Batch 50/116 loss: 0.0604\n",
      "Epoch 180: Batch 51/116 loss: 0.0348\n",
      "Epoch 180: Batch 52/116 loss: 0.0356\n",
      "Epoch 180: Batch 53/116 loss: 0.0322\n",
      "Epoch 180: Batch 54/116 loss: 0.0545\n",
      "Epoch 180: Batch 55/116 loss: 0.0354\n",
      "Epoch 180: Batch 56/116 loss: 0.0298\n",
      "Epoch 180: Batch 57/116 loss: 0.0346\n",
      "Epoch 180: Batch 58/116 loss: 0.0283\n",
      "Epoch 180: Batch 59/116 loss: 0.0385\n",
      "Epoch 180: Batch 60/116 loss: 0.0634\n",
      "Epoch 180: Batch 61/116 loss: 0.0326\n",
      "Epoch 180: Batch 62/116 loss: 0.0469\n",
      "Epoch 180: Batch 63/116 loss: 0.0457\n",
      "Epoch 180: Batch 64/116 loss: 0.0376\n",
      "Epoch 180: Batch 65/116 loss: 0.0505\n",
      "Epoch 180: Batch 66/116 loss: 0.0450\n",
      "Epoch 180: Batch 67/116 loss: 0.0569\n",
      "Epoch 180: Batch 68/116 loss: 0.0209\n",
      "Epoch 180: Batch 69/116 loss: 0.0730\n",
      "Epoch 180: Batch 70/116 loss: 0.0417\n",
      "Epoch 180: Batch 71/116 loss: 0.0326\n",
      "Epoch 180: Batch 72/116 loss: 0.0368\n",
      "Epoch 180: Batch 73/116 loss: 0.0264\n",
      "Epoch 180: Batch 74/116 loss: 0.0414\n",
      "Epoch 180: Batch 75/116 loss: 0.0436\n",
      "Epoch 180: Batch 76/116 loss: 0.0400\n",
      "Epoch 180: Batch 77/116 loss: 0.0430\n",
      "Epoch 180: Batch 78/116 loss: 0.0196\n",
      "Epoch 180: Batch 79/116 loss: 0.0171\n",
      "Epoch 180: Batch 80/116 loss: 0.0440\n",
      "Epoch 180: Batch 81/116 loss: 0.0456\n",
      "Epoch 180: Batch 82/116 loss: 0.0321\n",
      "Epoch 180: Batch 83/116 loss: 0.0309\n",
      "Epoch 180: Batch 84/116 loss: 0.0235\n",
      "Epoch 180: Batch 85/116 loss: 0.0494\n",
      "Epoch 180: Batch 86/116 loss: 0.0422\n",
      "Epoch 180: Batch 87/116 loss: 0.0324\n",
      "Epoch 180: Batch 88/116 loss: 0.0285\n",
      "Epoch 180: Batch 89/116 loss: 0.0216\n",
      "Epoch 180: Batch 90/116 loss: 0.0438\n",
      "Epoch 180: Batch 91/116 loss: 0.0537\n",
      "Epoch 180: Batch 92/116 loss: 0.0378\n",
      "Epoch 180: Batch 93/116 loss: 0.0367\n",
      "Epoch 180: Batch 94/116 loss: 0.0487\n",
      "Epoch 180: Batch 95/116 loss: 0.0734\n",
      "Epoch 180: Batch 96/116 loss: 0.0503\n",
      "Epoch 180: Batch 97/116 loss: 0.0437\n",
      "Epoch 180: Batch 98/116 loss: 0.0411\n",
      "Epoch 180: Batch 99/116 loss: 0.0340\n",
      "Epoch 180: Batch 100/116 loss: 0.0398\n",
      "Epoch 180: Batch 101/116 loss: 0.0432\n",
      "Epoch 180: Batch 102/116 loss: 0.0393\n",
      "Epoch 180: Batch 103/116 loss: 0.0410\n",
      "Epoch 180: Batch 104/116 loss: 0.0545\n",
      "Epoch 180: Batch 105/116 loss: 0.0736\n",
      "Epoch 180: Batch 106/116 loss: 0.0484\n",
      "Epoch 180: Batch 107/116 loss: 0.0449\n",
      "Epoch 180: Batch 108/116 loss: 0.0470\n",
      "Epoch 180: Batch 109/116 loss: 0.0396\n",
      "Epoch 180: Batch 110/116 loss: 0.0441\n",
      "Epoch 180: Batch 111/116 loss: 0.0342\n",
      "Epoch 180: Batch 112/116 loss: 0.0388\n",
      "Epoch 180: Batch 113/116 loss: 0.0261\n",
      "Epoch 180: Batch 114/116 loss: 0.0299\n",
      "Epoch 180: Batch 115/116 loss: 0.0426\n",
      "Epoch 180: Batch 116/116 loss: 0.0431\n",
      "Epoch 180 train loss: 0.0411 valid loss: 0.0485\n",
      "Epoch 181: Batch 1/116 loss: 0.0345\n",
      "Epoch 181: Batch 2/116 loss: 0.0369\n",
      "Epoch 181: Batch 3/116 loss: 0.0482\n",
      "Epoch 181: Batch 4/116 loss: 0.0483\n",
      "Epoch 181: Batch 5/116 loss: 0.0378\n",
      "Epoch 181: Batch 6/116 loss: 0.0468\n",
      "Epoch 181: Batch 7/116 loss: 0.0249\n",
      "Epoch 181: Batch 8/116 loss: 0.0368\n",
      "Epoch 181: Batch 9/116 loss: 0.0239\n",
      "Epoch 181: Batch 10/116 loss: 0.0381\n",
      "Epoch 181: Batch 11/116 loss: 0.0433\n",
      "Epoch 181: Batch 12/116 loss: 0.0495\n",
      "Epoch 181: Batch 13/116 loss: 0.0523\n",
      "Epoch 181: Batch 14/116 loss: 0.0228\n",
      "Epoch 181: Batch 15/116 loss: 0.0445\n",
      "Epoch 181: Batch 16/116 loss: 0.0287\n",
      "Epoch 181: Batch 17/116 loss: 0.0493\n",
      "Epoch 181: Batch 18/116 loss: 0.0567\n",
      "Epoch 181: Batch 19/116 loss: 0.0405\n",
      "Epoch 181: Batch 20/116 loss: 0.0519\n",
      "Epoch 181: Batch 21/116 loss: 0.0430\n",
      "Epoch 181: Batch 22/116 loss: 0.0422\n",
      "Epoch 181: Batch 23/116 loss: 0.0290\n",
      "Epoch 181: Batch 24/116 loss: 0.0253\n",
      "Epoch 181: Batch 25/116 loss: 0.0471\n",
      "Epoch 181: Batch 26/116 loss: 0.0324\n",
      "Epoch 181: Batch 27/116 loss: 0.0654\n",
      "Epoch 181: Batch 28/116 loss: 0.0346\n",
      "Epoch 181: Batch 29/116 loss: 0.0491\n",
      "Epoch 181: Batch 30/116 loss: 0.0280\n",
      "Epoch 181: Batch 31/116 loss: 0.0261\n",
      "Epoch 181: Batch 32/116 loss: 0.0339\n",
      "Epoch 181: Batch 33/116 loss: 0.0476\n",
      "Epoch 181: Batch 34/116 loss: 0.0619\n",
      "Epoch 181: Batch 35/116 loss: 0.0354\n",
      "Epoch 181: Batch 36/116 loss: 0.0462\n",
      "Epoch 181: Batch 37/116 loss: 0.0438\n",
      "Epoch 181: Batch 38/116 loss: 0.0366\n",
      "Epoch 181: Batch 39/116 loss: 0.0290\n",
      "Epoch 181: Batch 40/116 loss: 0.0618\n",
      "Epoch 181: Batch 41/116 loss: 0.0238\n",
      "Epoch 181: Batch 42/116 loss: 0.0276\n",
      "Epoch 181: Batch 43/116 loss: 0.0373\n",
      "Epoch 181: Batch 44/116 loss: 0.0448\n",
      "Epoch 181: Batch 45/116 loss: 0.0426\n",
      "Epoch 181: Batch 46/116 loss: 0.0330\n",
      "Epoch 181: Batch 47/116 loss: 0.0580\n",
      "Epoch 181: Batch 48/116 loss: 0.0325\n",
      "Epoch 181: Batch 49/116 loss: 0.0487\n",
      "Epoch 181: Batch 50/116 loss: 0.0537\n",
      "Epoch 181: Batch 51/116 loss: 0.0368\n",
      "Epoch 181: Batch 52/116 loss: 0.0415\n",
      "Epoch 181: Batch 53/116 loss: 0.0473\n",
      "Epoch 181: Batch 54/116 loss: 0.0420\n",
      "Epoch 181: Batch 55/116 loss: 0.0459\n",
      "Epoch 181: Batch 56/116 loss: 0.0313\n",
      "Epoch 181: Batch 57/116 loss: 0.0346\n",
      "Epoch 181: Batch 58/116 loss: 0.0549\n",
      "Epoch 181: Batch 59/116 loss: 0.0430\n",
      "Epoch 181: Batch 60/116 loss: 0.0441\n",
      "Epoch 181: Batch 61/116 loss: 0.0396\n",
      "Epoch 181: Batch 62/116 loss: 0.0582\n",
      "Epoch 181: Batch 63/116 loss: 0.0408\n",
      "Epoch 181: Batch 64/116 loss: 0.0638\n",
      "Epoch 181: Batch 65/116 loss: 0.0348\n",
      "Epoch 181: Batch 66/116 loss: 0.0487\n",
      "Epoch 181: Batch 67/116 loss: 0.0382\n",
      "Epoch 181: Batch 68/116 loss: 0.0317\n",
      "Epoch 181: Batch 69/116 loss: 0.0448\n",
      "Epoch 181: Batch 70/116 loss: 0.0329\n",
      "Epoch 181: Batch 71/116 loss: 0.0364\n",
      "Epoch 181: Batch 72/116 loss: 0.0335\n",
      "Epoch 181: Batch 73/116 loss: 0.0279\n",
      "Epoch 181: Batch 74/116 loss: 0.0252\n",
      "Epoch 181: Batch 75/116 loss: 0.0432\n",
      "Epoch 181: Batch 76/116 loss: 0.0580\n",
      "Epoch 181: Batch 77/116 loss: 0.0280\n",
      "Epoch 181: Batch 78/116 loss: 0.0276\n",
      "Epoch 181: Batch 79/116 loss: 0.0333\n",
      "Epoch 181: Batch 80/116 loss: 0.0170\n",
      "Epoch 181: Batch 81/116 loss: 0.0472\n",
      "Epoch 181: Batch 82/116 loss: 0.0336\n",
      "Epoch 181: Batch 83/116 loss: 0.0468\n",
      "Epoch 181: Batch 84/116 loss: 0.0295\n",
      "Epoch 181: Batch 85/116 loss: 0.0374\n",
      "Epoch 181: Batch 86/116 loss: 0.0711\n",
      "Epoch 181: Batch 87/116 loss: 0.0515\n",
      "Epoch 181: Batch 88/116 loss: 0.0418\n",
      "Epoch 181: Batch 89/116 loss: 0.0430\n",
      "Epoch 181: Batch 90/116 loss: 0.0427\n",
      "Epoch 181: Batch 91/116 loss: 0.0284\n",
      "Epoch 181: Batch 92/116 loss: 0.0710\n",
      "Epoch 181: Batch 93/116 loss: 0.0400\n",
      "Epoch 181: Batch 94/116 loss: 0.0408\n",
      "Epoch 181: Batch 95/116 loss: 0.0472\n",
      "Epoch 181: Batch 96/116 loss: 0.0302\n",
      "Epoch 181: Batch 97/116 loss: 0.0301\n",
      "Epoch 181: Batch 98/116 loss: 0.0534\n",
      "Epoch 181: Batch 99/116 loss: 0.0314\n",
      "Epoch 181: Batch 100/116 loss: 0.0282\n",
      "Epoch 181: Batch 101/116 loss: 0.0280\n",
      "Epoch 181: Batch 102/116 loss: 0.0503\n",
      "Epoch 181: Batch 103/116 loss: 0.0417\n",
      "Epoch 181: Batch 104/116 loss: 0.0448\n",
      "Epoch 181: Batch 105/116 loss: 0.0383\n",
      "Epoch 181: Batch 106/116 loss: 0.0392\n",
      "Epoch 181: Batch 107/116 loss: 0.0467\n",
      "Epoch 181: Batch 108/116 loss: 0.0246\n",
      "Epoch 181: Batch 109/116 loss: 0.0618\n",
      "Epoch 181: Batch 110/116 loss: 0.0352\n",
      "Epoch 181: Batch 111/116 loss: 0.0351\n",
      "Epoch 181: Batch 112/116 loss: 0.0529\n",
      "Epoch 181: Batch 113/116 loss: 0.0572\n",
      "Epoch 181: Batch 114/116 loss: 0.0390\n",
      "Epoch 181: Batch 115/116 loss: 0.0346\n",
      "Epoch 181: Batch 116/116 loss: 0.0368\n",
      "Epoch 181 train loss: 0.0408 valid loss: 0.0545\n",
      "performance reducing: 1\n",
      "Epoch 182: Batch 1/116 loss: 0.0433\n",
      "Epoch 182: Batch 2/116 loss: 0.0375\n",
      "Epoch 182: Batch 3/116 loss: 0.0479\n",
      "Epoch 182: Batch 4/116 loss: 0.0603\n",
      "Epoch 182: Batch 5/116 loss: 0.0296\n",
      "Epoch 182: Batch 6/116 loss: 0.0501\n",
      "Epoch 182: Batch 7/116 loss: 0.0227\n",
      "Epoch 182: Batch 8/116 loss: 0.0277\n",
      "Epoch 182: Batch 9/116 loss: 0.0470\n",
      "Epoch 182: Batch 10/116 loss: 0.0354\n",
      "Epoch 182: Batch 11/116 loss: 0.0280\n",
      "Epoch 182: Batch 12/116 loss: 0.0296\n",
      "Epoch 182: Batch 13/116 loss: 0.0382\n",
      "Epoch 182: Batch 14/116 loss: 0.0376\n",
      "Epoch 182: Batch 15/116 loss: 0.0325\n",
      "Epoch 182: Batch 16/116 loss: 0.0510\n",
      "Epoch 182: Batch 17/116 loss: 0.0588\n",
      "Epoch 182: Batch 18/116 loss: 0.0416\n",
      "Epoch 182: Batch 19/116 loss: 0.0434\n",
      "Epoch 182: Batch 20/116 loss: 0.0449\n",
      "Epoch 182: Batch 21/116 loss: 0.0390\n",
      "Epoch 182: Batch 22/116 loss: 0.0536\n",
      "Epoch 182: Batch 23/116 loss: 0.0360\n",
      "Epoch 182: Batch 24/116 loss: 0.0314\n",
      "Epoch 182: Batch 25/116 loss: 0.0457\n",
      "Epoch 182: Batch 26/116 loss: 0.0508\n",
      "Epoch 182: Batch 27/116 loss: 0.0383\n",
      "Epoch 182: Batch 28/116 loss: 0.0425\n",
      "Epoch 182: Batch 29/116 loss: 0.0509\n",
      "Epoch 182: Batch 30/116 loss: 0.0372\n",
      "Epoch 182: Batch 31/116 loss: 0.0309\n",
      "Epoch 182: Batch 32/116 loss: 0.0526\n",
      "Epoch 182: Batch 33/116 loss: 0.0375\n",
      "Epoch 182: Batch 34/116 loss: 0.0336\n",
      "Epoch 182: Batch 35/116 loss: 0.0405\n",
      "Epoch 182: Batch 36/116 loss: 0.0327\n",
      "Epoch 182: Batch 37/116 loss: 0.0344\n",
      "Epoch 182: Batch 38/116 loss: 0.0309\n",
      "Epoch 182: Batch 39/116 loss: 0.0606\n",
      "Epoch 182: Batch 40/116 loss: 0.0464\n",
      "Epoch 182: Batch 41/116 loss: 0.0376\n",
      "Epoch 182: Batch 42/116 loss: 0.0478\n",
      "Epoch 182: Batch 43/116 loss: 0.0404\n",
      "Epoch 182: Batch 44/116 loss: 0.0181\n",
      "Epoch 182: Batch 45/116 loss: 0.0484\n",
      "Epoch 182: Batch 46/116 loss: 0.0284\n",
      "Epoch 182: Batch 47/116 loss: 0.0403\n",
      "Epoch 182: Batch 48/116 loss: 0.0549\n",
      "Epoch 182: Batch 49/116 loss: 0.0387\n",
      "Epoch 182: Batch 50/116 loss: 0.0408\n",
      "Epoch 182: Batch 51/116 loss: 0.0442\n",
      "Epoch 182: Batch 52/116 loss: 0.0848\n",
      "Epoch 182: Batch 53/116 loss: 0.0396\n",
      "Epoch 182: Batch 54/116 loss: 0.0330\n",
      "Epoch 182: Batch 55/116 loss: 0.0342\n",
      "Epoch 182: Batch 56/116 loss: 0.0286\n",
      "Epoch 182: Batch 57/116 loss: 0.0231\n",
      "Epoch 182: Batch 58/116 loss: 0.0454\n",
      "Epoch 182: Batch 59/116 loss: 0.0333\n",
      "Epoch 182: Batch 60/116 loss: 0.0341\n",
      "Epoch 182: Batch 61/116 loss: 0.0265\n",
      "Epoch 182: Batch 62/116 loss: 0.0311\n",
      "Epoch 182: Batch 63/116 loss: 0.0357\n",
      "Epoch 182: Batch 64/116 loss: 0.0345\n",
      "Epoch 182: Batch 65/116 loss: 0.0555\n",
      "Epoch 182: Batch 66/116 loss: 0.0333\n",
      "Epoch 182: Batch 67/116 loss: 0.0549\n",
      "Epoch 182: Batch 68/116 loss: 0.0519\n",
      "Epoch 182: Batch 69/116 loss: 0.0307\n",
      "Epoch 182: Batch 70/116 loss: 0.0412\n",
      "Epoch 182: Batch 71/116 loss: 0.0218\n",
      "Epoch 182: Batch 72/116 loss: 0.0449\n",
      "Epoch 182: Batch 73/116 loss: 0.0313\n",
      "Epoch 182: Batch 74/116 loss: 0.0409\n",
      "Epoch 182: Batch 75/116 loss: 0.0471\n",
      "Epoch 182: Batch 76/116 loss: 0.0418\n",
      "Epoch 182: Batch 77/116 loss: 0.0420\n",
      "Epoch 182: Batch 78/116 loss: 0.0395\n",
      "Epoch 182: Batch 79/116 loss: 0.0366\n",
      "Epoch 182: Batch 80/116 loss: 0.0408\n",
      "Epoch 182: Batch 81/116 loss: 0.0340\n",
      "Epoch 182: Batch 82/116 loss: 0.0397\n",
      "Epoch 182: Batch 83/116 loss: 0.0385\n",
      "Epoch 182: Batch 84/116 loss: 0.0342\n",
      "Epoch 182: Batch 85/116 loss: 0.0529\n",
      "Epoch 182: Batch 86/116 loss: 0.0547\n",
      "Epoch 182: Batch 87/116 loss: 0.0472\n",
      "Epoch 182: Batch 88/116 loss: 0.0627\n",
      "Epoch 182: Batch 89/116 loss: 0.0330\n",
      "Epoch 182: Batch 90/116 loss: 0.0364\n",
      "Epoch 182: Batch 91/116 loss: 0.0356\n",
      "Epoch 182: Batch 92/116 loss: 0.0442\n",
      "Epoch 182: Batch 93/116 loss: 0.0582\n",
      "Epoch 182: Batch 94/116 loss: 0.0549\n",
      "Epoch 182: Batch 95/116 loss: 0.0350\n",
      "Epoch 182: Batch 96/116 loss: 0.0481\n",
      "Epoch 182: Batch 97/116 loss: 0.0467\n",
      "Epoch 182: Batch 98/116 loss: 0.0265\n",
      "Epoch 182: Batch 99/116 loss: 0.0492\n",
      "Epoch 182: Batch 100/116 loss: 0.0676\n",
      "Epoch 182: Batch 101/116 loss: 0.0285\n",
      "Epoch 182: Batch 102/116 loss: 0.0380\n",
      "Epoch 182: Batch 103/116 loss: 0.0585\n",
      "Epoch 182: Batch 104/116 loss: 0.0431\n",
      "Epoch 182: Batch 105/116 loss: 0.0375\n",
      "Epoch 182: Batch 106/116 loss: 0.0337\n",
      "Epoch 182: Batch 107/116 loss: 0.0659\n",
      "Epoch 182: Batch 108/116 loss: 0.0244\n",
      "Epoch 182: Batch 109/116 loss: 0.0612\n",
      "Epoch 182: Batch 110/116 loss: 0.0298\n",
      "Epoch 182: Batch 111/116 loss: 0.0410\n",
      "Epoch 182: Batch 112/116 loss: 0.0451\n",
      "Epoch 182: Batch 113/116 loss: 0.0330\n",
      "Epoch 182: Batch 114/116 loss: 0.0379\n",
      "Epoch 182: Batch 115/116 loss: 0.0350\n",
      "Epoch 182: Batch 116/116 loss: 0.0494\n",
      "Epoch 182 train loss: 0.0411 valid loss: 0.0513\n",
      "performance reducing: 2\n",
      "Epoch 183: Batch 1/116 loss: 0.0294\n",
      "Epoch 183: Batch 2/116 loss: 0.0350\n",
      "Epoch 183: Batch 3/116 loss: 0.0355\n",
      "Epoch 183: Batch 4/116 loss: 0.0462\n",
      "Epoch 183: Batch 5/116 loss: 0.0231\n",
      "Epoch 183: Batch 6/116 loss: 0.0327\n",
      "Epoch 183: Batch 7/116 loss: 0.0589\n",
      "Epoch 183: Batch 8/116 loss: 0.0319\n",
      "Epoch 183: Batch 9/116 loss: 0.0484\n",
      "Epoch 183: Batch 10/116 loss: 0.0452\n",
      "Epoch 183: Batch 11/116 loss: 0.0268\n",
      "Epoch 183: Batch 12/116 loss: 0.0433\n",
      "Epoch 183: Batch 13/116 loss: 0.0385\n",
      "Epoch 183: Batch 14/116 loss: 0.0299\n",
      "Epoch 183: Batch 15/116 loss: 0.0600\n",
      "Epoch 183: Batch 16/116 loss: 0.0494\n",
      "Epoch 183: Batch 17/116 loss: 0.0310\n",
      "Epoch 183: Batch 18/116 loss: 0.0586\n",
      "Epoch 183: Batch 19/116 loss: 0.0361\n",
      "Epoch 183: Batch 20/116 loss: 0.0331\n",
      "Epoch 183: Batch 21/116 loss: 0.0448\n",
      "Epoch 183: Batch 22/116 loss: 0.0315\n",
      "Epoch 183: Batch 23/116 loss: 0.0448\n",
      "Epoch 183: Batch 24/116 loss: 0.0505\n",
      "Epoch 183: Batch 25/116 loss: 0.0514\n",
      "Epoch 183: Batch 26/116 loss: 0.0287\n",
      "Epoch 183: Batch 27/116 loss: 0.0403\n",
      "Epoch 183: Batch 28/116 loss: 0.0287\n",
      "Epoch 183: Batch 29/116 loss: 0.0505\n",
      "Epoch 183: Batch 30/116 loss: 0.0438\n",
      "Epoch 183: Batch 31/116 loss: 0.0389\n",
      "Epoch 183: Batch 32/116 loss: 0.0295\n",
      "Epoch 183: Batch 33/116 loss: 0.0187\n",
      "Epoch 183: Batch 34/116 loss: 0.0341\n",
      "Epoch 183: Batch 35/116 loss: 0.0503\n",
      "Epoch 183: Batch 36/116 loss: 0.0531\n",
      "Epoch 183: Batch 37/116 loss: 0.0489\n",
      "Epoch 183: Batch 38/116 loss: 0.0439\n",
      "Epoch 183: Batch 39/116 loss: 0.0272\n",
      "Epoch 183: Batch 40/116 loss: 0.0458\n",
      "Epoch 183: Batch 41/116 loss: 0.0538\n",
      "Epoch 183: Batch 42/116 loss: 0.0316\n",
      "Epoch 183: Batch 43/116 loss: 0.0371\n",
      "Epoch 183: Batch 44/116 loss: 0.0415\n",
      "Epoch 183: Batch 45/116 loss: 0.0414\n",
      "Epoch 183: Batch 46/116 loss: 0.0577\n",
      "Epoch 183: Batch 47/116 loss: 0.0368\n",
      "Epoch 183: Batch 48/116 loss: 0.0383\n",
      "Epoch 183: Batch 49/116 loss: 0.0331\n",
      "Epoch 183: Batch 50/116 loss: 0.0478\n",
      "Epoch 183: Batch 51/116 loss: 0.0342\n",
      "Epoch 183: Batch 52/116 loss: 0.0701\n",
      "Epoch 183: Batch 53/116 loss: 0.0269\n",
      "Epoch 183: Batch 54/116 loss: 0.0359\n",
      "Epoch 183: Batch 55/116 loss: 0.0388\n",
      "Epoch 183: Batch 56/116 loss: 0.0339\n",
      "Epoch 183: Batch 57/116 loss: 0.0372\n",
      "Epoch 183: Batch 58/116 loss: 0.0350\n",
      "Epoch 183: Batch 59/116 loss: 0.0372\n",
      "Epoch 183: Batch 60/116 loss: 0.0356\n",
      "Epoch 183: Batch 61/116 loss: 0.0304\n",
      "Epoch 183: Batch 62/116 loss: 0.0303\n",
      "Epoch 183: Batch 63/116 loss: 0.0347\n",
      "Epoch 183: Batch 64/116 loss: 0.0271\n",
      "Epoch 183: Batch 65/116 loss: 0.0374\n",
      "Epoch 183: Batch 66/116 loss: 0.0257\n",
      "Epoch 183: Batch 67/116 loss: 0.0308\n",
      "Epoch 183: Batch 68/116 loss: 0.0430\n",
      "Epoch 183: Batch 69/116 loss: 0.0498\n",
      "Epoch 183: Batch 70/116 loss: 0.0327\n",
      "Epoch 183: Batch 71/116 loss: 0.0351\n",
      "Epoch 183: Batch 72/116 loss: 0.0363\n",
      "Epoch 183: Batch 73/116 loss: 0.0352\n",
      "Epoch 183: Batch 74/116 loss: 0.0388\n",
      "Epoch 183: Batch 75/116 loss: 0.0504\n",
      "Epoch 183: Batch 76/116 loss: 0.0419\n",
      "Epoch 183: Batch 77/116 loss: 0.0557\n",
      "Epoch 183: Batch 78/116 loss: 0.0357\n",
      "Epoch 183: Batch 79/116 loss: 0.0509\n",
      "Epoch 183: Batch 80/116 loss: 0.0399\n",
      "Epoch 183: Batch 81/116 loss: 0.0419\n",
      "Epoch 183: Batch 82/116 loss: 0.0419\n",
      "Epoch 183: Batch 83/116 loss: 0.0405\n",
      "Epoch 183: Batch 84/116 loss: 0.0630\n",
      "Epoch 183: Batch 85/116 loss: 0.0400\n",
      "Epoch 183: Batch 86/116 loss: 0.0430\n",
      "Epoch 183: Batch 87/116 loss: 0.0427\n",
      "Epoch 183: Batch 88/116 loss: 0.0363\n",
      "Epoch 183: Batch 89/116 loss: 0.0404\n",
      "Epoch 183: Batch 90/116 loss: 0.0355\n",
      "Epoch 183: Batch 91/116 loss: 0.0339\n",
      "Epoch 183: Batch 92/116 loss: 0.0456\n",
      "Epoch 183: Batch 93/116 loss: 0.0367\n",
      "Epoch 183: Batch 94/116 loss: 0.0340\n",
      "Epoch 183: Batch 95/116 loss: 0.0410\n",
      "Epoch 183: Batch 96/116 loss: 0.0374\n",
      "Epoch 183: Batch 97/116 loss: 0.0278\n",
      "Epoch 183: Batch 98/116 loss: 0.0635\n",
      "Epoch 183: Batch 99/116 loss: 0.0347\n",
      "Epoch 183: Batch 100/116 loss: 0.0426\n",
      "Epoch 183: Batch 101/116 loss: 0.0623\n",
      "Epoch 183: Batch 102/116 loss: 0.0464\n",
      "Epoch 183: Batch 103/116 loss: 0.0213\n",
      "Epoch 183: Batch 104/116 loss: 0.0249\n",
      "Epoch 183: Batch 105/116 loss: 0.0488\n",
      "Epoch 183: Batch 106/116 loss: 0.0472\n",
      "Epoch 183: Batch 107/116 loss: 0.0437\n",
      "Epoch 183: Batch 108/116 loss: 0.0448\n",
      "Epoch 183: Batch 109/116 loss: 0.0395\n",
      "Epoch 183: Batch 110/116 loss: 0.0400\n",
      "Epoch 183: Batch 111/116 loss: 0.0584\n",
      "Epoch 183: Batch 112/116 loss: 0.0365\n",
      "Epoch 183: Batch 113/116 loss: 0.0573\n",
      "Epoch 183: Batch 114/116 loss: 0.0492\n",
      "Epoch 183: Batch 115/116 loss: 0.0447\n",
      "Epoch 183: Batch 116/116 loss: 0.0345\n",
      "Epoch 183 train loss: 0.0404 valid loss: 0.0684\n",
      "performance reducing: 3\n",
      "Epoch 184: Batch 1/116 loss: 0.0469\n",
      "Epoch 184: Batch 2/116 loss: 0.0358\n",
      "Epoch 184: Batch 3/116 loss: 0.0360\n",
      "Epoch 184: Batch 4/116 loss: 0.0644\n",
      "Epoch 184: Batch 5/116 loss: 0.0254\n",
      "Epoch 184: Batch 6/116 loss: 0.0549\n",
      "Epoch 184: Batch 7/116 loss: 0.0441\n",
      "Epoch 184: Batch 8/116 loss: 0.0441\n",
      "Epoch 184: Batch 9/116 loss: 0.0348\n",
      "Epoch 184: Batch 10/116 loss: 0.0544\n",
      "Epoch 184: Batch 11/116 loss: 0.0337\n",
      "Epoch 184: Batch 12/116 loss: 0.0423\n",
      "Epoch 184: Batch 13/116 loss: 0.0436\n",
      "Epoch 184: Batch 14/116 loss: 0.0472\n",
      "Epoch 184: Batch 15/116 loss: 0.0404\n",
      "Epoch 184: Batch 16/116 loss: 0.0374\n",
      "Epoch 184: Batch 17/116 loss: 0.0350\n",
      "Epoch 184: Batch 18/116 loss: 0.0314\n",
      "Epoch 184: Batch 19/116 loss: 0.0432\n",
      "Epoch 184: Batch 20/116 loss: 0.0336\n",
      "Epoch 184: Batch 21/116 loss: 0.0345\n",
      "Epoch 184: Batch 22/116 loss: 0.0408\n",
      "Epoch 184: Batch 23/116 loss: 0.0493\n",
      "Epoch 184: Batch 24/116 loss: 0.0364\n",
      "Epoch 184: Batch 25/116 loss: 0.0456\n",
      "Epoch 184: Batch 26/116 loss: 0.0471\n",
      "Epoch 184: Batch 27/116 loss: 0.0346\n",
      "Epoch 184: Batch 28/116 loss: 0.0341\n",
      "Epoch 184: Batch 29/116 loss: 0.0421\n",
      "Epoch 184: Batch 30/116 loss: 0.0372\n",
      "Epoch 184: Batch 31/116 loss: 0.0561\n",
      "Epoch 184: Batch 32/116 loss: 0.0305\n",
      "Epoch 184: Batch 33/116 loss: 0.0272\n",
      "Epoch 184: Batch 34/116 loss: 0.0379\n",
      "Epoch 184: Batch 35/116 loss: 0.0377\n",
      "Epoch 184: Batch 36/116 loss: 0.0604\n",
      "Epoch 184: Batch 37/116 loss: 0.0379\n",
      "Epoch 184: Batch 38/116 loss: 0.0412\n",
      "Epoch 184: Batch 39/116 loss: 0.0360\n",
      "Epoch 184: Batch 40/116 loss: 0.0535\n",
      "Epoch 184: Batch 41/116 loss: 0.0311\n",
      "Epoch 184: Batch 42/116 loss: 0.0233\n",
      "Epoch 184: Batch 43/116 loss: 0.0273\n",
      "Epoch 184: Batch 44/116 loss: 0.0342\n",
      "Epoch 184: Batch 45/116 loss: 0.0442\n",
      "Epoch 184: Batch 46/116 loss: 0.0289\n",
      "Epoch 184: Batch 47/116 loss: 0.0404\n",
      "Epoch 184: Batch 48/116 loss: 0.0445\n",
      "Epoch 184: Batch 49/116 loss: 0.0290\n",
      "Epoch 184: Batch 50/116 loss: 0.0370\n",
      "Epoch 184: Batch 51/116 loss: 0.0434\n",
      "Epoch 184: Batch 52/116 loss: 0.0352\n",
      "Epoch 184: Batch 53/116 loss: 0.0653\n",
      "Epoch 184: Batch 54/116 loss: 0.0471\n",
      "Epoch 184: Batch 55/116 loss: 0.0388\n",
      "Epoch 184: Batch 56/116 loss: 0.0375\n",
      "Epoch 184: Batch 57/116 loss: 0.0486\n",
      "Epoch 184: Batch 58/116 loss: 0.0393\n",
      "Epoch 184: Batch 59/116 loss: 0.0416\n",
      "Epoch 184: Batch 60/116 loss: 0.0232\n",
      "Epoch 184: Batch 61/116 loss: 0.0340\n",
      "Epoch 184: Batch 62/116 loss: 0.0411\n",
      "Epoch 184: Batch 63/116 loss: 0.0590\n",
      "Epoch 184: Batch 64/116 loss: 0.0390\n",
      "Epoch 184: Batch 65/116 loss: 0.0343\n",
      "Epoch 184: Batch 66/116 loss: 0.0403\n",
      "Epoch 184: Batch 67/116 loss: 0.0381\n",
      "Epoch 184: Batch 68/116 loss: 0.0494\n",
      "Epoch 184: Batch 69/116 loss: 0.0473\n",
      "Epoch 184: Batch 70/116 loss: 0.0561\n",
      "Epoch 184: Batch 71/116 loss: 0.0294\n",
      "Epoch 184: Batch 72/116 loss: 0.0527\n",
      "Epoch 184: Batch 73/116 loss: 0.0460\n",
      "Epoch 184: Batch 74/116 loss: 0.0445\n",
      "Epoch 184: Batch 75/116 loss: 0.0384\n",
      "Epoch 184: Batch 76/116 loss: 0.0450\n",
      "Epoch 184: Batch 77/116 loss: 0.0427\n",
      "Epoch 184: Batch 78/116 loss: 0.0346\n",
      "Epoch 184: Batch 79/116 loss: 0.0349\n",
      "Epoch 184: Batch 80/116 loss: 0.0329\n",
      "Epoch 184: Batch 81/116 loss: 0.0391\n",
      "Epoch 184: Batch 82/116 loss: 0.0445\n",
      "Epoch 184: Batch 83/116 loss: 0.0329\n",
      "Epoch 184: Batch 84/116 loss: 0.0559\n",
      "Epoch 184: Batch 85/116 loss: 0.0557\n",
      "Epoch 184: Batch 86/116 loss: 0.0515\n",
      "Epoch 184: Batch 87/116 loss: 0.0351\n",
      "Epoch 184: Batch 88/116 loss: 0.0294\n",
      "Epoch 184: Batch 89/116 loss: 0.0379\n",
      "Epoch 184: Batch 90/116 loss: 0.0332\n",
      "Epoch 184: Batch 91/116 loss: 0.0326\n",
      "Epoch 184: Batch 92/116 loss: 0.0372\n",
      "Epoch 184: Batch 93/116 loss: 0.0332\n",
      "Epoch 184: Batch 94/116 loss: 0.0238\n",
      "Epoch 184: Batch 95/116 loss: 0.0414\n",
      "Epoch 184: Batch 96/116 loss: 0.0500\n",
      "Epoch 184: Batch 97/116 loss: 0.0410\n",
      "Epoch 184: Batch 98/116 loss: 0.0486\n",
      "Epoch 184: Batch 99/116 loss: 0.0472\n",
      "Epoch 184: Batch 100/116 loss: 0.0358\n",
      "Epoch 184: Batch 101/116 loss: 0.0506\n",
      "Epoch 184: Batch 102/116 loss: 0.0406\n",
      "Epoch 184: Batch 103/116 loss: 0.0359\n",
      "Epoch 184: Batch 104/116 loss: 0.0444\n",
      "Epoch 184: Batch 105/116 loss: 0.0355\n",
      "Epoch 184: Batch 106/116 loss: 0.0326\n",
      "Epoch 184: Batch 107/116 loss: 0.0378\n",
      "Epoch 184: Batch 108/116 loss: 0.0387\n",
      "Epoch 184: Batch 109/116 loss: 0.0474\n",
      "Epoch 184: Batch 110/116 loss: 0.0397\n",
      "Epoch 184: Batch 111/116 loss: 0.0321\n",
      "Epoch 184: Batch 112/116 loss: 0.0403\n",
      "Epoch 184: Batch 113/116 loss: 0.0377\n",
      "Epoch 184: Batch 114/116 loss: 0.0400\n",
      "Epoch 184: Batch 115/116 loss: 0.0275\n",
      "Epoch 184: Batch 116/116 loss: 0.0378\n",
      "Epoch 184 train loss: 0.0403 valid loss: 0.0564\n",
      "performance reducing: 4\n",
      "Epoch 185: Batch 1/116 loss: 0.0376\n",
      "Epoch 185: Batch 2/116 loss: 0.0577\n",
      "Epoch 185: Batch 3/116 loss: 0.0424\n",
      "Epoch 185: Batch 4/116 loss: 0.0265\n",
      "Epoch 185: Batch 5/116 loss: 0.0333\n",
      "Epoch 185: Batch 6/116 loss: 0.0351\n",
      "Epoch 185: Batch 7/116 loss: 0.0380\n",
      "Epoch 185: Batch 8/116 loss: 0.0394\n",
      "Epoch 185: Batch 9/116 loss: 0.0326\n",
      "Epoch 185: Batch 10/116 loss: 0.0372\n",
      "Epoch 185: Batch 11/116 loss: 0.0315\n",
      "Epoch 185: Batch 12/116 loss: 0.0262\n",
      "Epoch 185: Batch 13/116 loss: 0.0670\n",
      "Epoch 185: Batch 14/116 loss: 0.0435\n",
      "Epoch 185: Batch 15/116 loss: 0.0300\n",
      "Epoch 185: Batch 16/116 loss: 0.0367\n",
      "Epoch 185: Batch 17/116 loss: 0.0344\n",
      "Epoch 185: Batch 18/116 loss: 0.0347\n",
      "Epoch 185: Batch 19/116 loss: 0.0256\n",
      "Epoch 185: Batch 20/116 loss: 0.0276\n",
      "Epoch 185: Batch 21/116 loss: 0.0387\n",
      "Epoch 185: Batch 22/116 loss: 0.0402\n",
      "Epoch 185: Batch 23/116 loss: 0.0277\n",
      "Epoch 185: Batch 24/116 loss: 0.0436\n",
      "Epoch 185: Batch 25/116 loss: 0.0361\n",
      "Epoch 185: Batch 26/116 loss: 0.0353\n",
      "Epoch 185: Batch 27/116 loss: 0.0320\n",
      "Epoch 185: Batch 28/116 loss: 0.0485\n",
      "Epoch 185: Batch 29/116 loss: 0.0443\n",
      "Epoch 185: Batch 30/116 loss: 0.0333\n",
      "Epoch 185: Batch 31/116 loss: 0.0491\n",
      "Epoch 185: Batch 32/116 loss: 0.0315\n",
      "Epoch 185: Batch 33/116 loss: 0.0380\n",
      "Epoch 185: Batch 34/116 loss: 0.0578\n",
      "Epoch 185: Batch 35/116 loss: 0.0401\n",
      "Epoch 185: Batch 36/116 loss: 0.0359\n",
      "Epoch 185: Batch 37/116 loss: 0.0471\n",
      "Epoch 185: Batch 38/116 loss: 0.0466\n",
      "Epoch 185: Batch 39/116 loss: 0.0288\n",
      "Epoch 185: Batch 40/116 loss: 0.0370\n",
      "Epoch 185: Batch 41/116 loss: 0.0558\n",
      "Epoch 185: Batch 42/116 loss: 0.0300\n",
      "Epoch 185: Batch 43/116 loss: 0.0395\n",
      "Epoch 185: Batch 44/116 loss: 0.0444\n",
      "Epoch 185: Batch 45/116 loss: 0.0182\n",
      "Epoch 185: Batch 46/116 loss: 0.0377\n",
      "Epoch 185: Batch 47/116 loss: 0.0429\n",
      "Epoch 185: Batch 48/116 loss: 0.0370\n",
      "Epoch 185: Batch 49/116 loss: 0.0274\n",
      "Epoch 185: Batch 50/116 loss: 0.0344\n",
      "Epoch 185: Batch 51/116 loss: 0.0322\n",
      "Epoch 185: Batch 52/116 loss: 0.0533\n",
      "Epoch 185: Batch 53/116 loss: 0.0586\n",
      "Epoch 185: Batch 54/116 loss: 0.0310\n",
      "Epoch 185: Batch 55/116 loss: 0.0445\n",
      "Epoch 185: Batch 56/116 loss: 0.0441\n",
      "Epoch 185: Batch 57/116 loss: 0.0444\n",
      "Epoch 185: Batch 58/116 loss: 0.0568\n",
      "Epoch 185: Batch 59/116 loss: 0.0322\n",
      "Epoch 185: Batch 60/116 loss: 0.0326\n",
      "Epoch 185: Batch 61/116 loss: 0.0490\n",
      "Epoch 185: Batch 62/116 loss: 0.0391\n",
      "Epoch 185: Batch 63/116 loss: 0.0542\n",
      "Epoch 185: Batch 64/116 loss: 0.0225\n",
      "Epoch 185: Batch 65/116 loss: 0.0447\n",
      "Epoch 185: Batch 66/116 loss: 0.0314\n",
      "Epoch 185: Batch 67/116 loss: 0.0502\n",
      "Epoch 185: Batch 68/116 loss: 0.0408\n",
      "Epoch 185: Batch 69/116 loss: 0.0509\n",
      "Epoch 185: Batch 70/116 loss: 0.0419\n",
      "Epoch 185: Batch 71/116 loss: 0.0394\n",
      "Epoch 185: Batch 72/116 loss: 0.0328\n",
      "Epoch 185: Batch 73/116 loss: 0.0405\n",
      "Epoch 185: Batch 74/116 loss: 0.0279\n",
      "Epoch 185: Batch 75/116 loss: 0.0550\n",
      "Epoch 185: Batch 76/116 loss: 0.0447\n",
      "Epoch 185: Batch 77/116 loss: 0.0412\n",
      "Epoch 185: Batch 78/116 loss: 0.0317\n",
      "Epoch 185: Batch 79/116 loss: 0.0382\n",
      "Epoch 185: Batch 80/116 loss: 0.0370\n",
      "Epoch 185: Batch 81/116 loss: 0.0369\n",
      "Epoch 185: Batch 82/116 loss: 0.0420\n",
      "Epoch 185: Batch 83/116 loss: 0.0477\n",
      "Epoch 185: Batch 84/116 loss: 0.0359\n",
      "Epoch 185: Batch 85/116 loss: 0.0376\n",
      "Epoch 185: Batch 86/116 loss: 0.0461\n",
      "Epoch 185: Batch 87/116 loss: 0.0252\n",
      "Epoch 185: Batch 88/116 loss: 0.0217\n",
      "Epoch 185: Batch 89/116 loss: 0.0390\n",
      "Epoch 185: Batch 90/116 loss: 0.0301\n",
      "Epoch 185: Batch 91/116 loss: 0.0269\n",
      "Epoch 185: Batch 92/116 loss: 0.0295\n",
      "Epoch 185: Batch 93/116 loss: 0.0377\n",
      "Epoch 185: Batch 94/116 loss: 0.0311\n",
      "Epoch 185: Batch 95/116 loss: 0.0517\n",
      "Epoch 185: Batch 96/116 loss: 0.0339\n",
      "Epoch 185: Batch 97/116 loss: 0.0458\n",
      "Epoch 185: Batch 98/116 loss: 0.0399\n",
      "Epoch 185: Batch 99/116 loss: 0.0246\n",
      "Epoch 185: Batch 100/116 loss: 0.0378\n",
      "Epoch 185: Batch 101/116 loss: 0.0407\n",
      "Epoch 185: Batch 102/116 loss: 0.0368\n",
      "Epoch 185: Batch 103/116 loss: 0.0459\n",
      "Epoch 185: Batch 104/116 loss: 0.0483\n",
      "Epoch 185: Batch 105/116 loss: 0.0364\n",
      "Epoch 185: Batch 106/116 loss: 0.0475\n",
      "Epoch 185: Batch 107/116 loss: 0.0424\n",
      "Epoch 185: Batch 108/116 loss: 0.0510\n",
      "Epoch 185: Batch 109/116 loss: 0.0421\n",
      "Epoch 185: Batch 110/116 loss: 0.0608\n",
      "Epoch 185: Batch 111/116 loss: 0.0372\n",
      "Epoch 185: Batch 112/116 loss: 0.0435\n",
      "Epoch 185: Batch 113/116 loss: 0.0406\n",
      "Epoch 185: Batch 114/116 loss: 0.0497\n",
      "Epoch 185: Batch 115/116 loss: 0.0483\n",
      "Epoch 185: Batch 116/116 loss: 0.0542\n",
      "Epoch 185 train loss: 0.0395 valid loss: 0.0544\n",
      "performance reducing: 5\n",
      "Epoch 186: Batch 1/116 loss: 0.0365\n",
      "Epoch 186: Batch 2/116 loss: 0.0451\n",
      "Epoch 186: Batch 3/116 loss: 0.0320\n",
      "Epoch 186: Batch 4/116 loss: 0.0342\n",
      "Epoch 186: Batch 5/116 loss: 0.0358\n",
      "Epoch 186: Batch 6/116 loss: 0.0222\n",
      "Epoch 186: Batch 7/116 loss: 0.0455\n",
      "Epoch 186: Batch 8/116 loss: 0.0323\n",
      "Epoch 186: Batch 9/116 loss: 0.0234\n",
      "Epoch 186: Batch 10/116 loss: 0.0474\n",
      "Epoch 186: Batch 11/116 loss: 0.0473\n",
      "Epoch 186: Batch 12/116 loss: 0.0377\n",
      "Epoch 186: Batch 13/116 loss: 0.0308\n",
      "Epoch 186: Batch 14/116 loss: 0.0288\n",
      "Epoch 186: Batch 15/116 loss: 0.0435\n",
      "Epoch 186: Batch 16/116 loss: 0.0311\n",
      "Epoch 186: Batch 17/116 loss: 0.0368\n",
      "Epoch 186: Batch 18/116 loss: 0.0361\n",
      "Epoch 186: Batch 19/116 loss: 0.0444\n",
      "Epoch 186: Batch 20/116 loss: 0.0383\n",
      "Epoch 186: Batch 21/116 loss: 0.0415\n",
      "Epoch 186: Batch 22/116 loss: 0.0309\n",
      "Epoch 186: Batch 23/116 loss: 0.0299\n",
      "Epoch 186: Batch 24/116 loss: 0.0651\n",
      "Epoch 186: Batch 25/116 loss: 0.0329\n",
      "Epoch 186: Batch 26/116 loss: 0.0326\n",
      "Epoch 186: Batch 27/116 loss: 0.0413\n",
      "Epoch 186: Batch 28/116 loss: 0.0511\n",
      "Epoch 186: Batch 29/116 loss: 0.0367\n",
      "Epoch 186: Batch 30/116 loss: 0.0572\n",
      "Epoch 186: Batch 31/116 loss: 0.0495\n",
      "Epoch 186: Batch 32/116 loss: 0.0545\n",
      "Epoch 186: Batch 33/116 loss: 0.0576\n",
      "Epoch 186: Batch 34/116 loss: 0.0426\n",
      "Epoch 186: Batch 35/116 loss: 0.0395\n",
      "Epoch 186: Batch 36/116 loss: 0.0441\n",
      "Epoch 186: Batch 37/116 loss: 0.0374\n",
      "Epoch 186: Batch 38/116 loss: 0.0297\n",
      "Epoch 186: Batch 39/116 loss: 0.0511\n",
      "Epoch 186: Batch 40/116 loss: 0.0493\n",
      "Epoch 186: Batch 41/116 loss: 0.0265\n",
      "Epoch 186: Batch 42/116 loss: 0.0387\n",
      "Epoch 186: Batch 43/116 loss: 0.0306\n",
      "Epoch 186: Batch 44/116 loss: 0.0461\n",
      "Epoch 186: Batch 45/116 loss: 0.0645\n",
      "Epoch 186: Batch 46/116 loss: 0.0331\n",
      "Epoch 186: Batch 47/116 loss: 0.0358\n",
      "Epoch 186: Batch 48/116 loss: 0.0499\n",
      "Epoch 186: Batch 49/116 loss: 0.0313\n",
      "Epoch 186: Batch 50/116 loss: 0.0338\n",
      "Epoch 186: Batch 51/116 loss: 0.0411\n",
      "Epoch 186: Batch 52/116 loss: 0.0257\n",
      "Epoch 186: Batch 53/116 loss: 0.0466\n",
      "Epoch 186: Batch 54/116 loss: 0.0394\n",
      "Epoch 186: Batch 55/116 loss: 0.0441\n",
      "Epoch 186: Batch 56/116 loss: 0.0414\n",
      "Epoch 186: Batch 57/116 loss: 0.0341\n",
      "Epoch 186: Batch 58/116 loss: 0.0379\n",
      "Epoch 186: Batch 59/116 loss: 0.0344\n",
      "Epoch 186: Batch 60/116 loss: 0.0363\n",
      "Epoch 186: Batch 61/116 loss: 0.0351\n",
      "Epoch 186: Batch 62/116 loss: 0.0458\n",
      "Epoch 186: Batch 63/116 loss: 0.0524\n",
      "Epoch 186: Batch 64/116 loss: 0.0411\n",
      "Epoch 186: Batch 65/116 loss: 0.0692\n",
      "Epoch 186: Batch 66/116 loss: 0.0334\n",
      "Epoch 186: Batch 67/116 loss: 0.0430\n",
      "Epoch 186: Batch 68/116 loss: 0.0527\n",
      "Epoch 186: Batch 69/116 loss: 0.0417\n",
      "Epoch 186: Batch 70/116 loss: 0.0336\n",
      "Epoch 186: Batch 71/116 loss: 0.0485\n",
      "Epoch 186: Batch 72/116 loss: 0.0418\n",
      "Epoch 186: Batch 73/116 loss: 0.0411\n",
      "Epoch 186: Batch 74/116 loss: 0.0449\n",
      "Epoch 186: Batch 75/116 loss: 0.0512\n",
      "Epoch 186: Batch 76/116 loss: 0.0521\n",
      "Epoch 186: Batch 77/116 loss: 0.0488\n",
      "Epoch 186: Batch 78/116 loss: 0.0403\n",
      "Epoch 186: Batch 79/116 loss: 0.0426\n",
      "Epoch 186: Batch 80/116 loss: 0.0282\n",
      "Epoch 186: Batch 81/116 loss: 0.0409\n",
      "Epoch 186: Batch 82/116 loss: 0.0523\n",
      "Epoch 186: Batch 83/116 loss: 0.0524\n",
      "Epoch 186: Batch 84/116 loss: 0.0427\n",
      "Epoch 186: Batch 85/116 loss: 0.0406\n",
      "Epoch 186: Batch 86/116 loss: 0.0393\n",
      "Epoch 186: Batch 87/116 loss: 0.0341\n",
      "Epoch 186: Batch 88/116 loss: 0.0423\n",
      "Epoch 186: Batch 89/116 loss: 0.0323\n",
      "Epoch 186: Batch 90/116 loss: 0.0591\n",
      "Epoch 186: Batch 91/116 loss: 0.0449\n",
      "Epoch 186: Batch 92/116 loss: 0.0589\n",
      "Epoch 186: Batch 93/116 loss: 0.0283\n",
      "Epoch 186: Batch 94/116 loss: 0.0321\n",
      "Epoch 186: Batch 95/116 loss: 0.0464\n",
      "Epoch 186: Batch 96/116 loss: 0.0384\n",
      "Epoch 186: Batch 97/116 loss: 0.0495\n",
      "Epoch 186: Batch 98/116 loss: 0.0493\n",
      "Epoch 186: Batch 99/116 loss: 0.0430\n",
      "Epoch 186: Batch 100/116 loss: 0.0509\n",
      "Epoch 186: Batch 101/116 loss: 0.0268\n",
      "Epoch 186: Batch 102/116 loss: 0.0387\n",
      "Epoch 186: Batch 103/116 loss: 0.0319\n",
      "Epoch 186: Batch 104/116 loss: 0.0296\n",
      "Epoch 186: Batch 105/116 loss: 0.0390\n",
      "Epoch 186: Batch 106/116 loss: 0.0202\n",
      "Epoch 186: Batch 107/116 loss: 0.0223\n",
      "Epoch 186: Batch 108/116 loss: 0.0358\n",
      "Epoch 186: Batch 109/116 loss: 0.0404\n",
      "Epoch 186: Batch 110/116 loss: 0.0320\n",
      "Epoch 186: Batch 111/116 loss: 0.0403\n",
      "Epoch 186: Batch 112/116 loss: 0.0416\n",
      "Epoch 186: Batch 113/116 loss: 0.0528\n",
      "Epoch 186: Batch 114/116 loss: 0.0337\n",
      "Epoch 186: Batch 115/116 loss: 0.0286\n",
      "Epoch 186: Batch 116/116 loss: 0.0386\n",
      "Epoch 186 train loss: 0.0405 valid loss: 0.0546\n",
      "performance reducing: 6\n",
      "Epoch 187: Batch 1/116 loss: 0.0359\n",
      "Epoch 187: Batch 2/116 loss: 0.0371\n",
      "Epoch 187: Batch 3/116 loss: 0.0379\n",
      "Epoch 187: Batch 4/116 loss: 0.0420\n",
      "Epoch 187: Batch 5/116 loss: 0.0351\n",
      "Epoch 187: Batch 6/116 loss: 0.0314\n",
      "Epoch 187: Batch 7/116 loss: 0.0518\n",
      "Epoch 187: Batch 8/116 loss: 0.0478\n",
      "Epoch 187: Batch 9/116 loss: 0.0412\n",
      "Epoch 187: Batch 10/116 loss: 0.0313\n",
      "Epoch 187: Batch 11/116 loss: 0.0643\n",
      "Epoch 187: Batch 12/116 loss: 0.0483\n",
      "Epoch 187: Batch 13/116 loss: 0.0433\n",
      "Epoch 187: Batch 14/116 loss: 0.0351\n",
      "Epoch 187: Batch 15/116 loss: 0.0369\n",
      "Epoch 187: Batch 16/116 loss: 0.0279\n",
      "Epoch 187: Batch 17/116 loss: 0.0425\n",
      "Epoch 187: Batch 18/116 loss: 0.0424\n",
      "Epoch 187: Batch 19/116 loss: 0.0388\n",
      "Epoch 187: Batch 20/116 loss: 0.0341\n",
      "Epoch 187: Batch 21/116 loss: 0.0441\n",
      "Epoch 187: Batch 22/116 loss: 0.0346\n",
      "Epoch 187: Batch 23/116 loss: 0.0427\n",
      "Epoch 187: Batch 24/116 loss: 0.0509\n",
      "Epoch 187: Batch 25/116 loss: 0.0368\n",
      "Epoch 187: Batch 26/116 loss: 0.0270\n",
      "Epoch 187: Batch 27/116 loss: 0.0265\n",
      "Epoch 187: Batch 28/116 loss: 0.0324\n",
      "Epoch 187: Batch 29/116 loss: 0.0571\n",
      "Epoch 187: Batch 30/116 loss: 0.0436\n",
      "Epoch 187: Batch 31/116 loss: 0.0421\n",
      "Epoch 187: Batch 32/116 loss: 0.0379\n",
      "Epoch 187: Batch 33/116 loss: 0.0398\n",
      "Epoch 187: Batch 34/116 loss: 0.0414\n",
      "Epoch 187: Batch 35/116 loss: 0.0461\n",
      "Epoch 187: Batch 36/116 loss: 0.0340\n",
      "Epoch 187: Batch 37/116 loss: 0.0362\n",
      "Epoch 187: Batch 38/116 loss: 0.0352\n",
      "Epoch 187: Batch 39/116 loss: 0.0425\n",
      "Epoch 187: Batch 40/116 loss: 0.0430\n",
      "Epoch 187: Batch 41/116 loss: 0.0446\n",
      "Epoch 187: Batch 42/116 loss: 0.0382\n",
      "Epoch 187: Batch 43/116 loss: 0.0469\n",
      "Epoch 187: Batch 44/116 loss: 0.0297\n",
      "Epoch 187: Batch 45/116 loss: 0.0316\n",
      "Epoch 187: Batch 46/116 loss: 0.0434\n",
      "Epoch 187: Batch 47/116 loss: 0.0457\n",
      "Epoch 187: Batch 48/116 loss: 0.0666\n",
      "Epoch 187: Batch 49/116 loss: 0.0439\n",
      "Epoch 187: Batch 50/116 loss: 0.0460\n",
      "Epoch 187: Batch 51/116 loss: 0.0593\n",
      "Epoch 187: Batch 52/116 loss: 0.0652\n",
      "Epoch 187: Batch 53/116 loss: 0.0497\n",
      "Epoch 187: Batch 54/116 loss: 0.0263\n",
      "Epoch 187: Batch 55/116 loss: 0.0496\n",
      "Epoch 187: Batch 56/116 loss: 0.0471\n",
      "Epoch 187: Batch 57/116 loss: 0.0615\n",
      "Epoch 187: Batch 58/116 loss: 0.0293\n",
      "Epoch 187: Batch 59/116 loss: 0.0366\n",
      "Epoch 187: Batch 60/116 loss: 0.0249\n",
      "Epoch 187: Batch 61/116 loss: 0.0279\n",
      "Epoch 187: Batch 62/116 loss: 0.0357\n",
      "Epoch 187: Batch 63/116 loss: 0.0514\n",
      "Epoch 187: Batch 64/116 loss: 0.0344\n",
      "Epoch 187: Batch 65/116 loss: 0.0592\n",
      "Epoch 187: Batch 66/116 loss: 0.0414\n",
      "Epoch 187: Batch 67/116 loss: 0.0317\n",
      "Epoch 187: Batch 68/116 loss: 0.0456\n",
      "Epoch 187: Batch 69/116 loss: 0.0409\n",
      "Epoch 187: Batch 70/116 loss: 0.0343\n",
      "Epoch 187: Batch 71/116 loss: 0.0315\n",
      "Epoch 187: Batch 72/116 loss: 0.0296\n",
      "Epoch 187: Batch 73/116 loss: 0.0315\n",
      "Epoch 187: Batch 74/116 loss: 0.0536\n",
      "Epoch 187: Batch 75/116 loss: 0.0390\n",
      "Epoch 187: Batch 76/116 loss: 0.0470\n",
      "Epoch 187: Batch 77/116 loss: 0.0336\n",
      "Epoch 187: Batch 78/116 loss: 0.0433\n",
      "Epoch 187: Batch 79/116 loss: 0.0278\n",
      "Epoch 187: Batch 80/116 loss: 0.0370\n",
      "Epoch 187: Batch 81/116 loss: 0.0350\n",
      "Epoch 187: Batch 82/116 loss: 0.0595\n",
      "Epoch 187: Batch 83/116 loss: 0.0267\n",
      "Epoch 187: Batch 84/116 loss: 0.0363\n",
      "Epoch 187: Batch 85/116 loss: 0.0379\n",
      "Epoch 187: Batch 86/116 loss: 0.0466\n",
      "Epoch 187: Batch 87/116 loss: 0.0358\n",
      "Epoch 187: Batch 88/116 loss: 0.0380\n",
      "Epoch 187: Batch 89/116 loss: 0.0502\n",
      "Epoch 187: Batch 90/116 loss: 0.0369\n",
      "Epoch 187: Batch 91/116 loss: 0.0394\n",
      "Epoch 187: Batch 92/116 loss: 0.0537\n",
      "Epoch 187: Batch 93/116 loss: 0.0544\n",
      "Epoch 187: Batch 94/116 loss: 0.0343\n",
      "Epoch 187: Batch 95/116 loss: 0.0445\n",
      "Epoch 187: Batch 96/116 loss: 0.0453\n",
      "Epoch 187: Batch 97/116 loss: 0.0312\n",
      "Epoch 187: Batch 98/116 loss: 0.0505\n",
      "Epoch 187: Batch 99/116 loss: 0.0493\n",
      "Epoch 187: Batch 100/116 loss: 0.0437\n",
      "Epoch 187: Batch 101/116 loss: 0.0509\n",
      "Epoch 187: Batch 102/116 loss: 0.0425\n",
      "Epoch 187: Batch 103/116 loss: 0.0459\n",
      "Epoch 187: Batch 104/116 loss: 0.0343\n",
      "Epoch 187: Batch 105/116 loss: 0.0520\n",
      "Epoch 187: Batch 106/116 loss: 0.0319\n",
      "Epoch 187: Batch 107/116 loss: 0.0451\n",
      "Epoch 187: Batch 108/116 loss: 0.0398\n",
      "Epoch 187: Batch 109/116 loss: 0.0544\n",
      "Epoch 187: Batch 110/116 loss: 0.0463\n",
      "Epoch 187: Batch 111/116 loss: 0.0536\n",
      "Epoch 187: Batch 112/116 loss: 0.0363\n",
      "Epoch 187: Batch 113/116 loss: 0.0528\n",
      "Epoch 187: Batch 114/116 loss: 0.0308\n",
      "Epoch 187: Batch 115/116 loss: 0.0390\n",
      "Epoch 187: Batch 116/116 loss: 0.0528\n",
      "Epoch 187 train loss: 0.0415 valid loss: 0.0519\n",
      "performance reducing: 7\n",
      "Epoch 188: Batch 1/116 loss: 0.0419\n",
      "Epoch 188: Batch 2/116 loss: 0.0328\n",
      "Epoch 188: Batch 3/116 loss: 0.0866\n",
      "Epoch 188: Batch 4/116 loss: 0.0504\n",
      "Epoch 188: Batch 5/116 loss: 0.0265\n",
      "Epoch 188: Batch 6/116 loss: 0.0645\n",
      "Epoch 188: Batch 7/116 loss: 0.0445\n",
      "Epoch 188: Batch 8/116 loss: 0.0423\n",
      "Epoch 188: Batch 9/116 loss: 0.0502\n",
      "Epoch 188: Batch 10/116 loss: 0.0470\n",
      "Epoch 188: Batch 11/116 loss: 0.0338\n",
      "Epoch 188: Batch 12/116 loss: 0.0362\n",
      "Epoch 188: Batch 13/116 loss: 0.0314\n",
      "Epoch 188: Batch 14/116 loss: 0.0357\n",
      "Epoch 188: Batch 15/116 loss: 0.0401\n",
      "Epoch 188: Batch 16/116 loss: 0.0540\n",
      "Epoch 188: Batch 17/116 loss: 0.0307\n",
      "Epoch 188: Batch 18/116 loss: 0.0279\n",
      "Epoch 188: Batch 19/116 loss: 0.0313\n",
      "Epoch 188: Batch 20/116 loss: 0.0197\n",
      "Epoch 188: Batch 21/116 loss: 0.0717\n",
      "Epoch 188: Batch 22/116 loss: 0.0548\n",
      "Epoch 188: Batch 23/116 loss: 0.0303\n",
      "Epoch 188: Batch 24/116 loss: 0.0496\n",
      "Epoch 188: Batch 25/116 loss: 0.0471\n",
      "Epoch 188: Batch 26/116 loss: 0.0550\n",
      "Epoch 188: Batch 27/116 loss: 0.0379\n",
      "Epoch 188: Batch 28/116 loss: 0.0248\n",
      "Epoch 188: Batch 29/116 loss: 0.0275\n",
      "Epoch 188: Batch 30/116 loss: 0.0553\n",
      "Epoch 188: Batch 31/116 loss: 0.0560\n",
      "Epoch 188: Batch 32/116 loss: 0.0461\n",
      "Epoch 188: Batch 33/116 loss: 0.0271\n",
      "Epoch 188: Batch 34/116 loss: 0.0496\n",
      "Epoch 188: Batch 35/116 loss: 0.0388\n",
      "Epoch 188: Batch 36/116 loss: 0.0313\n",
      "Epoch 188: Batch 37/116 loss: 0.0369\n",
      "Epoch 188: Batch 38/116 loss: 0.0450\n",
      "Epoch 188: Batch 39/116 loss: 0.0254\n",
      "Epoch 188: Batch 40/116 loss: 0.0481\n",
      "Epoch 188: Batch 41/116 loss: 0.0407\n",
      "Epoch 188: Batch 42/116 loss: 0.0303\n",
      "Epoch 188: Batch 43/116 loss: 0.0383\n",
      "Epoch 188: Batch 44/116 loss: 0.0413\n",
      "Epoch 188: Batch 45/116 loss: 0.0277\n",
      "Epoch 188: Batch 46/116 loss: 0.0464\n",
      "Epoch 188: Batch 47/116 loss: 0.0428\n",
      "Epoch 188: Batch 48/116 loss: 0.0553\n",
      "Epoch 188: Batch 49/116 loss: 0.0510\n",
      "Epoch 188: Batch 50/116 loss: 0.0404\n",
      "Epoch 188: Batch 51/116 loss: 0.0365\n",
      "Epoch 188: Batch 52/116 loss: 0.0379\n",
      "Epoch 188: Batch 53/116 loss: 0.0268\n",
      "Epoch 188: Batch 54/116 loss: 0.0255\n",
      "Epoch 188: Batch 55/116 loss: 0.0327\n",
      "Epoch 188: Batch 56/116 loss: 0.0424\n",
      "Epoch 188: Batch 57/116 loss: 0.0614\n",
      "Epoch 188: Batch 58/116 loss: 0.0412\n",
      "Epoch 188: Batch 59/116 loss: 0.0368\n",
      "Epoch 188: Batch 60/116 loss: 0.0442\n",
      "Epoch 188: Batch 61/116 loss: 0.0281\n",
      "Epoch 188: Batch 62/116 loss: 0.0435\n",
      "Epoch 188: Batch 63/116 loss: 0.0316\n",
      "Epoch 188: Batch 64/116 loss: 0.0440\n",
      "Epoch 188: Batch 65/116 loss: 0.0384\n",
      "Epoch 188: Batch 66/116 loss: 0.0340\n",
      "Epoch 188: Batch 67/116 loss: 0.0433\n",
      "Epoch 188: Batch 68/116 loss: 0.0292\n",
      "Epoch 188: Batch 69/116 loss: 0.0318\n",
      "Epoch 188: Batch 70/116 loss: 0.0286\n",
      "Epoch 188: Batch 71/116 loss: 0.0398\n",
      "Epoch 188: Batch 72/116 loss: 0.0356\n",
      "Epoch 188: Batch 73/116 loss: 0.0377\n",
      "Epoch 188: Batch 74/116 loss: 0.0442\n",
      "Epoch 188: Batch 75/116 loss: 0.0502\n",
      "Epoch 188: Batch 76/116 loss: 0.0401\n",
      "Epoch 188: Batch 77/116 loss: 0.0451\n",
      "Epoch 188: Batch 78/116 loss: 0.0480\n",
      "Epoch 188: Batch 79/116 loss: 0.0465\n",
      "Epoch 188: Batch 80/116 loss: 0.0415\n",
      "Epoch 188: Batch 81/116 loss: 0.0418\n",
      "Epoch 188: Batch 82/116 loss: 0.0374\n",
      "Epoch 188: Batch 83/116 loss: 0.0333\n",
      "Epoch 188: Batch 84/116 loss: 0.0262\n",
      "Epoch 188: Batch 85/116 loss: 0.0453\n",
      "Epoch 188: Batch 86/116 loss: 0.0468\n",
      "Epoch 188: Batch 87/116 loss: 0.0227\n",
      "Epoch 188: Batch 88/116 loss: 0.0590\n",
      "Epoch 188: Batch 89/116 loss: 0.0505\n",
      "Epoch 188: Batch 90/116 loss: 0.0311\n",
      "Epoch 188: Batch 91/116 loss: 0.0399\n",
      "Epoch 188: Batch 92/116 loss: 0.0413\n",
      "Epoch 188: Batch 93/116 loss: 0.0293\n",
      "Epoch 188: Batch 94/116 loss: 0.0471\n",
      "Epoch 188: Batch 95/116 loss: 0.0380\n",
      "Epoch 188: Batch 96/116 loss: 0.0349\n",
      "Epoch 188: Batch 97/116 loss: 0.0452\n",
      "Epoch 188: Batch 98/116 loss: 0.0487\n",
      "Epoch 188: Batch 99/116 loss: 0.0220\n",
      "Epoch 188: Batch 100/116 loss: 0.0516\n",
      "Epoch 188: Batch 101/116 loss: 0.0544\n",
      "Epoch 188: Batch 102/116 loss: 0.0547\n",
      "Epoch 188: Batch 103/116 loss: 0.0414\n",
      "Epoch 188: Batch 104/116 loss: 0.0395\n",
      "Epoch 188: Batch 105/116 loss: 0.0385\n",
      "Epoch 188: Batch 106/116 loss: 0.0414\n",
      "Epoch 188: Batch 107/116 loss: 0.0543\n",
      "Epoch 188: Batch 108/116 loss: 0.0336\n",
      "Epoch 188: Batch 109/116 loss: 0.0327\n",
      "Epoch 188: Batch 110/116 loss: 0.0290\n",
      "Epoch 188: Batch 111/116 loss: 0.0512\n",
      "Epoch 188: Batch 112/116 loss: 0.0265\n",
      "Epoch 188: Batch 113/116 loss: 0.0541\n",
      "Epoch 188: Batch 114/116 loss: 0.0428\n",
      "Epoch 188: Batch 115/116 loss: 0.0479\n",
      "Epoch 188: Batch 116/116 loss: 0.0275\n",
      "Epoch 188 train loss: 0.0408 valid loss: 0.0537\n",
      "performance reducing: 8\n",
      "Epoch 189: Batch 1/116 loss: 0.0570\n",
      "Epoch 189: Batch 2/116 loss: 0.0393\n",
      "Epoch 189: Batch 3/116 loss: 0.0406\n",
      "Epoch 189: Batch 4/116 loss: 0.0346\n",
      "Epoch 189: Batch 5/116 loss: 0.0223\n",
      "Epoch 189: Batch 6/116 loss: 0.0470\n",
      "Epoch 189: Batch 7/116 loss: 0.0464\n",
      "Epoch 189: Batch 8/116 loss: 0.0230\n",
      "Epoch 189: Batch 9/116 loss: 0.0319\n",
      "Epoch 189: Batch 10/116 loss: 0.0269\n",
      "Epoch 189: Batch 11/116 loss: 0.0414\n",
      "Epoch 189: Batch 12/116 loss: 0.0618\n",
      "Epoch 189: Batch 13/116 loss: 0.0495\n",
      "Epoch 189: Batch 14/116 loss: 0.0323\n",
      "Epoch 189: Batch 15/116 loss: 0.0758\n",
      "Epoch 189: Batch 16/116 loss: 0.0480\n",
      "Epoch 189: Batch 17/116 loss: 0.0345\n",
      "Epoch 189: Batch 18/116 loss: 0.0301\n",
      "Epoch 189: Batch 19/116 loss: 0.0311\n",
      "Epoch 189: Batch 20/116 loss: 0.0317\n",
      "Epoch 189: Batch 21/116 loss: 0.0344\n",
      "Epoch 189: Batch 22/116 loss: 0.0388\n",
      "Epoch 189: Batch 23/116 loss: 0.0262\n",
      "Epoch 189: Batch 24/116 loss: 0.0333\n",
      "Epoch 189: Batch 25/116 loss: 0.0511\n",
      "Epoch 189: Batch 26/116 loss: 0.0336\n",
      "Epoch 189: Batch 27/116 loss: 0.0540\n",
      "Epoch 189: Batch 28/116 loss: 0.0332\n",
      "Epoch 189: Batch 29/116 loss: 0.0417\n",
      "Epoch 189: Batch 30/116 loss: 0.0255\n",
      "Epoch 189: Batch 31/116 loss: 0.0505\n",
      "Epoch 189: Batch 32/116 loss: 0.0302\n",
      "Epoch 189: Batch 33/116 loss: 0.0272\n",
      "Epoch 189: Batch 34/116 loss: 0.0347\n",
      "Epoch 189: Batch 35/116 loss: 0.0295\n",
      "Epoch 189: Batch 36/116 loss: 0.0316\n",
      "Epoch 189: Batch 37/116 loss: 0.0455\n",
      "Epoch 189: Batch 38/116 loss: 0.0575\n",
      "Epoch 189: Batch 39/116 loss: 0.0405\n",
      "Epoch 189: Batch 40/116 loss: 0.0310\n",
      "Epoch 189: Batch 41/116 loss: 0.0480\n",
      "Epoch 189: Batch 42/116 loss: 0.0489\n",
      "Epoch 189: Batch 43/116 loss: 0.0319\n",
      "Epoch 189: Batch 44/116 loss: 0.0543\n",
      "Epoch 189: Batch 45/116 loss: 0.0376\n",
      "Epoch 189: Batch 46/116 loss: 0.0444\n",
      "Epoch 189: Batch 47/116 loss: 0.0405\n",
      "Epoch 189: Batch 48/116 loss: 0.0340\n",
      "Epoch 189: Batch 49/116 loss: 0.0312\n",
      "Epoch 189: Batch 50/116 loss: 0.0323\n",
      "Epoch 189: Batch 51/116 loss: 0.0248\n",
      "Epoch 189: Batch 52/116 loss: 0.0583\n",
      "Epoch 189: Batch 53/116 loss: 0.0334\n",
      "Epoch 189: Batch 54/116 loss: 0.0452\n",
      "Epoch 189: Batch 55/116 loss: 0.0441\n",
      "Epoch 189: Batch 56/116 loss: 0.0357\n",
      "Epoch 189: Batch 57/116 loss: 0.0484\n",
      "Epoch 189: Batch 58/116 loss: 0.0436\n",
      "Epoch 189: Batch 59/116 loss: 0.0390\n",
      "Epoch 189: Batch 60/116 loss: 0.0398\n",
      "Epoch 189: Batch 61/116 loss: 0.0468\n",
      "Epoch 189: Batch 62/116 loss: 0.0253\n",
      "Epoch 189: Batch 63/116 loss: 0.0392\n",
      "Epoch 189: Batch 64/116 loss: 0.0372\n",
      "Epoch 189: Batch 65/116 loss: 0.0380\n",
      "Epoch 189: Batch 66/116 loss: 0.0422\n",
      "Epoch 189: Batch 67/116 loss: 0.0542\n",
      "Epoch 189: Batch 68/116 loss: 0.0621\n",
      "Epoch 189: Batch 69/116 loss: 0.0298\n",
      "Epoch 189: Batch 70/116 loss: 0.0375\n",
      "Epoch 189: Batch 71/116 loss: 0.0514\n",
      "Epoch 189: Batch 72/116 loss: 0.0340\n",
      "Epoch 189: Batch 73/116 loss: 0.0274\n",
      "Epoch 189: Batch 74/116 loss: 0.0363\n",
      "Epoch 189: Batch 75/116 loss: 0.0352\n",
      "Epoch 189: Batch 76/116 loss: 0.0447\n",
      "Epoch 189: Batch 77/116 loss: 0.0291\n",
      "Epoch 189: Batch 78/116 loss: 0.0578\n",
      "Epoch 189: Batch 79/116 loss: 0.0472\n",
      "Epoch 189: Batch 80/116 loss: 0.0489\n",
      "Epoch 189: Batch 81/116 loss: 0.0228\n",
      "Epoch 189: Batch 82/116 loss: 0.0348\n",
      "Epoch 189: Batch 83/116 loss: 0.0389\n",
      "Epoch 189: Batch 84/116 loss: 0.0318\n",
      "Epoch 189: Batch 85/116 loss: 0.0361\n",
      "Epoch 189: Batch 86/116 loss: 0.0461\n",
      "Epoch 189: Batch 87/116 loss: 0.0435\n",
      "Epoch 189: Batch 88/116 loss: 0.0350\n",
      "Epoch 189: Batch 89/116 loss: 0.0333\n",
      "Epoch 189: Batch 90/116 loss: 0.0331\n",
      "Epoch 189: Batch 91/116 loss: 0.0398\n",
      "Epoch 189: Batch 92/116 loss: 0.0299\n",
      "Epoch 189: Batch 93/116 loss: 0.0256\n",
      "Epoch 189: Batch 94/116 loss: 0.0253\n",
      "Epoch 189: Batch 95/116 loss: 0.0426\n",
      "Epoch 189: Batch 96/116 loss: 0.0362\n",
      "Epoch 189: Batch 97/116 loss: 0.0500\n",
      "Epoch 189: Batch 98/116 loss: 0.0435\n",
      "Epoch 189: Batch 99/116 loss: 0.0309\n",
      "Epoch 189: Batch 100/116 loss: 0.0480\n",
      "Epoch 189: Batch 101/116 loss: 0.0380\n",
      "Epoch 189: Batch 102/116 loss: 0.0273\n",
      "Epoch 189: Batch 103/116 loss: 0.0280\n",
      "Epoch 189: Batch 104/116 loss: 0.0421\n",
      "Epoch 189: Batch 105/116 loss: 0.0468\n",
      "Epoch 189: Batch 106/116 loss: 0.0290\n",
      "Epoch 189: Batch 107/116 loss: 0.0615\n",
      "Epoch 189: Batch 108/116 loss: 0.0368\n",
      "Epoch 189: Batch 109/116 loss: 0.0510\n",
      "Epoch 189: Batch 110/116 loss: 0.0473\n",
      "Epoch 189: Batch 111/116 loss: 0.0262\n",
      "Epoch 189: Batch 112/116 loss: 0.0554\n",
      "Epoch 189: Batch 113/116 loss: 0.0358\n",
      "Epoch 189: Batch 114/116 loss: 0.0544\n",
      "Epoch 189: Batch 115/116 loss: 0.0303\n",
      "Epoch 189: Batch 116/116 loss: 0.0419\n",
      "Epoch 189 train loss: 0.0394 valid loss: 0.0610\n",
      "performance reducing: 9\n",
      "Epoch 190: Batch 1/116 loss: 0.0283\n",
      "Epoch 190: Batch 2/116 loss: 0.0462\n",
      "Epoch 190: Batch 3/116 loss: 0.0243\n",
      "Epoch 190: Batch 4/116 loss: 0.0341\n",
      "Epoch 190: Batch 5/116 loss: 0.0495\n",
      "Epoch 190: Batch 6/116 loss: 0.0546\n",
      "Epoch 190: Batch 7/116 loss: 0.0385\n",
      "Epoch 190: Batch 8/116 loss: 0.0406\n",
      "Epoch 190: Batch 9/116 loss: 0.0416\n",
      "Epoch 190: Batch 10/116 loss: 0.0370\n",
      "Epoch 190: Batch 11/116 loss: 0.0232\n",
      "Epoch 190: Batch 12/116 loss: 0.0484\n",
      "Epoch 190: Batch 13/116 loss: 0.0394\n",
      "Epoch 190: Batch 14/116 loss: 0.0363\n",
      "Epoch 190: Batch 15/116 loss: 0.0371\n",
      "Epoch 190: Batch 16/116 loss: 0.0552\n",
      "Epoch 190: Batch 17/116 loss: 0.0553\n",
      "Epoch 190: Batch 18/116 loss: 0.0454\n",
      "Epoch 190: Batch 19/116 loss: 0.0317\n",
      "Epoch 190: Batch 20/116 loss: 0.0403\n",
      "Epoch 190: Batch 21/116 loss: 0.0504\n",
      "Epoch 190: Batch 22/116 loss: 0.0539\n",
      "Epoch 190: Batch 23/116 loss: 0.0467\n",
      "Epoch 190: Batch 24/116 loss: 0.0425\n",
      "Epoch 190: Batch 25/116 loss: 0.0504\n",
      "Epoch 190: Batch 26/116 loss: 0.0500\n",
      "Epoch 190: Batch 27/116 loss: 0.0241\n",
      "Epoch 190: Batch 28/116 loss: 0.0341\n",
      "Epoch 190: Batch 29/116 loss: 0.0529\n",
      "Epoch 190: Batch 30/116 loss: 0.0235\n",
      "Epoch 190: Batch 31/116 loss: 0.0374\n",
      "Epoch 190: Batch 32/116 loss: 0.0388\n",
      "Epoch 190: Batch 33/116 loss: 0.0438\n",
      "Epoch 190: Batch 34/116 loss: 0.0265\n",
      "Epoch 190: Batch 35/116 loss: 0.0393\n",
      "Epoch 190: Batch 36/116 loss: 0.0433\n",
      "Epoch 190: Batch 37/116 loss: 0.0342\n",
      "Epoch 190: Batch 38/116 loss: 0.0418\n",
      "Epoch 190: Batch 39/116 loss: 0.0353\n",
      "Epoch 190: Batch 40/116 loss: 0.0333\n",
      "Epoch 190: Batch 41/116 loss: 0.0358\n",
      "Epoch 190: Batch 42/116 loss: 0.0325\n",
      "Epoch 190: Batch 43/116 loss: 0.0217\n",
      "Epoch 190: Batch 44/116 loss: 0.0287\n",
      "Epoch 190: Batch 45/116 loss: 0.0300\n",
      "Epoch 190: Batch 46/116 loss: 0.0429\n",
      "Epoch 190: Batch 47/116 loss: 0.0332\n",
      "Epoch 190: Batch 48/116 loss: 0.0379\n",
      "Epoch 190: Batch 49/116 loss: 0.0452\n",
      "Epoch 190: Batch 50/116 loss: 0.0329\n",
      "Epoch 190: Batch 51/116 loss: 0.0435\n",
      "Epoch 190: Batch 52/116 loss: 0.0300\n",
      "Epoch 190: Batch 53/116 loss: 0.0310\n",
      "Epoch 190: Batch 54/116 loss: 0.0144\n",
      "Epoch 190: Batch 55/116 loss: 0.0453\n",
      "Epoch 190: Batch 56/116 loss: 0.0417\n",
      "Epoch 190: Batch 57/116 loss: 0.0346\n",
      "Epoch 190: Batch 58/116 loss: 0.0344\n",
      "Epoch 190: Batch 59/116 loss: 0.0584\n",
      "Epoch 190: Batch 60/116 loss: 0.0425\n",
      "Epoch 190: Batch 61/116 loss: 0.0315\n",
      "Epoch 190: Batch 62/116 loss: 0.0462\n",
      "Epoch 190: Batch 63/116 loss: 0.0306\n",
      "Epoch 190: Batch 64/116 loss: 0.0376\n",
      "Epoch 190: Batch 65/116 loss: 0.0312\n",
      "Epoch 190: Batch 66/116 loss: 0.0407\n",
      "Epoch 190: Batch 67/116 loss: 0.0315\n",
      "Epoch 190: Batch 68/116 loss: 0.0397\n",
      "Epoch 190: Batch 69/116 loss: 0.0444\n",
      "Epoch 190: Batch 70/116 loss: 0.0283\n",
      "Epoch 190: Batch 71/116 loss: 0.0376\n",
      "Epoch 190: Batch 72/116 loss: 0.0348\n",
      "Epoch 190: Batch 73/116 loss: 0.0431\n",
      "Epoch 190: Batch 74/116 loss: 0.0304\n",
      "Epoch 190: Batch 75/116 loss: 0.0284\n",
      "Epoch 190: Batch 76/116 loss: 0.0385\n",
      "Epoch 190: Batch 77/116 loss: 0.0349\n",
      "Epoch 190: Batch 78/116 loss: 0.0487\n",
      "Epoch 190: Batch 79/116 loss: 0.0315\n",
      "Epoch 190: Batch 80/116 loss: 0.0319\n",
      "Epoch 190: Batch 81/116 loss: 0.0323\n",
      "Epoch 190: Batch 82/116 loss: 0.0268\n",
      "Epoch 190: Batch 83/116 loss: 0.0395\n",
      "Epoch 190: Batch 84/116 loss: 0.0385\n",
      "Epoch 190: Batch 85/116 loss: 0.0343\n",
      "Epoch 190: Batch 86/116 loss: 0.0353\n",
      "Epoch 190: Batch 87/116 loss: 0.0444\n",
      "Epoch 190: Batch 88/116 loss: 0.0361\n",
      "Epoch 190: Batch 89/116 loss: 0.0449\n",
      "Epoch 190: Batch 90/116 loss: 0.0349\n",
      "Epoch 190: Batch 91/116 loss: 0.0412\n",
      "Epoch 190: Batch 92/116 loss: 0.0426\n",
      "Epoch 190: Batch 93/116 loss: 0.0635\n",
      "Epoch 190: Batch 94/116 loss: 0.0376\n",
      "Epoch 190: Batch 95/116 loss: 0.0528\n",
      "Epoch 190: Batch 96/116 loss: 0.0327\n",
      "Epoch 190: Batch 97/116 loss: 0.0526\n",
      "Epoch 190: Batch 98/116 loss: 0.0536\n",
      "Epoch 190: Batch 99/116 loss: 0.0411\n",
      "Epoch 190: Batch 100/116 loss: 0.0445\n",
      "Epoch 190: Batch 101/116 loss: 0.0408\n",
      "Epoch 190: Batch 102/116 loss: 0.0223\n",
      "Epoch 190: Batch 103/116 loss: 0.0402\n",
      "Epoch 190: Batch 104/116 loss: 0.0290\n",
      "Epoch 190: Batch 105/116 loss: 0.0214\n",
      "Epoch 190: Batch 106/116 loss: 0.0238\n",
      "Epoch 190: Batch 107/116 loss: 0.0392\n",
      "Epoch 190: Batch 108/116 loss: 0.0411\n",
      "Epoch 190: Batch 109/116 loss: 0.0302\n",
      "Epoch 190: Batch 110/116 loss: 0.0470\n",
      "Epoch 190: Batch 111/116 loss: 0.0410\n",
      "Epoch 190: Batch 112/116 loss: 0.0344\n",
      "Epoch 190: Batch 113/116 loss: 0.0347\n",
      "Epoch 190: Batch 114/116 loss: 0.0538\n",
      "Epoch 190: Batch 115/116 loss: 0.0505\n",
      "Epoch 190: Batch 116/116 loss: 0.0374\n",
      "Epoch 190 train loss: 0.0385 valid loss: 0.0490\n",
      "performance reducing: 10\n",
      "Epoch 191: Batch 1/116 loss: 0.0332\n",
      "Epoch 191: Batch 2/116 loss: 0.0337\n",
      "Epoch 191: Batch 3/116 loss: 0.0376\n",
      "Epoch 191: Batch 4/116 loss: 0.0445\n",
      "Epoch 191: Batch 5/116 loss: 0.0381\n",
      "Epoch 191: Batch 6/116 loss: 0.0319\n",
      "Epoch 191: Batch 7/116 loss: 0.0396\n",
      "Epoch 191: Batch 8/116 loss: 0.0437\n",
      "Epoch 191: Batch 9/116 loss: 0.0310\n",
      "Epoch 191: Batch 10/116 loss: 0.0541\n",
      "Epoch 191: Batch 11/116 loss: 0.0314\n",
      "Epoch 191: Batch 12/116 loss: 0.0241\n",
      "Epoch 191: Batch 13/116 loss: 0.0284\n",
      "Epoch 191: Batch 14/116 loss: 0.0358\n",
      "Epoch 191: Batch 15/116 loss: 0.0256\n",
      "Epoch 191: Batch 16/116 loss: 0.0297\n",
      "Epoch 191: Batch 17/116 loss: 0.0292\n",
      "Epoch 191: Batch 18/116 loss: 0.0605\n",
      "Epoch 191: Batch 19/116 loss: 0.0392\n",
      "Epoch 191: Batch 20/116 loss: 0.0431\n",
      "Epoch 191: Batch 21/116 loss: 0.0365\n",
      "Epoch 191: Batch 22/116 loss: 0.0694\n",
      "Epoch 191: Batch 23/116 loss: 0.0306\n",
      "Epoch 191: Batch 24/116 loss: 0.0276\n",
      "Epoch 191: Batch 25/116 loss: 0.0339\n",
      "Epoch 191: Batch 26/116 loss: 0.0479\n",
      "Epoch 191: Batch 27/116 loss: 0.0435\n",
      "Epoch 191: Batch 28/116 loss: 0.0434\n",
      "Epoch 191: Batch 29/116 loss: 0.0396\n",
      "Epoch 191: Batch 30/116 loss: 0.0234\n",
      "Epoch 191: Batch 31/116 loss: 0.0292\n",
      "Epoch 191: Batch 32/116 loss: 0.0451\n",
      "Epoch 191: Batch 33/116 loss: 0.0478\n",
      "Epoch 191: Batch 34/116 loss: 0.0235\n",
      "Epoch 191: Batch 35/116 loss: 0.0362\n",
      "Epoch 191: Batch 36/116 loss: 0.0373\n",
      "Epoch 191: Batch 37/116 loss: 0.0495\n",
      "Epoch 191: Batch 38/116 loss: 0.0372\n",
      "Epoch 191: Batch 39/116 loss: 0.0443\n",
      "Epoch 191: Batch 40/116 loss: 0.0623\n",
      "Epoch 191: Batch 41/116 loss: 0.0341\n",
      "Epoch 191: Batch 42/116 loss: 0.0381\n",
      "Epoch 191: Batch 43/116 loss: 0.0374\n",
      "Epoch 191: Batch 44/116 loss: 0.0434\n",
      "Epoch 191: Batch 45/116 loss: 0.0367\n",
      "Epoch 191: Batch 46/116 loss: 0.0415\n",
      "Epoch 191: Batch 47/116 loss: 0.0309\n",
      "Epoch 191: Batch 48/116 loss: 0.0513\n",
      "Epoch 191: Batch 49/116 loss: 0.0410\n",
      "Epoch 191: Batch 50/116 loss: 0.0513\n",
      "Epoch 191: Batch 51/116 loss: 0.0453\n",
      "Epoch 191: Batch 52/116 loss: 0.0260\n",
      "Epoch 191: Batch 53/116 loss: 0.0549\n",
      "Epoch 191: Batch 54/116 loss: 0.0291\n",
      "Epoch 191: Batch 55/116 loss: 0.0408\n",
      "Epoch 191: Batch 56/116 loss: 0.0427\n",
      "Epoch 191: Batch 57/116 loss: 0.0411\n",
      "Epoch 191: Batch 58/116 loss: 0.0346\n",
      "Epoch 191: Batch 59/116 loss: 0.0426\n",
      "Epoch 191: Batch 60/116 loss: 0.0258\n",
      "Epoch 191: Batch 61/116 loss: 0.0368\n",
      "Epoch 191: Batch 62/116 loss: 0.0434\n",
      "Epoch 191: Batch 63/116 loss: 0.0445\n",
      "Epoch 191: Batch 64/116 loss: 0.0588\n",
      "Epoch 191: Batch 65/116 loss: 0.0537\n",
      "Epoch 191: Batch 66/116 loss: 0.0541\n",
      "Epoch 191: Batch 67/116 loss: 0.0266\n",
      "Epoch 191: Batch 68/116 loss: 0.0389\n",
      "Epoch 191: Batch 69/116 loss: 0.0576\n",
      "Epoch 191: Batch 70/116 loss: 0.0375\n",
      "Epoch 191: Batch 71/116 loss: 0.0365\n",
      "Epoch 191: Batch 72/116 loss: 0.0452\n",
      "Epoch 191: Batch 73/116 loss: 0.0429\n",
      "Epoch 191: Batch 74/116 loss: 0.0546\n",
      "Epoch 191: Batch 75/116 loss: 0.0449\n",
      "Epoch 191: Batch 76/116 loss: 0.0267\n",
      "Epoch 191: Batch 77/116 loss: 0.0437\n",
      "Epoch 191: Batch 78/116 loss: 0.0534\n",
      "Epoch 191: Batch 79/116 loss: 0.0402\n",
      "Epoch 191: Batch 80/116 loss: 0.0337\n",
      "Epoch 191: Batch 81/116 loss: 0.0446\n",
      "Epoch 191: Batch 82/116 loss: 0.0294\n",
      "Epoch 191: Batch 83/116 loss: 0.0452\n",
      "Epoch 191: Batch 84/116 loss: 0.0368\n",
      "Epoch 191: Batch 85/116 loss: 0.0359\n",
      "Epoch 191: Batch 86/116 loss: 0.0372\n",
      "Epoch 191: Batch 87/116 loss: 0.0294\n",
      "Epoch 191: Batch 88/116 loss: 0.0261\n",
      "Epoch 191: Batch 89/116 loss: 0.0504\n",
      "Epoch 191: Batch 90/116 loss: 0.0281\n",
      "Epoch 191: Batch 91/116 loss: 0.0380\n",
      "Epoch 191: Batch 92/116 loss: 0.0407\n",
      "Epoch 191: Batch 93/116 loss: 0.0442\n",
      "Epoch 191: Batch 94/116 loss: 0.0427\n",
      "Epoch 191: Batch 95/116 loss: 0.0284\n",
      "Epoch 191: Batch 96/116 loss: 0.0523\n",
      "Epoch 191: Batch 97/116 loss: 0.0306\n",
      "Epoch 191: Batch 98/116 loss: 0.0409\n",
      "Epoch 191: Batch 99/116 loss: 0.0626\n",
      "Epoch 191: Batch 100/116 loss: 0.0319\n",
      "Epoch 191: Batch 101/116 loss: 0.0408\n",
      "Epoch 191: Batch 102/116 loss: 0.0262\n",
      "Epoch 191: Batch 103/116 loss: 0.0359\n",
      "Epoch 191: Batch 104/116 loss: 0.0419\n",
      "Epoch 191: Batch 105/116 loss: 0.0490\n",
      "Epoch 191: Batch 106/116 loss: 0.0382\n",
      "Epoch 191: Batch 107/116 loss: 0.0261\n",
      "Epoch 191: Batch 108/116 loss: 0.0225\n",
      "Epoch 191: Batch 109/116 loss: 0.0447\n",
      "Epoch 191: Batch 110/116 loss: 0.0434\n",
      "Epoch 191: Batch 111/116 loss: 0.0633\n",
      "Epoch 191: Batch 112/116 loss: 0.0583\n",
      "Epoch 191: Batch 113/116 loss: 0.0421\n",
      "Epoch 191: Batch 114/116 loss: 0.0536\n",
      "Epoch 191: Batch 115/116 loss: 0.0390\n",
      "Epoch 191: Batch 116/116 loss: 0.0486\n",
      "Epoch 191 train loss: 0.0401 valid loss: 0.0588\n",
      "performance reducing: 11\n",
      "Epoch 192: Batch 1/116 loss: 0.0374\n",
      "Epoch 192: Batch 2/116 loss: 0.0428\n",
      "Epoch 192: Batch 3/116 loss: 0.0364\n",
      "Epoch 192: Batch 4/116 loss: 0.0439\n",
      "Epoch 192: Batch 5/116 loss: 0.0310\n",
      "Epoch 192: Batch 6/116 loss: 0.0378\n",
      "Epoch 192: Batch 7/116 loss: 0.0283\n",
      "Epoch 192: Batch 8/116 loss: 0.0342\n",
      "Epoch 192: Batch 9/116 loss: 0.0401\n",
      "Epoch 192: Batch 10/116 loss: 0.0277\n",
      "Epoch 192: Batch 11/116 loss: 0.0289\n",
      "Epoch 192: Batch 12/116 loss: 0.0405\n",
      "Epoch 192: Batch 13/116 loss: 0.0389\n",
      "Epoch 192: Batch 14/116 loss: 0.0282\n",
      "Epoch 192: Batch 15/116 loss: 0.0348\n",
      "Epoch 192: Batch 16/116 loss: 0.0475\n",
      "Epoch 192: Batch 17/116 loss: 0.0486\n",
      "Epoch 192: Batch 18/116 loss: 0.0380\n",
      "Epoch 192: Batch 19/116 loss: 0.0237\n",
      "Epoch 192: Batch 20/116 loss: 0.0473\n",
      "Epoch 192: Batch 21/116 loss: 0.0357\n",
      "Epoch 192: Batch 22/116 loss: 0.0353\n",
      "Epoch 192: Batch 23/116 loss: 0.0445\n",
      "Epoch 192: Batch 24/116 loss: 0.0256\n",
      "Epoch 192: Batch 25/116 loss: 0.0299\n",
      "Epoch 192: Batch 26/116 loss: 0.0274\n",
      "Epoch 192: Batch 27/116 loss: 0.0601\n",
      "Epoch 192: Batch 28/116 loss: 0.0594\n",
      "Epoch 192: Batch 29/116 loss: 0.0589\n",
      "Epoch 192: Batch 30/116 loss: 0.0954\n",
      "Epoch 192: Batch 31/116 loss: 0.0378\n",
      "Epoch 192: Batch 32/116 loss: 0.0886\n",
      "Epoch 192: Batch 33/116 loss: 0.0504\n",
      "Epoch 192: Batch 34/116 loss: 0.0622\n",
      "Epoch 192: Batch 35/116 loss: 0.0636\n",
      "Epoch 192: Batch 36/116 loss: 0.0729\n",
      "Epoch 192: Batch 37/116 loss: 0.0520\n",
      "Epoch 192: Batch 38/116 loss: 0.0554\n",
      "Epoch 192: Batch 39/116 loss: 0.0546\n",
      "Epoch 192: Batch 40/116 loss: 0.0689\n",
      "Epoch 192: Batch 41/116 loss: 0.0367\n",
      "Epoch 192: Batch 42/116 loss: 0.0512\n",
      "Epoch 192: Batch 43/116 loss: 0.0551\n",
      "Epoch 192: Batch 44/116 loss: 0.0348\n",
      "Epoch 192: Batch 45/116 loss: 0.0453\n",
      "Epoch 192: Batch 46/116 loss: 0.0603\n",
      "Epoch 192: Batch 47/116 loss: 0.0429\n",
      "Epoch 192: Batch 48/116 loss: 0.0400\n",
      "Epoch 192: Batch 49/116 loss: 0.0611\n",
      "Epoch 192: Batch 50/116 loss: 0.0525\n",
      "Epoch 192: Batch 51/116 loss: 0.0488\n",
      "Epoch 192: Batch 52/116 loss: 0.0449\n",
      "Epoch 192: Batch 53/116 loss: 0.0359\n",
      "Epoch 192: Batch 54/116 loss: 0.0581\n",
      "Epoch 192: Batch 55/116 loss: 0.0358\n",
      "Epoch 192: Batch 56/116 loss: 0.0365\n",
      "Epoch 192: Batch 57/116 loss: 0.0626\n",
      "Epoch 192: Batch 58/116 loss: 0.0409\n",
      "Epoch 192: Batch 59/116 loss: 0.0292\n",
      "Epoch 192: Batch 60/116 loss: 0.0498\n",
      "Epoch 192: Batch 61/116 loss: 0.0363\n",
      "Epoch 192: Batch 62/116 loss: 0.0395\n",
      "Epoch 192: Batch 63/116 loss: 0.0408\n",
      "Epoch 192: Batch 64/116 loss: 0.0368\n",
      "Epoch 192: Batch 65/116 loss: 0.0499\n",
      "Epoch 192: Batch 66/116 loss: 0.0407\n",
      "Epoch 192: Batch 67/116 loss: 0.0355\n",
      "Epoch 192: Batch 68/116 loss: 0.0395\n",
      "Epoch 192: Batch 69/116 loss: 0.0597\n",
      "Epoch 192: Batch 70/116 loss: 0.0551\n",
      "Epoch 192: Batch 71/116 loss: 0.0457\n",
      "Epoch 192: Batch 72/116 loss: 0.0300\n",
      "Epoch 192: Batch 73/116 loss: 0.0543\n",
      "Epoch 192: Batch 74/116 loss: 0.0367\n",
      "Epoch 192: Batch 75/116 loss: 0.0323\n",
      "Epoch 192: Batch 76/116 loss: 0.0299\n",
      "Epoch 192: Batch 77/116 loss: 0.0465\n",
      "Epoch 192: Batch 78/116 loss: 0.0546\n",
      "Epoch 192: Batch 79/116 loss: 0.0640\n",
      "Epoch 192: Batch 80/116 loss: 0.0370\n",
      "Epoch 192: Batch 81/116 loss: 0.0544\n",
      "Epoch 192: Batch 82/116 loss: 0.0353\n",
      "Epoch 192: Batch 83/116 loss: 0.0445\n",
      "Epoch 192: Batch 84/116 loss: 0.0656\n",
      "Epoch 192: Batch 85/116 loss: 0.0469\n",
      "Epoch 192: Batch 86/116 loss: 0.0416\n",
      "Epoch 192: Batch 87/116 loss: 0.0432\n",
      "Epoch 192: Batch 88/116 loss: 0.0395\n",
      "Epoch 192: Batch 89/116 loss: 0.0499\n",
      "Epoch 192: Batch 90/116 loss: 0.0337\n",
      "Epoch 192: Batch 91/116 loss: 0.0340\n",
      "Epoch 192: Batch 92/116 loss: 0.0339\n",
      "Epoch 192: Batch 93/116 loss: 0.0400\n",
      "Epoch 192: Batch 94/116 loss: 0.0298\n",
      "Epoch 192: Batch 95/116 loss: 0.0329\n",
      "Epoch 192: Batch 96/116 loss: 0.0420\n",
      "Epoch 192: Batch 97/116 loss: 0.0355\n",
      "Epoch 192: Batch 98/116 loss: 0.0494\n",
      "Epoch 192: Batch 99/116 loss: 0.0389\n",
      "Epoch 192: Batch 100/116 loss: 0.0539\n",
      "Epoch 192: Batch 101/116 loss: 0.0551\n",
      "Epoch 192: Batch 102/116 loss: 0.0350\n",
      "Epoch 192: Batch 103/116 loss: 0.0423\n",
      "Epoch 192: Batch 104/116 loss: 0.0310\n",
      "Epoch 192: Batch 105/116 loss: 0.0424\n",
      "Epoch 192: Batch 106/116 loss: 0.0378\n",
      "Epoch 192: Batch 107/116 loss: 0.0382\n",
      "Epoch 192: Batch 108/116 loss: 0.0272\n",
      "Epoch 192: Batch 109/116 loss: 0.0338\n",
      "Epoch 192: Batch 110/116 loss: 0.0391\n",
      "Epoch 192: Batch 111/116 loss: 0.0380\n",
      "Epoch 192: Batch 112/116 loss: 0.0386\n",
      "Epoch 192: Batch 113/116 loss: 0.0470\n",
      "Epoch 192: Batch 114/116 loss: 0.0253\n",
      "Epoch 192: Batch 115/116 loss: 0.0318\n",
      "Epoch 192: Batch 116/116 loss: 0.0678\n",
      "Epoch 192 train loss: 0.0438 valid loss: 0.0535\n",
      "performance reducing: 12\n",
      "Epoch 193: Batch 1/116 loss: 0.0310\n",
      "Epoch 193: Batch 2/116 loss: 0.0379\n",
      "Epoch 193: Batch 3/116 loss: 0.0346\n",
      "Epoch 193: Batch 4/116 loss: 0.0399\n",
      "Epoch 193: Batch 5/116 loss: 0.0336\n",
      "Epoch 193: Batch 6/116 loss: 0.0414\n",
      "Epoch 193: Batch 7/116 loss: 0.0420\n",
      "Epoch 193: Batch 8/116 loss: 0.0432\n",
      "Epoch 193: Batch 9/116 loss: 0.0216\n",
      "Epoch 193: Batch 10/116 loss: 0.0318\n",
      "Epoch 193: Batch 11/116 loss: 0.0573\n",
      "Epoch 193: Batch 12/116 loss: 0.0508\n",
      "Epoch 193: Batch 13/116 loss: 0.0438\n",
      "Epoch 193: Batch 14/116 loss: 0.0546\n",
      "Epoch 193: Batch 15/116 loss: 0.0450\n",
      "Epoch 193: Batch 16/116 loss: 0.0591\n",
      "Epoch 193: Batch 17/116 loss: 0.0382\n",
      "Epoch 193: Batch 18/116 loss: 0.0270\n",
      "Epoch 193: Batch 19/116 loss: 0.0681\n",
      "Epoch 193: Batch 20/116 loss: 0.0293\n",
      "Epoch 193: Batch 21/116 loss: 0.0333\n",
      "Epoch 193: Batch 22/116 loss: 0.0271\n",
      "Epoch 193: Batch 23/116 loss: 0.0303\n",
      "Epoch 193: Batch 24/116 loss: 0.0264\n",
      "Epoch 193: Batch 25/116 loss: 0.0361\n",
      "Epoch 193: Batch 26/116 loss: 0.0521\n",
      "Epoch 193: Batch 27/116 loss: 0.0288\n",
      "Epoch 193: Batch 28/116 loss: 0.0360\n",
      "Epoch 193: Batch 29/116 loss: 0.0350\n",
      "Epoch 193: Batch 30/116 loss: 0.0507\n",
      "Epoch 193: Batch 31/116 loss: 0.0555\n",
      "Epoch 193: Batch 32/116 loss: 0.0357\n",
      "Epoch 193: Batch 33/116 loss: 0.0641\n",
      "Epoch 193: Batch 34/116 loss: 0.0287\n",
      "Epoch 193: Batch 35/116 loss: 0.0602\n",
      "Epoch 193: Batch 36/116 loss: 0.0632\n",
      "Epoch 193: Batch 37/116 loss: 0.0593\n",
      "Epoch 193: Batch 38/116 loss: 0.0363\n",
      "Epoch 193: Batch 39/116 loss: 0.0378\n",
      "Epoch 193: Batch 40/116 loss: 0.0371\n",
      "Epoch 193: Batch 41/116 loss: 0.0523\n",
      "Epoch 193: Batch 42/116 loss: 0.0343\n",
      "Epoch 193: Batch 43/116 loss: 0.0364\n",
      "Epoch 193: Batch 44/116 loss: 0.0500\n",
      "Epoch 193: Batch 45/116 loss: 0.0339\n",
      "Epoch 193: Batch 46/116 loss: 0.0596\n",
      "Epoch 193: Batch 47/116 loss: 0.0315\n",
      "Epoch 193: Batch 48/116 loss: 0.0407\n",
      "Epoch 193: Batch 49/116 loss: 0.0448\n",
      "Epoch 193: Batch 50/116 loss: 0.0392\n",
      "Epoch 193: Batch 51/116 loss: 0.0296\n",
      "Epoch 193: Batch 52/116 loss: 0.0499\n",
      "Epoch 193: Batch 53/116 loss: 0.0529\n",
      "Epoch 193: Batch 54/116 loss: 0.0376\n",
      "Epoch 193: Batch 55/116 loss: 0.0606\n",
      "Epoch 193: Batch 56/116 loss: 0.0404\n",
      "Epoch 193: Batch 57/116 loss: 0.0287\n",
      "Epoch 193: Batch 58/116 loss: 0.0569\n",
      "Epoch 193: Batch 59/116 loss: 0.0489\n",
      "Epoch 193: Batch 60/116 loss: 0.0494\n",
      "Epoch 193: Batch 61/116 loss: 0.0401\n",
      "Epoch 193: Batch 62/116 loss: 0.0332\n",
      "Epoch 193: Batch 63/116 loss: 0.0235\n",
      "Epoch 193: Batch 64/116 loss: 0.0388\n",
      "Epoch 193: Batch 65/116 loss: 0.0470\n",
      "Epoch 193: Batch 66/116 loss: 0.0517\n",
      "Epoch 193: Batch 67/116 loss: 0.0507\n",
      "Epoch 193: Batch 68/116 loss: 0.0301\n",
      "Epoch 193: Batch 69/116 loss: 0.0554\n",
      "Epoch 193: Batch 70/116 loss: 0.0306\n",
      "Epoch 193: Batch 71/116 loss: 0.0350\n",
      "Epoch 193: Batch 72/116 loss: 0.0302\n",
      "Epoch 193: Batch 73/116 loss: 0.0411\n",
      "Epoch 193: Batch 74/116 loss: 0.0511\n",
      "Epoch 193: Batch 75/116 loss: 0.0360\n",
      "Epoch 193: Batch 76/116 loss: 0.0353\n",
      "Epoch 193: Batch 77/116 loss: 0.0393\n",
      "Epoch 193: Batch 78/116 loss: 0.0450\n",
      "Epoch 193: Batch 79/116 loss: 0.0463\n",
      "Epoch 193: Batch 80/116 loss: 0.0517\n",
      "Epoch 193: Batch 81/116 loss: 0.0338\n",
      "Epoch 193: Batch 82/116 loss: 0.0399\n",
      "Epoch 193: Batch 83/116 loss: 0.0232\n",
      "Epoch 193: Batch 84/116 loss: 0.0409\n",
      "Epoch 193: Batch 85/116 loss: 0.0365\n",
      "Epoch 193: Batch 86/116 loss: 0.0387\n",
      "Epoch 193: Batch 87/116 loss: 0.0333\n",
      "Epoch 193: Batch 88/116 loss: 0.0236\n",
      "Epoch 193: Batch 89/116 loss: 0.0410\n",
      "Epoch 193: Batch 90/116 loss: 0.0257\n",
      "Epoch 193: Batch 91/116 loss: 0.0480\n",
      "Epoch 193: Batch 92/116 loss: 0.0277\n",
      "Epoch 193: Batch 93/116 loss: 0.0433\n",
      "Epoch 193: Batch 94/116 loss: 0.0266\n",
      "Epoch 193: Batch 95/116 loss: 0.0553\n",
      "Epoch 193: Batch 96/116 loss: 0.0352\n",
      "Epoch 193: Batch 97/116 loss: 0.0437\n",
      "Epoch 193: Batch 98/116 loss: 0.0423\n",
      "Epoch 193: Batch 99/116 loss: 0.0583\n",
      "Epoch 193: Batch 100/116 loss: 0.0551\n",
      "Epoch 193: Batch 101/116 loss: 0.0407\n",
      "Epoch 193: Batch 102/116 loss: 0.0372\n",
      "Epoch 193: Batch 103/116 loss: 0.0433\n",
      "Epoch 193: Batch 104/116 loss: 0.0409\n",
      "Epoch 193: Batch 105/116 loss: 0.0397\n",
      "Epoch 193: Batch 106/116 loss: 0.0506\n",
      "Epoch 193: Batch 107/116 loss: 0.0273\n",
      "Epoch 193: Batch 108/116 loss: 0.0347\n",
      "Epoch 193: Batch 109/116 loss: 0.0350\n",
      "Epoch 193: Batch 110/116 loss: 0.0345\n",
      "Epoch 193: Batch 111/116 loss: 0.0482\n",
      "Epoch 193: Batch 112/116 loss: 0.0522\n",
      "Epoch 193: Batch 113/116 loss: 0.0522\n",
      "Epoch 193: Batch 114/116 loss: 0.0433\n",
      "Epoch 193: Batch 115/116 loss: 0.0448\n",
      "Epoch 193: Batch 116/116 loss: 0.0368\n",
      "Epoch 193 train loss: 0.0413 valid loss: 0.0586\n",
      "performance reducing: 13\n",
      "Epoch 194: Batch 1/116 loss: 0.0261\n",
      "Epoch 194: Batch 2/116 loss: 0.0425\n",
      "Epoch 194: Batch 3/116 loss: 0.0515\n",
      "Epoch 194: Batch 4/116 loss: 0.0234\n",
      "Epoch 194: Batch 5/116 loss: 0.0354\n",
      "Epoch 194: Batch 6/116 loss: 0.0535\n",
      "Epoch 194: Batch 7/116 loss: 0.0498\n",
      "Epoch 194: Batch 8/116 loss: 0.0395\n",
      "Epoch 194: Batch 9/116 loss: 0.0461\n",
      "Epoch 194: Batch 10/116 loss: 0.0552\n",
      "Epoch 194: Batch 11/116 loss: 0.0540\n",
      "Epoch 194: Batch 12/116 loss: 0.0375\n",
      "Epoch 194: Batch 13/116 loss: 0.0432\n",
      "Epoch 194: Batch 14/116 loss: 0.0304\n",
      "Epoch 194: Batch 15/116 loss: 0.0445\n",
      "Epoch 194: Batch 16/116 loss: 0.0355\n",
      "Epoch 194: Batch 17/116 loss: 0.0356\n",
      "Epoch 194: Batch 18/116 loss: 0.0394\n",
      "Epoch 194: Batch 19/116 loss: 0.0285\n",
      "Epoch 194: Batch 20/116 loss: 0.0291\n",
      "Epoch 194: Batch 21/116 loss: 0.0326\n",
      "Epoch 194: Batch 22/116 loss: 0.0332\n",
      "Epoch 194: Batch 23/116 loss: 0.0448\n",
      "Epoch 194: Batch 24/116 loss: 0.0425\n",
      "Epoch 194: Batch 25/116 loss: 0.0332\n",
      "Epoch 194: Batch 26/116 loss: 0.0377\n",
      "Epoch 194: Batch 27/116 loss: 0.0306\n",
      "Epoch 194: Batch 28/116 loss: 0.0283\n",
      "Epoch 194: Batch 29/116 loss: 0.0272\n",
      "Epoch 194: Batch 30/116 loss: 0.0199\n",
      "Epoch 194: Batch 31/116 loss: 0.0406\n",
      "Epoch 194: Batch 32/116 loss: 0.0445\n",
      "Epoch 194: Batch 33/116 loss: 0.0467\n",
      "Epoch 194: Batch 34/116 loss: 0.0392\n",
      "Epoch 194: Batch 35/116 loss: 0.0336\n",
      "Epoch 194: Batch 36/116 loss: 0.0326\n",
      "Epoch 194: Batch 37/116 loss: 0.0588\n",
      "Epoch 194: Batch 38/116 loss: 0.0474\n",
      "Epoch 194: Batch 39/116 loss: 0.0439\n",
      "Epoch 194: Batch 40/116 loss: 0.0526\n",
      "Epoch 194: Batch 41/116 loss: 0.0303\n",
      "Epoch 194: Batch 42/116 loss: 0.0246\n",
      "Epoch 194: Batch 43/116 loss: 0.0360\n",
      "Epoch 194: Batch 44/116 loss: 0.0466\n",
      "Epoch 194: Batch 45/116 loss: 0.0568\n",
      "Epoch 194: Batch 46/116 loss: 0.0458\n",
      "Epoch 194: Batch 47/116 loss: 0.0648\n",
      "Epoch 194: Batch 48/116 loss: 0.0400\n",
      "Epoch 194: Batch 49/116 loss: 0.0489\n",
      "Epoch 194: Batch 50/116 loss: 0.0282\n",
      "Epoch 194: Batch 51/116 loss: 0.0478\n",
      "Epoch 194: Batch 52/116 loss: 0.0639\n",
      "Epoch 194: Batch 53/116 loss: 0.0414\n",
      "Epoch 194: Batch 54/116 loss: 0.0335\n",
      "Epoch 194: Batch 55/116 loss: 0.0422\n",
      "Epoch 194: Batch 56/116 loss: 0.0427\n",
      "Epoch 194: Batch 57/116 loss: 0.0427\n",
      "Epoch 194: Batch 58/116 loss: 0.0430\n",
      "Epoch 194: Batch 59/116 loss: 0.0400\n",
      "Epoch 194: Batch 60/116 loss: 0.0347\n",
      "Epoch 194: Batch 61/116 loss: 0.0315\n",
      "Epoch 194: Batch 62/116 loss: 0.0376\n",
      "Epoch 194: Batch 63/116 loss: 0.0274\n",
      "Epoch 194: Batch 64/116 loss: 0.0305\n",
      "Epoch 194: Batch 65/116 loss: 0.0596\n",
      "Epoch 194: Batch 66/116 loss: 0.0444\n",
      "Epoch 194: Batch 67/116 loss: 0.0519\n",
      "Epoch 194: Batch 68/116 loss: 0.0559\n",
      "Epoch 194: Batch 69/116 loss: 0.0404\n",
      "Epoch 194: Batch 70/116 loss: 0.0407\n",
      "Epoch 194: Batch 71/116 loss: 0.0413\n",
      "Epoch 194: Batch 72/116 loss: 0.0364\n",
      "Epoch 194: Batch 73/116 loss: 0.0352\n",
      "Epoch 194: Batch 74/116 loss: 0.0425\n",
      "Epoch 194: Batch 75/116 loss: 0.0409\n",
      "Epoch 194: Batch 76/116 loss: 0.0366\n",
      "Epoch 194: Batch 77/116 loss: 0.0365\n",
      "Epoch 194: Batch 78/116 loss: 0.0355\n",
      "Epoch 194: Batch 79/116 loss: 0.0382\n",
      "Epoch 194: Batch 80/116 loss: 0.0611\n",
      "Epoch 194: Batch 81/116 loss: 0.0490\n",
      "Epoch 194: Batch 82/116 loss: 0.0423\n",
      "Epoch 194: Batch 83/116 loss: 0.0488\n",
      "Epoch 194: Batch 84/116 loss: 0.0444\n",
      "Epoch 194: Batch 85/116 loss: 0.0239\n",
      "Epoch 194: Batch 86/116 loss: 0.0299\n",
      "Epoch 194: Batch 87/116 loss: 0.0366\n",
      "Epoch 194: Batch 88/116 loss: 0.0462\n",
      "Epoch 194: Batch 89/116 loss: 0.0301\n",
      "Epoch 194: Batch 90/116 loss: 0.0364\n",
      "Epoch 194: Batch 91/116 loss: 0.0506\n",
      "Epoch 194: Batch 92/116 loss: 0.0507\n",
      "Epoch 194: Batch 93/116 loss: 0.0555\n",
      "Epoch 194: Batch 94/116 loss: 0.0300\n",
      "Epoch 194: Batch 95/116 loss: 0.0253\n",
      "Epoch 194: Batch 96/116 loss: 0.0416\n",
      "Epoch 194: Batch 97/116 loss: 0.0439\n",
      "Epoch 194: Batch 98/116 loss: 0.0246\n",
      "Epoch 194: Batch 99/116 loss: 0.0412\n",
      "Epoch 194: Batch 100/116 loss: 0.0337\n",
      "Epoch 194: Batch 101/116 loss: 0.0230\n",
      "Epoch 194: Batch 102/116 loss: 0.0416\n",
      "Epoch 194: Batch 103/116 loss: 0.0386\n",
      "Epoch 194: Batch 104/116 loss: 0.0320\n",
      "Epoch 194: Batch 105/116 loss: 0.0529\n",
      "Epoch 194: Batch 106/116 loss: 0.0363\n",
      "Epoch 194: Batch 107/116 loss: 0.0320\n",
      "Epoch 194: Batch 108/116 loss: 0.0332\n",
      "Epoch 194: Batch 109/116 loss: 0.0353\n",
      "Epoch 194: Batch 110/116 loss: 0.0359\n",
      "Epoch 194: Batch 111/116 loss: 0.0255\n",
      "Epoch 194: Batch 112/116 loss: 0.0326\n",
      "Epoch 194: Batch 113/116 loss: 0.0440\n",
      "Epoch 194: Batch 114/116 loss: 0.0434\n",
      "Epoch 194: Batch 115/116 loss: 0.0378\n",
      "Epoch 194: Batch 116/116 loss: 0.0466\n",
      "Epoch 194 train loss: 0.0399 valid loss: 0.0520\n",
      "performance reducing: 14\n",
      "Epoch 195: Batch 1/116 loss: 0.0327\n",
      "Epoch 195: Batch 2/116 loss: 0.0251\n",
      "Epoch 195: Batch 3/116 loss: 0.0339\n",
      "Epoch 195: Batch 4/116 loss: 0.0380\n",
      "Epoch 195: Batch 5/116 loss: 0.0306\n",
      "Epoch 195: Batch 6/116 loss: 0.0495\n",
      "Epoch 195: Batch 7/116 loss: 0.0308\n",
      "Epoch 195: Batch 8/116 loss: 0.0299\n",
      "Epoch 195: Batch 9/116 loss: 0.0305\n",
      "Epoch 195: Batch 10/116 loss: 0.0313\n",
      "Epoch 195: Batch 11/116 loss: 0.0322\n",
      "Epoch 195: Batch 12/116 loss: 0.0246\n",
      "Epoch 195: Batch 13/116 loss: 0.0279\n",
      "Epoch 195: Batch 14/116 loss: 0.0341\n",
      "Epoch 195: Batch 15/116 loss: 0.0404\n",
      "Epoch 195: Batch 16/116 loss: 0.0267\n",
      "Epoch 195: Batch 17/116 loss: 0.0356\n",
      "Epoch 195: Batch 18/116 loss: 0.0346\n",
      "Epoch 195: Batch 19/116 loss: 0.0368\n",
      "Epoch 195: Batch 20/116 loss: 0.0441\n",
      "Epoch 195: Batch 21/116 loss: 0.0487\n",
      "Epoch 195: Batch 22/116 loss: 0.0314\n",
      "Epoch 195: Batch 23/116 loss: 0.0361\n",
      "Epoch 195: Batch 24/116 loss: 0.0378\n",
      "Epoch 195: Batch 25/116 loss: 0.0480\n",
      "Epoch 195: Batch 26/116 loss: 0.0396\n",
      "Epoch 195: Batch 27/116 loss: 0.0443\n",
      "Epoch 195: Batch 28/116 loss: 0.0407\n",
      "Epoch 195: Batch 29/116 loss: 0.0417\n",
      "Epoch 195: Batch 30/116 loss: 0.0520\n",
      "Epoch 195: Batch 31/116 loss: 0.0420\n",
      "Epoch 195: Batch 32/116 loss: 0.0613\n",
      "Epoch 195: Batch 33/116 loss: 0.0365\n",
      "Epoch 195: Batch 34/116 loss: 0.0242\n",
      "Epoch 195: Batch 35/116 loss: 0.0587\n",
      "Epoch 195: Batch 36/116 loss: 0.0257\n",
      "Epoch 195: Batch 37/116 loss: 0.0356\n",
      "Epoch 195: Batch 38/116 loss: 0.0433\n",
      "Epoch 195: Batch 39/116 loss: 0.0195\n",
      "Epoch 195: Batch 40/116 loss: 0.0249\n",
      "Epoch 195: Batch 41/116 loss: 0.0416\n",
      "Epoch 195: Batch 42/116 loss: 0.0537\n",
      "Epoch 195: Batch 43/116 loss: 0.0417\n",
      "Epoch 195: Batch 44/116 loss: 0.0332\n",
      "Epoch 195: Batch 45/116 loss: 0.0357\n",
      "Epoch 195: Batch 46/116 loss: 0.0391\n",
      "Epoch 195: Batch 47/116 loss: 0.0331\n",
      "Epoch 195: Batch 48/116 loss: 0.0329\n",
      "Epoch 195: Batch 49/116 loss: 0.0593\n",
      "Epoch 195: Batch 50/116 loss: 0.0385\n",
      "Epoch 195: Batch 51/116 loss: 0.0401\n",
      "Epoch 195: Batch 52/116 loss: 0.0438\n",
      "Epoch 195: Batch 53/116 loss: 0.0350\n",
      "Epoch 195: Batch 54/116 loss: 0.0377\n",
      "Epoch 195: Batch 55/116 loss: 0.0244\n",
      "Epoch 195: Batch 56/116 loss: 0.0319\n",
      "Epoch 195: Batch 57/116 loss: 0.0316\n",
      "Epoch 195: Batch 58/116 loss: 0.0303\n",
      "Epoch 195: Batch 59/116 loss: 0.0540\n",
      "Epoch 195: Batch 60/116 loss: 0.0434\n",
      "Epoch 195: Batch 61/116 loss: 0.0335\n",
      "Epoch 195: Batch 62/116 loss: 0.0257\n",
      "Epoch 195: Batch 63/116 loss: 0.0430\n",
      "Epoch 195: Batch 64/116 loss: 0.0439\n",
      "Epoch 195: Batch 65/116 loss: 0.0317\n",
      "Epoch 195: Batch 66/116 loss: 0.0582\n",
      "Epoch 195: Batch 67/116 loss: 0.0465\n",
      "Epoch 195: Batch 68/116 loss: 0.0318\n",
      "Epoch 195: Batch 69/116 loss: 0.0384\n",
      "Epoch 195: Batch 70/116 loss: 0.0439\n",
      "Epoch 195: Batch 71/116 loss: 0.0353\n",
      "Epoch 195: Batch 72/116 loss: 0.0254\n",
      "Epoch 195: Batch 73/116 loss: 0.0308\n",
      "Epoch 195: Batch 74/116 loss: 0.0223\n",
      "Epoch 195: Batch 75/116 loss: 0.0337\n",
      "Epoch 195: Batch 76/116 loss: 0.0299\n",
      "Epoch 195: Batch 77/116 loss: 0.0336\n",
      "Epoch 195: Batch 78/116 loss: 0.0548\n",
      "Epoch 195: Batch 79/116 loss: 0.0301\n",
      "Epoch 195: Batch 80/116 loss: 0.0430\n",
      "Epoch 195: Batch 81/116 loss: 0.0399\n",
      "Epoch 195: Batch 82/116 loss: 0.0330\n",
      "Epoch 195: Batch 83/116 loss: 0.0337\n",
      "Epoch 195: Batch 84/116 loss: 0.0440\n",
      "Epoch 195: Batch 85/116 loss: 0.0234\n",
      "Epoch 195: Batch 86/116 loss: 0.0539\n",
      "Epoch 195: Batch 87/116 loss: 0.0475\n",
      "Epoch 195: Batch 88/116 loss: 0.0506\n",
      "Epoch 195: Batch 89/116 loss: 0.0271\n",
      "Epoch 195: Batch 90/116 loss: 0.0294\n",
      "Epoch 195: Batch 91/116 loss: 0.0375\n",
      "Epoch 195: Batch 92/116 loss: 0.0347\n",
      "Epoch 195: Batch 93/116 loss: 0.0288\n",
      "Epoch 195: Batch 94/116 loss: 0.0343\n",
      "Epoch 195: Batch 95/116 loss: 0.0338\n",
      "Epoch 195: Batch 96/116 loss: 0.0336\n",
      "Epoch 195: Batch 97/116 loss: 0.0385\n",
      "Epoch 195: Batch 98/116 loss: 0.0446\n",
      "Epoch 195: Batch 99/116 loss: 0.0492\n",
      "Epoch 195: Batch 100/116 loss: 0.0529\n",
      "Epoch 195: Batch 101/116 loss: 0.0295\n",
      "Epoch 195: Batch 102/116 loss: 0.0426\n",
      "Epoch 195: Batch 103/116 loss: 0.0429\n",
      "Epoch 195: Batch 104/116 loss: 0.0565\n",
      "Epoch 195: Batch 105/116 loss: 0.0412\n",
      "Epoch 195: Batch 106/116 loss: 0.0545\n",
      "Epoch 195: Batch 107/116 loss: 0.0357\n",
      "Epoch 195: Batch 108/116 loss: 0.0304\n",
      "Epoch 195: Batch 109/116 loss: 0.0542\n",
      "Epoch 195: Batch 110/116 loss: 0.0431\n",
      "Epoch 195: Batch 111/116 loss: 0.0403\n",
      "Epoch 195: Batch 112/116 loss: 0.0537\n",
      "Epoch 195: Batch 113/116 loss: 0.0406\n",
      "Epoch 195: Batch 114/116 loss: 0.0910\n",
      "Epoch 195: Batch 115/116 loss: 0.0406\n",
      "Epoch 195: Batch 116/116 loss: 0.0551\n",
      "Epoch 195 train loss: 0.0388 valid loss: 0.0752\n",
      "performance reducing: 15\n",
      "Epoch 196: Batch 1/116 loss: 0.0534\n",
      "Epoch 196: Batch 2/116 loss: 0.0304\n",
      "Epoch 196: Batch 3/116 loss: 0.0363\n",
      "Epoch 196: Batch 4/116 loss: 0.0336\n",
      "Epoch 196: Batch 5/116 loss: 0.0282\n",
      "Epoch 196: Batch 6/116 loss: 0.0344\n",
      "Epoch 196: Batch 7/116 loss: 0.0371\n",
      "Epoch 196: Batch 8/116 loss: 0.0506\n",
      "Epoch 196: Batch 9/116 loss: 0.0395\n",
      "Epoch 196: Batch 10/116 loss: 0.0343\n",
      "Epoch 196: Batch 11/116 loss: 0.0405\n",
      "Epoch 196: Batch 12/116 loss: 0.0353\n",
      "Epoch 196: Batch 13/116 loss: 0.0317\n",
      "Epoch 196: Batch 14/116 loss: 0.0425\n",
      "Epoch 196: Batch 15/116 loss: 0.0349\n",
      "Epoch 196: Batch 16/116 loss: 0.0516\n",
      "Epoch 196: Batch 17/116 loss: 0.0708\n",
      "Epoch 196: Batch 18/116 loss: 0.0350\n",
      "Epoch 196: Batch 19/116 loss: 0.0348\n",
      "Epoch 196: Batch 20/116 loss: 0.0439\n",
      "Epoch 196: Batch 21/116 loss: 0.0290\n",
      "Epoch 196: Batch 22/116 loss: 0.0438\n",
      "Epoch 196: Batch 23/116 loss: 0.0292\n",
      "Epoch 196: Batch 24/116 loss: 0.0296\n",
      "Epoch 196: Batch 25/116 loss: 0.0373\n",
      "Epoch 196: Batch 26/116 loss: 0.0388\n",
      "Epoch 196: Batch 27/116 loss: 0.0380\n",
      "Epoch 196: Batch 28/116 loss: 0.0423\n",
      "Epoch 196: Batch 29/116 loss: 0.0244\n",
      "Epoch 196: Batch 30/116 loss: 0.0435\n",
      "Epoch 196: Batch 31/116 loss: 0.0319\n",
      "Epoch 196: Batch 32/116 loss: 0.0353\n",
      "Epoch 196: Batch 33/116 loss: 0.0351\n",
      "Epoch 196: Batch 34/116 loss: 0.0505\n",
      "Epoch 196: Batch 35/116 loss: 0.0397\n",
      "Epoch 196: Batch 36/116 loss: 0.0451\n",
      "Epoch 196: Batch 37/116 loss: 0.0333\n",
      "Epoch 196: Batch 38/116 loss: 0.0462\n",
      "Epoch 196: Batch 39/116 loss: 0.0487\n",
      "Epoch 196: Batch 40/116 loss: 0.0358\n",
      "Epoch 196: Batch 41/116 loss: 0.0335\n",
      "Epoch 196: Batch 42/116 loss: 0.0249\n",
      "Epoch 196: Batch 43/116 loss: 0.0616\n",
      "Epoch 196: Batch 44/116 loss: 0.0300\n",
      "Epoch 196: Batch 45/116 loss: 0.0673\n",
      "Epoch 196: Batch 46/116 loss: 0.0331\n",
      "Epoch 196: Batch 47/116 loss: 0.0380\n",
      "Epoch 196: Batch 48/116 loss: 0.0393\n",
      "Epoch 196: Batch 49/116 loss: 0.0281\n",
      "Epoch 196: Batch 50/116 loss: 0.0463\n",
      "Epoch 196: Batch 51/116 loss: 0.0289\n",
      "Epoch 196: Batch 52/116 loss: 0.0301\n",
      "Epoch 196: Batch 53/116 loss: 0.0334\n",
      "Epoch 196: Batch 54/116 loss: 0.0405\n",
      "Epoch 196: Batch 55/116 loss: 0.0486\n",
      "Epoch 196: Batch 56/116 loss: 0.0510\n",
      "Epoch 196: Batch 57/116 loss: 0.0468\n",
      "Epoch 196: Batch 58/116 loss: 0.0450\n",
      "Epoch 196: Batch 59/116 loss: 0.0370\n",
      "Epoch 196: Batch 60/116 loss: 0.0429\n",
      "Epoch 196: Batch 61/116 loss: 0.0373\n",
      "Epoch 196: Batch 62/116 loss: 0.0307\n",
      "Epoch 196: Batch 63/116 loss: 0.0278\n",
      "Epoch 196: Batch 64/116 loss: 0.0555\n",
      "Epoch 196: Batch 65/116 loss: 0.0341\n",
      "Epoch 196: Batch 66/116 loss: 0.0274\n",
      "Epoch 196: Batch 67/116 loss: 0.0342\n",
      "Epoch 196: Batch 68/116 loss: 0.0275\n",
      "Epoch 196: Batch 69/116 loss: 0.0287\n",
      "Epoch 196: Batch 70/116 loss: 0.0321\n",
      "Epoch 196: Batch 71/116 loss: 0.0388\n",
      "Epoch 196: Batch 72/116 loss: 0.0383\n",
      "Epoch 196: Batch 73/116 loss: 0.0589\n",
      "Epoch 196: Batch 74/116 loss: 0.0418\n",
      "Epoch 196: Batch 75/116 loss: 0.0316\n",
      "Epoch 196: Batch 76/116 loss: 0.0387\n",
      "Epoch 196: Batch 77/116 loss: 0.0380\n",
      "Epoch 196: Batch 78/116 loss: 0.0400\n",
      "Epoch 196: Batch 79/116 loss: 0.0330\n",
      "Epoch 196: Batch 80/116 loss: 0.0438\n",
      "Epoch 196: Batch 81/116 loss: 0.0281\n",
      "Epoch 196: Batch 82/116 loss: 0.0735\n",
      "Epoch 196: Batch 83/116 loss: 0.0412\n",
      "Epoch 196: Batch 84/116 loss: 0.0571\n",
      "Epoch 196: Batch 85/116 loss: 0.0319\n",
      "Epoch 196: Batch 86/116 loss: 0.0478\n",
      "Epoch 196: Batch 87/116 loss: 0.0579\n",
      "Epoch 196: Batch 88/116 loss: 0.0343\n",
      "Epoch 196: Batch 89/116 loss: 0.0318\n",
      "Epoch 196: Batch 90/116 loss: 0.0423\n",
      "Epoch 196: Batch 91/116 loss: 0.0467\n",
      "Epoch 196: Batch 92/116 loss: 0.0305\n",
      "Epoch 196: Batch 93/116 loss: 0.0374\n",
      "Epoch 196: Batch 94/116 loss: 0.0428\n",
      "Epoch 196: Batch 95/116 loss: 0.0443\n",
      "Epoch 196: Batch 96/116 loss: 0.0388\n",
      "Epoch 196: Batch 97/116 loss: 0.0617\n",
      "Epoch 196: Batch 98/116 loss: 0.0222\n",
      "Epoch 196: Batch 99/116 loss: 0.0504\n",
      "Epoch 196: Batch 100/116 loss: 0.0337\n",
      "Epoch 196: Batch 101/116 loss: 0.0381\n",
      "Epoch 196: Batch 102/116 loss: 0.0436\n",
      "Epoch 196: Batch 103/116 loss: 0.0376\n",
      "Epoch 196: Batch 104/116 loss: 0.0462\n",
      "Epoch 196: Batch 105/116 loss: 0.0580\n",
      "Epoch 196: Batch 106/116 loss: 0.0435\n",
      "Epoch 196: Batch 107/116 loss: 0.0490\n",
      "Epoch 196: Batch 108/116 loss: 0.0466\n",
      "Epoch 196: Batch 109/116 loss: 0.0445\n",
      "Epoch 196: Batch 110/116 loss: 0.0300\n",
      "Epoch 196: Batch 111/116 loss: 0.0419\n",
      "Epoch 196: Batch 112/116 loss: 0.0312\n",
      "Epoch 196: Batch 113/116 loss: 0.0245\n",
      "Epoch 196: Batch 114/116 loss: 0.0541\n",
      "Epoch 196: Batch 115/116 loss: 0.0405\n",
      "Epoch 196: Batch 116/116 loss: 0.0323\n",
      "Epoch 196 train loss: 0.0398 valid loss: 0.0524\n",
      "performance reducing: 16\n",
      "Epoch 197: Batch 1/116 loss: 0.0292\n",
      "Epoch 197: Batch 2/116 loss: 0.0287\n",
      "Epoch 197: Batch 3/116 loss: 0.0383\n",
      "Epoch 197: Batch 4/116 loss: 0.0472\n",
      "Epoch 197: Batch 5/116 loss: 0.0373\n",
      "Epoch 197: Batch 6/116 loss: 0.0294\n",
      "Epoch 197: Batch 7/116 loss: 0.0337\n",
      "Epoch 197: Batch 8/116 loss: 0.0194\n",
      "Epoch 197: Batch 9/116 loss: 0.0498\n",
      "Epoch 197: Batch 10/116 loss: 0.0437\n",
      "Epoch 197: Batch 11/116 loss: 0.0446\n",
      "Epoch 197: Batch 12/116 loss: 0.0488\n",
      "Epoch 197: Batch 13/116 loss: 0.0354\n",
      "Epoch 197: Batch 14/116 loss: 0.0443\n",
      "Epoch 197: Batch 15/116 loss: 0.0482\n",
      "Epoch 197: Batch 16/116 loss: 0.0463\n",
      "Epoch 197: Batch 17/116 loss: 0.0421\n",
      "Epoch 197: Batch 18/116 loss: 0.0560\n",
      "Epoch 197: Batch 19/116 loss: 0.0263\n",
      "Epoch 197: Batch 20/116 loss: 0.0365\n",
      "Epoch 197: Batch 21/116 loss: 0.0425\n",
      "Epoch 197: Batch 22/116 loss: 0.0552\n",
      "Epoch 197: Batch 23/116 loss: 0.0326\n",
      "Epoch 197: Batch 24/116 loss: 0.0296\n",
      "Epoch 197: Batch 25/116 loss: 0.0478\n",
      "Epoch 197: Batch 26/116 loss: 0.0259\n",
      "Epoch 197: Batch 27/116 loss: 0.0370\n",
      "Epoch 197: Batch 28/116 loss: 0.0389\n",
      "Epoch 197: Batch 29/116 loss: 0.0448\n",
      "Epoch 197: Batch 30/116 loss: 0.0424\n",
      "Epoch 197: Batch 31/116 loss: 0.0338\n",
      "Epoch 197: Batch 32/116 loss: 0.0384\n",
      "Epoch 197: Batch 33/116 loss: 0.0418\n",
      "Epoch 197: Batch 34/116 loss: 0.0532\n",
      "Epoch 197: Batch 35/116 loss: 0.0314\n",
      "Epoch 197: Batch 36/116 loss: 0.0415\n",
      "Epoch 197: Batch 37/116 loss: 0.0486\n",
      "Epoch 197: Batch 38/116 loss: 0.0427\n",
      "Epoch 197: Batch 39/116 loss: 0.0589\n",
      "Epoch 197: Batch 40/116 loss: 0.0459\n",
      "Epoch 197: Batch 41/116 loss: 0.0286\n",
      "Epoch 197: Batch 42/116 loss: 0.0520\n",
      "Epoch 197: Batch 43/116 loss: 0.0359\n",
      "Epoch 197: Batch 44/116 loss: 0.0358\n",
      "Epoch 197: Batch 45/116 loss: 0.0294\n",
      "Epoch 197: Batch 46/116 loss: 0.0425\n",
      "Epoch 197: Batch 47/116 loss: 0.0248\n",
      "Epoch 197: Batch 48/116 loss: 0.0403\n",
      "Epoch 197: Batch 49/116 loss: 0.0414\n",
      "Epoch 197: Batch 50/116 loss: 0.0387\n",
      "Epoch 197: Batch 51/116 loss: 0.0427\n",
      "Epoch 197: Batch 52/116 loss: 0.0410\n",
      "Epoch 197: Batch 53/116 loss: 0.0324\n",
      "Epoch 197: Batch 54/116 loss: 0.0265\n",
      "Epoch 197: Batch 55/116 loss: 0.0412\n",
      "Epoch 197: Batch 56/116 loss: 0.0374\n",
      "Epoch 197: Batch 57/116 loss: 0.0435\n",
      "Epoch 197: Batch 58/116 loss: 0.0255\n",
      "Epoch 197: Batch 59/116 loss: 0.0341\n",
      "Epoch 197: Batch 60/116 loss: 0.0206\n",
      "Epoch 197: Batch 61/116 loss: 0.0426\n",
      "Epoch 197: Batch 62/116 loss: 0.0425\n",
      "Epoch 197: Batch 63/116 loss: 0.0367\n",
      "Epoch 197: Batch 64/116 loss: 0.0353\n",
      "Epoch 197: Batch 65/116 loss: 0.0396\n",
      "Epoch 197: Batch 66/116 loss: 0.0475\n",
      "Epoch 197: Batch 67/116 loss: 0.0441\n",
      "Epoch 197: Batch 68/116 loss: 0.0400\n",
      "Epoch 197: Batch 69/116 loss: 0.0553\n",
      "Epoch 197: Batch 70/116 loss: 0.0396\n",
      "Epoch 197: Batch 71/116 loss: 0.0392\n",
      "Epoch 197: Batch 72/116 loss: 0.0375\n",
      "Epoch 197: Batch 73/116 loss: 0.0419\n",
      "Epoch 197: Batch 74/116 loss: 0.0492\n",
      "Epoch 197: Batch 75/116 loss: 0.0526\n",
      "Epoch 197: Batch 76/116 loss: 0.0423\n",
      "Epoch 197: Batch 77/116 loss: 0.0456\n",
      "Epoch 197: Batch 78/116 loss: 0.0452\n",
      "Epoch 197: Batch 79/116 loss: 0.0609\n",
      "Epoch 197: Batch 80/116 loss: 0.0583\n",
      "Epoch 197: Batch 81/116 loss: 0.0531\n",
      "Epoch 197: Batch 82/116 loss: 0.0389\n",
      "Epoch 197: Batch 83/116 loss: 0.0435\n",
      "Epoch 197: Batch 84/116 loss: 0.0496\n",
      "Epoch 197: Batch 85/116 loss: 0.0545\n",
      "Epoch 197: Batch 86/116 loss: 0.0388\n",
      "Epoch 197: Batch 87/116 loss: 0.0270\n",
      "Epoch 197: Batch 88/116 loss: 0.0353\n",
      "Epoch 197: Batch 89/116 loss: 0.0423\n",
      "Epoch 197: Batch 90/116 loss: 0.0475\n",
      "Epoch 197: Batch 91/116 loss: 0.0362\n",
      "Epoch 197: Batch 92/116 loss: 0.0331\n",
      "Epoch 197: Batch 93/116 loss: 0.0322\n",
      "Epoch 197: Batch 94/116 loss: 0.0332\n",
      "Epoch 197: Batch 95/116 loss: 0.0313\n",
      "Epoch 197: Batch 96/116 loss: 0.0476\n",
      "Epoch 197: Batch 97/116 loss: 0.0551\n",
      "Epoch 197: Batch 98/116 loss: 0.0664\n",
      "Epoch 197: Batch 99/116 loss: 0.0324\n",
      "Epoch 197: Batch 100/116 loss: 0.0351\n",
      "Epoch 197: Batch 101/116 loss: 0.0216\n",
      "Epoch 197: Batch 102/116 loss: 0.0332\n",
      "Epoch 197: Batch 103/116 loss: 0.0347\n",
      "Epoch 197: Batch 104/116 loss: 0.0419\n",
      "Epoch 197: Batch 105/116 loss: 0.0558\n",
      "Epoch 197: Batch 106/116 loss: 0.0354\n",
      "Epoch 197: Batch 107/116 loss: 0.0333\n",
      "Epoch 197: Batch 108/116 loss: 0.0341\n",
      "Epoch 197: Batch 109/116 loss: 0.0396\n",
      "Epoch 197: Batch 110/116 loss: 0.0454\n",
      "Epoch 197: Batch 111/116 loss: 0.0658\n",
      "Epoch 197: Batch 112/116 loss: 0.0340\n",
      "Epoch 197: Batch 113/116 loss: 0.0328\n",
      "Epoch 197: Batch 114/116 loss: 0.0423\n",
      "Epoch 197: Batch 115/116 loss: 0.0336\n",
      "Epoch 197: Batch 116/116 loss: 0.0322\n",
      "Epoch 197 train loss: 0.0403 valid loss: 0.0522\n",
      "performance reducing: 17\n",
      "Epoch 198: Batch 1/116 loss: 0.0378\n",
      "Epoch 198: Batch 2/116 loss: 0.0313\n",
      "Epoch 198: Batch 3/116 loss: 0.0465\n",
      "Epoch 198: Batch 4/116 loss: 0.0406\n",
      "Epoch 198: Batch 5/116 loss: 0.0721\n",
      "Epoch 198: Batch 6/116 loss: 0.0500\n",
      "Epoch 198: Batch 7/116 loss: 0.0397\n",
      "Epoch 198: Batch 8/116 loss: 0.0400\n",
      "Epoch 198: Batch 9/116 loss: 0.0329\n",
      "Epoch 198: Batch 10/116 loss: 0.0497\n",
      "Epoch 198: Batch 11/116 loss: 0.0368\n",
      "Epoch 198: Batch 12/116 loss: 0.0349\n",
      "Epoch 198: Batch 13/116 loss: 0.0289\n",
      "Epoch 198: Batch 14/116 loss: 0.0341\n",
      "Epoch 198: Batch 15/116 loss: 0.0335\n",
      "Epoch 198: Batch 16/116 loss: 0.0392\n",
      "Epoch 198: Batch 17/116 loss: 0.0396\n",
      "Epoch 198: Batch 18/116 loss: 0.0288\n",
      "Epoch 198: Batch 19/116 loss: 0.0430\n",
      "Epoch 198: Batch 20/116 loss: 0.0436\n",
      "Epoch 198: Batch 21/116 loss: 0.0413\n",
      "Epoch 198: Batch 22/116 loss: 0.0249\n",
      "Epoch 198: Batch 23/116 loss: 0.0461\n",
      "Epoch 198: Batch 24/116 loss: 0.0466\n",
      "Epoch 198: Batch 25/116 loss: 0.0497\n",
      "Epoch 198: Batch 26/116 loss: 0.0280\n",
      "Epoch 198: Batch 27/116 loss: 0.0399\n",
      "Epoch 198: Batch 28/116 loss: 0.0387\n",
      "Epoch 198: Batch 29/116 loss: 0.0241\n",
      "Epoch 198: Batch 30/116 loss: 0.0341\n",
      "Epoch 198: Batch 31/116 loss: 0.0379\n",
      "Epoch 198: Batch 32/116 loss: 0.0292\n",
      "Epoch 198: Batch 33/116 loss: 0.0469\n",
      "Epoch 198: Batch 34/116 loss: 0.0338\n",
      "Epoch 198: Batch 35/116 loss: 0.0281\n",
      "Epoch 198: Batch 36/116 loss: 0.0365\n",
      "Epoch 198: Batch 37/116 loss: 0.0421\n",
      "Epoch 198: Batch 38/116 loss: 0.0214\n",
      "Epoch 198: Batch 39/116 loss: 0.0342\n",
      "Epoch 198: Batch 40/116 loss: 0.0298\n",
      "Epoch 198: Batch 41/116 loss: 0.0249\n",
      "Epoch 198: Batch 42/116 loss: 0.0409\n",
      "Epoch 198: Batch 43/116 loss: 0.0493\n",
      "Epoch 198: Batch 44/116 loss: 0.0315\n",
      "Epoch 198: Batch 45/116 loss: 0.0576\n",
      "Epoch 198: Batch 46/116 loss: 0.0252\n",
      "Epoch 198: Batch 47/116 loss: 0.0404\n",
      "Epoch 198: Batch 48/116 loss: 0.0361\n",
      "Epoch 198: Batch 49/116 loss: 0.0390\n",
      "Epoch 198: Batch 50/116 loss: 0.0405\n",
      "Epoch 198: Batch 51/116 loss: 0.0372\n",
      "Epoch 198: Batch 52/116 loss: 0.0417\n",
      "Epoch 198: Batch 53/116 loss: 0.0303\n",
      "Epoch 198: Batch 54/116 loss: 0.0345\n",
      "Epoch 198: Batch 55/116 loss: 0.0389\n",
      "Epoch 198: Batch 56/116 loss: 0.0315\n",
      "Epoch 198: Batch 57/116 loss: 0.0531\n",
      "Epoch 198: Batch 58/116 loss: 0.0359\n",
      "Epoch 198: Batch 59/116 loss: 0.0509\n",
      "Epoch 198: Batch 60/116 loss: 0.0311\n",
      "Epoch 198: Batch 61/116 loss: 0.0299\n",
      "Epoch 198: Batch 62/116 loss: 0.0233\n",
      "Epoch 198: Batch 63/116 loss: 0.0407\n",
      "Epoch 198: Batch 64/116 loss: 0.0337\n",
      "Epoch 198: Batch 65/116 loss: 0.0592\n",
      "Epoch 198: Batch 66/116 loss: 0.0366\n",
      "Epoch 198: Batch 67/116 loss: 0.0300\n",
      "Epoch 198: Batch 68/116 loss: 0.0314\n",
      "Epoch 198: Batch 69/116 loss: 0.0401\n",
      "Epoch 198: Batch 70/116 loss: 0.0507\n",
      "Epoch 198: Batch 71/116 loss: 0.0295\n",
      "Epoch 198: Batch 72/116 loss: 0.0349\n",
      "Epoch 198: Batch 73/116 loss: 0.0509\n",
      "Epoch 198: Batch 74/116 loss: 0.0398\n",
      "Epoch 198: Batch 75/116 loss: 0.0330\n",
      "Epoch 198: Batch 76/116 loss: 0.0534\n",
      "Epoch 198: Batch 77/116 loss: 0.0259\n",
      "Epoch 198: Batch 78/116 loss: 0.0438\n",
      "Epoch 198: Batch 79/116 loss: 0.0362\n",
      "Epoch 198: Batch 80/116 loss: 0.0318\n",
      "Epoch 198: Batch 81/116 loss: 0.0328\n",
      "Epoch 198: Batch 82/116 loss: 0.0408\n",
      "Epoch 198: Batch 83/116 loss: 0.0408\n",
      "Epoch 198: Batch 84/116 loss: 0.0408\n",
      "Epoch 198: Batch 85/116 loss: 0.0336\n",
      "Epoch 198: Batch 86/116 loss: 0.0502\n",
      "Epoch 198: Batch 87/116 loss: 0.0353\n",
      "Epoch 198: Batch 88/116 loss: 0.0538\n",
      "Epoch 198: Batch 89/116 loss: 0.0365\n",
      "Epoch 198: Batch 90/116 loss: 0.0516\n",
      "Epoch 198: Batch 91/116 loss: 0.0545\n",
      "Epoch 198: Batch 92/116 loss: 0.0495\n",
      "Epoch 198: Batch 93/116 loss: 0.0336\n",
      "Epoch 198: Batch 94/116 loss: 0.0439\n",
      "Epoch 198: Batch 95/116 loss: 0.0606\n",
      "Epoch 198: Batch 96/116 loss: 0.0320\n",
      "Epoch 198: Batch 97/116 loss: 0.0472\n",
      "Epoch 198: Batch 98/116 loss: 0.0485\n",
      "Epoch 198: Batch 99/116 loss: 0.0405\n",
      "Epoch 198: Batch 100/116 loss: 0.0240\n",
      "Epoch 198: Batch 101/116 loss: 0.0505\n",
      "Epoch 198: Batch 102/116 loss: 0.0369\n",
      "Epoch 198: Batch 103/116 loss: 0.0431\n",
      "Epoch 198: Batch 104/116 loss: 0.0350\n",
      "Epoch 198: Batch 105/116 loss: 0.0308\n",
      "Epoch 198: Batch 106/116 loss: 0.0427\n",
      "Epoch 198: Batch 107/116 loss: 0.0330\n",
      "Epoch 198: Batch 108/116 loss: 0.0419\n",
      "Epoch 198: Batch 109/116 loss: 0.0561\n",
      "Epoch 198: Batch 110/116 loss: 0.0498\n",
      "Epoch 198: Batch 111/116 loss: 0.0340\n",
      "Epoch 198: Batch 112/116 loss: 0.0312\n",
      "Epoch 198: Batch 113/116 loss: 0.0339\n",
      "Epoch 198: Batch 114/116 loss: 0.0288\n",
      "Epoch 198: Batch 115/116 loss: 0.0521\n",
      "Epoch 198: Batch 116/116 loss: 0.0311\n",
      "Epoch 198 train loss: 0.0390 valid loss: 0.0499\n",
      "performance reducing: 18\n",
      "Epoch 199: Batch 1/116 loss: 0.0381\n",
      "Epoch 199: Batch 2/116 loss: 0.0385\n",
      "Epoch 199: Batch 3/116 loss: 0.0530\n",
      "Epoch 199: Batch 4/116 loss: 0.0358\n",
      "Epoch 199: Batch 5/116 loss: 0.0567\n",
      "Epoch 199: Batch 6/116 loss: 0.0344\n",
      "Epoch 199: Batch 7/116 loss: 0.0364\n",
      "Epoch 199: Batch 8/116 loss: 0.0389\n",
      "Epoch 199: Batch 9/116 loss: 0.0371\n",
      "Epoch 199: Batch 10/116 loss: 0.0448\n",
      "Epoch 199: Batch 11/116 loss: 0.0278\n",
      "Epoch 199: Batch 12/116 loss: 0.0299\n",
      "Epoch 199: Batch 13/116 loss: 0.0389\n",
      "Epoch 199: Batch 14/116 loss: 0.0292\n",
      "Epoch 199: Batch 15/116 loss: 0.0292\n",
      "Epoch 199: Batch 16/116 loss: 0.0353\n",
      "Epoch 199: Batch 17/116 loss: 0.0332\n",
      "Epoch 199: Batch 18/116 loss: 0.0289\n",
      "Epoch 199: Batch 19/116 loss: 0.0320\n",
      "Epoch 199: Batch 20/116 loss: 0.0407\n",
      "Epoch 199: Batch 21/116 loss: 0.0211\n",
      "Epoch 199: Batch 22/116 loss: 0.0390\n",
      "Epoch 199: Batch 23/116 loss: 0.0477\n",
      "Epoch 199: Batch 24/116 loss: 0.0463\n",
      "Epoch 199: Batch 25/116 loss: 0.0378\n",
      "Epoch 199: Batch 26/116 loss: 0.0532\n",
      "Epoch 199: Batch 27/116 loss: 0.0236\n",
      "Epoch 199: Batch 28/116 loss: 0.0316\n",
      "Epoch 199: Batch 29/116 loss: 0.0397\n",
      "Epoch 199: Batch 30/116 loss: 0.0403\n",
      "Epoch 199: Batch 31/116 loss: 0.0308\n",
      "Epoch 199: Batch 32/116 loss: 0.0392\n",
      "Epoch 199: Batch 33/116 loss: 0.0623\n",
      "Epoch 199: Batch 34/116 loss: 0.0359\n",
      "Epoch 199: Batch 35/116 loss: 0.0437\n",
      "Epoch 199: Batch 36/116 loss: 0.0318\n",
      "Epoch 199: Batch 37/116 loss: 0.0291\n",
      "Epoch 199: Batch 38/116 loss: 0.0503\n",
      "Epoch 199: Batch 39/116 loss: 0.0382\n",
      "Epoch 199: Batch 40/116 loss: 0.0275\n",
      "Epoch 199: Batch 41/116 loss: 0.0264\n",
      "Epoch 199: Batch 42/116 loss: 0.0358\n",
      "Epoch 199: Batch 43/116 loss: 0.0316\n",
      "Epoch 199: Batch 44/116 loss: 0.0373\n",
      "Epoch 199: Batch 45/116 loss: 0.0378\n",
      "Epoch 199: Batch 46/116 loss: 0.0407\n",
      "Epoch 199: Batch 47/116 loss: 0.0485\n",
      "Epoch 199: Batch 48/116 loss: 0.0462\n",
      "Epoch 199: Batch 49/116 loss: 0.0292\n",
      "Epoch 199: Batch 50/116 loss: 0.0437\n",
      "Epoch 199: Batch 51/116 loss: 0.0568\n",
      "Epoch 199: Batch 52/116 loss: 0.0494\n",
      "Epoch 199: Batch 53/116 loss: 0.0456\n",
      "Epoch 199: Batch 54/116 loss: 0.0253\n",
      "Epoch 199: Batch 55/116 loss: 0.0290\n",
      "Epoch 199: Batch 56/116 loss: 0.0370\n",
      "Epoch 199: Batch 57/116 loss: 0.0335\n",
      "Epoch 199: Batch 58/116 loss: 0.0328\n",
      "Epoch 199: Batch 59/116 loss: 0.0409\n",
      "Epoch 199: Batch 60/116 loss: 0.0252\n",
      "Epoch 199: Batch 61/116 loss: 0.0319\n",
      "Epoch 199: Batch 62/116 loss: 0.0381\n",
      "Epoch 199: Batch 63/116 loss: 0.0326\n",
      "Epoch 199: Batch 64/116 loss: 0.0402\n",
      "Epoch 199: Batch 65/116 loss: 0.0415\n",
      "Epoch 199: Batch 66/116 loss: 0.0221\n",
      "Epoch 199: Batch 67/116 loss: 0.0346\n",
      "Epoch 199: Batch 68/116 loss: 0.0391\n",
      "Epoch 199: Batch 69/116 loss: 0.0634\n",
      "Epoch 199: Batch 70/116 loss: 0.0457\n",
      "Epoch 199: Batch 71/116 loss: 0.0389\n",
      "Epoch 199: Batch 72/116 loss: 0.0311\n",
      "Epoch 199: Batch 73/116 loss: 0.0435\n",
      "Epoch 199: Batch 74/116 loss: 0.0331\n",
      "Epoch 199: Batch 75/116 loss: 0.0356\n",
      "Epoch 199: Batch 76/116 loss: 0.0362\n",
      "Epoch 199: Batch 77/116 loss: 0.0441\n",
      "Epoch 199: Batch 78/116 loss: 0.0409\n",
      "Epoch 199: Batch 79/116 loss: 0.0377\n",
      "Epoch 199: Batch 80/116 loss: 0.0218\n",
      "Epoch 199: Batch 81/116 loss: 0.0304\n",
      "Epoch 199: Batch 82/116 loss: 0.0298\n",
      "Epoch 199: Batch 83/116 loss: 0.0405\n",
      "Epoch 199: Batch 84/116 loss: 0.0303\n",
      "Epoch 199: Batch 85/116 loss: 0.0229\n",
      "Epoch 199: Batch 86/116 loss: 0.0358\n",
      "Epoch 199: Batch 87/116 loss: 0.0363\n",
      "Epoch 199: Batch 88/116 loss: 0.0405\n",
      "Epoch 199: Batch 89/116 loss: 0.0396\n",
      "Epoch 199: Batch 90/116 loss: 0.0412\n",
      "Epoch 199: Batch 91/116 loss: 0.0453\n",
      "Epoch 199: Batch 92/116 loss: 0.0443\n",
      "Epoch 199: Batch 93/116 loss: 0.0614\n",
      "Epoch 199: Batch 94/116 loss: 0.0336\n",
      "Epoch 199: Batch 95/116 loss: 0.0354\n",
      "Epoch 199: Batch 96/116 loss: 0.0282\n",
      "Epoch 199: Batch 97/116 loss: 0.0341\n",
      "Epoch 199: Batch 98/116 loss: 0.0402\n",
      "Epoch 199: Batch 99/116 loss: 0.0415\n",
      "Epoch 199: Batch 100/116 loss: 0.0413\n",
      "Epoch 199: Batch 101/116 loss: 0.0292\n",
      "Epoch 199: Batch 102/116 loss: 0.0427\n",
      "Epoch 199: Batch 103/116 loss: 0.0487\n",
      "Epoch 199: Batch 104/116 loss: 0.0323\n",
      "Epoch 199: Batch 105/116 loss: 0.0302\n",
      "Epoch 199: Batch 106/116 loss: 0.0448\n",
      "Epoch 199: Batch 107/116 loss: 0.0311\n",
      "Epoch 199: Batch 108/116 loss: 0.0440\n",
      "Epoch 199: Batch 109/116 loss: 0.0399\n",
      "Epoch 199: Batch 110/116 loss: 0.0399\n",
      "Epoch 199: Batch 111/116 loss: 0.0459\n",
      "Epoch 199: Batch 112/116 loss: 0.0321\n",
      "Epoch 199: Batch 113/116 loss: 0.0355\n",
      "Epoch 199: Batch 114/116 loss: 0.0494\n",
      "Epoch 199: Batch 115/116 loss: 0.0365\n",
      "Epoch 199: Batch 116/116 loss: 0.0405\n",
      "Epoch 199 train loss: 0.0378 valid loss: 0.0573\n",
      "performance reducing: 19\n",
      "Epoch 200: Batch 1/116 loss: 0.0378\n",
      "Epoch 200: Batch 2/116 loss: 0.0382\n",
      "Epoch 200: Batch 3/116 loss: 0.0382\n",
      "Epoch 200: Batch 4/116 loss: 0.0557\n",
      "Epoch 200: Batch 5/116 loss: 0.0318\n",
      "Epoch 200: Batch 6/116 loss: 0.0422\n",
      "Epoch 200: Batch 7/116 loss: 0.0354\n",
      "Epoch 200: Batch 8/116 loss: 0.0367\n",
      "Epoch 200: Batch 9/116 loss: 0.0346\n",
      "Epoch 200: Batch 10/116 loss: 0.0323\n",
      "Epoch 200: Batch 11/116 loss: 0.0292\n",
      "Epoch 200: Batch 12/116 loss: 0.0465\n",
      "Epoch 200: Batch 13/116 loss: 0.0395\n",
      "Epoch 200: Batch 14/116 loss: 0.0364\n",
      "Epoch 200: Batch 15/116 loss: 0.0442\n",
      "Epoch 200: Batch 16/116 loss: 0.0442\n",
      "Epoch 200: Batch 17/116 loss: 0.0376\n",
      "Epoch 200: Batch 18/116 loss: 0.0396\n",
      "Epoch 200: Batch 19/116 loss: 0.0360\n",
      "Epoch 200: Batch 20/116 loss: 0.0457\n",
      "Epoch 200: Batch 21/116 loss: 0.0570\n",
      "Epoch 200: Batch 22/116 loss: 0.0375\n",
      "Epoch 200: Batch 23/116 loss: 0.0389\n",
      "Epoch 200: Batch 24/116 loss: 0.0477\n",
      "Epoch 200: Batch 25/116 loss: 0.0564\n",
      "Epoch 200: Batch 26/116 loss: 0.0283\n",
      "Epoch 200: Batch 27/116 loss: 0.0476\n",
      "Epoch 200: Batch 28/116 loss: 0.0433\n",
      "Epoch 200: Batch 29/116 loss: 0.0298\n",
      "Epoch 200: Batch 30/116 loss: 0.0338\n",
      "Epoch 200: Batch 31/116 loss: 0.0306\n",
      "Epoch 200: Batch 32/116 loss: 0.0470\n",
      "Epoch 200: Batch 33/116 loss: 0.0495\n",
      "Epoch 200: Batch 34/116 loss: 0.0317\n",
      "Epoch 200: Batch 35/116 loss: 0.0433\n",
      "Epoch 200: Batch 36/116 loss: 0.0317\n",
      "Epoch 200: Batch 37/116 loss: 0.0301\n",
      "Epoch 200: Batch 38/116 loss: 0.0384\n",
      "Epoch 200: Batch 39/116 loss: 0.0301\n",
      "Epoch 200: Batch 40/116 loss: 0.0500\n",
      "Epoch 200: Batch 41/116 loss: 0.0460\n",
      "Epoch 200: Batch 42/116 loss: 0.0281\n",
      "Epoch 200: Batch 43/116 loss: 0.0525\n",
      "Epoch 200: Batch 44/116 loss: 0.0364\n",
      "Epoch 200: Batch 45/116 loss: 0.0276\n",
      "Epoch 200: Batch 46/116 loss: 0.0360\n",
      "Epoch 200: Batch 47/116 loss: 0.0265\n",
      "Epoch 200: Batch 48/116 loss: 0.0339\n",
      "Epoch 200: Batch 49/116 loss: 0.0494\n",
      "Epoch 200: Batch 50/116 loss: 0.0545\n",
      "Epoch 200: Batch 51/116 loss: 0.0318\n",
      "Epoch 200: Batch 52/116 loss: 0.0292\n",
      "Epoch 200: Batch 53/116 loss: 0.0306\n",
      "Epoch 200: Batch 54/116 loss: 0.0347\n",
      "Epoch 200: Batch 55/116 loss: 0.0231\n",
      "Epoch 200: Batch 56/116 loss: 0.0467\n",
      "Epoch 200: Batch 57/116 loss: 0.0405\n",
      "Epoch 200: Batch 58/116 loss: 0.0345\n",
      "Epoch 200: Batch 59/116 loss: 0.0286\n",
      "Epoch 200: Batch 60/116 loss: 0.0353\n",
      "Epoch 200: Batch 61/116 loss: 0.0344\n",
      "Epoch 200: Batch 62/116 loss: 0.0310\n",
      "Epoch 200: Batch 63/116 loss: 0.0360\n",
      "Epoch 200: Batch 64/116 loss: 0.0350\n",
      "Epoch 200: Batch 65/116 loss: 0.0373\n",
      "Epoch 200: Batch 66/116 loss: 0.0497\n",
      "Epoch 200: Batch 67/116 loss: 0.0390\n",
      "Epoch 200: Batch 68/116 loss: 0.0389\n",
      "Epoch 200: Batch 69/116 loss: 0.0452\n",
      "Epoch 200: Batch 70/116 loss: 0.0221\n",
      "Epoch 200: Batch 71/116 loss: 0.0434\n",
      "Epoch 200: Batch 72/116 loss: 0.0440\n",
      "Epoch 200: Batch 73/116 loss: 0.0317\n",
      "Epoch 200: Batch 74/116 loss: 0.0266\n",
      "Epoch 200: Batch 75/116 loss: 0.0252\n",
      "Epoch 200: Batch 76/116 loss: 0.0378\n",
      "Epoch 200: Batch 77/116 loss: 0.0203\n",
      "Epoch 200: Batch 78/116 loss: 0.0299\n",
      "Epoch 200: Batch 79/116 loss: 0.0612\n",
      "Epoch 200: Batch 80/116 loss: 0.0355\n",
      "Epoch 200: Batch 81/116 loss: 0.0424\n",
      "Epoch 200: Batch 82/116 loss: 0.0611\n",
      "Epoch 200: Batch 83/116 loss: 0.0472\n",
      "Epoch 200: Batch 84/116 loss: 0.0279\n",
      "Epoch 200: Batch 85/116 loss: 0.0408\n",
      "Epoch 200: Batch 86/116 loss: 0.0223\n",
      "Epoch 200: Batch 87/116 loss: 0.0555\n",
      "Epoch 200: Batch 88/116 loss: 0.0315\n",
      "Epoch 200: Batch 89/116 loss: 0.0339\n",
      "Epoch 200: Batch 90/116 loss: 0.0340\n",
      "Epoch 200: Batch 91/116 loss: 0.0341\n",
      "Epoch 200: Batch 92/116 loss: 0.0247\n",
      "Epoch 200: Batch 93/116 loss: 0.0536\n",
      "Epoch 200: Batch 94/116 loss: 0.0358\n",
      "Epoch 200: Batch 95/116 loss: 0.0512\n",
      "Epoch 200: Batch 96/116 loss: 0.0435\n",
      "Epoch 200: Batch 97/116 loss: 0.0410\n",
      "Epoch 200: Batch 98/116 loss: 0.0393\n",
      "Epoch 200: Batch 99/116 loss: 0.0360\n",
      "Epoch 200: Batch 100/116 loss: 0.0268\n",
      "Epoch 200: Batch 101/116 loss: 0.0389\n",
      "Epoch 200: Batch 102/116 loss: 0.0443\n",
      "Epoch 200: Batch 103/116 loss: 0.0382\n",
      "Epoch 200: Batch 104/116 loss: 0.0379\n",
      "Epoch 200: Batch 105/116 loss: 0.0317\n",
      "Epoch 200: Batch 106/116 loss: 0.0328\n",
      "Epoch 200: Batch 107/116 loss: 0.0310\n",
      "Epoch 200: Batch 108/116 loss: 0.0319\n",
      "Epoch 200: Batch 109/116 loss: 0.0298\n",
      "Epoch 200: Batch 110/116 loss: 0.0297\n",
      "Epoch 200: Batch 111/116 loss: 0.0334\n",
      "Epoch 200: Batch 112/116 loss: 0.0337\n",
      "Epoch 200: Batch 113/116 loss: 0.0397\n",
      "Epoch 200: Batch 114/116 loss: 0.0382\n",
      "Epoch 200: Batch 115/116 loss: 0.0223\n",
      "Epoch 200: Batch 116/116 loss: 0.0432\n",
      "Epoch 200 train loss: 0.0377 valid loss: 0.0569\n",
      "performance reducing: 20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lr = 1e-3\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # 后面修改为Dice损失函数看看效果\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/images/*')\n",
    "    train_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/masks/*')\n",
    "    \n",
    "    val_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/images/*')\n",
    "    val_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/masks/*')\n",
    "\n",
    "    train_transforms = Compose([\n",
    "        # todo 先Resize试一下\n",
    "        ToPILImage(),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomRotation(degrees=(0, 180)),\n",
    "#         ColorJitter(), # 这个有问题，但是是什么问题？\n",
    "#         GrayScale(), # 这个貌似也有问题，问题更大\n",
    "        RandomCrop(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    valid_transforms = Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = load_data(train_image_path, train_mask_path, batch_size=32, drop_last=True,\n",
    "                             transforms=train_transforms)\n",
    "    valid_loader = load_data(val_image_path, val_mask_path, batch_size=32, transforms=valid_transforms)\n",
    "    \n",
    "    continue_train = True\n",
    "    \n",
    "    if not continue_train:\n",
    "        epoch = 50\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, epoch, device=device)\n",
    "    else:\n",
    "        total_epoch = 200\n",
    "        pretrain_params = torch.load('../input/covid-xray-unet/last_model.pth')\n",
    "        model.load_state_dict(pretrain_params['last_model_state_dict'])\n",
    "        optimizer.load_state_dict(pretrain_params['last_optimizer_state_dict'])\n",
    "        current_epoch = pretrain_params['epoch']\n",
    "        model.train()\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9435.800799,
   "end_time": "2022-08-11T10:50:35.167782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-11T08:13:19.366983",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
