{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd826a93",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:33.655991Z",
     "iopub.status.busy": "2022-08-11T02:03:33.655170Z",
     "iopub.status.idle": "2022-08-11T02:03:33.668202Z",
     "shell.execute_reply": "2022-08-11T02:03:33.666958Z"
    },
    "papermill": {
     "duration": 0.020569,
     "end_time": "2022-08-11T02:03:33.670528",
     "exception": false,
     "start_time": "2022-08-11T02:03:33.649959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b2b8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:33.677992Z",
     "iopub.status.busy": "2022-08-11T02:03:33.677354Z",
     "iopub.status.idle": "2022-08-11T02:03:35.870603Z",
     "shell.execute_reply": "2022-08-11T02:03:35.869648Z"
    },
    "papermill": {
     "duration": 2.199648,
     "end_time": "2022-08-11T02:03:35.872998",
     "exception": false,
     "start_time": "2022-08-11T02:03:33.673350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "import torchvision.transforms.functional as Func\n",
    "\n",
    "\n",
    "class ToPILImage(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_pil_image(image)\n",
    "        if target is not None:\n",
    "            target = Func.to_pil_image(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        seed = np.random.randint(65536)\n",
    "        torch.manual_seed(seed)\n",
    "        crop = torchvision.transforms.RandomCrop(self.size)\n",
    "        image = crop(image)\n",
    "        if target is not None:\n",
    "            target = crop(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        super(Resize, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.resize(image, self.size)\n",
    "        if target is not None:\n",
    "            target = Func.resize(target, self.size, interpolation=Func.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = Func.hflip(image)\n",
    "            if target is not None:\n",
    "                target = Func.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __call__(self, image, target):\n",
    "        color_jitter = torchvision.transforms.ColorJitter()\n",
    "        image = color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class GrayScale(object):\n",
    "    def __call__(self, image, target):\n",
    "        gray_scale = torchvision.transforms.Grayscale()\n",
    "        image = gray_scale(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, degrees):\n",
    "        super(RandomRotation, self).__init__()\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        degree = random.randint(self.degrees[0], self.degrees[1])\n",
    "        image = Func.rotate(image, degree)\n",
    "        if target is not None:\n",
    "            target = Func.rotate(target, degree)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = Func.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_tensor(image)\n",
    "        if target is not None:\n",
    "            target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bae335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:35.879444Z",
     "iopub.status.busy": "2022-08-11T02:03:35.879001Z",
     "iopub.status.idle": "2022-08-11T02:03:35.888726Z",
     "shell.execute_reply": "2022-08-11T02:03:35.887918Z"
    },
    "papermill": {
     "duration": 0.015257,
     "end_time": "2022-08-11T02:03:35.890853",
     "exception": false,
     "start_time": "2022-08-11T02:03:35.875596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, target_path, transforms=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data_paths = data_path\n",
    "        self.target_paths = target_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        image = plt.imread(self.data_paths[item])\n",
    "        target = plt.imread(self.target_paths[item])\n",
    "\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # image = torch.from_numpy(image)\n",
    "        # target = torch.from_numpy(target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "\n",
    "def load_data(data_path, target_path, batch_size, drop_last=False, transforms=None):\n",
    "    datas = CustomDataset(data_path=data_path, target_path=target_path, transforms=transforms)\n",
    "    data_loader = DataLoader(datas, shuffle=True, batch_size=batch_size, drop_last=drop_last)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a421c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:35.897667Z",
     "iopub.status.busy": "2022-08-11T02:03:35.897385Z",
     "iopub.status.idle": "2022-08-11T02:03:35.919966Z",
     "shell.execute_reply": "2022-08-11T02:03:35.919016Z"
    },
    "papermill": {
     "duration": 0.028932,
     "end_time": "2022-08-11T02:03:35.922128",
     "exception": false,
     "start_time": "2022-08-11T02:03:35.893196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net模型的pytorch实现。\n",
    "    论文地址：https://arxiv.org/abs/1505.04597\n",
    "    模型的总体结构: 编码器 -> 一个ConvBlock -> 解码器 -> 一个Conv 1 * 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.eb1 = EncoderBlock(1, 64, 64, kernel_size=2)\n",
    "        self.eb2 = EncoderBlock(64, 128, 128, kernel_size=2)\n",
    "        self.eb3 = EncoderBlock(128, 256, 256, kernel_size=2)\n",
    "        self.eb4 = EncoderBlock(256, 512, 512, kernel_size=2)\n",
    "        # 编码器与解码器之间有一个ConvBlock\n",
    "        self.cb = ConvBlock(512, 1024, 1024)\n",
    "        # 解码器部分\n",
    "        self.db1 = DecoderBlock(1024, 512, 512)\n",
    "        self.db2 = DecoderBlock(512, 512, 256)\n",
    "        self.db3 = DecoderBlock(256, 128, 128)\n",
    "        self.db4 = DecoderBlock(128, 64, 64)\n",
    "        # 一个Conv 1 * 1, 二分类，结果为两个通道\n",
    "        self.conv1x1 = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ex1, skip_x1 = self.eb1(x)\n",
    "        ex2, skip_x2 = self.eb2(ex1)\n",
    "        ex3, skip_x3 = self.eb3(ex2)\n",
    "        ex4, skip_x4 = self.eb4(ex3)\n",
    "        cbx = self.cb(ex4)\n",
    "        dx1 = self.db1(cbx, skip_x4)\n",
    "        dx2 = self.db2(dx1, skip_x3)\n",
    "        dx3 = self.db3(dx2, skip_x2)\n",
    "        dx4 = self.db4(dx3, skip_x1)\n",
    "        crop = transforms.CenterCrop(size=(x.shape[-1], x.shape[-2]))\n",
    "        # normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        return self.sigmoid(self.conv1x1(crop(dx4)))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"一个Conv2d卷积后跟一个Relu激活函数，卷积核大小为3 * 3\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_relu_list = [nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(mid_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(out_channels),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "        self.conv_relu = nn.Sequential(*conv_relu_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_relu(x)\n",
    "\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    \"\"\"下采样，使用max pool方法执行，核大小为 2 * 2，用在编码器的ConvBlock后面\n",
    "\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    \"\"\"上采样，用在解码器的ConvBlock前面，使用转置卷积，同时通道数减半，\n",
    "\n",
    "    C_out = out_channels\n",
    "    H_out = (H_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    W_out = (W_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "\n",
    "    :param in_channels: 转置卷积的输入通道数\n",
    "    :param out_channels: 转置卷积的输出通道数\n",
    "    :param kernel_size: 转置卷积的卷积核大小，默认为2\n",
    "    :param stride: 转置卷积的步幅，默认为2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, dilation=1, padding=0, output_padding=1):\n",
    "        super(UpSampling, self).__init__()\n",
    "        # self.up_sample = nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "        # stride=2, kernel_size=2相当于宽高翻倍\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            dilation=dilation, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_sample(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"编码器中的一个层次块\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "        self.down_sample = DownSampling(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block(x)\n",
    "        return self.down_sample(x1), x1\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    \"\"\"跳跃连接，在通道维上连接\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 将从编码器传过来的特征图裁剪到与输入相同尺寸\n",
    "        x1 = functional.center_crop(skip_x, [x.shape[-2], x.shape[-1]])\n",
    "        if x1.shape != x.shape:\n",
    "            raise Exception('要连接的两个特征图尺寸不一致，skip_x.shape={}，x.shape={}'.format(skip_x.shape, x.shape))\n",
    "        # 通道维连接\n",
    "        return torch.cat([x, x1], dim=1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的层次块，每个层次块都是UpSampling -> Concat -> ConvBlock\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up_sample = UpSampling(in_channels, out_channels)\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x1 = self.up_sample(x)\n",
    "        concat = ConcatLayer()\n",
    "        x2 = concat(x1, skip_x)\n",
    "        return self.conv_block(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b6053d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:35.928368Z",
     "iopub.status.busy": "2022-08-11T02:03:35.928107Z",
     "iopub.status.idle": "2022-08-11T02:03:35.944834Z",
     "shell.execute_reply": "2022-08-11T02:03:35.943872Z"
    },
    "papermill": {
     "duration": 0.022313,
     "end_time": "2022-08-11T02:03:35.946807",
     "exception": false,
     "start_time": "2022-08-11T02:03:35.924494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames,SpellCheckingInspection\n",
    "def train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch=0, num_classes=2,\n",
    "          device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    loss_change_list = []\n",
    "    valid_loss_change_list = []\n",
    "    saved_last = {}\n",
    "    saved_best = {}\n",
    "    loss_change = {}\n",
    "\n",
    "    search_best = SearchBest()\n",
    "\n",
    "    for i in range(current_epoch, total_epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_onehot = convert_to_one_hot(y, num_classes=num_classes)\n",
    "            predict = model(x)\n",
    "\n",
    "            loss_value = criterion(predict, y_onehot)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "            print('Epoch {}: Batch {}/{} loss: {:.4f}'.format(i + 1, index + 1, len(train_loader), loss_value.item()))\n",
    "\n",
    "        loss_change_list.append(total_loss / len(train_loader))\n",
    "        valid_avg_loss = valid(model, criterion, valid_loader, num_classes, device)\n",
    "        valid_loss_change_list.append(valid_avg_loss)\n",
    "        print('Epoch {} train loss: {:.4f} valid loss: {:.4f}'.format(i + 1, total_loss / len(train_loader),\n",
    "                                                                      valid_avg_loss))\n",
    "        search_best(valid_avg_loss)\n",
    "        if search_best.counter == 0:\n",
    "            # save the relevant params of the best model state in the current time.\n",
    "            saved_best['best_model_state_dict'] = model.state_dict()\n",
    "            saved_best['best_optimizer_state_dict'] = optimizer.state_dict()\n",
    "            saved_best['epoch'] = i + 1\n",
    "    loss_change['train_loss_change_history'] = loss_change_list\n",
    "    loss_change['valid_loss_change_history'] = valid_loss_change_list\n",
    "    saved_last['last_model_state_dict'] = model.state_dict()\n",
    "    saved_last['last_optimizer_state_dict'] = optimizer.state_dict()\n",
    "    saved_last['epoch'] = total_epoch\n",
    "    torch.save(saved_best, './best_model.pth')\n",
    "    torch.save(saved_last, './last_model.pth')\n",
    "    torch.save(loss_change, './loss_change.pth')\n",
    "\n",
    "\n",
    "def convert_to_one_hot(data, num_classes):\n",
    "    if type(data) is not torch.Tensor:\n",
    "        raise RuntimeError('data must be a torch.Tensor')\n",
    "    if data.dtype is not torch.int64:\n",
    "        data = data.to(torch.int64)\n",
    "    data = F.one_hot(data, num_classes=num_classes).permute((0, -1, 1, 2))\n",
    "    return data.to(torch.float32)\n",
    "\n",
    "\n",
    "class SearchBest(object):\n",
    "    def __init__(self, min_delta=0, verbose=True):\n",
    "        super(SearchBest, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, valid_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = valid_loss\n",
    "        elif self.best_score - valid_loss >= self.min_delta:\n",
    "            self.best_score = valid_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print('performance reducing: {}'.format(self.counter))\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def valid(model, criterion, valid_loader, num_classes, device):\n",
    "    \"\"\"\n",
    "    :return: validate loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    valid_total_loss = 0.0\n",
    "    for index, (x, y) in enumerate(valid_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_onehot = convert_to_one_hot(y, num_classes)\n",
    "        with torch.no_grad():\n",
    "            predict = model(x)\n",
    "            valid_loss = criterion(predict, y_onehot)\n",
    "            valid_total_loss += valid_loss.item()\n",
    "    return valid_total_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8ca29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T02:03:35.953224Z",
     "iopub.status.busy": "2022-08-11T02:03:35.952495Z",
     "iopub.status.idle": "2022-08-11T04:42:16.273277Z",
     "shell.execute_reply": "2022-08-11T04:42:16.272034Z"
    },
    "papermill": {
     "duration": 9520.327366,
     "end_time": "2022-08-11T04:42:16.276525",
     "exception": false,
     "start_time": "2022-08-11T02:03:35.949159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: Batch 1/116 loss: 0.0463\n",
      "Epoch 101: Batch 2/116 loss: 0.0684\n",
      "Epoch 101: Batch 3/116 loss: 0.0573\n",
      "Epoch 101: Batch 4/116 loss: 0.0564\n",
      "Epoch 101: Batch 5/116 loss: 0.0476\n",
      "Epoch 101: Batch 6/116 loss: 0.0629\n",
      "Epoch 101: Batch 7/116 loss: 0.0336\n",
      "Epoch 101: Batch 8/116 loss: 0.0497\n",
      "Epoch 101: Batch 9/116 loss: 0.0565\n",
      "Epoch 101: Batch 10/116 loss: 0.0617\n",
      "Epoch 101: Batch 11/116 loss: 0.0507\n",
      "Epoch 101: Batch 12/116 loss: 0.0696\n",
      "Epoch 101: Batch 13/116 loss: 0.0379\n",
      "Epoch 101: Batch 14/116 loss: 0.0539\n",
      "Epoch 101: Batch 15/116 loss: 0.0464\n",
      "Epoch 101: Batch 16/116 loss: 0.0419\n",
      "Epoch 101: Batch 17/116 loss: 0.0390\n",
      "Epoch 101: Batch 18/116 loss: 0.0605\n",
      "Epoch 101: Batch 19/116 loss: 0.0631\n",
      "Epoch 101: Batch 20/116 loss: 0.0638\n",
      "Epoch 101: Batch 21/116 loss: 0.0452\n",
      "Epoch 101: Batch 22/116 loss: 0.0647\n",
      "Epoch 101: Batch 23/116 loss: 0.0405\n",
      "Epoch 101: Batch 24/116 loss: 0.0946\n",
      "Epoch 101: Batch 25/116 loss: 0.0585\n",
      "Epoch 101: Batch 26/116 loss: 0.0566\n",
      "Epoch 101: Batch 27/116 loss: 0.0701\n",
      "Epoch 101: Batch 28/116 loss: 0.0572\n",
      "Epoch 101: Batch 29/116 loss: 0.0539\n",
      "Epoch 101: Batch 30/116 loss: 0.0510\n",
      "Epoch 101: Batch 31/116 loss: 0.0483\n",
      "Epoch 101: Batch 32/116 loss: 0.0478\n",
      "Epoch 101: Batch 33/116 loss: 0.0490\n",
      "Epoch 101: Batch 34/116 loss: 0.0482\n",
      "Epoch 101: Batch 35/116 loss: 0.0435\n",
      "Epoch 101: Batch 36/116 loss: 0.0576\n",
      "Epoch 101: Batch 37/116 loss: 0.0617\n",
      "Epoch 101: Batch 38/116 loss: 0.0403\n",
      "Epoch 101: Batch 39/116 loss: 0.0586\n",
      "Epoch 101: Batch 40/116 loss: 0.0448\n",
      "Epoch 101: Batch 41/116 loss: 0.0583\n",
      "Epoch 101: Batch 42/116 loss: 0.0376\n",
      "Epoch 101: Batch 43/116 loss: 0.0601\n",
      "Epoch 101: Batch 44/116 loss: 0.0471\n",
      "Epoch 101: Batch 45/116 loss: 0.0387\n",
      "Epoch 101: Batch 46/116 loss: 0.0529\n",
      "Epoch 101: Batch 47/116 loss: 0.0684\n",
      "Epoch 101: Batch 48/116 loss: 0.0778\n",
      "Epoch 101: Batch 49/116 loss: 0.0507\n",
      "Epoch 101: Batch 50/116 loss: 0.0466\n",
      "Epoch 101: Batch 51/116 loss: 0.0720\n",
      "Epoch 101: Batch 52/116 loss: 0.0485\n",
      "Epoch 101: Batch 53/116 loss: 0.0598\n",
      "Epoch 101: Batch 54/116 loss: 0.0392\n",
      "Epoch 101: Batch 55/116 loss: 0.0361\n",
      "Epoch 101: Batch 56/116 loss: 0.0538\n",
      "Epoch 101: Batch 57/116 loss: 0.0425\n",
      "Epoch 101: Batch 58/116 loss: 0.0503\n",
      "Epoch 101: Batch 59/116 loss: 0.0558\n",
      "Epoch 101: Batch 60/116 loss: 0.0679\n",
      "Epoch 101: Batch 61/116 loss: 0.0442\n",
      "Epoch 101: Batch 62/116 loss: 0.0427\n",
      "Epoch 101: Batch 63/116 loss: 0.0670\n",
      "Epoch 101: Batch 64/116 loss: 0.0484\n",
      "Epoch 101: Batch 65/116 loss: 0.0813\n",
      "Epoch 101: Batch 66/116 loss: 0.0585\n",
      "Epoch 101: Batch 67/116 loss: 0.0675\n",
      "Epoch 101: Batch 68/116 loss: 0.0548\n",
      "Epoch 101: Batch 69/116 loss: 0.0447\n",
      "Epoch 101: Batch 70/116 loss: 0.0389\n",
      "Epoch 101: Batch 71/116 loss: 0.0557\n",
      "Epoch 101: Batch 72/116 loss: 0.0641\n",
      "Epoch 101: Batch 73/116 loss: 0.0519\n",
      "Epoch 101: Batch 74/116 loss: 0.0345\n",
      "Epoch 101: Batch 75/116 loss: 0.0681\n",
      "Epoch 101: Batch 76/116 loss: 0.0668\n",
      "Epoch 101: Batch 77/116 loss: 0.0479\n",
      "Epoch 101: Batch 78/116 loss: 0.0526\n",
      "Epoch 101: Batch 79/116 loss: 0.0712\n",
      "Epoch 101: Batch 80/116 loss: 0.0458\n",
      "Epoch 101: Batch 81/116 loss: 0.0752\n",
      "Epoch 101: Batch 82/116 loss: 0.0441\n",
      "Epoch 101: Batch 83/116 loss: 0.0655\n",
      "Epoch 101: Batch 84/116 loss: 0.0380\n",
      "Epoch 101: Batch 85/116 loss: 0.0360\n",
      "Epoch 101: Batch 86/116 loss: 0.0603\n",
      "Epoch 101: Batch 87/116 loss: 0.0434\n",
      "Epoch 101: Batch 88/116 loss: 0.0608\n",
      "Epoch 101: Batch 89/116 loss: 0.0456\n",
      "Epoch 101: Batch 90/116 loss: 0.0522\n",
      "Epoch 101: Batch 91/116 loss: 0.0608\n",
      "Epoch 101: Batch 92/116 loss: 0.0847\n",
      "Epoch 101: Batch 93/116 loss: 0.0640\n",
      "Epoch 101: Batch 94/116 loss: 0.0394\n",
      "Epoch 101: Batch 95/116 loss: 0.0424\n",
      "Epoch 101: Batch 96/116 loss: 0.0483\n",
      "Epoch 101: Batch 97/116 loss: 0.0693\n",
      "Epoch 101: Batch 98/116 loss: 0.0454\n",
      "Epoch 101: Batch 99/116 loss: 0.0693\n",
      "Epoch 101: Batch 100/116 loss: 0.0434\n",
      "Epoch 101: Batch 101/116 loss: 0.0476\n",
      "Epoch 101: Batch 102/116 loss: 0.0625\n",
      "Epoch 101: Batch 103/116 loss: 0.0566\n",
      "Epoch 101: Batch 104/116 loss: 0.0575\n",
      "Epoch 101: Batch 105/116 loss: 0.0517\n",
      "Epoch 101: Batch 106/116 loss: 0.0452\n",
      "Epoch 101: Batch 107/116 loss: 0.0436\n",
      "Epoch 101: Batch 108/116 loss: 0.0438\n",
      "Epoch 101: Batch 109/116 loss: 0.0694\n",
      "Epoch 101: Batch 110/116 loss: 0.0605\n",
      "Epoch 101: Batch 111/116 loss: 0.0457\n",
      "Epoch 101: Batch 112/116 loss: 0.0665\n",
      "Epoch 101: Batch 113/116 loss: 0.0483\n",
      "Epoch 101: Batch 114/116 loss: 0.0668\n",
      "Epoch 101: Batch 115/116 loss: 0.0598\n",
      "Epoch 101: Batch 116/116 loss: 0.0452\n",
      "Epoch 101 train loss: 0.0542 valid loss: 0.0700\n",
      "Epoch 102: Batch 1/116 loss: 0.0450\n",
      "Epoch 102: Batch 2/116 loss: 0.0477\n",
      "Epoch 102: Batch 3/116 loss: 0.0578\n",
      "Epoch 102: Batch 4/116 loss: 0.0596\n",
      "Epoch 102: Batch 5/116 loss: 0.0453\n",
      "Epoch 102: Batch 6/116 loss: 0.0709\n",
      "Epoch 102: Batch 7/116 loss: 0.0403\n",
      "Epoch 102: Batch 8/116 loss: 0.0742\n",
      "Epoch 102: Batch 9/116 loss: 0.0618\n",
      "Epoch 102: Batch 10/116 loss: 0.0516\n",
      "Epoch 102: Batch 11/116 loss: 0.0402\n",
      "Epoch 102: Batch 12/116 loss: 0.0507\n",
      "Epoch 102: Batch 13/116 loss: 0.0410\n",
      "Epoch 102: Batch 14/116 loss: 0.0802\n",
      "Epoch 102: Batch 15/116 loss: 0.0559\n",
      "Epoch 102: Batch 16/116 loss: 0.0400\n",
      "Epoch 102: Batch 17/116 loss: 0.0786\n",
      "Epoch 102: Batch 18/116 loss: 0.0700\n",
      "Epoch 102: Batch 19/116 loss: 0.0648\n",
      "Epoch 102: Batch 20/116 loss: 0.0570\n",
      "Epoch 102: Batch 21/116 loss: 0.0447\n",
      "Epoch 102: Batch 22/116 loss: 0.0461\n",
      "Epoch 102: Batch 23/116 loss: 0.0334\n",
      "Epoch 102: Batch 24/116 loss: 0.0573\n",
      "Epoch 102: Batch 25/116 loss: 0.0512\n",
      "Epoch 102: Batch 26/116 loss: 0.0674\n",
      "Epoch 102: Batch 27/116 loss: 0.0473\n",
      "Epoch 102: Batch 28/116 loss: 0.0656\n",
      "Epoch 102: Batch 29/116 loss: 0.0460\n",
      "Epoch 102: Batch 30/116 loss: 0.0375\n",
      "Epoch 102: Batch 31/116 loss: 0.0504\n",
      "Epoch 102: Batch 32/116 loss: 0.0392\n",
      "Epoch 102: Batch 33/116 loss: 0.0667\n",
      "Epoch 102: Batch 34/116 loss: 0.0391\n",
      "Epoch 102: Batch 35/116 loss: 0.0394\n",
      "Epoch 102: Batch 36/116 loss: 0.0569\n",
      "Epoch 102: Batch 37/116 loss: 0.0360\n",
      "Epoch 102: Batch 38/116 loss: 0.0660\n",
      "Epoch 102: Batch 39/116 loss: 0.0548\n",
      "Epoch 102: Batch 40/116 loss: 0.0415\n",
      "Epoch 102: Batch 41/116 loss: 0.0696\n",
      "Epoch 102: Batch 42/116 loss: 0.0682\n",
      "Epoch 102: Batch 43/116 loss: 0.0468\n",
      "Epoch 102: Batch 44/116 loss: 0.0341\n",
      "Epoch 102: Batch 45/116 loss: 0.0392\n",
      "Epoch 102: Batch 46/116 loss: 0.0420\n",
      "Epoch 102: Batch 47/116 loss: 0.0426\n",
      "Epoch 102: Batch 48/116 loss: 0.0601\n",
      "Epoch 102: Batch 49/116 loss: 0.0668\n",
      "Epoch 102: Batch 50/116 loss: 0.0528\n",
      "Epoch 102: Batch 51/116 loss: 0.0646\n",
      "Epoch 102: Batch 52/116 loss: 0.0496\n",
      "Epoch 102: Batch 53/116 loss: 0.0553\n",
      "Epoch 102: Batch 54/116 loss: 0.0678\n",
      "Epoch 102: Batch 55/116 loss: 0.0407\n",
      "Epoch 102: Batch 56/116 loss: 0.0308\n",
      "Epoch 102: Batch 57/116 loss: 0.0745\n",
      "Epoch 102: Batch 58/116 loss: 0.0615\n",
      "Epoch 102: Batch 59/116 loss: 0.0456\n",
      "Epoch 102: Batch 60/116 loss: 0.0636\n",
      "Epoch 102: Batch 61/116 loss: 0.0417\n",
      "Epoch 102: Batch 62/116 loss: 0.0527\n",
      "Epoch 102: Batch 63/116 loss: 0.0510\n",
      "Epoch 102: Batch 64/116 loss: 0.0487\n",
      "Epoch 102: Batch 65/116 loss: 0.0382\n",
      "Epoch 102: Batch 66/116 loss: 0.0489\n",
      "Epoch 102: Batch 67/116 loss: 0.0438\n",
      "Epoch 102: Batch 68/116 loss: 0.0703\n",
      "Epoch 102: Batch 69/116 loss: 0.0724\n",
      "Epoch 102: Batch 70/116 loss: 0.0463\n",
      "Epoch 102: Batch 71/116 loss: 0.0439\n",
      "Epoch 102: Batch 72/116 loss: 0.0560\n",
      "Epoch 102: Batch 73/116 loss: 0.0378\n",
      "Epoch 102: Batch 74/116 loss: 0.0474\n",
      "Epoch 102: Batch 75/116 loss: 0.0488\n",
      "Epoch 102: Batch 76/116 loss: 0.0488\n",
      "Epoch 102: Batch 77/116 loss: 0.0488\n",
      "Epoch 102: Batch 78/116 loss: 0.0696\n",
      "Epoch 102: Batch 79/116 loss: 0.0483\n",
      "Epoch 102: Batch 80/116 loss: 0.0731\n",
      "Epoch 102: Batch 81/116 loss: 0.0500\n",
      "Epoch 102: Batch 82/116 loss: 0.0655\n",
      "Epoch 102: Batch 83/116 loss: 0.0523\n",
      "Epoch 102: Batch 84/116 loss: 0.0505\n",
      "Epoch 102: Batch 85/116 loss: 0.0535\n",
      "Epoch 102: Batch 86/116 loss: 0.0422\n",
      "Epoch 102: Batch 87/116 loss: 0.0310\n",
      "Epoch 102: Batch 88/116 loss: 0.0541\n",
      "Epoch 102: Batch 89/116 loss: 0.0448\n",
      "Epoch 102: Batch 90/116 loss: 0.0474\n",
      "Epoch 102: Batch 91/116 loss: 0.0254\n",
      "Epoch 102: Batch 92/116 loss: 0.0428\n",
      "Epoch 102: Batch 93/116 loss: 0.0376\n",
      "Epoch 102: Batch 94/116 loss: 0.0471\n",
      "Epoch 102: Batch 95/116 loss: 0.0419\n",
      "Epoch 102: Batch 96/116 loss: 0.0333\n",
      "Epoch 102: Batch 97/116 loss: 0.0663\n",
      "Epoch 102: Batch 98/116 loss: 0.0218\n",
      "Epoch 102: Batch 99/116 loss: 0.0584\n",
      "Epoch 102: Batch 100/116 loss: 0.0529\n",
      "Epoch 102: Batch 101/116 loss: 0.0499\n",
      "Epoch 102: Batch 102/116 loss: 0.0445\n",
      "Epoch 102: Batch 103/116 loss: 0.0569\n",
      "Epoch 102: Batch 104/116 loss: 0.0538\n",
      "Epoch 102: Batch 105/116 loss: 0.0434\n",
      "Epoch 102: Batch 106/116 loss: 0.0671\n",
      "Epoch 102: Batch 107/116 loss: 0.0996\n",
      "Epoch 102: Batch 108/116 loss: 0.0652\n",
      "Epoch 102: Batch 109/116 loss: 0.0595\n",
      "Epoch 102: Batch 110/116 loss: 0.0657\n",
      "Epoch 102: Batch 111/116 loss: 0.0634\n",
      "Epoch 102: Batch 112/116 loss: 0.0568\n",
      "Epoch 102: Batch 113/116 loss: 0.0458\n",
      "Epoch 102: Batch 114/116 loss: 0.0508\n",
      "Epoch 102: Batch 115/116 loss: 0.0611\n",
      "Epoch 102: Batch 116/116 loss: 0.0418\n",
      "Epoch 102 train loss: 0.0524 valid loss: 0.0578\n",
      "Epoch 103: Batch 1/116 loss: 0.0541\n",
      "Epoch 103: Batch 2/116 loss: 0.0562\n",
      "Epoch 103: Batch 3/116 loss: 0.0522\n",
      "Epoch 103: Batch 4/116 loss: 0.0454\n",
      "Epoch 103: Batch 5/116 loss: 0.0523\n",
      "Epoch 103: Batch 6/116 loss: 0.0535\n",
      "Epoch 103: Batch 7/116 loss: 0.0597\n",
      "Epoch 103: Batch 8/116 loss: 0.0367\n",
      "Epoch 103: Batch 9/116 loss: 0.0623\n",
      "Epoch 103: Batch 10/116 loss: 0.0524\n",
      "Epoch 103: Batch 11/116 loss: 0.0454\n",
      "Epoch 103: Batch 12/116 loss: 0.0452\n",
      "Epoch 103: Batch 13/116 loss: 0.0356\n",
      "Epoch 103: Batch 14/116 loss: 0.0581\n",
      "Epoch 103: Batch 15/116 loss: 0.0780\n",
      "Epoch 103: Batch 16/116 loss: 0.0536\n",
      "Epoch 103: Batch 17/116 loss: 0.0763\n",
      "Epoch 103: Batch 18/116 loss: 0.0793\n",
      "Epoch 103: Batch 19/116 loss: 0.0514\n",
      "Epoch 103: Batch 20/116 loss: 0.0540\n",
      "Epoch 103: Batch 21/116 loss: 0.0422\n",
      "Epoch 103: Batch 22/116 loss: 0.0631\n",
      "Epoch 103: Batch 23/116 loss: 0.0596\n",
      "Epoch 103: Batch 24/116 loss: 0.0766\n",
      "Epoch 103: Batch 25/116 loss: 0.0470\n",
      "Epoch 103: Batch 26/116 loss: 0.0760\n",
      "Epoch 103: Batch 27/116 loss: 0.0549\n",
      "Epoch 103: Batch 28/116 loss: 0.0558\n",
      "Epoch 103: Batch 29/116 loss: 0.0385\n",
      "Epoch 103: Batch 30/116 loss: 0.0496\n",
      "Epoch 103: Batch 31/116 loss: 0.0543\n",
      "Epoch 103: Batch 32/116 loss: 0.0449\n",
      "Epoch 103: Batch 33/116 loss: 0.0546\n",
      "Epoch 103: Batch 34/116 loss: 0.0449\n",
      "Epoch 103: Batch 35/116 loss: 0.0339\n",
      "Epoch 103: Batch 36/116 loss: 0.0324\n",
      "Epoch 103: Batch 37/116 loss: 0.0685\n",
      "Epoch 103: Batch 38/116 loss: 0.0547\n",
      "Epoch 103: Batch 39/116 loss: 0.0390\n",
      "Epoch 103: Batch 40/116 loss: 0.0406\n",
      "Epoch 103: Batch 41/116 loss: 0.0508\n",
      "Epoch 103: Batch 42/116 loss: 0.0344\n",
      "Epoch 103: Batch 43/116 loss: 0.0392\n",
      "Epoch 103: Batch 44/116 loss: 0.0551\n",
      "Epoch 103: Batch 45/116 loss: 0.0587\n",
      "Epoch 103: Batch 46/116 loss: 0.0501\n",
      "Epoch 103: Batch 47/116 loss: 0.0497\n",
      "Epoch 103: Batch 48/116 loss: 0.0653\n",
      "Epoch 103: Batch 49/116 loss: 0.0335\n",
      "Epoch 103: Batch 50/116 loss: 0.0505\n",
      "Epoch 103: Batch 51/116 loss: 0.0701\n",
      "Epoch 103: Batch 52/116 loss: 0.0378\n",
      "Epoch 103: Batch 53/116 loss: 0.0350\n",
      "Epoch 103: Batch 54/116 loss: 0.0302\n",
      "Epoch 103: Batch 55/116 loss: 0.0444\n",
      "Epoch 103: Batch 56/116 loss: 0.0488\n",
      "Epoch 103: Batch 57/116 loss: 0.0346\n",
      "Epoch 103: Batch 58/116 loss: 0.0374\n",
      "Epoch 103: Batch 59/116 loss: 0.0684\n",
      "Epoch 103: Batch 60/116 loss: 0.0686\n",
      "Epoch 103: Batch 61/116 loss: 0.0521\n",
      "Epoch 103: Batch 62/116 loss: 0.0461\n",
      "Epoch 103: Batch 63/116 loss: 0.0648\n",
      "Epoch 103: Batch 64/116 loss: 0.0785\n",
      "Epoch 103: Batch 65/116 loss: 0.0352\n",
      "Epoch 103: Batch 66/116 loss: 0.0478\n",
      "Epoch 103: Batch 67/116 loss: 0.0770\n",
      "Epoch 103: Batch 68/116 loss: 0.0453\n",
      "Epoch 103: Batch 69/116 loss: 0.0709\n",
      "Epoch 103: Batch 70/116 loss: 0.0688\n",
      "Epoch 103: Batch 71/116 loss: 0.0498\n",
      "Epoch 103: Batch 72/116 loss: 0.0625\n",
      "Epoch 103: Batch 73/116 loss: 0.0565\n",
      "Epoch 103: Batch 74/116 loss: 0.0330\n",
      "Epoch 103: Batch 75/116 loss: 0.0520\n",
      "Epoch 103: Batch 76/116 loss: 0.0484\n",
      "Epoch 103: Batch 77/116 loss: 0.0429\n",
      "Epoch 103: Batch 78/116 loss: 0.0977\n",
      "Epoch 103: Batch 79/116 loss: 0.0572\n",
      "Epoch 103: Batch 80/116 loss: 0.0407\n",
      "Epoch 103: Batch 81/116 loss: 0.0765\n",
      "Epoch 103: Batch 82/116 loss: 0.0533\n",
      "Epoch 103: Batch 83/116 loss: 0.0373\n",
      "Epoch 103: Batch 84/116 loss: 0.0579\n",
      "Epoch 103: Batch 85/116 loss: 0.0557\n",
      "Epoch 103: Batch 86/116 loss: 0.0349\n",
      "Epoch 103: Batch 87/116 loss: 0.0789\n",
      "Epoch 103: Batch 88/116 loss: 0.0685\n",
      "Epoch 103: Batch 89/116 loss: 0.0466\n",
      "Epoch 103: Batch 90/116 loss: 0.0669\n",
      "Epoch 103: Batch 91/116 loss: 0.0603\n",
      "Epoch 103: Batch 92/116 loss: 0.0330\n",
      "Epoch 103: Batch 93/116 loss: 0.0566\n",
      "Epoch 103: Batch 94/116 loss: 0.0525\n",
      "Epoch 103: Batch 95/116 loss: 0.0456\n",
      "Epoch 103: Batch 96/116 loss: 0.0647\n",
      "Epoch 103: Batch 97/116 loss: 0.0551\n",
      "Epoch 103: Batch 98/116 loss: 0.0565\n",
      "Epoch 103: Batch 99/116 loss: 0.0358\n",
      "Epoch 103: Batch 100/116 loss: 0.0419\n",
      "Epoch 103: Batch 101/116 loss: 0.0449\n",
      "Epoch 103: Batch 102/116 loss: 0.0418\n",
      "Epoch 103: Batch 103/116 loss: 0.0517\n",
      "Epoch 103: Batch 104/116 loss: 0.0532\n",
      "Epoch 103: Batch 105/116 loss: 0.0603\n",
      "Epoch 103: Batch 106/116 loss: 0.0459\n",
      "Epoch 103: Batch 107/116 loss: 0.0478\n",
      "Epoch 103: Batch 108/116 loss: 0.0510\n",
      "Epoch 103: Batch 109/116 loss: 0.0295\n",
      "Epoch 103: Batch 110/116 loss: 0.0511\n",
      "Epoch 103: Batch 111/116 loss: 0.0409\n",
      "Epoch 103: Batch 112/116 loss: 0.0497\n",
      "Epoch 103: Batch 113/116 loss: 0.0452\n",
      "Epoch 103: Batch 114/116 loss: 0.0588\n",
      "Epoch 103: Batch 115/116 loss: 0.0580\n",
      "Epoch 103: Batch 116/116 loss: 0.0438\n",
      "Epoch 103 train loss: 0.0524 valid loss: 0.0564\n",
      "Epoch 104: Batch 1/116 loss: 0.0749\n",
      "Epoch 104: Batch 2/116 loss: 0.0565\n",
      "Epoch 104: Batch 3/116 loss: 0.0617\n",
      "Epoch 104: Batch 4/116 loss: 0.0468\n",
      "Epoch 104: Batch 5/116 loss: 0.0520\n",
      "Epoch 104: Batch 6/116 loss: 0.0699\n",
      "Epoch 104: Batch 7/116 loss: 0.0747\n",
      "Epoch 104: Batch 8/116 loss: 0.0566\n",
      "Epoch 104: Batch 9/116 loss: 0.0564\n",
      "Epoch 104: Batch 10/116 loss: 0.0590\n",
      "Epoch 104: Batch 11/116 loss: 0.0719\n",
      "Epoch 104: Batch 12/116 loss: 0.0381\n",
      "Epoch 104: Batch 13/116 loss: 0.0407\n",
      "Epoch 104: Batch 14/116 loss: 0.0435\n",
      "Epoch 104: Batch 15/116 loss: 0.0711\n",
      "Epoch 104: Batch 16/116 loss: 0.0517\n",
      "Epoch 104: Batch 17/116 loss: 0.0541\n",
      "Epoch 104: Batch 18/116 loss: 0.0487\n",
      "Epoch 104: Batch 19/116 loss: 0.0365\n",
      "Epoch 104: Batch 20/116 loss: 0.0413\n",
      "Epoch 104: Batch 21/116 loss: 0.0509\n",
      "Epoch 104: Batch 22/116 loss: 0.0288\n",
      "Epoch 104: Batch 23/116 loss: 0.0482\n",
      "Epoch 104: Batch 24/116 loss: 0.0424\n",
      "Epoch 104: Batch 25/116 loss: 0.0438\n",
      "Epoch 104: Batch 26/116 loss: 0.0815\n",
      "Epoch 104: Batch 27/116 loss: 0.0654\n",
      "Epoch 104: Batch 28/116 loss: 0.0405\n",
      "Epoch 104: Batch 29/116 loss: 0.0533\n",
      "Epoch 104: Batch 30/116 loss: 0.0623\n",
      "Epoch 104: Batch 31/116 loss: 0.0726\n",
      "Epoch 104: Batch 32/116 loss: 0.0430\n",
      "Epoch 104: Batch 33/116 loss: 0.0467\n",
      "Epoch 104: Batch 34/116 loss: 0.0529\n",
      "Epoch 104: Batch 35/116 loss: 0.0461\n",
      "Epoch 104: Batch 36/116 loss: 0.0457\n",
      "Epoch 104: Batch 37/116 loss: 0.0596\n",
      "Epoch 104: Batch 38/116 loss: 0.0604\n",
      "Epoch 104: Batch 39/116 loss: 0.0676\n",
      "Epoch 104: Batch 40/116 loss: 0.0622\n",
      "Epoch 104: Batch 41/116 loss: 0.0456\n",
      "Epoch 104: Batch 42/116 loss: 0.0544\n",
      "Epoch 104: Batch 43/116 loss: 0.0406\n",
      "Epoch 104: Batch 44/116 loss: 0.0437\n",
      "Epoch 104: Batch 45/116 loss: 0.0476\n",
      "Epoch 104: Batch 46/116 loss: 0.0425\n",
      "Epoch 104: Batch 47/116 loss: 0.0641\n",
      "Epoch 104: Batch 48/116 loss: 0.0465\n",
      "Epoch 104: Batch 49/116 loss: 0.0503\n",
      "Epoch 104: Batch 50/116 loss: 0.0491\n",
      "Epoch 104: Batch 51/116 loss: 0.0554\n",
      "Epoch 104: Batch 52/116 loss: 0.0394\n",
      "Epoch 104: Batch 53/116 loss: 0.0425\n",
      "Epoch 104: Batch 54/116 loss: 0.0525\n",
      "Epoch 104: Batch 55/116 loss: 0.0408\n",
      "Epoch 104: Batch 56/116 loss: 0.0458\n",
      "Epoch 104: Batch 57/116 loss: 0.0435\n",
      "Epoch 104: Batch 58/116 loss: 0.0379\n",
      "Epoch 104: Batch 59/116 loss: 0.0523\n",
      "Epoch 104: Batch 60/116 loss: 0.0527\n",
      "Epoch 104: Batch 61/116 loss: 0.0686\n",
      "Epoch 104: Batch 62/116 loss: 0.0511\n",
      "Epoch 104: Batch 63/116 loss: 0.0452\n",
      "Epoch 104: Batch 64/116 loss: 0.0360\n",
      "Epoch 104: Batch 65/116 loss: 0.0542\n",
      "Epoch 104: Batch 66/116 loss: 0.0411\n",
      "Epoch 104: Batch 67/116 loss: 0.0475\n",
      "Epoch 104: Batch 68/116 loss: 0.0438\n",
      "Epoch 104: Batch 69/116 loss: 0.0452\n",
      "Epoch 104: Batch 70/116 loss: 0.0458\n",
      "Epoch 104: Batch 71/116 loss: 0.0876\n",
      "Epoch 104: Batch 72/116 loss: 0.0361\n",
      "Epoch 104: Batch 73/116 loss: 0.0487\n",
      "Epoch 104: Batch 74/116 loss: 0.0548\n",
      "Epoch 104: Batch 75/116 loss: 0.0480\n",
      "Epoch 104: Batch 76/116 loss: 0.0596\n",
      "Epoch 104: Batch 77/116 loss: 0.0742\n",
      "Epoch 104: Batch 78/116 loss: 0.0323\n",
      "Epoch 104: Batch 79/116 loss: 0.0628\n",
      "Epoch 104: Batch 80/116 loss: 0.0481\n",
      "Epoch 104: Batch 81/116 loss: 0.0574\n",
      "Epoch 104: Batch 82/116 loss: 0.0448\n",
      "Epoch 104: Batch 83/116 loss: 0.0469\n",
      "Epoch 104: Batch 84/116 loss: 0.0396\n",
      "Epoch 104: Batch 85/116 loss: 0.0495\n",
      "Epoch 104: Batch 86/116 loss: 0.0487\n",
      "Epoch 104: Batch 87/116 loss: 0.0645\n",
      "Epoch 104: Batch 88/116 loss: 0.0579\n",
      "Epoch 104: Batch 89/116 loss: 0.0804\n",
      "Epoch 104: Batch 90/116 loss: 0.0357\n",
      "Epoch 104: Batch 91/116 loss: 0.0433\n",
      "Epoch 104: Batch 92/116 loss: 0.0520\n",
      "Epoch 104: Batch 93/116 loss: 0.0705\n",
      "Epoch 104: Batch 94/116 loss: 0.0518\n",
      "Epoch 104: Batch 95/116 loss: 0.0670\n",
      "Epoch 104: Batch 96/116 loss: 0.0642\n",
      "Epoch 104: Batch 97/116 loss: 0.0466\n",
      "Epoch 104: Batch 98/116 loss: 0.0607\n",
      "Epoch 104: Batch 99/116 loss: 0.0324\n",
      "Epoch 104: Batch 100/116 loss: 0.0419\n",
      "Epoch 104: Batch 101/116 loss: 0.0560\n",
      "Epoch 104: Batch 102/116 loss: 0.0386\n",
      "Epoch 104: Batch 103/116 loss: 0.1006\n",
      "Epoch 104: Batch 104/116 loss: 0.0426\n",
      "Epoch 104: Batch 105/116 loss: 0.0408\n",
      "Epoch 104: Batch 106/116 loss: 0.0484\n",
      "Epoch 104: Batch 107/116 loss: 0.0415\n",
      "Epoch 104: Batch 108/116 loss: 0.0439\n",
      "Epoch 104: Batch 109/116 loss: 0.0474\n",
      "Epoch 104: Batch 110/116 loss: 0.0496\n",
      "Epoch 104: Batch 111/116 loss: 0.0380\n",
      "Epoch 104: Batch 112/116 loss: 0.0387\n",
      "Epoch 104: Batch 113/116 loss: 0.0506\n",
      "Epoch 104: Batch 114/116 loss: 0.0345\n",
      "Epoch 104: Batch 115/116 loss: 0.0401\n",
      "Epoch 104: Batch 116/116 loss: 0.0628\n",
      "Epoch 104 train loss: 0.0516 valid loss: 0.0547\n",
      "Epoch 105: Batch 1/116 loss: 0.0444\n",
      "Epoch 105: Batch 2/116 loss: 0.0579\n",
      "Epoch 105: Batch 3/116 loss: 0.0480\n",
      "Epoch 105: Batch 4/116 loss: 0.0513\n",
      "Epoch 105: Batch 5/116 loss: 0.0479\n",
      "Epoch 105: Batch 6/116 loss: 0.0419\n",
      "Epoch 105: Batch 7/116 loss: 0.0426\n",
      "Epoch 105: Batch 8/116 loss: 0.0486\n",
      "Epoch 105: Batch 9/116 loss: 0.0392\n",
      "Epoch 105: Batch 10/116 loss: 0.0564\n",
      "Epoch 105: Batch 11/116 loss: 0.0564\n",
      "Epoch 105: Batch 12/116 loss: 0.0541\n",
      "Epoch 105: Batch 13/116 loss: 0.0499\n",
      "Epoch 105: Batch 14/116 loss: 0.0702\n",
      "Epoch 105: Batch 15/116 loss: 0.0351\n",
      "Epoch 105: Batch 16/116 loss: 0.0635\n",
      "Epoch 105: Batch 17/116 loss: 0.0643\n",
      "Epoch 105: Batch 18/116 loss: 0.0612\n",
      "Epoch 105: Batch 19/116 loss: 0.0422\n",
      "Epoch 105: Batch 20/116 loss: 0.0654\n",
      "Epoch 105: Batch 21/116 loss: 0.0536\n",
      "Epoch 105: Batch 22/116 loss: 0.0571\n",
      "Epoch 105: Batch 23/116 loss: 0.0668\n",
      "Epoch 105: Batch 24/116 loss: 0.0481\n",
      "Epoch 105: Batch 25/116 loss: 0.0662\n",
      "Epoch 105: Batch 26/116 loss: 0.0495\n",
      "Epoch 105: Batch 27/116 loss: 0.0821\n",
      "Epoch 105: Batch 28/116 loss: 0.0587\n",
      "Epoch 105: Batch 29/116 loss: 0.0318\n",
      "Epoch 105: Batch 30/116 loss: 0.0425\n",
      "Epoch 105: Batch 31/116 loss: 0.0404\n",
      "Epoch 105: Batch 32/116 loss: 0.0682\n",
      "Epoch 105: Batch 33/116 loss: 0.0395\n",
      "Epoch 105: Batch 34/116 loss: 0.0569\n",
      "Epoch 105: Batch 35/116 loss: 0.0394\n",
      "Epoch 105: Batch 36/116 loss: 0.0490\n",
      "Epoch 105: Batch 37/116 loss: 0.0513\n",
      "Epoch 105: Batch 38/116 loss: 0.0368\n",
      "Epoch 105: Batch 39/116 loss: 0.0535\n",
      "Epoch 105: Batch 40/116 loss: 0.0546\n",
      "Epoch 105: Batch 41/116 loss: 0.0467\n",
      "Epoch 105: Batch 42/116 loss: 0.0504\n",
      "Epoch 105: Batch 43/116 loss: 0.0474\n",
      "Epoch 105: Batch 44/116 loss: 0.0329\n",
      "Epoch 105: Batch 45/116 loss: 0.0528\n",
      "Epoch 105: Batch 46/116 loss: 0.0462\n",
      "Epoch 105: Batch 47/116 loss: 0.0615\n",
      "Epoch 105: Batch 48/116 loss: 0.0562\n",
      "Epoch 105: Batch 49/116 loss: 0.0311\n",
      "Epoch 105: Batch 50/116 loss: 0.0436\n",
      "Epoch 105: Batch 51/116 loss: 0.0451\n",
      "Epoch 105: Batch 52/116 loss: 0.0698\n",
      "Epoch 105: Batch 53/116 loss: 0.0612\n",
      "Epoch 105: Batch 54/116 loss: 0.0486\n",
      "Epoch 105: Batch 55/116 loss: 0.0522\n",
      "Epoch 105: Batch 56/116 loss: 0.0559\n",
      "Epoch 105: Batch 57/116 loss: 0.0699\n",
      "Epoch 105: Batch 58/116 loss: 0.0592\n",
      "Epoch 105: Batch 59/116 loss: 0.0766\n",
      "Epoch 105: Batch 60/116 loss: 0.0456\n",
      "Epoch 105: Batch 61/116 loss: 0.0551\n",
      "Epoch 105: Batch 62/116 loss: 0.0404\n",
      "Epoch 105: Batch 63/116 loss: 0.0432\n",
      "Epoch 105: Batch 64/116 loss: 0.0420\n",
      "Epoch 105: Batch 65/116 loss: 0.0423\n",
      "Epoch 105: Batch 66/116 loss: 0.0420\n",
      "Epoch 105: Batch 67/116 loss: 0.0699\n",
      "Epoch 105: Batch 68/116 loss: 0.0337\n",
      "Epoch 105: Batch 69/116 loss: 0.0352\n",
      "Epoch 105: Batch 70/116 loss: 0.0431\n",
      "Epoch 105: Batch 71/116 loss: 0.0341\n",
      "Epoch 105: Batch 72/116 loss: 0.0557\n",
      "Epoch 105: Batch 73/116 loss: 0.0278\n",
      "Epoch 105: Batch 74/116 loss: 0.0550\n",
      "Epoch 105: Batch 75/116 loss: 0.0400\n",
      "Epoch 105: Batch 76/116 loss: 0.0605\n",
      "Epoch 105: Batch 77/116 loss: 0.0545\n",
      "Epoch 105: Batch 78/116 loss: 0.0491\n",
      "Epoch 105: Batch 79/116 loss: 0.0526\n",
      "Epoch 105: Batch 80/116 loss: 0.0554\n",
      "Epoch 105: Batch 81/116 loss: 0.0546\n",
      "Epoch 105: Batch 82/116 loss: 0.0604\n",
      "Epoch 105: Batch 83/116 loss: 0.0379\n",
      "Epoch 105: Batch 84/116 loss: 0.0753\n",
      "Epoch 105: Batch 85/116 loss: 0.0273\n",
      "Epoch 105: Batch 86/116 loss: 0.0675\n",
      "Epoch 105: Batch 87/116 loss: 0.0490\n",
      "Epoch 105: Batch 88/116 loss: 0.0423\n",
      "Epoch 105: Batch 89/116 loss: 0.0670\n",
      "Epoch 105: Batch 90/116 loss: 0.0612\n",
      "Epoch 105: Batch 91/116 loss: 0.0735\n",
      "Epoch 105: Batch 92/116 loss: 0.0611\n",
      "Epoch 105: Batch 93/116 loss: 0.0660\n",
      "Epoch 105: Batch 94/116 loss: 0.0401\n",
      "Epoch 105: Batch 95/116 loss: 0.0489\n",
      "Epoch 105: Batch 96/116 loss: 0.0618\n",
      "Epoch 105: Batch 97/116 loss: 0.0548\n",
      "Epoch 105: Batch 98/116 loss: 0.0273\n",
      "Epoch 105: Batch 99/116 loss: 0.0354\n",
      "Epoch 105: Batch 100/116 loss: 0.0490\n",
      "Epoch 105: Batch 101/116 loss: 0.0392\n",
      "Epoch 105: Batch 102/116 loss: 0.0489\n",
      "Epoch 105: Batch 103/116 loss: 0.0343\n",
      "Epoch 105: Batch 104/116 loss: 0.0554\n",
      "Epoch 105: Batch 105/116 loss: 0.0478\n",
      "Epoch 105: Batch 106/116 loss: 0.0605\n",
      "Epoch 105: Batch 107/116 loss: 0.0703\n",
      "Epoch 105: Batch 108/116 loss: 0.0646\n",
      "Epoch 105: Batch 109/116 loss: 0.0600\n",
      "Epoch 105: Batch 110/116 loss: 0.0560\n",
      "Epoch 105: Batch 111/116 loss: 0.0671\n",
      "Epoch 105: Batch 112/116 loss: 0.0495\n",
      "Epoch 105: Batch 113/116 loss: 0.0423\n",
      "Epoch 105: Batch 114/116 loss: 0.0454\n",
      "Epoch 105: Batch 115/116 loss: 0.0494\n",
      "Epoch 105: Batch 116/116 loss: 0.0455\n",
      "Epoch 105 train loss: 0.0514 valid loss: 0.0543\n",
      "Epoch 106: Batch 1/116 loss: 0.0465\n",
      "Epoch 106: Batch 2/116 loss: 0.0409\n",
      "Epoch 106: Batch 3/116 loss: 0.0470\n",
      "Epoch 106: Batch 4/116 loss: 0.0502\n",
      "Epoch 106: Batch 5/116 loss: 0.0560\n",
      "Epoch 106: Batch 6/116 loss: 0.0651\n",
      "Epoch 106: Batch 7/116 loss: 0.0445\n",
      "Epoch 106: Batch 8/116 loss: 0.0471\n",
      "Epoch 106: Batch 9/116 loss: 0.0444\n",
      "Epoch 106: Batch 10/116 loss: 0.0582\n",
      "Epoch 106: Batch 11/116 loss: 0.0915\n",
      "Epoch 106: Batch 12/116 loss: 0.0392\n",
      "Epoch 106: Batch 13/116 loss: 0.0319\n",
      "Epoch 106: Batch 14/116 loss: 0.0571\n",
      "Epoch 106: Batch 15/116 loss: 0.0363\n",
      "Epoch 106: Batch 16/116 loss: 0.0372\n",
      "Epoch 106: Batch 17/116 loss: 0.0670\n",
      "Epoch 106: Batch 18/116 loss: 0.0474\n",
      "Epoch 106: Batch 19/116 loss: 0.0371\n",
      "Epoch 106: Batch 20/116 loss: 0.0409\n",
      "Epoch 106: Batch 21/116 loss: 0.0489\n",
      "Epoch 106: Batch 22/116 loss: 0.0658\n",
      "Epoch 106: Batch 23/116 loss: 0.0542\n",
      "Epoch 106: Batch 24/116 loss: 0.0402\n",
      "Epoch 106: Batch 25/116 loss: 0.0764\n",
      "Epoch 106: Batch 26/116 loss: 0.0590\n",
      "Epoch 106: Batch 27/116 loss: 0.0544\n",
      "Epoch 106: Batch 28/116 loss: 0.0633\n",
      "Epoch 106: Batch 29/116 loss: 0.0331\n",
      "Epoch 106: Batch 30/116 loss: 0.0512\n",
      "Epoch 106: Batch 31/116 loss: 0.0567\n",
      "Epoch 106: Batch 32/116 loss: 0.0334\n",
      "Epoch 106: Batch 33/116 loss: 0.0438\n",
      "Epoch 106: Batch 34/116 loss: 0.0926\n",
      "Epoch 106: Batch 35/116 loss: 0.0508\n",
      "Epoch 106: Batch 36/116 loss: 0.0602\n",
      "Epoch 106: Batch 37/116 loss: 0.0458\n",
      "Epoch 106: Batch 38/116 loss: 0.0380\n",
      "Epoch 106: Batch 39/116 loss: 0.0367\n",
      "Epoch 106: Batch 40/116 loss: 0.0726\n",
      "Epoch 106: Batch 41/116 loss: 0.0405\n",
      "Epoch 106: Batch 42/116 loss: 0.0734\n",
      "Epoch 106: Batch 43/116 loss: 0.0557\n",
      "Epoch 106: Batch 44/116 loss: 0.0615\n",
      "Epoch 106: Batch 45/116 loss: 0.0651\n",
      "Epoch 106: Batch 46/116 loss: 0.0403\n",
      "Epoch 106: Batch 47/116 loss: 0.0425\n",
      "Epoch 106: Batch 48/116 loss: 0.0681\n",
      "Epoch 106: Batch 49/116 loss: 0.0666\n",
      "Epoch 106: Batch 50/116 loss: 0.0535\n",
      "Epoch 106: Batch 51/116 loss: 0.0648\n",
      "Epoch 106: Batch 52/116 loss: 0.0376\n",
      "Epoch 106: Batch 53/116 loss: 0.0417\n",
      "Epoch 106: Batch 54/116 loss: 0.0501\n",
      "Epoch 106: Batch 55/116 loss: 0.0529\n",
      "Epoch 106: Batch 56/116 loss: 0.0285\n",
      "Epoch 106: Batch 57/116 loss: 0.0488\n",
      "Epoch 106: Batch 58/116 loss: 0.0611\n",
      "Epoch 106: Batch 59/116 loss: 0.0474\n",
      "Epoch 106: Batch 60/116 loss: 0.0637\n",
      "Epoch 106: Batch 61/116 loss: 0.0522\n",
      "Epoch 106: Batch 62/116 loss: 0.0678\n",
      "Epoch 106: Batch 63/116 loss: 0.0407\n",
      "Epoch 106: Batch 64/116 loss: 0.0385\n",
      "Epoch 106: Batch 65/116 loss: 0.0552\n",
      "Epoch 106: Batch 66/116 loss: 0.0825\n",
      "Epoch 106: Batch 67/116 loss: 0.0500\n",
      "Epoch 106: Batch 68/116 loss: 0.0412\n",
      "Epoch 106: Batch 69/116 loss: 0.0520\n",
      "Epoch 106: Batch 70/116 loss: 0.0514\n",
      "Epoch 106: Batch 71/116 loss: 0.0593\n",
      "Epoch 106: Batch 72/116 loss: 0.0472\n",
      "Epoch 106: Batch 73/116 loss: 0.0615\n",
      "Epoch 106: Batch 74/116 loss: 0.0407\n",
      "Epoch 106: Batch 75/116 loss: 0.0467\n",
      "Epoch 106: Batch 76/116 loss: 0.0430\n",
      "Epoch 106: Batch 77/116 loss: 0.0350\n",
      "Epoch 106: Batch 78/116 loss: 0.0463\n",
      "Epoch 106: Batch 79/116 loss: 0.0418\n",
      "Epoch 106: Batch 80/116 loss: 0.0521\n",
      "Epoch 106: Batch 81/116 loss: 0.0444\n",
      "Epoch 106: Batch 82/116 loss: 0.0526\n",
      "Epoch 106: Batch 83/116 loss: 0.0537\n",
      "Epoch 106: Batch 84/116 loss: 0.0613\n",
      "Epoch 106: Batch 85/116 loss: 0.0481\n",
      "Epoch 106: Batch 86/116 loss: 0.0654\n",
      "Epoch 106: Batch 87/116 loss: 0.0492\n",
      "Epoch 106: Batch 88/116 loss: 0.0534\n",
      "Epoch 106: Batch 89/116 loss: 0.0436\n",
      "Epoch 106: Batch 90/116 loss: 0.0333\n",
      "Epoch 106: Batch 91/116 loss: 0.0657\n",
      "Epoch 106: Batch 92/116 loss: 0.0687\n",
      "Epoch 106: Batch 93/116 loss: 0.0476\n",
      "Epoch 106: Batch 94/116 loss: 0.0451\n",
      "Epoch 106: Batch 95/116 loss: 0.0393\n",
      "Epoch 106: Batch 96/116 loss: 0.0354\n",
      "Epoch 106: Batch 97/116 loss: 0.0591\n",
      "Epoch 106: Batch 98/116 loss: 0.0609\n",
      "Epoch 106: Batch 99/116 loss: 0.0402\n",
      "Epoch 106: Batch 100/116 loss: 0.0388\n",
      "Epoch 106: Batch 101/116 loss: 0.0461\n",
      "Epoch 106: Batch 102/116 loss: 0.0577\n",
      "Epoch 106: Batch 103/116 loss: 0.0271\n",
      "Epoch 106: Batch 104/116 loss: 0.0467\n",
      "Epoch 106: Batch 105/116 loss: 0.0636\n",
      "Epoch 106: Batch 106/116 loss: 0.0610\n",
      "Epoch 106: Batch 107/116 loss: 0.0473\n",
      "Epoch 106: Batch 108/116 loss: 0.0519\n",
      "Epoch 106: Batch 109/116 loss: 0.0527\n",
      "Epoch 106: Batch 110/116 loss: 0.0355\n",
      "Epoch 106: Batch 111/116 loss: 0.0442\n",
      "Epoch 106: Batch 112/116 loss: 0.0524\n",
      "Epoch 106: Batch 113/116 loss: 0.0457\n",
      "Epoch 106: Batch 114/116 loss: 0.0775\n",
      "Epoch 106: Batch 115/116 loss: 0.0481\n",
      "Epoch 106: Batch 116/116 loss: 0.0765\n",
      "Epoch 106 train loss: 0.0515 valid loss: 0.0575\n",
      "performance reducing: 1\n",
      "Epoch 107: Batch 1/116 loss: 0.0431\n",
      "Epoch 107: Batch 2/116 loss: 0.0459\n",
      "Epoch 107: Batch 3/116 loss: 0.0544\n",
      "Epoch 107: Batch 4/116 loss: 0.0378\n",
      "Epoch 107: Batch 5/116 loss: 0.0355\n",
      "Epoch 107: Batch 6/116 loss: 0.0403\n",
      "Epoch 107: Batch 7/116 loss: 0.0410\n",
      "Epoch 107: Batch 8/116 loss: 0.0419\n",
      "Epoch 107: Batch 9/116 loss: 0.0629\n",
      "Epoch 107: Batch 10/116 loss: 0.0599\n",
      "Epoch 107: Batch 11/116 loss: 0.0366\n",
      "Epoch 107: Batch 12/116 loss: 0.0972\n",
      "Epoch 107: Batch 13/116 loss: 0.0578\n",
      "Epoch 107: Batch 14/116 loss: 0.0585\n",
      "Epoch 107: Batch 15/116 loss: 0.0597\n",
      "Epoch 107: Batch 16/116 loss: 0.0556\n",
      "Epoch 107: Batch 17/116 loss: 0.0476\n",
      "Epoch 107: Batch 18/116 loss: 0.0459\n",
      "Epoch 107: Batch 19/116 loss: 0.0666\n",
      "Epoch 107: Batch 20/116 loss: 0.0845\n",
      "Epoch 107: Batch 21/116 loss: 0.0565\n",
      "Epoch 107: Batch 22/116 loss: 0.0593\n",
      "Epoch 107: Batch 23/116 loss: 0.0704\n",
      "Epoch 107: Batch 24/116 loss: 0.0413\n",
      "Epoch 107: Batch 25/116 loss: 0.0397\n",
      "Epoch 107: Batch 26/116 loss: 0.0542\n",
      "Epoch 107: Batch 27/116 loss: 0.0687\n",
      "Epoch 107: Batch 28/116 loss: 0.0459\n",
      "Epoch 107: Batch 29/116 loss: 0.0253\n",
      "Epoch 107: Batch 30/116 loss: 0.0433\n",
      "Epoch 107: Batch 31/116 loss: 0.0734\n",
      "Epoch 107: Batch 32/116 loss: 0.0505\n",
      "Epoch 107: Batch 33/116 loss: 0.0365\n",
      "Epoch 107: Batch 34/116 loss: 0.0580\n",
      "Epoch 107: Batch 35/116 loss: 0.0395\n",
      "Epoch 107: Batch 36/116 loss: 0.0713\n",
      "Epoch 107: Batch 37/116 loss: 0.0410\n",
      "Epoch 107: Batch 38/116 loss: 0.0388\n",
      "Epoch 107: Batch 39/116 loss: 0.0453\n",
      "Epoch 107: Batch 40/116 loss: 0.0362\n",
      "Epoch 107: Batch 41/116 loss: 0.0428\n",
      "Epoch 107: Batch 42/116 loss: 0.0642\n",
      "Epoch 107: Batch 43/116 loss: 0.0447\n",
      "Epoch 107: Batch 44/116 loss: 0.0521\n",
      "Epoch 107: Batch 45/116 loss: 0.0724\n",
      "Epoch 107: Batch 46/116 loss: 0.0544\n",
      "Epoch 107: Batch 47/116 loss: 0.0308\n",
      "Epoch 107: Batch 48/116 loss: 0.0428\n",
      "Epoch 107: Batch 49/116 loss: 0.0362\n",
      "Epoch 107: Batch 50/116 loss: 0.0388\n",
      "Epoch 107: Batch 51/116 loss: 0.0468\n",
      "Epoch 107: Batch 52/116 loss: 0.0650\n",
      "Epoch 107: Batch 53/116 loss: 0.0547\n",
      "Epoch 107: Batch 54/116 loss: 0.0418\n",
      "Epoch 107: Batch 55/116 loss: 0.0661\n",
      "Epoch 107: Batch 56/116 loss: 0.0566\n",
      "Epoch 107: Batch 57/116 loss: 0.0420\n",
      "Epoch 107: Batch 58/116 loss: 0.0580\n",
      "Epoch 107: Batch 59/116 loss: 0.0485\n",
      "Epoch 107: Batch 60/116 loss: 0.0412\n",
      "Epoch 107: Batch 61/116 loss: 0.0365\n",
      "Epoch 107: Batch 62/116 loss: 0.0433\n",
      "Epoch 107: Batch 63/116 loss: 0.0622\n",
      "Epoch 107: Batch 64/116 loss: 0.0609\n",
      "Epoch 107: Batch 65/116 loss: 0.0465\n",
      "Epoch 107: Batch 66/116 loss: 0.0570\n",
      "Epoch 107: Batch 67/116 loss: 0.0573\n",
      "Epoch 107: Batch 68/116 loss: 0.0438\n",
      "Epoch 107: Batch 69/116 loss: 0.0405\n",
      "Epoch 107: Batch 70/116 loss: 0.0478\n",
      "Epoch 107: Batch 71/116 loss: 0.0410\n",
      "Epoch 107: Batch 72/116 loss: 0.0341\n",
      "Epoch 107: Batch 73/116 loss: 0.0488\n",
      "Epoch 107: Batch 74/116 loss: 0.0326\n",
      "Epoch 107: Batch 75/116 loss: 0.0647\n",
      "Epoch 107: Batch 76/116 loss: 0.0398\n",
      "Epoch 107: Batch 77/116 loss: 0.0553\n",
      "Epoch 107: Batch 78/116 loss: 0.0636\n",
      "Epoch 107: Batch 79/116 loss: 0.0326\n",
      "Epoch 107: Batch 80/116 loss: 0.0479\n",
      "Epoch 107: Batch 81/116 loss: 0.0543\n",
      "Epoch 107: Batch 82/116 loss: 0.0362\n",
      "Epoch 107: Batch 83/116 loss: 0.0486\n",
      "Epoch 107: Batch 84/116 loss: 0.0440\n",
      "Epoch 107: Batch 85/116 loss: 0.0492\n",
      "Epoch 107: Batch 86/116 loss: 0.0669\n",
      "Epoch 107: Batch 87/116 loss: 0.0483\n",
      "Epoch 107: Batch 88/116 loss: 0.0569\n",
      "Epoch 107: Batch 89/116 loss: 0.0665\n",
      "Epoch 107: Batch 90/116 loss: 0.0484\n",
      "Epoch 107: Batch 91/116 loss: 0.0415\n",
      "Epoch 107: Batch 92/116 loss: 0.0622\n",
      "Epoch 107: Batch 93/116 loss: 0.0568\n",
      "Epoch 107: Batch 94/116 loss: 0.0513\n",
      "Epoch 107: Batch 95/116 loss: 0.0645\n",
      "Epoch 107: Batch 96/116 loss: 0.0649\n",
      "Epoch 107: Batch 97/116 loss: 0.0526\n",
      "Epoch 107: Batch 98/116 loss: 0.0431\n",
      "Epoch 107: Batch 99/116 loss: 0.0490\n",
      "Epoch 107: Batch 100/116 loss: 0.0450\n",
      "Epoch 107: Batch 101/116 loss: 0.0445\n",
      "Epoch 107: Batch 102/116 loss: 0.0367\n",
      "Epoch 107: Batch 103/116 loss: 0.0515\n",
      "Epoch 107: Batch 104/116 loss: 0.0562\n",
      "Epoch 107: Batch 105/116 loss: 0.0464\n",
      "Epoch 107: Batch 106/116 loss: 0.0474\n",
      "Epoch 107: Batch 107/116 loss: 0.0494\n",
      "Epoch 107: Batch 108/116 loss: 0.0696\n",
      "Epoch 107: Batch 109/116 loss: 0.0700\n",
      "Epoch 107: Batch 110/116 loss: 0.0406\n",
      "Epoch 107: Batch 111/116 loss: 0.0745\n",
      "Epoch 107: Batch 112/116 loss: 0.0567\n",
      "Epoch 107: Batch 113/116 loss: 0.0486\n",
      "Epoch 107: Batch 114/116 loss: 0.0509\n",
      "Epoch 107: Batch 115/116 loss: 0.0505\n",
      "Epoch 107: Batch 116/116 loss: 0.0670\n",
      "Epoch 107 train loss: 0.0512 valid loss: 0.0645\n",
      "performance reducing: 2\n",
      "Epoch 108: Batch 1/116 loss: 0.0442\n",
      "Epoch 108: Batch 2/116 loss: 0.0377\n",
      "Epoch 108: Batch 3/116 loss: 0.0543\n",
      "Epoch 108: Batch 4/116 loss: 0.0601\n",
      "Epoch 108: Batch 5/116 loss: 0.0655\n",
      "Epoch 108: Batch 6/116 loss: 0.0443\n",
      "Epoch 108: Batch 7/116 loss: 0.0613\n",
      "Epoch 108: Batch 8/116 loss: 0.0510\n",
      "Epoch 108: Batch 9/116 loss: 0.0535\n",
      "Epoch 108: Batch 10/116 loss: 0.0375\n",
      "Epoch 108: Batch 11/116 loss: 0.0611\n",
      "Epoch 108: Batch 12/116 loss: 0.0450\n",
      "Epoch 108: Batch 13/116 loss: 0.0613\n",
      "Epoch 108: Batch 14/116 loss: 0.0574\n",
      "Epoch 108: Batch 15/116 loss: 0.0684\n",
      "Epoch 108: Batch 16/116 loss: 0.0548\n",
      "Epoch 108: Batch 17/116 loss: 0.0533\n",
      "Epoch 108: Batch 18/116 loss: 0.0472\n",
      "Epoch 108: Batch 19/116 loss: 0.0664\n",
      "Epoch 108: Batch 20/116 loss: 0.0434\n",
      "Epoch 108: Batch 21/116 loss: 0.0510\n",
      "Epoch 108: Batch 22/116 loss: 0.0444\n",
      "Epoch 108: Batch 23/116 loss: 0.0479\n",
      "Epoch 108: Batch 24/116 loss: 0.0357\n",
      "Epoch 108: Batch 25/116 loss: 0.0511\n",
      "Epoch 108: Batch 26/116 loss: 0.0369\n",
      "Epoch 108: Batch 27/116 loss: 0.0356\n",
      "Epoch 108: Batch 28/116 loss: 0.0471\n",
      "Epoch 108: Batch 29/116 loss: 0.0388\n",
      "Epoch 108: Batch 30/116 loss: 0.0264\n",
      "Epoch 108: Batch 31/116 loss: 0.0201\n",
      "Epoch 108: Batch 32/116 loss: 0.0582\n",
      "Epoch 108: Batch 33/116 loss: 0.0564\n",
      "Epoch 108: Batch 34/116 loss: 0.0421\n",
      "Epoch 108: Batch 35/116 loss: 0.0411\n",
      "Epoch 108: Batch 36/116 loss: 0.0502\n",
      "Epoch 108: Batch 37/116 loss: 0.0447\n",
      "Epoch 108: Batch 38/116 loss: 0.0371\n",
      "Epoch 108: Batch 39/116 loss: 0.0504\n",
      "Epoch 108: Batch 40/116 loss: 0.0569\n",
      "Epoch 108: Batch 41/116 loss: 0.0532\n",
      "Epoch 108: Batch 42/116 loss: 0.0403\n",
      "Epoch 108: Batch 43/116 loss: 0.0299\n",
      "Epoch 108: Batch 44/116 loss: 0.0364\n",
      "Epoch 108: Batch 45/116 loss: 0.0454\n",
      "Epoch 108: Batch 46/116 loss: 0.0498\n",
      "Epoch 108: Batch 47/116 loss: 0.0562\n",
      "Epoch 108: Batch 48/116 loss: 0.0459\n",
      "Epoch 108: Batch 49/116 loss: 0.0527\n",
      "Epoch 108: Batch 50/116 loss: 0.0562\n",
      "Epoch 108: Batch 51/116 loss: 0.0451\n",
      "Epoch 108: Batch 52/116 loss: 0.0348\n",
      "Epoch 108: Batch 53/116 loss: 0.0512\n",
      "Epoch 108: Batch 54/116 loss: 0.0735\n",
      "Epoch 108: Batch 55/116 loss: 0.0448\n",
      "Epoch 108: Batch 56/116 loss: 0.0391\n",
      "Epoch 108: Batch 57/116 loss: 0.0549\n",
      "Epoch 108: Batch 58/116 loss: 0.0636\n",
      "Epoch 108: Batch 59/116 loss: 0.0697\n",
      "Epoch 108: Batch 60/116 loss: 0.0411\n",
      "Epoch 108: Batch 61/116 loss: 0.0483\n",
      "Epoch 108: Batch 62/116 loss: 0.0396\n",
      "Epoch 108: Batch 63/116 loss: 0.0547\n",
      "Epoch 108: Batch 64/116 loss: 0.0565\n",
      "Epoch 108: Batch 65/116 loss: 0.0432\n",
      "Epoch 108: Batch 66/116 loss: 0.0824\n",
      "Epoch 108: Batch 67/116 loss: 0.0642\n",
      "Epoch 108: Batch 68/116 loss: 0.0397\n",
      "Epoch 108: Batch 69/116 loss: 0.0605\n",
      "Epoch 108: Batch 70/116 loss: 0.0444\n",
      "Epoch 108: Batch 71/116 loss: 0.0602\n",
      "Epoch 108: Batch 72/116 loss: 0.0573\n",
      "Epoch 108: Batch 73/116 loss: 0.0456\n",
      "Epoch 108: Batch 74/116 loss: 0.0538\n",
      "Epoch 108: Batch 75/116 loss: 0.0787\n",
      "Epoch 108: Batch 76/116 loss: 0.0510\n",
      "Epoch 108: Batch 77/116 loss: 0.0770\n",
      "Epoch 108: Batch 78/116 loss: 0.0455\n",
      "Epoch 108: Batch 79/116 loss: 0.0438\n",
      "Epoch 108: Batch 80/116 loss: 0.0446\n",
      "Epoch 108: Batch 81/116 loss: 0.0357\n",
      "Epoch 108: Batch 82/116 loss: 0.0724\n",
      "Epoch 108: Batch 83/116 loss: 0.0457\n",
      "Epoch 108: Batch 84/116 loss: 0.0480\n",
      "Epoch 108: Batch 85/116 loss: 0.0538\n",
      "Epoch 108: Batch 86/116 loss: 0.0600\n",
      "Epoch 108: Batch 87/116 loss: 0.0448\n",
      "Epoch 108: Batch 88/116 loss: 0.0507\n",
      "Epoch 108: Batch 89/116 loss: 0.0633\n",
      "Epoch 108: Batch 90/116 loss: 0.0645\n",
      "Epoch 108: Batch 91/116 loss: 0.0392\n",
      "Epoch 108: Batch 92/116 loss: 0.0421\n",
      "Epoch 108: Batch 93/116 loss: 0.0473\n",
      "Epoch 108: Batch 94/116 loss: 0.0469\n",
      "Epoch 108: Batch 95/116 loss: 0.0523\n",
      "Epoch 108: Batch 96/116 loss: 0.0537\n",
      "Epoch 108: Batch 97/116 loss: 0.0412\n",
      "Epoch 108: Batch 98/116 loss: 0.0618\n",
      "Epoch 108: Batch 99/116 loss: 0.0457\n",
      "Epoch 108: Batch 100/116 loss: 0.0428\n",
      "Epoch 108: Batch 101/116 loss: 0.0761\n",
      "Epoch 108: Batch 102/116 loss: 0.0518\n",
      "Epoch 108: Batch 103/116 loss: 0.0614\n",
      "Epoch 108: Batch 104/116 loss: 0.0667\n",
      "Epoch 108: Batch 105/116 loss: 0.0723\n",
      "Epoch 108: Batch 106/116 loss: 0.0589\n",
      "Epoch 108: Batch 107/116 loss: 0.0639\n",
      "Epoch 108: Batch 108/116 loss: 0.0507\n",
      "Epoch 108: Batch 109/116 loss: 0.0407\n",
      "Epoch 108: Batch 110/116 loss: 0.0415\n",
      "Epoch 108: Batch 111/116 loss: 0.0516\n",
      "Epoch 108: Batch 112/116 loss: 0.0520\n",
      "Epoch 108: Batch 113/116 loss: 0.0418\n",
      "Epoch 108: Batch 114/116 loss: 0.0503\n",
      "Epoch 108: Batch 115/116 loss: 0.0433\n",
      "Epoch 108: Batch 116/116 loss: 0.0494\n",
      "Epoch 108 train loss: 0.0509 valid loss: 0.0596\n",
      "performance reducing: 3\n",
      "Epoch 109: Batch 1/116 loss: 0.0411\n",
      "Epoch 109: Batch 2/116 loss: 0.0539\n",
      "Epoch 109: Batch 3/116 loss: 0.0494\n",
      "Epoch 109: Batch 4/116 loss: 0.0527\n",
      "Epoch 109: Batch 5/116 loss: 0.0422\n",
      "Epoch 109: Batch 6/116 loss: 0.0498\n",
      "Epoch 109: Batch 7/116 loss: 0.0487\n",
      "Epoch 109: Batch 8/116 loss: 0.0521\n",
      "Epoch 109: Batch 9/116 loss: 0.0399\n",
      "Epoch 109: Batch 10/116 loss: 0.0315\n",
      "Epoch 109: Batch 11/116 loss: 0.0380\n",
      "Epoch 109: Batch 12/116 loss: 0.0664\n",
      "Epoch 109: Batch 13/116 loss: 0.0438\n",
      "Epoch 109: Batch 14/116 loss: 0.0545\n",
      "Epoch 109: Batch 15/116 loss: 0.0539\n",
      "Epoch 109: Batch 16/116 loss: 0.0617\n",
      "Epoch 109: Batch 17/116 loss: 0.0369\n",
      "Epoch 109: Batch 18/116 loss: 0.0643\n",
      "Epoch 109: Batch 19/116 loss: 0.0327\n",
      "Epoch 109: Batch 20/116 loss: 0.0321\n",
      "Epoch 109: Batch 21/116 loss: 0.0497\n",
      "Epoch 109: Batch 22/116 loss: 0.0663\n",
      "Epoch 109: Batch 23/116 loss: 0.0569\n",
      "Epoch 109: Batch 24/116 loss: 0.0466\n",
      "Epoch 109: Batch 25/116 loss: 0.0503\n",
      "Epoch 109: Batch 26/116 loss: 0.0514\n",
      "Epoch 109: Batch 27/116 loss: 0.0532\n",
      "Epoch 109: Batch 28/116 loss: 0.0605\n",
      "Epoch 109: Batch 29/116 loss: 0.0433\n",
      "Epoch 109: Batch 30/116 loss: 0.0676\n",
      "Epoch 109: Batch 31/116 loss: 0.0740\n",
      "Epoch 109: Batch 32/116 loss: 0.0491\n",
      "Epoch 109: Batch 33/116 loss: 0.0493\n",
      "Epoch 109: Batch 34/116 loss: 0.0450\n",
      "Epoch 109: Batch 35/116 loss: 0.0495\n",
      "Epoch 109: Batch 36/116 loss: 0.0521\n",
      "Epoch 109: Batch 37/116 loss: 0.0478\n",
      "Epoch 109: Batch 38/116 loss: 0.0474\n",
      "Epoch 109: Batch 39/116 loss: 0.0633\n",
      "Epoch 109: Batch 40/116 loss: 0.0455\n",
      "Epoch 109: Batch 41/116 loss: 0.0570\n",
      "Epoch 109: Batch 42/116 loss: 0.0429\n",
      "Epoch 109: Batch 43/116 loss: 0.0536\n",
      "Epoch 109: Batch 44/116 loss: 0.0534\n",
      "Epoch 109: Batch 45/116 loss: 0.0483\n",
      "Epoch 109: Batch 46/116 loss: 0.0351\n",
      "Epoch 109: Batch 47/116 loss: 0.0589\n",
      "Epoch 109: Batch 48/116 loss: 0.0348\n",
      "Epoch 109: Batch 49/116 loss: 0.0955\n",
      "Epoch 109: Batch 50/116 loss: 0.0655\n",
      "Epoch 109: Batch 51/116 loss: 0.0607\n",
      "Epoch 109: Batch 52/116 loss: 0.0640\n",
      "Epoch 109: Batch 53/116 loss: 0.0414\n",
      "Epoch 109: Batch 54/116 loss: 0.0594\n",
      "Epoch 109: Batch 55/116 loss: 0.0394\n",
      "Epoch 109: Batch 56/116 loss: 0.0422\n",
      "Epoch 109: Batch 57/116 loss: 0.0525\n",
      "Epoch 109: Batch 58/116 loss: 0.1008\n",
      "Epoch 109: Batch 59/116 loss: 0.0579\n",
      "Epoch 109: Batch 60/116 loss: 0.0842\n",
      "Epoch 109: Batch 61/116 loss: 0.0492\n",
      "Epoch 109: Batch 62/116 loss: 0.0753\n",
      "Epoch 109: Batch 63/116 loss: 0.0527\n",
      "Epoch 109: Batch 64/116 loss: 0.0503\n",
      "Epoch 109: Batch 65/116 loss: 0.0413\n",
      "Epoch 109: Batch 66/116 loss: 0.0448\n",
      "Epoch 109: Batch 67/116 loss: 0.0567\n",
      "Epoch 109: Batch 68/116 loss: 0.0345\n",
      "Epoch 109: Batch 69/116 loss: 0.0518\n",
      "Epoch 109: Batch 70/116 loss: 0.0435\n",
      "Epoch 109: Batch 71/116 loss: 0.0518\n",
      "Epoch 109: Batch 72/116 loss: 0.0568\n",
      "Epoch 109: Batch 73/116 loss: 0.0350\n",
      "Epoch 109: Batch 74/116 loss: 0.0685\n",
      "Epoch 109: Batch 75/116 loss: 0.0532\n",
      "Epoch 109: Batch 76/116 loss: 0.0731\n",
      "Epoch 109: Batch 77/116 loss: 0.0413\n",
      "Epoch 109: Batch 78/116 loss: 0.0563\n",
      "Epoch 109: Batch 79/116 loss: 0.0390\n",
      "Epoch 109: Batch 80/116 loss: 0.0421\n",
      "Epoch 109: Batch 81/116 loss: 0.0769\n",
      "Epoch 109: Batch 82/116 loss: 0.0423\n",
      "Epoch 109: Batch 83/116 loss: 0.0548\n",
      "Epoch 109: Batch 84/116 loss: 0.0496\n",
      "Epoch 109: Batch 85/116 loss: 0.0591\n",
      "Epoch 109: Batch 86/116 loss: 0.0574\n",
      "Epoch 109: Batch 87/116 loss: 0.0449\n",
      "Epoch 109: Batch 88/116 loss: 0.0657\n",
      "Epoch 109: Batch 89/116 loss: 0.0525\n",
      "Epoch 109: Batch 90/116 loss: 0.0611\n",
      "Epoch 109: Batch 91/116 loss: 0.0537\n",
      "Epoch 109: Batch 92/116 loss: 0.0453\n",
      "Epoch 109: Batch 93/116 loss: 0.0636\n",
      "Epoch 109: Batch 94/116 loss: 0.0558\n",
      "Epoch 109: Batch 95/116 loss: 0.0387\n",
      "Epoch 109: Batch 96/116 loss: 0.0464\n",
      "Epoch 109: Batch 97/116 loss: 0.0476\n",
      "Epoch 109: Batch 98/116 loss: 0.0399\n",
      "Epoch 109: Batch 99/116 loss: 0.0380\n",
      "Epoch 109: Batch 100/116 loss: 0.0471\n",
      "Epoch 109: Batch 101/116 loss: 0.0455\n",
      "Epoch 109: Batch 102/116 loss: 0.0477\n",
      "Epoch 109: Batch 103/116 loss: 0.0438\n",
      "Epoch 109: Batch 104/116 loss: 0.0653\n",
      "Epoch 109: Batch 105/116 loss: 0.0481\n",
      "Epoch 109: Batch 106/116 loss: 0.0475\n",
      "Epoch 109: Batch 107/116 loss: 0.0542\n",
      "Epoch 109: Batch 108/116 loss: 0.0633\n",
      "Epoch 109: Batch 109/116 loss: 0.0539\n",
      "Epoch 109: Batch 110/116 loss: 0.0438\n",
      "Epoch 109: Batch 111/116 loss: 0.0369\n",
      "Epoch 109: Batch 112/116 loss: 0.0560\n",
      "Epoch 109: Batch 113/116 loss: 0.0453\n",
      "Epoch 109: Batch 114/116 loss: 0.0491\n",
      "Epoch 109: Batch 115/116 loss: 0.0390\n",
      "Epoch 109: Batch 116/116 loss: 0.0691\n",
      "Epoch 109 train loss: 0.0520 valid loss: 0.0564\n",
      "performance reducing: 4\n",
      "Epoch 110: Batch 1/116 loss: 0.0506\n",
      "Epoch 110: Batch 2/116 loss: 0.0447\n",
      "Epoch 110: Batch 3/116 loss: 0.0474\n",
      "Epoch 110: Batch 4/116 loss: 0.0528\n",
      "Epoch 110: Batch 5/116 loss: 0.0496\n",
      "Epoch 110: Batch 6/116 loss: 0.0777\n",
      "Epoch 110: Batch 7/116 loss: 0.0504\n",
      "Epoch 110: Batch 8/116 loss: 0.0491\n",
      "Epoch 110: Batch 9/116 loss: 0.0434\n",
      "Epoch 110: Batch 10/116 loss: 0.0422\n",
      "Epoch 110: Batch 11/116 loss: 0.0316\n",
      "Epoch 110: Batch 12/116 loss: 0.0549\n",
      "Epoch 110: Batch 13/116 loss: 0.0549\n",
      "Epoch 110: Batch 14/116 loss: 0.0495\n",
      "Epoch 110: Batch 15/116 loss: 0.0493\n",
      "Epoch 110: Batch 16/116 loss: 0.0344\n",
      "Epoch 110: Batch 17/116 loss: 0.0558\n",
      "Epoch 110: Batch 18/116 loss: 0.0503\n",
      "Epoch 110: Batch 19/116 loss: 0.0467\n",
      "Epoch 110: Batch 20/116 loss: 0.0501\n",
      "Epoch 110: Batch 21/116 loss: 0.0508\n",
      "Epoch 110: Batch 22/116 loss: 0.0405\n",
      "Epoch 110: Batch 23/116 loss: 0.0387\n",
      "Epoch 110: Batch 24/116 loss: 0.0588\n",
      "Epoch 110: Batch 25/116 loss: 0.0849\n",
      "Epoch 110: Batch 26/116 loss: 0.0667\n",
      "Epoch 110: Batch 27/116 loss: 0.0385\n",
      "Epoch 110: Batch 28/116 loss: 0.0467\n",
      "Epoch 110: Batch 29/116 loss: 0.0488\n",
      "Epoch 110: Batch 30/116 loss: 0.0554\n",
      "Epoch 110: Batch 31/116 loss: 0.0567\n",
      "Epoch 110: Batch 32/116 loss: 0.0437\n",
      "Epoch 110: Batch 33/116 loss: 0.0419\n",
      "Epoch 110: Batch 34/116 loss: 0.0568\n",
      "Epoch 110: Batch 35/116 loss: 0.0526\n",
      "Epoch 110: Batch 36/116 loss: 0.0414\n",
      "Epoch 110: Batch 37/116 loss: 0.0416\n",
      "Epoch 110: Batch 38/116 loss: 0.0436\n",
      "Epoch 110: Batch 39/116 loss: 0.0614\n",
      "Epoch 110: Batch 40/116 loss: 0.0493\n",
      "Epoch 110: Batch 41/116 loss: 0.0507\n",
      "Epoch 110: Batch 42/116 loss: 0.0307\n",
      "Epoch 110: Batch 43/116 loss: 0.0450\n",
      "Epoch 110: Batch 44/116 loss: 0.0228\n",
      "Epoch 110: Batch 45/116 loss: 0.0471\n",
      "Epoch 110: Batch 46/116 loss: 0.0571\n",
      "Epoch 110: Batch 47/116 loss: 0.0361\n",
      "Epoch 110: Batch 48/116 loss: 0.0757\n",
      "Epoch 110: Batch 49/116 loss: 0.0515\n",
      "Epoch 110: Batch 50/116 loss: 0.0621\n",
      "Epoch 110: Batch 51/116 loss: 0.0305\n",
      "Epoch 110: Batch 52/116 loss: 0.0651\n",
      "Epoch 110: Batch 53/116 loss: 0.0553\n",
      "Epoch 110: Batch 54/116 loss: 0.0544\n",
      "Epoch 110: Batch 55/116 loss: 0.0312\n",
      "Epoch 110: Batch 56/116 loss: 0.0445\n",
      "Epoch 110: Batch 57/116 loss: 0.0515\n",
      "Epoch 110: Batch 58/116 loss: 0.1539\n",
      "Epoch 110: Batch 59/116 loss: 0.0583\n",
      "Epoch 110: Batch 60/116 loss: 0.0808\n",
      "Epoch 110: Batch 61/116 loss: 0.1075\n",
      "Epoch 110: Batch 62/116 loss: 0.0581\n",
      "Epoch 110: Batch 63/116 loss: 0.0926\n",
      "Epoch 110: Batch 64/116 loss: 0.0521\n",
      "Epoch 110: Batch 65/116 loss: 0.0671\n",
      "Epoch 110: Batch 66/116 loss: 0.0865\n",
      "Epoch 110: Batch 67/116 loss: 0.0460\n",
      "Epoch 110: Batch 68/116 loss: 0.0701\n",
      "Epoch 110: Batch 69/116 loss: 0.0425\n",
      "Epoch 110: Batch 70/116 loss: 0.0688\n",
      "Epoch 110: Batch 71/116 loss: 0.0788\n",
      "Epoch 110: Batch 72/116 loss: 0.0592\n",
      "Epoch 110: Batch 73/116 loss: 0.0561\n",
      "Epoch 110: Batch 74/116 loss: 0.0680\n",
      "Epoch 110: Batch 75/116 loss: 0.0776\n",
      "Epoch 110: Batch 76/116 loss: 0.0670\n",
      "Epoch 110: Batch 77/116 loss: 0.0439\n",
      "Epoch 110: Batch 78/116 loss: 0.0788\n",
      "Epoch 110: Batch 79/116 loss: 0.0578\n",
      "Epoch 110: Batch 80/116 loss: 0.0389\n",
      "Epoch 110: Batch 81/116 loss: 0.0516\n",
      "Epoch 110: Batch 82/116 loss: 0.0383\n",
      "Epoch 110: Batch 83/116 loss: 0.0506\n",
      "Epoch 110: Batch 84/116 loss: 0.0546\n",
      "Epoch 110: Batch 85/116 loss: 0.1058\n",
      "Epoch 110: Batch 86/116 loss: 0.0415\n",
      "Epoch 110: Batch 87/116 loss: 0.0323\n",
      "Epoch 110: Batch 88/116 loss: 0.0373\n",
      "Epoch 110: Batch 89/116 loss: 0.0629\n",
      "Epoch 110: Batch 90/116 loss: 0.0627\n",
      "Epoch 110: Batch 91/116 loss: 0.0534\n",
      "Epoch 110: Batch 92/116 loss: 0.0537\n",
      "Epoch 110: Batch 93/116 loss: 0.0411\n",
      "Epoch 110: Batch 94/116 loss: 0.0515\n",
      "Epoch 110: Batch 95/116 loss: 0.0455\n",
      "Epoch 110: Batch 96/116 loss: 0.0524\n",
      "Epoch 110: Batch 97/116 loss: 0.0748\n",
      "Epoch 110: Batch 98/116 loss: 0.0568\n",
      "Epoch 110: Batch 99/116 loss: 0.0537\n",
      "Epoch 110: Batch 100/116 loss: 0.0476\n",
      "Epoch 110: Batch 101/116 loss: 0.0390\n",
      "Epoch 110: Batch 102/116 loss: 0.0474\n",
      "Epoch 110: Batch 103/116 loss: 0.0963\n",
      "Epoch 110: Batch 104/116 loss: 0.0470\n",
      "Epoch 110: Batch 105/116 loss: 0.0750\n",
      "Epoch 110: Batch 106/116 loss: 0.0544\n",
      "Epoch 110: Batch 107/116 loss: 0.0454\n",
      "Epoch 110: Batch 108/116 loss: 0.0665\n",
      "Epoch 110: Batch 109/116 loss: 0.0503\n",
      "Epoch 110: Batch 110/116 loss: 0.0404\n",
      "Epoch 110: Batch 111/116 loss: 0.0445\n",
      "Epoch 110: Batch 112/116 loss: 0.0484\n",
      "Epoch 110: Batch 113/116 loss: 0.0628\n",
      "Epoch 110: Batch 114/116 loss: 0.0628\n",
      "Epoch 110: Batch 115/116 loss: 0.0503\n",
      "Epoch 110: Batch 116/116 loss: 0.0472\n",
      "Epoch 110 train loss: 0.0549 valid loss: 0.0576\n",
      "performance reducing: 5\n",
      "Epoch 111: Batch 1/116 loss: 0.0657\n",
      "Epoch 111: Batch 2/116 loss: 0.0636\n",
      "Epoch 111: Batch 3/116 loss: 0.0487\n",
      "Epoch 111: Batch 4/116 loss: 0.0505\n",
      "Epoch 111: Batch 5/116 loss: 0.0428\n",
      "Epoch 111: Batch 6/116 loss: 0.0546\n",
      "Epoch 111: Batch 7/116 loss: 0.0519\n",
      "Epoch 111: Batch 8/116 loss: 0.0429\n",
      "Epoch 111: Batch 9/116 loss: 0.0487\n",
      "Epoch 111: Batch 10/116 loss: 0.0526\n",
      "Epoch 111: Batch 11/116 loss: 0.0354\n",
      "Epoch 111: Batch 12/116 loss: 0.0548\n",
      "Epoch 111: Batch 13/116 loss: 0.0579\n",
      "Epoch 111: Batch 14/116 loss: 0.0429\n",
      "Epoch 111: Batch 15/116 loss: 0.0340\n",
      "Epoch 111: Batch 16/116 loss: 0.0615\n",
      "Epoch 111: Batch 17/116 loss: 0.0483\n",
      "Epoch 111: Batch 18/116 loss: 0.0613\n",
      "Epoch 111: Batch 19/116 loss: 0.0832\n",
      "Epoch 111: Batch 20/116 loss: 0.0526\n",
      "Epoch 111: Batch 21/116 loss: 0.0580\n",
      "Epoch 111: Batch 22/116 loss: 0.0577\n",
      "Epoch 111: Batch 23/116 loss: 0.0580\n",
      "Epoch 111: Batch 24/116 loss: 0.0316\n",
      "Epoch 111: Batch 25/116 loss: 0.0296\n",
      "Epoch 111: Batch 26/116 loss: 0.0555\n",
      "Epoch 111: Batch 27/116 loss: 0.0370\n",
      "Epoch 111: Batch 28/116 loss: 0.0389\n",
      "Epoch 111: Batch 29/116 loss: 0.0643\n",
      "Epoch 111: Batch 30/116 loss: 0.0475\n",
      "Epoch 111: Batch 31/116 loss: 0.0544\n",
      "Epoch 111: Batch 32/116 loss: 0.0585\n",
      "Epoch 111: Batch 33/116 loss: 0.0586\n",
      "Epoch 111: Batch 34/116 loss: 0.0544\n",
      "Epoch 111: Batch 35/116 loss: 0.0580\n",
      "Epoch 111: Batch 36/116 loss: 0.0535\n",
      "Epoch 111: Batch 37/116 loss: 0.0526\n",
      "Epoch 111: Batch 38/116 loss: 0.0515\n",
      "Epoch 111: Batch 39/116 loss: 0.0513\n",
      "Epoch 111: Batch 40/116 loss: 0.0697\n",
      "Epoch 111: Batch 41/116 loss: 0.0411\n",
      "Epoch 111: Batch 42/116 loss: 0.0384\n",
      "Epoch 111: Batch 43/116 loss: 0.0472\n",
      "Epoch 111: Batch 44/116 loss: 0.0444\n",
      "Epoch 111: Batch 45/116 loss: 0.0447\n",
      "Epoch 111: Batch 46/116 loss: 0.0458\n",
      "Epoch 111: Batch 47/116 loss: 0.0435\n",
      "Epoch 111: Batch 48/116 loss: 0.0334\n",
      "Epoch 111: Batch 49/116 loss: 0.0703\n",
      "Epoch 111: Batch 50/116 loss: 0.0416\n",
      "Epoch 111: Batch 51/116 loss: 0.0658\n",
      "Epoch 111: Batch 52/116 loss: 0.0425\n",
      "Epoch 111: Batch 53/116 loss: 0.0411\n",
      "Epoch 111: Batch 54/116 loss: 0.0471\n",
      "Epoch 111: Batch 55/116 loss: 0.0454\n",
      "Epoch 111: Batch 56/116 loss: 0.0545\n",
      "Epoch 111: Batch 57/116 loss: 0.0456\n",
      "Epoch 111: Batch 58/116 loss: 0.0420\n",
      "Epoch 111: Batch 59/116 loss: 0.0584\n",
      "Epoch 111: Batch 60/116 loss: 0.0468\n",
      "Epoch 111: Batch 61/116 loss: 0.0470\n",
      "Epoch 111: Batch 62/116 loss: 0.0506\n",
      "Epoch 111: Batch 63/116 loss: 0.0442\n",
      "Epoch 111: Batch 64/116 loss: 0.0637\n",
      "Epoch 111: Batch 65/116 loss: 0.0474\n",
      "Epoch 111: Batch 66/116 loss: 0.0514\n",
      "Epoch 111: Batch 67/116 loss: 0.0447\n",
      "Epoch 111: Batch 68/116 loss: 0.0536\n",
      "Epoch 111: Batch 69/116 loss: 0.0609\n",
      "Epoch 111: Batch 70/116 loss: 0.0458\n",
      "Epoch 111: Batch 71/116 loss: 0.0362\n",
      "Epoch 111: Batch 72/116 loss: 0.0434\n",
      "Epoch 111: Batch 73/116 loss: 0.0572\n",
      "Epoch 111: Batch 74/116 loss: 0.0534\n",
      "Epoch 111: Batch 75/116 loss: 0.0487\n",
      "Epoch 111: Batch 76/116 loss: 0.0462\n",
      "Epoch 111: Batch 77/116 loss: 0.0586\n",
      "Epoch 111: Batch 78/116 loss: 0.0485\n",
      "Epoch 111: Batch 79/116 loss: 0.0453\n",
      "Epoch 111: Batch 80/116 loss: 0.0670\n",
      "Epoch 111: Batch 81/116 loss: 0.0581\n",
      "Epoch 111: Batch 82/116 loss: 0.0621\n",
      "Epoch 111: Batch 83/116 loss: 0.0548\n",
      "Epoch 111: Batch 84/116 loss: 0.0569\n",
      "Epoch 111: Batch 85/116 loss: 0.0568\n",
      "Epoch 111: Batch 86/116 loss: 0.0455\n",
      "Epoch 111: Batch 87/116 loss: 0.0688\n",
      "Epoch 111: Batch 88/116 loss: 0.0453\n",
      "Epoch 111: Batch 89/116 loss: 0.0435\n",
      "Epoch 111: Batch 90/116 loss: 0.0386\n",
      "Epoch 111: Batch 91/116 loss: 0.0468\n",
      "Epoch 111: Batch 92/116 loss: 0.0394\n",
      "Epoch 111: Batch 93/116 loss: 0.0578\n",
      "Epoch 111: Batch 94/116 loss: 0.0499\n",
      "Epoch 111: Batch 95/116 loss: 0.0602\n",
      "Epoch 111: Batch 96/116 loss: 0.0413\n",
      "Epoch 111: Batch 97/116 loss: 0.0453\n",
      "Epoch 111: Batch 98/116 loss: 0.0446\n",
      "Epoch 111: Batch 99/116 loss: 0.0459\n",
      "Epoch 111: Batch 100/116 loss: 0.0659\n",
      "Epoch 111: Batch 101/116 loss: 0.0397\n",
      "Epoch 111: Batch 102/116 loss: 0.0559\n",
      "Epoch 111: Batch 103/116 loss: 0.1009\n",
      "Epoch 111: Batch 104/116 loss: 0.0356\n",
      "Epoch 111: Batch 105/116 loss: 0.0517\n",
      "Epoch 111: Batch 106/116 loss: 0.0486\n",
      "Epoch 111: Batch 107/116 loss: 0.0458\n",
      "Epoch 111: Batch 108/116 loss: 0.0478\n",
      "Epoch 111: Batch 109/116 loss: 0.0553\n",
      "Epoch 111: Batch 110/116 loss: 0.0554\n",
      "Epoch 111: Batch 111/116 loss: 0.0461\n",
      "Epoch 111: Batch 112/116 loss: 0.0712\n",
      "Epoch 111: Batch 113/116 loss: 0.0482\n",
      "Epoch 111: Batch 114/116 loss: 0.0584\n",
      "Epoch 111: Batch 115/116 loss: 0.0419\n",
      "Epoch 111: Batch 116/116 loss: 0.0415\n",
      "Epoch 111 train loss: 0.0510 valid loss: 0.0550\n",
      "performance reducing: 6\n",
      "Epoch 112: Batch 1/116 loss: 0.0655\n",
      "Epoch 112: Batch 2/116 loss: 0.0531\n",
      "Epoch 112: Batch 3/116 loss: 0.0483\n",
      "Epoch 112: Batch 4/116 loss: 0.0516\n",
      "Epoch 112: Batch 5/116 loss: 0.0596\n",
      "Epoch 112: Batch 6/116 loss: 0.0496\n",
      "Epoch 112: Batch 7/116 loss: 0.0551\n",
      "Epoch 112: Batch 8/116 loss: 0.0309\n",
      "Epoch 112: Batch 9/116 loss: 0.0509\n",
      "Epoch 112: Batch 10/116 loss: 0.0653\n",
      "Epoch 112: Batch 11/116 loss: 0.0320\n",
      "Epoch 112: Batch 12/116 loss: 0.0619\n",
      "Epoch 112: Batch 13/116 loss: 0.0632\n",
      "Epoch 112: Batch 14/116 loss: 0.0398\n",
      "Epoch 112: Batch 15/116 loss: 0.0504\n",
      "Epoch 112: Batch 16/116 loss: 0.0595\n",
      "Epoch 112: Batch 17/116 loss: 0.0459\n",
      "Epoch 112: Batch 18/116 loss: 0.0623\n",
      "Epoch 112: Batch 19/116 loss: 0.0506\n",
      "Epoch 112: Batch 20/116 loss: 0.0653\n",
      "Epoch 112: Batch 21/116 loss: 0.0396\n",
      "Epoch 112: Batch 22/116 loss: 0.0534\n",
      "Epoch 112: Batch 23/116 loss: 0.0423\n",
      "Epoch 112: Batch 24/116 loss: 0.0512\n",
      "Epoch 112: Batch 25/116 loss: 0.0507\n",
      "Epoch 112: Batch 26/116 loss: 0.0575\n",
      "Epoch 112: Batch 27/116 loss: 0.0487\n",
      "Epoch 112: Batch 28/116 loss: 0.0386\n",
      "Epoch 112: Batch 29/116 loss: 0.0570\n",
      "Epoch 112: Batch 30/116 loss: 0.0981\n",
      "Epoch 112: Batch 31/116 loss: 0.0491\n",
      "Epoch 112: Batch 32/116 loss: 0.0431\n",
      "Epoch 112: Batch 33/116 loss: 0.0617\n",
      "Epoch 112: Batch 34/116 loss: 0.0597\n",
      "Epoch 112: Batch 35/116 loss: 0.1300\n",
      "Epoch 112: Batch 36/116 loss: 0.0662\n",
      "Epoch 112: Batch 37/116 loss: 0.0691\n",
      "Epoch 112: Batch 38/116 loss: 0.0626\n",
      "Epoch 112: Batch 39/116 loss: 0.0673\n",
      "Epoch 112: Batch 40/116 loss: 0.0685\n",
      "Epoch 112: Batch 41/116 loss: 0.0504\n",
      "Epoch 112: Batch 42/116 loss: 0.0360\n",
      "Epoch 112: Batch 43/116 loss: 0.0647\n",
      "Epoch 112: Batch 44/116 loss: 0.0579\n",
      "Epoch 112: Batch 45/116 loss: 0.0645\n",
      "Epoch 112: Batch 46/116 loss: 0.0563\n",
      "Epoch 112: Batch 47/116 loss: 0.0399\n",
      "Epoch 112: Batch 48/116 loss: 0.0824\n",
      "Epoch 112: Batch 49/116 loss: 0.0577\n",
      "Epoch 112: Batch 50/116 loss: 0.0548\n",
      "Epoch 112: Batch 51/116 loss: 0.0484\n",
      "Epoch 112: Batch 52/116 loss: 0.0517\n",
      "Epoch 112: Batch 53/116 loss: 0.0658\n",
      "Epoch 112: Batch 54/116 loss: 0.0588\n",
      "Epoch 112: Batch 55/116 loss: 0.0486\n",
      "Epoch 112: Batch 56/116 loss: 0.0371\n",
      "Epoch 112: Batch 57/116 loss: 0.0497\n",
      "Epoch 112: Batch 58/116 loss: 0.0497\n",
      "Epoch 112: Batch 59/116 loss: 0.0639\n",
      "Epoch 112: Batch 60/116 loss: 0.0359\n",
      "Epoch 112: Batch 61/116 loss: 0.0375\n",
      "Epoch 112: Batch 62/116 loss: 0.0509\n",
      "Epoch 112: Batch 63/116 loss: 0.0513\n",
      "Epoch 112: Batch 64/116 loss: 0.0525\n",
      "Epoch 112: Batch 65/116 loss: 0.0576\n",
      "Epoch 112: Batch 66/116 loss: 0.0425\n",
      "Epoch 112: Batch 67/116 loss: 0.0464\n",
      "Epoch 112: Batch 68/116 loss: 0.0674\n",
      "Epoch 112: Batch 69/116 loss: 0.0789\n",
      "Epoch 112: Batch 70/116 loss: 0.0356\n",
      "Epoch 112: Batch 71/116 loss: 0.0438\n",
      "Epoch 112: Batch 72/116 loss: 0.0671\n",
      "Epoch 112: Batch 73/116 loss: 0.0532\n",
      "Epoch 112: Batch 74/116 loss: 0.0320\n",
      "Epoch 112: Batch 75/116 loss: 0.0455\n",
      "Epoch 112: Batch 76/116 loss: 0.0424\n",
      "Epoch 112: Batch 77/116 loss: 0.0376\n",
      "Epoch 112: Batch 78/116 loss: 0.0593\n",
      "Epoch 112: Batch 79/116 loss: 0.0772\n",
      "Epoch 112: Batch 80/116 loss: 0.0454\n",
      "Epoch 112: Batch 81/116 loss: 0.0496\n",
      "Epoch 112: Batch 82/116 loss: 0.0469\n",
      "Epoch 112: Batch 83/116 loss: 0.0472\n",
      "Epoch 112: Batch 84/116 loss: 0.0558\n",
      "Epoch 112: Batch 85/116 loss: 0.0597\n",
      "Epoch 112: Batch 86/116 loss: 0.0396\n",
      "Epoch 112: Batch 87/116 loss: 0.0590\n",
      "Epoch 112: Batch 88/116 loss: 0.0441\n",
      "Epoch 112: Batch 89/116 loss: 0.0477\n",
      "Epoch 112: Batch 90/116 loss: 0.0318\n",
      "Epoch 112: Batch 91/116 loss: 0.0448\n",
      "Epoch 112: Batch 92/116 loss: 0.0399\n",
      "Epoch 112: Batch 93/116 loss: 0.0483\n",
      "Epoch 112: Batch 94/116 loss: 0.0391\n",
      "Epoch 112: Batch 95/116 loss: 0.0469\n",
      "Epoch 112: Batch 96/116 loss: 0.0470\n",
      "Epoch 112: Batch 97/116 loss: 0.0359\n",
      "Epoch 112: Batch 98/116 loss: 0.0389\n",
      "Epoch 112: Batch 99/116 loss: 0.0568\n",
      "Epoch 112: Batch 100/116 loss: 0.0592\n",
      "Epoch 112: Batch 101/116 loss: 0.0801\n",
      "Epoch 112: Batch 102/116 loss: 0.0279\n",
      "Epoch 112: Batch 103/116 loss: 0.0535\n",
      "Epoch 112: Batch 104/116 loss: 0.0580\n",
      "Epoch 112: Batch 105/116 loss: 0.0422\n",
      "Epoch 112: Batch 106/116 loss: 0.0726\n",
      "Epoch 112: Batch 107/116 loss: 0.0403\n",
      "Epoch 112: Batch 108/116 loss: 0.0546\n",
      "Epoch 112: Batch 109/116 loss: 0.0461\n",
      "Epoch 112: Batch 110/116 loss: 0.0544\n",
      "Epoch 112: Batch 111/116 loss: 0.0455\n",
      "Epoch 112: Batch 112/116 loss: 0.0329\n",
      "Epoch 112: Batch 113/116 loss: 0.0447\n",
      "Epoch 112: Batch 114/116 loss: 0.0568\n",
      "Epoch 112: Batch 115/116 loss: 0.0513\n",
      "Epoch 112: Batch 116/116 loss: 0.0564\n",
      "Epoch 112 train loss: 0.0528 valid loss: 0.0564\n",
      "performance reducing: 7\n",
      "Epoch 113: Batch 1/116 loss: 0.0796\n",
      "Epoch 113: Batch 2/116 loss: 0.0551\n",
      "Epoch 113: Batch 3/116 loss: 0.0409\n",
      "Epoch 113: Batch 4/116 loss: 0.0465\n",
      "Epoch 113: Batch 5/116 loss: 0.0450\n",
      "Epoch 113: Batch 6/116 loss: 0.0391\n",
      "Epoch 113: Batch 7/116 loss: 0.0598\n",
      "Epoch 113: Batch 8/116 loss: 0.0440\n",
      "Epoch 113: Batch 9/116 loss: 0.0683\n",
      "Epoch 113: Batch 10/116 loss: 0.0491\n",
      "Epoch 113: Batch 11/116 loss: 0.0538\n",
      "Epoch 113: Batch 12/116 loss: 0.0550\n",
      "Epoch 113: Batch 13/116 loss: 0.0482\n",
      "Epoch 113: Batch 14/116 loss: 0.0433\n",
      "Epoch 113: Batch 15/116 loss: 0.0430\n",
      "Epoch 113: Batch 16/116 loss: 0.0634\n",
      "Epoch 113: Batch 17/116 loss: 0.0627\n",
      "Epoch 113: Batch 18/116 loss: 0.0456\n",
      "Epoch 113: Batch 19/116 loss: 0.0465\n",
      "Epoch 113: Batch 20/116 loss: 0.0524\n",
      "Epoch 113: Batch 21/116 loss: 0.0573\n",
      "Epoch 113: Batch 22/116 loss: 0.0519\n",
      "Epoch 113: Batch 23/116 loss: 0.0460\n",
      "Epoch 113: Batch 24/116 loss: 0.0570\n",
      "Epoch 113: Batch 25/116 loss: 0.0535\n",
      "Epoch 113: Batch 26/116 loss: 0.0415\n",
      "Epoch 113: Batch 27/116 loss: 0.0491\n",
      "Epoch 113: Batch 28/116 loss: 0.0422\n",
      "Epoch 113: Batch 29/116 loss: 0.0449\n",
      "Epoch 113: Batch 30/116 loss: 0.0378\n",
      "Epoch 113: Batch 31/116 loss: 0.0599\n",
      "Epoch 113: Batch 32/116 loss: 0.0810\n",
      "Epoch 113: Batch 33/116 loss: 0.0553\n",
      "Epoch 113: Batch 34/116 loss: 0.0503\n",
      "Epoch 113: Batch 35/116 loss: 0.0587\n",
      "Epoch 113: Batch 36/116 loss: 0.0444\n",
      "Epoch 113: Batch 37/116 loss: 0.0396\n",
      "Epoch 113: Batch 38/116 loss: 0.0556\n",
      "Epoch 113: Batch 39/116 loss: 0.0365\n",
      "Epoch 113: Batch 40/116 loss: 0.0543\n",
      "Epoch 113: Batch 41/116 loss: 0.0555\n",
      "Epoch 113: Batch 42/116 loss: 0.0334\n",
      "Epoch 113: Batch 43/116 loss: 0.0362\n",
      "Epoch 113: Batch 44/116 loss: 0.0553\n",
      "Epoch 113: Batch 45/116 loss: 0.0417\n",
      "Epoch 113: Batch 46/116 loss: 0.0461\n",
      "Epoch 113: Batch 47/116 loss: 0.0757\n",
      "Epoch 113: Batch 48/116 loss: 0.0520\n",
      "Epoch 113: Batch 49/116 loss: 0.0716\n",
      "Epoch 113: Batch 50/116 loss: 0.0468\n",
      "Epoch 113: Batch 51/116 loss: 0.0587\n",
      "Epoch 113: Batch 52/116 loss: 0.0502\n",
      "Epoch 113: Batch 53/116 loss: 0.0739\n",
      "Epoch 113: Batch 54/116 loss: 0.0578\n",
      "Epoch 113: Batch 55/116 loss: 0.0356\n",
      "Epoch 113: Batch 56/116 loss: 0.0372\n",
      "Epoch 113: Batch 57/116 loss: 0.0463\n",
      "Epoch 113: Batch 58/116 loss: 0.0384\n",
      "Epoch 113: Batch 59/116 loss: 0.0566\n",
      "Epoch 113: Batch 60/116 loss: 0.0673\n",
      "Epoch 113: Batch 61/116 loss: 0.0571\n",
      "Epoch 113: Batch 62/116 loss: 0.0478\n",
      "Epoch 113: Batch 63/116 loss: 0.0456\n",
      "Epoch 113: Batch 64/116 loss: 0.0613\n",
      "Epoch 113: Batch 65/116 loss: 0.0389\n",
      "Epoch 113: Batch 66/116 loss: 0.0496\n",
      "Epoch 113: Batch 67/116 loss: 0.0475\n",
      "Epoch 113: Batch 68/116 loss: 0.0498\n",
      "Epoch 113: Batch 69/116 loss: 0.0527\n",
      "Epoch 113: Batch 70/116 loss: 0.0579\n",
      "Epoch 113: Batch 71/116 loss: 0.0484\n",
      "Epoch 113: Batch 72/116 loss: 0.0714\n",
      "Epoch 113: Batch 73/116 loss: 0.0507\n",
      "Epoch 113: Batch 74/116 loss: 0.0403\n",
      "Epoch 113: Batch 75/116 loss: 0.0501\n",
      "Epoch 113: Batch 76/116 loss: 0.0375\n",
      "Epoch 113: Batch 77/116 loss: 0.0483\n",
      "Epoch 113: Batch 78/116 loss: 0.0537\n",
      "Epoch 113: Batch 79/116 loss: 0.0441\n",
      "Epoch 113: Batch 80/116 loss: 0.0382\n",
      "Epoch 113: Batch 81/116 loss: 0.0903\n",
      "Epoch 113: Batch 82/116 loss: 0.0469\n",
      "Epoch 113: Batch 83/116 loss: 0.0605\n",
      "Epoch 113: Batch 84/116 loss: 0.0671\n",
      "Epoch 113: Batch 85/116 loss: 0.0531\n",
      "Epoch 113: Batch 86/116 loss: 0.0391\n",
      "Epoch 113: Batch 87/116 loss: 0.0406\n",
      "Epoch 113: Batch 88/116 loss: 0.0322\n",
      "Epoch 113: Batch 89/116 loss: 0.0291\n",
      "Epoch 113: Batch 90/116 loss: 0.0456\n",
      "Epoch 113: Batch 91/116 loss: 0.0418\n",
      "Epoch 113: Batch 92/116 loss: 0.0547\n",
      "Epoch 113: Batch 93/116 loss: 0.0394\n",
      "Epoch 113: Batch 94/116 loss: 0.0420\n",
      "Epoch 113: Batch 95/116 loss: 0.0481\n",
      "Epoch 113: Batch 96/116 loss: 0.0611\n",
      "Epoch 113: Batch 97/116 loss: 0.0556\n",
      "Epoch 113: Batch 98/116 loss: 0.0503\n",
      "Epoch 113: Batch 99/116 loss: 0.0574\n",
      "Epoch 113: Batch 100/116 loss: 0.0474\n",
      "Epoch 113: Batch 101/116 loss: 0.0306\n",
      "Epoch 113: Batch 102/116 loss: 0.0426\n",
      "Epoch 113: Batch 103/116 loss: 0.0290\n",
      "Epoch 113: Batch 104/116 loss: 0.0374\n",
      "Epoch 113: Batch 105/116 loss: 0.0979\n",
      "Epoch 113: Batch 106/116 loss: 0.0699\n",
      "Epoch 113: Batch 107/116 loss: 0.0511\n",
      "Epoch 113: Batch 108/116 loss: 0.0520\n",
      "Epoch 113: Batch 109/116 loss: 0.0534\n",
      "Epoch 113: Batch 110/116 loss: 0.0636\n",
      "Epoch 113: Batch 111/116 loss: 0.0538\n",
      "Epoch 113: Batch 112/116 loss: 0.0452\n",
      "Epoch 113: Batch 113/116 loss: 0.0548\n",
      "Epoch 113: Batch 114/116 loss: 0.0581\n",
      "Epoch 113: Batch 115/116 loss: 0.0414\n",
      "Epoch 113: Batch 116/116 loss: 0.0445\n",
      "Epoch 113 train loss: 0.0510 valid loss: 0.0576\n",
      "performance reducing: 8\n",
      "Epoch 114: Batch 1/116 loss: 0.0563\n",
      "Epoch 114: Batch 2/116 loss: 0.0522\n",
      "Epoch 114: Batch 3/116 loss: 0.0413\n",
      "Epoch 114: Batch 4/116 loss: 0.0524\n",
      "Epoch 114: Batch 5/116 loss: 0.0486\n",
      "Epoch 114: Batch 6/116 loss: 0.0525\n",
      "Epoch 114: Batch 7/116 loss: 0.0365\n",
      "Epoch 114: Batch 8/116 loss: 0.0556\n",
      "Epoch 114: Batch 9/116 loss: 0.0465\n",
      "Epoch 114: Batch 10/116 loss: 0.0352\n",
      "Epoch 114: Batch 11/116 loss: 0.0316\n",
      "Epoch 114: Batch 12/116 loss: 0.0454\n",
      "Epoch 114: Batch 13/116 loss: 0.0328\n",
      "Epoch 114: Batch 14/116 loss: 0.0535\n",
      "Epoch 114: Batch 15/116 loss: 0.0630\n",
      "Epoch 114: Batch 16/116 loss: 0.0640\n",
      "Epoch 114: Batch 17/116 loss: 0.0519\n",
      "Epoch 114: Batch 18/116 loss: 0.0410\n",
      "Epoch 114: Batch 19/116 loss: 0.0533\n",
      "Epoch 114: Batch 20/116 loss: 0.0387\n",
      "Epoch 114: Batch 21/116 loss: 0.0561\n",
      "Epoch 114: Batch 22/116 loss: 0.0543\n",
      "Epoch 114: Batch 23/116 loss: 0.0527\n",
      "Epoch 114: Batch 24/116 loss: 0.0279\n",
      "Epoch 114: Batch 25/116 loss: 0.0530\n",
      "Epoch 114: Batch 26/116 loss: 0.0385\n",
      "Epoch 114: Batch 27/116 loss: 0.0508\n",
      "Epoch 114: Batch 28/116 loss: 0.0358\n",
      "Epoch 114: Batch 29/116 loss: 0.0638\n",
      "Epoch 114: Batch 30/116 loss: 0.0721\n",
      "Epoch 114: Batch 31/116 loss: 0.0492\n",
      "Epoch 114: Batch 32/116 loss: 0.0896\n",
      "Epoch 114: Batch 33/116 loss: 0.0698\n",
      "Epoch 114: Batch 34/116 loss: 0.0425\n",
      "Epoch 114: Batch 35/116 loss: 0.0588\n",
      "Epoch 114: Batch 36/116 loss: 0.0456\n",
      "Epoch 114: Batch 37/116 loss: 0.0447\n",
      "Epoch 114: Batch 38/116 loss: 0.0544\n",
      "Epoch 114: Batch 39/116 loss: 0.0427\n",
      "Epoch 114: Batch 40/116 loss: 0.0344\n",
      "Epoch 114: Batch 41/116 loss: 0.0400\n",
      "Epoch 114: Batch 42/116 loss: 0.0360\n",
      "Epoch 114: Batch 43/116 loss: 0.0599\n",
      "Epoch 114: Batch 44/116 loss: 0.0304\n",
      "Epoch 114: Batch 45/116 loss: 0.0594\n",
      "Epoch 114: Batch 46/116 loss: 0.0593\n",
      "Epoch 114: Batch 47/116 loss: 0.0440\n",
      "Epoch 114: Batch 48/116 loss: 0.0437\n",
      "Epoch 114: Batch 49/116 loss: 0.0412\n",
      "Epoch 114: Batch 50/116 loss: 0.0396\n",
      "Epoch 114: Batch 51/116 loss: 0.0483\n",
      "Epoch 114: Batch 52/116 loss: 0.0506\n",
      "Epoch 114: Batch 53/116 loss: 0.0553\n",
      "Epoch 114: Batch 54/116 loss: 0.0515\n",
      "Epoch 114: Batch 55/116 loss: 0.0450\n",
      "Epoch 114: Batch 56/116 loss: 0.0330\n",
      "Epoch 114: Batch 57/116 loss: 0.0364\n",
      "Epoch 114: Batch 58/116 loss: 0.0468\n",
      "Epoch 114: Batch 59/116 loss: 0.0507\n",
      "Epoch 114: Batch 60/116 loss: 0.0722\n",
      "Epoch 114: Batch 61/116 loss: 0.0396\n",
      "Epoch 114: Batch 62/116 loss: 0.0611\n",
      "Epoch 114: Batch 63/116 loss: 0.0753\n",
      "Epoch 114: Batch 64/116 loss: 0.0355\n",
      "Epoch 114: Batch 65/116 loss: 0.0538\n",
      "Epoch 114: Batch 66/116 loss: 0.0613\n",
      "Epoch 114: Batch 67/116 loss: 0.0550\n",
      "Epoch 114: Batch 68/116 loss: 0.0501\n",
      "Epoch 114: Batch 69/116 loss: 0.0624\n",
      "Epoch 114: Batch 70/116 loss: 0.0489\n",
      "Epoch 114: Batch 71/116 loss: 0.0384\n",
      "Epoch 114: Batch 72/116 loss: 0.0491\n",
      "Epoch 114: Batch 73/116 loss: 0.0762\n",
      "Epoch 114: Batch 74/116 loss: 0.0524\n",
      "Epoch 114: Batch 75/116 loss: 0.0496\n",
      "Epoch 114: Batch 76/116 loss: 0.0644\n",
      "Epoch 114: Batch 77/116 loss: 0.0500\n",
      "Epoch 114: Batch 78/116 loss: 0.0590\n",
      "Epoch 114: Batch 79/116 loss: 0.0410\n",
      "Epoch 114: Batch 80/116 loss: 0.0571\n",
      "Epoch 114: Batch 81/116 loss: 0.0398\n",
      "Epoch 114: Batch 82/116 loss: 0.0558\n",
      "Epoch 114: Batch 83/116 loss: 0.0610\n",
      "Epoch 114: Batch 84/116 loss: 0.0698\n",
      "Epoch 114: Batch 85/116 loss: 0.0636\n",
      "Epoch 114: Batch 86/116 loss: 0.0372\n",
      "Epoch 114: Batch 87/116 loss: 0.0547\n",
      "Epoch 114: Batch 88/116 loss: 0.0452\n",
      "Epoch 114: Batch 89/116 loss: 0.0387\n",
      "Epoch 114: Batch 90/116 loss: 0.0558\n",
      "Epoch 114: Batch 91/116 loss: 0.0627\n",
      "Epoch 114: Batch 92/116 loss: 0.0391\n",
      "Epoch 114: Batch 93/116 loss: 0.0281\n",
      "Epoch 114: Batch 94/116 loss: 0.0543\n",
      "Epoch 114: Batch 95/116 loss: 0.0570\n",
      "Epoch 114: Batch 96/116 loss: 0.0446\n",
      "Epoch 114: Batch 97/116 loss: 0.0489\n",
      "Epoch 114: Batch 98/116 loss: 0.0425\n",
      "Epoch 114: Batch 99/116 loss: 0.0518\n",
      "Epoch 114: Batch 100/116 loss: 0.0528\n",
      "Epoch 114: Batch 101/116 loss: 0.0318\n",
      "Epoch 114: Batch 102/116 loss: 0.0927\n",
      "Epoch 114: Batch 103/116 loss: 0.0474\n",
      "Epoch 114: Batch 104/116 loss: 0.0479\n",
      "Epoch 114: Batch 105/116 loss: 0.0452\n",
      "Epoch 114: Batch 106/116 loss: 0.0450\n",
      "Epoch 114: Batch 107/116 loss: 0.0523\n",
      "Epoch 114: Batch 108/116 loss: 0.0464\n",
      "Epoch 114: Batch 109/116 loss: 0.0568\n",
      "Epoch 114: Batch 110/116 loss: 0.0393\n",
      "Epoch 114: Batch 111/116 loss: 0.0761\n",
      "Epoch 114: Batch 112/116 loss: 0.0346\n",
      "Epoch 114: Batch 113/116 loss: 0.0584\n",
      "Epoch 114: Batch 114/116 loss: 0.0667\n",
      "Epoch 114: Batch 115/116 loss: 0.0657\n",
      "Epoch 114: Batch 116/116 loss: 0.0358\n",
      "Epoch 114 train loss: 0.0505 valid loss: 0.0558\n",
      "performance reducing: 9\n",
      "Epoch 115: Batch 1/116 loss: 0.0475\n",
      "Epoch 115: Batch 2/116 loss: 0.0643\n",
      "Epoch 115: Batch 3/116 loss: 0.0489\n",
      "Epoch 115: Batch 4/116 loss: 0.0439\n",
      "Epoch 115: Batch 5/116 loss: 0.0354\n",
      "Epoch 115: Batch 6/116 loss: 0.0416\n",
      "Epoch 115: Batch 7/116 loss: 0.0387\n",
      "Epoch 115: Batch 8/116 loss: 0.0524\n",
      "Epoch 115: Batch 9/116 loss: 0.0473\n",
      "Epoch 115: Batch 10/116 loss: 0.1414\n",
      "Epoch 115: Batch 11/116 loss: 0.0554\n",
      "Epoch 115: Batch 12/116 loss: 0.0556\n",
      "Epoch 115: Batch 13/116 loss: 0.0663\n",
      "Epoch 115: Batch 14/116 loss: 0.0587\n",
      "Epoch 115: Batch 15/116 loss: 0.0593\n",
      "Epoch 115: Batch 16/116 loss: 0.0530\n",
      "Epoch 115: Batch 17/116 loss: 0.0427\n",
      "Epoch 115: Batch 18/116 loss: 0.0432\n",
      "Epoch 115: Batch 19/116 loss: 0.0644\n",
      "Epoch 115: Batch 20/116 loss: 0.0521\n",
      "Epoch 115: Batch 21/116 loss: 0.0582\n",
      "Epoch 115: Batch 22/116 loss: 0.0574\n",
      "Epoch 115: Batch 23/116 loss: 0.0434\n",
      "Epoch 115: Batch 24/116 loss: 0.0470\n",
      "Epoch 115: Batch 25/116 loss: 0.0512\n",
      "Epoch 115: Batch 26/116 loss: 0.0575\n",
      "Epoch 115: Batch 27/116 loss: 0.0475\n",
      "Epoch 115: Batch 28/116 loss: 0.0511\n",
      "Epoch 115: Batch 29/116 loss: 0.0645\n",
      "Epoch 115: Batch 30/116 loss: 0.0408\n",
      "Epoch 115: Batch 31/116 loss: 0.0414\n",
      "Epoch 115: Batch 32/116 loss: 0.0617\n",
      "Epoch 115: Batch 33/116 loss: 0.0474\n",
      "Epoch 115: Batch 34/116 loss: 0.0574\n",
      "Epoch 115: Batch 35/116 loss: 0.0504\n",
      "Epoch 115: Batch 36/116 loss: 0.0474\n",
      "Epoch 115: Batch 37/116 loss: 0.0548\n",
      "Epoch 115: Batch 38/116 loss: 0.0503\n",
      "Epoch 115: Batch 39/116 loss: 0.0476\n",
      "Epoch 115: Batch 40/116 loss: 0.0811\n",
      "Epoch 115: Batch 41/116 loss: 0.0275\n",
      "Epoch 115: Batch 42/116 loss: 0.0381\n",
      "Epoch 115: Batch 43/116 loss: 0.0640\n",
      "Epoch 115: Batch 44/116 loss: 0.0357\n",
      "Epoch 115: Batch 45/116 loss: 0.0634\n",
      "Epoch 115: Batch 46/116 loss: 0.0432\n",
      "Epoch 115: Batch 47/116 loss: 0.0545\n",
      "Epoch 115: Batch 48/116 loss: 0.0551\n",
      "Epoch 115: Batch 49/116 loss: 0.0538\n",
      "Epoch 115: Batch 50/116 loss: 0.0895\n",
      "Epoch 115: Batch 51/116 loss: 0.0689\n",
      "Epoch 115: Batch 52/116 loss: 0.0565\n",
      "Epoch 115: Batch 53/116 loss: 0.0446\n",
      "Epoch 115: Batch 54/116 loss: 0.0557\n",
      "Epoch 115: Batch 55/116 loss: 0.0549\n",
      "Epoch 115: Batch 56/116 loss: 0.0440\n",
      "Epoch 115: Batch 57/116 loss: 0.0531\n",
      "Epoch 115: Batch 58/116 loss: 0.0554\n",
      "Epoch 115: Batch 59/116 loss: 0.0416\n",
      "Epoch 115: Batch 60/116 loss: 0.0436\n",
      "Epoch 115: Batch 61/116 loss: 0.0375\n",
      "Epoch 115: Batch 62/116 loss: 0.0421\n",
      "Epoch 115: Batch 63/116 loss: 0.0548\n",
      "Epoch 115: Batch 64/116 loss: 0.0532\n",
      "Epoch 115: Batch 65/116 loss: 0.0591\n",
      "Epoch 115: Batch 66/116 loss: 0.0378\n",
      "Epoch 115: Batch 67/116 loss: 0.0498\n",
      "Epoch 115: Batch 68/116 loss: 0.0537\n",
      "Epoch 115: Batch 69/116 loss: 0.0541\n",
      "Epoch 115: Batch 70/116 loss: 0.0416\n",
      "Epoch 115: Batch 71/116 loss: 0.0482\n",
      "Epoch 115: Batch 72/116 loss: 0.0631\n",
      "Epoch 115: Batch 73/116 loss: 0.0557\n",
      "Epoch 115: Batch 74/116 loss: 0.0430\n",
      "Epoch 115: Batch 75/116 loss: 0.0627\n",
      "Epoch 115: Batch 76/116 loss: 0.0414\n",
      "Epoch 115: Batch 77/116 loss: 0.0476\n",
      "Epoch 115: Batch 78/116 loss: 0.0538\n",
      "Epoch 115: Batch 79/116 loss: 0.0477\n",
      "Epoch 115: Batch 80/116 loss: 0.0429\n",
      "Epoch 115: Batch 81/116 loss: 0.0690\n",
      "Epoch 115: Batch 82/116 loss: 0.0308\n",
      "Epoch 115: Batch 83/116 loss: 0.0516\n",
      "Epoch 115: Batch 84/116 loss: 0.0493\n",
      "Epoch 115: Batch 85/116 loss: 0.0393\n",
      "Epoch 115: Batch 86/116 loss: 0.0591\n",
      "Epoch 115: Batch 87/116 loss: 0.0336\n",
      "Epoch 115: Batch 88/116 loss: 0.0629\n",
      "Epoch 115: Batch 89/116 loss: 0.0359\n",
      "Epoch 115: Batch 90/116 loss: 0.0573\n",
      "Epoch 115: Batch 91/116 loss: 0.0599\n",
      "Epoch 115: Batch 92/116 loss: 0.0394\n",
      "Epoch 115: Batch 93/116 loss: 0.0451\n",
      "Epoch 115: Batch 94/116 loss: 0.0430\n",
      "Epoch 115: Batch 95/116 loss: 0.0272\n",
      "Epoch 115: Batch 96/116 loss: 0.0517\n",
      "Epoch 115: Batch 97/116 loss: 0.0540\n",
      "Epoch 115: Batch 98/116 loss: 0.0435\n",
      "Epoch 115: Batch 99/116 loss: 0.0371\n",
      "Epoch 115: Batch 100/116 loss: 0.0661\n",
      "Epoch 115: Batch 101/116 loss: 0.0381\n",
      "Epoch 115: Batch 102/116 loss: 0.0655\n",
      "Epoch 115: Batch 103/116 loss: 0.0460\n",
      "Epoch 115: Batch 104/116 loss: 0.0605\n",
      "Epoch 115: Batch 105/116 loss: 0.0650\n",
      "Epoch 115: Batch 106/116 loss: 0.0473\n",
      "Epoch 115: Batch 107/116 loss: 0.0455\n",
      "Epoch 115: Batch 108/116 loss: 0.0594\n",
      "Epoch 115: Batch 109/116 loss: 0.0713\n",
      "Epoch 115: Batch 110/116 loss: 0.0414\n",
      "Epoch 115: Batch 111/116 loss: 0.0511\n",
      "Epoch 115: Batch 112/116 loss: 0.0501\n",
      "Epoch 115: Batch 113/116 loss: 0.0481\n",
      "Epoch 115: Batch 114/116 loss: 0.0441\n",
      "Epoch 115: Batch 115/116 loss: 0.0544\n",
      "Epoch 115: Batch 116/116 loss: 0.0961\n",
      "Epoch 115 train loss: 0.0521 valid loss: 0.0603\n",
      "performance reducing: 10\n",
      "Epoch 116: Batch 1/116 loss: 0.0322\n",
      "Epoch 116: Batch 2/116 loss: 0.0411\n",
      "Epoch 116: Batch 3/116 loss: 0.0409\n",
      "Epoch 116: Batch 4/116 loss: 0.0467\n",
      "Epoch 116: Batch 5/116 loss: 0.0429\n",
      "Epoch 116: Batch 6/116 loss: 0.0484\n",
      "Epoch 116: Batch 7/116 loss: 0.0522\n",
      "Epoch 116: Batch 8/116 loss: 0.0404\n",
      "Epoch 116: Batch 9/116 loss: 0.0575\n",
      "Epoch 116: Batch 10/116 loss: 0.0441\n",
      "Epoch 116: Batch 11/116 loss: 0.0514\n",
      "Epoch 116: Batch 12/116 loss: 0.0518\n",
      "Epoch 116: Batch 13/116 loss: 0.0421\n",
      "Epoch 116: Batch 14/116 loss: 0.0479\n",
      "Epoch 116: Batch 15/116 loss: 0.0498\n",
      "Epoch 116: Batch 16/116 loss: 0.0443\n",
      "Epoch 116: Batch 17/116 loss: 0.0565\n",
      "Epoch 116: Batch 18/116 loss: 0.0533\n",
      "Epoch 116: Batch 19/116 loss: 0.0337\n",
      "Epoch 116: Batch 20/116 loss: 0.0496\n",
      "Epoch 116: Batch 21/116 loss: 0.0661\n",
      "Epoch 116: Batch 22/116 loss: 0.0262\n",
      "Epoch 116: Batch 23/116 loss: 0.0742\n",
      "Epoch 116: Batch 24/116 loss: 0.0639\n",
      "Epoch 116: Batch 25/116 loss: 0.0464\n",
      "Epoch 116: Batch 26/116 loss: 0.0472\n",
      "Epoch 116: Batch 27/116 loss: 0.0563\n",
      "Epoch 116: Batch 28/116 loss: 0.0336\n",
      "Epoch 116: Batch 29/116 loss: 0.0449\n",
      "Epoch 116: Batch 30/116 loss: 0.0460\n",
      "Epoch 116: Batch 31/116 loss: 0.0623\n",
      "Epoch 116: Batch 32/116 loss: 0.0308\n",
      "Epoch 116: Batch 33/116 loss: 0.0585\n",
      "Epoch 116: Batch 34/116 loss: 0.0630\n",
      "Epoch 116: Batch 35/116 loss: 0.0505\n",
      "Epoch 116: Batch 36/116 loss: 0.0530\n",
      "Epoch 116: Batch 37/116 loss: 0.0372\n",
      "Epoch 116: Batch 38/116 loss: 0.0400\n",
      "Epoch 116: Batch 39/116 loss: 0.0391\n",
      "Epoch 116: Batch 40/116 loss: 0.0551\n",
      "Epoch 116: Batch 41/116 loss: 0.0728\n",
      "Epoch 116: Batch 42/116 loss: 0.0390\n",
      "Epoch 116: Batch 43/116 loss: 0.0440\n",
      "Epoch 116: Batch 44/116 loss: 0.0326\n",
      "Epoch 116: Batch 45/116 loss: 0.0370\n",
      "Epoch 116: Batch 46/116 loss: 0.0541\n",
      "Epoch 116: Batch 47/116 loss: 0.0345\n",
      "Epoch 116: Batch 48/116 loss: 0.0658\n",
      "Epoch 116: Batch 49/116 loss: 0.0538\n",
      "Epoch 116: Batch 50/116 loss: 0.0524\n",
      "Epoch 116: Batch 51/116 loss: 0.0546\n",
      "Epoch 116: Batch 52/116 loss: 0.0433\n",
      "Epoch 116: Batch 53/116 loss: 0.0549\n",
      "Epoch 116: Batch 54/116 loss: 0.0436\n",
      "Epoch 116: Batch 55/116 loss: 0.0473\n",
      "Epoch 116: Batch 56/116 loss: 0.0450\n",
      "Epoch 116: Batch 57/116 loss: 0.0862\n",
      "Epoch 116: Batch 58/116 loss: 0.0644\n",
      "Epoch 116: Batch 59/116 loss: 0.0505\n",
      "Epoch 116: Batch 60/116 loss: 0.0516\n",
      "Epoch 116: Batch 61/116 loss: 0.0368\n",
      "Epoch 116: Batch 62/116 loss: 0.0353\n",
      "Epoch 116: Batch 63/116 loss: 0.0641\n",
      "Epoch 116: Batch 64/116 loss: 0.0809\n",
      "Epoch 116: Batch 65/116 loss: 0.0598\n",
      "Epoch 116: Batch 66/116 loss: 0.0505\n",
      "Epoch 116: Batch 67/116 loss: 0.0582\n",
      "Epoch 116: Batch 68/116 loss: 0.0544\n",
      "Epoch 116: Batch 69/116 loss: 0.0686\n",
      "Epoch 116: Batch 70/116 loss: 0.0568\n",
      "Epoch 116: Batch 71/116 loss: 0.0476\n",
      "Epoch 116: Batch 72/116 loss: 0.0422\n",
      "Epoch 116: Batch 73/116 loss: 0.0341\n",
      "Epoch 116: Batch 74/116 loss: 0.0615\n",
      "Epoch 116: Batch 75/116 loss: 0.0404\n",
      "Epoch 116: Batch 76/116 loss: 0.0648\n",
      "Epoch 116: Batch 77/116 loss: 0.0587\n",
      "Epoch 116: Batch 78/116 loss: 0.0539\n",
      "Epoch 116: Batch 79/116 loss: 0.0588\n",
      "Epoch 116: Batch 80/116 loss: 0.0654\n",
      "Epoch 116: Batch 81/116 loss: 0.0319\n",
      "Epoch 116: Batch 82/116 loss: 0.0437\n",
      "Epoch 116: Batch 83/116 loss: 0.0398\n",
      "Epoch 116: Batch 84/116 loss: 0.0619\n",
      "Epoch 116: Batch 85/116 loss: 0.0373\n",
      "Epoch 116: Batch 86/116 loss: 0.0506\n",
      "Epoch 116: Batch 87/116 loss: 0.0572\n",
      "Epoch 116: Batch 88/116 loss: 0.0547\n",
      "Epoch 116: Batch 89/116 loss: 0.0468\n",
      "Epoch 116: Batch 90/116 loss: 0.0580\n",
      "Epoch 116: Batch 91/116 loss: 0.0566\n",
      "Epoch 116: Batch 92/116 loss: 0.0434\n",
      "Epoch 116: Batch 93/116 loss: 0.0382\n",
      "Epoch 116: Batch 94/116 loss: 0.0454\n",
      "Epoch 116: Batch 95/116 loss: 0.0516\n",
      "Epoch 116: Batch 96/116 loss: 0.0433\n",
      "Epoch 116: Batch 97/116 loss: 0.0454\n",
      "Epoch 116: Batch 98/116 loss: 0.0275\n",
      "Epoch 116: Batch 99/116 loss: 0.0539\n",
      "Epoch 116: Batch 100/116 loss: 0.0549\n",
      "Epoch 116: Batch 101/116 loss: 0.0324\n",
      "Epoch 116: Batch 102/116 loss: 0.0715\n",
      "Epoch 116: Batch 103/116 loss: 0.0530\n",
      "Epoch 116: Batch 104/116 loss: 0.0771\n",
      "Epoch 116: Batch 105/116 loss: 0.0479\n",
      "Epoch 116: Batch 106/116 loss: 0.0455\n",
      "Epoch 116: Batch 107/116 loss: 0.0779\n",
      "Epoch 116: Batch 108/116 loss: 0.0419\n",
      "Epoch 116: Batch 109/116 loss: 0.0370\n",
      "Epoch 116: Batch 110/116 loss: 0.0755\n",
      "Epoch 116: Batch 111/116 loss: 0.0552\n",
      "Epoch 116: Batch 112/116 loss: 0.0446\n",
      "Epoch 116: Batch 113/116 loss: 0.0484\n",
      "Epoch 116: Batch 114/116 loss: 0.0502\n",
      "Epoch 116: Batch 115/116 loss: 0.0398\n",
      "Epoch 116: Batch 116/116 loss: 0.0381\n",
      "Epoch 116 train loss: 0.0502 valid loss: 0.0604\n",
      "performance reducing: 11\n",
      "Epoch 117: Batch 1/116 loss: 0.0396\n",
      "Epoch 117: Batch 2/116 loss: 0.0476\n",
      "Epoch 117: Batch 3/116 loss: 0.0346\n",
      "Epoch 117: Batch 4/116 loss: 0.0544\n",
      "Epoch 117: Batch 5/116 loss: 0.0473\n",
      "Epoch 117: Batch 6/116 loss: 0.0477\n",
      "Epoch 117: Batch 7/116 loss: 0.0358\n",
      "Epoch 117: Batch 8/116 loss: 0.0488\n",
      "Epoch 117: Batch 9/116 loss: 0.0647\n",
      "Epoch 117: Batch 10/116 loss: 0.0507\n",
      "Epoch 117: Batch 11/116 loss: 0.0533\n",
      "Epoch 117: Batch 12/116 loss: 0.0852\n",
      "Epoch 117: Batch 13/116 loss: 0.0452\n",
      "Epoch 117: Batch 14/116 loss: 0.0610\n",
      "Epoch 117: Batch 15/116 loss: 0.0447\n",
      "Epoch 117: Batch 16/116 loss: 0.0538\n",
      "Epoch 117: Batch 17/116 loss: 0.0465\n",
      "Epoch 117: Batch 18/116 loss: 0.0532\n",
      "Epoch 117: Batch 19/116 loss: 0.0378\n",
      "Epoch 117: Batch 20/116 loss: 0.0452\n",
      "Epoch 117: Batch 21/116 loss: 0.0491\n",
      "Epoch 117: Batch 22/116 loss: 0.0441\n",
      "Epoch 117: Batch 23/116 loss: 0.0387\n",
      "Epoch 117: Batch 24/116 loss: 0.0532\n",
      "Epoch 117: Batch 25/116 loss: 0.0467\n",
      "Epoch 117: Batch 26/116 loss: 0.0443\n",
      "Epoch 117: Batch 27/116 loss: 0.0418\n",
      "Epoch 117: Batch 28/116 loss: 0.0206\n",
      "Epoch 117: Batch 29/116 loss: 0.0509\n",
      "Epoch 117: Batch 30/116 loss: 0.0294\n",
      "Epoch 117: Batch 31/116 loss: 0.0426\n",
      "Epoch 117: Batch 32/116 loss: 0.0541\n",
      "Epoch 117: Batch 33/116 loss: 0.0430\n",
      "Epoch 117: Batch 34/116 loss: 0.0451\n",
      "Epoch 117: Batch 35/116 loss: 0.0410\n",
      "Epoch 117: Batch 36/116 loss: 0.0578\n",
      "Epoch 117: Batch 37/116 loss: 0.0605\n",
      "Epoch 117: Batch 38/116 loss: 0.0526\n",
      "Epoch 117: Batch 39/116 loss: 0.0523\n",
      "Epoch 117: Batch 40/116 loss: 0.0486\n",
      "Epoch 117: Batch 41/116 loss: 0.0496\n",
      "Epoch 117: Batch 42/116 loss: 0.0347\n",
      "Epoch 117: Batch 43/116 loss: 0.0557\n",
      "Epoch 117: Batch 44/116 loss: 0.0635\n",
      "Epoch 117: Batch 45/116 loss: 0.0719\n",
      "Epoch 117: Batch 46/116 loss: 0.0469\n",
      "Epoch 117: Batch 47/116 loss: 0.0826\n",
      "Epoch 117: Batch 48/116 loss: 0.0429\n",
      "Epoch 117: Batch 49/116 loss: 0.0466\n",
      "Epoch 117: Batch 50/116 loss: 0.0580\n",
      "Epoch 117: Batch 51/116 loss: 0.0544\n",
      "Epoch 117: Batch 52/116 loss: 0.0503\n",
      "Epoch 117: Batch 53/116 loss: 0.0741\n",
      "Epoch 117: Batch 54/116 loss: 0.0449\n",
      "Epoch 117: Batch 55/116 loss: 0.0377\n",
      "Epoch 117: Batch 56/116 loss: 0.0512\n",
      "Epoch 117: Batch 57/116 loss: 0.0457\n",
      "Epoch 117: Batch 58/116 loss: 0.0428\n",
      "Epoch 117: Batch 59/116 loss: 0.0322\n",
      "Epoch 117: Batch 60/116 loss: 0.0679\n",
      "Epoch 117: Batch 61/116 loss: 0.0580\n",
      "Epoch 117: Batch 62/116 loss: 0.0726\n",
      "Epoch 117: Batch 63/116 loss: 0.0740\n",
      "Epoch 117: Batch 64/116 loss: 0.0363\n",
      "Epoch 117: Batch 65/116 loss: 0.0570\n",
      "Epoch 117: Batch 66/116 loss: 0.0494\n",
      "Epoch 117: Batch 67/116 loss: 0.0534\n",
      "Epoch 117: Batch 68/116 loss: 0.0428\n",
      "Epoch 117: Batch 69/116 loss: 0.0642\n",
      "Epoch 117: Batch 70/116 loss: 0.0589\n",
      "Epoch 117: Batch 71/116 loss: 0.0650\n",
      "Epoch 117: Batch 72/116 loss: 0.0409\n",
      "Epoch 117: Batch 73/116 loss: 0.0599\n",
      "Epoch 117: Batch 74/116 loss: 0.0563\n",
      "Epoch 117: Batch 75/116 loss: 0.0483\n",
      "Epoch 117: Batch 76/116 loss: 0.0894\n",
      "Epoch 117: Batch 77/116 loss: 0.0343\n",
      "Epoch 117: Batch 78/116 loss: 0.0606\n",
      "Epoch 117: Batch 79/116 loss: 0.0514\n",
      "Epoch 117: Batch 80/116 loss: 0.0476\n",
      "Epoch 117: Batch 81/116 loss: 0.0502\n",
      "Epoch 117: Batch 82/116 loss: 0.0352\n",
      "Epoch 117: Batch 83/116 loss: 0.0520\n",
      "Epoch 117: Batch 84/116 loss: 0.0385\n",
      "Epoch 117: Batch 85/116 loss: 0.0596\n",
      "Epoch 117: Batch 86/116 loss: 0.0487\n",
      "Epoch 117: Batch 87/116 loss: 0.0385\n",
      "Epoch 117: Batch 88/116 loss: 0.0600\n",
      "Epoch 117: Batch 89/116 loss: 0.0650\n",
      "Epoch 117: Batch 90/116 loss: 0.0835\n",
      "Epoch 117: Batch 91/116 loss: 0.0626\n",
      "Epoch 117: Batch 92/116 loss: 0.0295\n",
      "Epoch 117: Batch 93/116 loss: 0.0445\n",
      "Epoch 117: Batch 94/116 loss: 0.0427\n",
      "Epoch 117: Batch 95/116 loss: 0.0491\n",
      "Epoch 117: Batch 96/116 loss: 0.0537\n",
      "Epoch 117: Batch 97/116 loss: 0.0510\n",
      "Epoch 117: Batch 98/116 loss: 0.0337\n",
      "Epoch 117: Batch 99/116 loss: 0.0433\n",
      "Epoch 117: Batch 100/116 loss: 0.0477\n",
      "Epoch 117: Batch 101/116 loss: 0.0709\n",
      "Epoch 117: Batch 102/116 loss: 0.0486\n",
      "Epoch 117: Batch 103/116 loss: 0.0402\n",
      "Epoch 117: Batch 104/116 loss: 0.0465\n",
      "Epoch 117: Batch 105/116 loss: 0.0402\n",
      "Epoch 117: Batch 106/116 loss: 0.0661\n",
      "Epoch 117: Batch 107/116 loss: 0.0523\n",
      "Epoch 117: Batch 108/116 loss: 0.0403\n",
      "Epoch 117: Batch 109/116 loss: 0.0420\n",
      "Epoch 117: Batch 110/116 loss: 0.0511\n",
      "Epoch 117: Batch 111/116 loss: 0.0470\n",
      "Epoch 117: Batch 112/116 loss: 0.0532\n",
      "Epoch 117: Batch 113/116 loss: 0.0777\n",
      "Epoch 117: Batch 114/116 loss: 0.0549\n",
      "Epoch 117: Batch 115/116 loss: 0.0350\n",
      "Epoch 117: Batch 116/116 loss: 0.0854\n",
      "Epoch 117 train loss: 0.0511 valid loss: 0.0540\n",
      "Epoch 118: Batch 1/116 loss: 0.0461\n",
      "Epoch 118: Batch 2/116 loss: 0.0466\n",
      "Epoch 118: Batch 3/116 loss: 0.0467\n",
      "Epoch 118: Batch 4/116 loss: 0.0652\n",
      "Epoch 118: Batch 5/116 loss: 0.0520\n",
      "Epoch 118: Batch 6/116 loss: 0.0410\n",
      "Epoch 118: Batch 7/116 loss: 0.0297\n",
      "Epoch 118: Batch 8/116 loss: 0.0896\n",
      "Epoch 118: Batch 9/116 loss: 0.0316\n",
      "Epoch 118: Batch 10/116 loss: 0.0451\n",
      "Epoch 118: Batch 11/116 loss: 0.0504\n",
      "Epoch 118: Batch 12/116 loss: 0.0452\n",
      "Epoch 118: Batch 13/116 loss: 0.0531\n",
      "Epoch 118: Batch 14/116 loss: 0.0522\n",
      "Epoch 118: Batch 15/116 loss: 0.0505\n",
      "Epoch 118: Batch 16/116 loss: 0.0466\n",
      "Epoch 118: Batch 17/116 loss: 0.0964\n",
      "Epoch 118: Batch 18/116 loss: 0.0394\n",
      "Epoch 118: Batch 19/116 loss: 0.0434\n",
      "Epoch 118: Batch 20/116 loss: 0.0441\n",
      "Epoch 118: Batch 21/116 loss: 0.0427\n",
      "Epoch 118: Batch 22/116 loss: 0.0503\n",
      "Epoch 118: Batch 23/116 loss: 0.0468\n",
      "Epoch 118: Batch 24/116 loss: 0.0319\n",
      "Epoch 118: Batch 25/116 loss: 0.0693\n",
      "Epoch 118: Batch 26/116 loss: 0.0530\n",
      "Epoch 118: Batch 27/116 loss: 0.0351\n",
      "Epoch 118: Batch 28/116 loss: 0.0730\n",
      "Epoch 118: Batch 29/116 loss: 0.0659\n",
      "Epoch 118: Batch 30/116 loss: 0.0401\n",
      "Epoch 118: Batch 31/116 loss: 0.0404\n",
      "Epoch 118: Batch 32/116 loss: 0.0414\n",
      "Epoch 118: Batch 33/116 loss: 0.0487\n",
      "Epoch 118: Batch 34/116 loss: 0.0384\n",
      "Epoch 118: Batch 35/116 loss: 0.0383\n",
      "Epoch 118: Batch 36/116 loss: 0.0487\n",
      "Epoch 118: Batch 37/116 loss: 0.0322\n",
      "Epoch 118: Batch 38/116 loss: 0.0705\n",
      "Epoch 118: Batch 39/116 loss: 0.0257\n",
      "Epoch 118: Batch 40/116 loss: 0.0519\n",
      "Epoch 118: Batch 41/116 loss: 0.0568\n",
      "Epoch 118: Batch 42/116 loss: 0.0577\n",
      "Epoch 118: Batch 43/116 loss: 0.0405\n",
      "Epoch 118: Batch 44/116 loss: 0.0595\n",
      "Epoch 118: Batch 45/116 loss: 0.0403\n",
      "Epoch 118: Batch 46/116 loss: 0.0352\n",
      "Epoch 118: Batch 47/116 loss: 0.0574\n",
      "Epoch 118: Batch 48/116 loss: 0.0276\n",
      "Epoch 118: Batch 49/116 loss: 0.0437\n",
      "Epoch 118: Batch 50/116 loss: 0.0398\n",
      "Epoch 118: Batch 51/116 loss: 0.0550\n",
      "Epoch 118: Batch 52/116 loss: 0.0446\n",
      "Epoch 118: Batch 53/116 loss: 0.0365\n",
      "Epoch 118: Batch 54/116 loss: 0.0529\n",
      "Epoch 118: Batch 55/116 loss: 0.0465\n",
      "Epoch 118: Batch 56/116 loss: 0.0478\n",
      "Epoch 118: Batch 57/116 loss: 0.0736\n",
      "Epoch 118: Batch 58/116 loss: 0.0623\n",
      "Epoch 118: Batch 59/116 loss: 0.0438\n",
      "Epoch 118: Batch 60/116 loss: 0.0457\n",
      "Epoch 118: Batch 61/116 loss: 0.0474\n",
      "Epoch 118: Batch 62/116 loss: 0.0406\n",
      "Epoch 118: Batch 63/116 loss: 0.0710\n",
      "Epoch 118: Batch 64/116 loss: 0.0545\n",
      "Epoch 118: Batch 65/116 loss: 0.0434\n",
      "Epoch 118: Batch 66/116 loss: 0.0517\n",
      "Epoch 118: Batch 67/116 loss: 0.0552\n",
      "Epoch 118: Batch 68/116 loss: 0.0445\n",
      "Epoch 118: Batch 69/116 loss: 0.0585\n",
      "Epoch 118: Batch 70/116 loss: 0.0451\n",
      "Epoch 118: Batch 71/116 loss: 0.0540\n",
      "Epoch 118: Batch 72/116 loss: 0.0573\n",
      "Epoch 118: Batch 73/116 loss: 0.0513\n",
      "Epoch 118: Batch 74/116 loss: 0.0614\n",
      "Epoch 118: Batch 75/116 loss: 0.0581\n",
      "Epoch 118: Batch 76/116 loss: 0.0389\n",
      "Epoch 118: Batch 77/116 loss: 0.0527\n",
      "Epoch 118: Batch 78/116 loss: 0.0578\n",
      "Epoch 118: Batch 79/116 loss: 0.0494\n",
      "Epoch 118: Batch 80/116 loss: 0.0474\n",
      "Epoch 118: Batch 81/116 loss: 0.0594\n",
      "Epoch 118: Batch 82/116 loss: 0.0499\n",
      "Epoch 118: Batch 83/116 loss: 0.0328\n",
      "Epoch 118: Batch 84/116 loss: 0.0472\n",
      "Epoch 118: Batch 85/116 loss: 0.0430\n",
      "Epoch 118: Batch 86/116 loss: 0.0497\n",
      "Epoch 118: Batch 87/116 loss: 0.0469\n",
      "Epoch 118: Batch 88/116 loss: 0.0274\n",
      "Epoch 118: Batch 89/116 loss: 0.0583\n",
      "Epoch 118: Batch 90/116 loss: 0.0505\n",
      "Epoch 118: Batch 91/116 loss: 0.0407\n",
      "Epoch 118: Batch 92/116 loss: 0.0569\n",
      "Epoch 118: Batch 93/116 loss: 0.0599\n",
      "Epoch 118: Batch 94/116 loss: 0.0462\n",
      "Epoch 118: Batch 95/116 loss: 0.0632\n",
      "Epoch 118: Batch 96/116 loss: 0.0622\n",
      "Epoch 118: Batch 97/116 loss: 0.0528\n",
      "Epoch 118: Batch 98/116 loss: 0.0604\n",
      "Epoch 118: Batch 99/116 loss: 0.0418\n",
      "Epoch 118: Batch 100/116 loss: 0.0519\n",
      "Epoch 118: Batch 101/116 loss: 0.0753\n",
      "Epoch 118: Batch 102/116 loss: 0.0567\n",
      "Epoch 118: Batch 103/116 loss: 0.0424\n",
      "Epoch 118: Batch 104/116 loss: 0.0505\n",
      "Epoch 118: Batch 105/116 loss: 0.0443\n",
      "Epoch 118: Batch 106/116 loss: 0.0388\n",
      "Epoch 118: Batch 107/116 loss: 0.0234\n",
      "Epoch 118: Batch 108/116 loss: 0.0330\n",
      "Epoch 118: Batch 109/116 loss: 0.0606\n",
      "Epoch 118: Batch 110/116 loss: 0.0494\n",
      "Epoch 118: Batch 111/116 loss: 0.0709\n",
      "Epoch 118: Batch 112/116 loss: 0.0605\n",
      "Epoch 118: Batch 113/116 loss: 0.0397\n",
      "Epoch 118: Batch 114/116 loss: 0.0485\n",
      "Epoch 118: Batch 115/116 loss: 0.0513\n",
      "Epoch 118: Batch 116/116 loss: 0.0572\n",
      "Epoch 118 train loss: 0.0497 valid loss: 0.0594\n",
      "performance reducing: 1\n",
      "Epoch 119: Batch 1/116 loss: 0.0664\n",
      "Epoch 119: Batch 2/116 loss: 0.0473\n",
      "Epoch 119: Batch 3/116 loss: 0.0473\n",
      "Epoch 119: Batch 4/116 loss: 0.0401\n",
      "Epoch 119: Batch 5/116 loss: 0.0579\n",
      "Epoch 119: Batch 6/116 loss: 0.0368\n",
      "Epoch 119: Batch 7/116 loss: 0.0525\n",
      "Epoch 119: Batch 8/116 loss: 0.0410\n",
      "Epoch 119: Batch 9/116 loss: 0.0419\n",
      "Epoch 119: Batch 10/116 loss: 0.0654\n",
      "Epoch 119: Batch 11/116 loss: 0.0456\n",
      "Epoch 119: Batch 12/116 loss: 0.0540\n",
      "Epoch 119: Batch 13/116 loss: 0.0403\n",
      "Epoch 119: Batch 14/116 loss: 0.0457\n",
      "Epoch 119: Batch 15/116 loss: 0.0513\n",
      "Epoch 119: Batch 16/116 loss: 0.0634\n",
      "Epoch 119: Batch 17/116 loss: 0.0682\n",
      "Epoch 119: Batch 18/116 loss: 0.0603\n",
      "Epoch 119: Batch 19/116 loss: 0.0343\n",
      "Epoch 119: Batch 20/116 loss: 0.0299\n",
      "Epoch 119: Batch 21/116 loss: 0.0546\n",
      "Epoch 119: Batch 22/116 loss: 0.0461\n",
      "Epoch 119: Batch 23/116 loss: 0.0392\n",
      "Epoch 119: Batch 24/116 loss: 0.0373\n",
      "Epoch 119: Batch 25/116 loss: 0.0423\n",
      "Epoch 119: Batch 26/116 loss: 0.0676\n",
      "Epoch 119: Batch 27/116 loss: 0.0628\n",
      "Epoch 119: Batch 28/116 loss: 0.0500\n",
      "Epoch 119: Batch 29/116 loss: 0.0392\n",
      "Epoch 119: Batch 30/116 loss: 0.0416\n",
      "Epoch 119: Batch 31/116 loss: 0.0598\n",
      "Epoch 119: Batch 32/116 loss: 0.0423\n",
      "Epoch 119: Batch 33/116 loss: 0.0365\n",
      "Epoch 119: Batch 34/116 loss: 0.0438\n",
      "Epoch 119: Batch 35/116 loss: 0.0546\n",
      "Epoch 119: Batch 36/116 loss: 0.0412\n",
      "Epoch 119: Batch 37/116 loss: 0.0330\n",
      "Epoch 119: Batch 38/116 loss: 0.0360\n",
      "Epoch 119: Batch 39/116 loss: 0.0437\n",
      "Epoch 119: Batch 40/116 loss: 0.0502\n",
      "Epoch 119: Batch 41/116 loss: 0.0511\n",
      "Epoch 119: Batch 42/116 loss: 0.0688\n",
      "Epoch 119: Batch 43/116 loss: 0.0515\n",
      "Epoch 119: Batch 44/116 loss: 0.0644\n",
      "Epoch 119: Batch 45/116 loss: 0.0518\n",
      "Epoch 119: Batch 46/116 loss: 0.0515\n",
      "Epoch 119: Batch 47/116 loss: 0.0411\n",
      "Epoch 119: Batch 48/116 loss: 0.0608\n",
      "Epoch 119: Batch 49/116 loss: 0.0646\n",
      "Epoch 119: Batch 50/116 loss: 0.0424\n",
      "Epoch 119: Batch 51/116 loss: 0.0400\n",
      "Epoch 119: Batch 52/116 loss: 0.0327\n",
      "Epoch 119: Batch 53/116 loss: 0.0480\n",
      "Epoch 119: Batch 54/116 loss: 0.0333\n",
      "Epoch 119: Batch 55/116 loss: 0.0359\n",
      "Epoch 119: Batch 56/116 loss: 0.0789\n",
      "Epoch 119: Batch 57/116 loss: 0.0424\n",
      "Epoch 119: Batch 58/116 loss: 0.0464\n",
      "Epoch 119: Batch 59/116 loss: 0.0908\n",
      "Epoch 119: Batch 60/116 loss: 0.0697\n",
      "Epoch 119: Batch 61/116 loss: 0.0488\n",
      "Epoch 119: Batch 62/116 loss: 0.0503\n",
      "Epoch 119: Batch 63/116 loss: 0.0694\n",
      "Epoch 119: Batch 64/116 loss: 0.0336\n",
      "Epoch 119: Batch 65/116 loss: 0.0475\n",
      "Epoch 119: Batch 66/116 loss: 0.0639\n",
      "Epoch 119: Batch 67/116 loss: 0.0377\n",
      "Epoch 119: Batch 68/116 loss: 0.0566\n",
      "Epoch 119: Batch 69/116 loss: 0.0396\n",
      "Epoch 119: Batch 70/116 loss: 0.0504\n",
      "Epoch 119: Batch 71/116 loss: 0.0461\n",
      "Epoch 119: Batch 72/116 loss: 0.0733\n",
      "Epoch 119: Batch 73/116 loss: 0.0423\n",
      "Epoch 119: Batch 74/116 loss: 0.0412\n",
      "Epoch 119: Batch 75/116 loss: 0.0531\n",
      "Epoch 119: Batch 76/116 loss: 0.0528\n",
      "Epoch 119: Batch 77/116 loss: 0.0500\n",
      "Epoch 119: Batch 78/116 loss: 0.0642\n",
      "Epoch 119: Batch 79/116 loss: 0.0499\n",
      "Epoch 119: Batch 80/116 loss: 0.0553\n",
      "Epoch 119: Batch 81/116 loss: 0.0522\n",
      "Epoch 119: Batch 82/116 loss: 0.0468\n",
      "Epoch 119: Batch 83/116 loss: 0.0331\n",
      "Epoch 119: Batch 84/116 loss: 0.0821\n",
      "Epoch 119: Batch 85/116 loss: 0.0363\n",
      "Epoch 119: Batch 86/116 loss: 0.0576\n",
      "Epoch 119: Batch 87/116 loss: 0.0539\n",
      "Epoch 119: Batch 88/116 loss: 0.0376\n",
      "Epoch 119: Batch 89/116 loss: 0.0477\n",
      "Epoch 119: Batch 90/116 loss: 0.0439\n",
      "Epoch 119: Batch 91/116 loss: 0.0438\n",
      "Epoch 119: Batch 92/116 loss: 0.0349\n",
      "Epoch 119: Batch 93/116 loss: 0.0504\n",
      "Epoch 119: Batch 94/116 loss: 0.0402\n",
      "Epoch 119: Batch 95/116 loss: 0.0596\n",
      "Epoch 119: Batch 96/116 loss: 0.0521\n",
      "Epoch 119: Batch 97/116 loss: 0.0461\n",
      "Epoch 119: Batch 98/116 loss: 0.0326\n",
      "Epoch 119: Batch 99/116 loss: 0.0400\n",
      "Epoch 119: Batch 100/116 loss: 0.0482\n",
      "Epoch 119: Batch 101/116 loss: 0.0485\n",
      "Epoch 119: Batch 102/116 loss: 0.0412\n",
      "Epoch 119: Batch 103/116 loss: 0.0719\n",
      "Epoch 119: Batch 104/116 loss: 0.0355\n",
      "Epoch 119: Batch 105/116 loss: 0.0536\n",
      "Epoch 119: Batch 106/116 loss: 0.0750\n",
      "Epoch 119: Batch 107/116 loss: 0.0346\n",
      "Epoch 119: Batch 108/116 loss: 0.0509\n",
      "Epoch 119: Batch 109/116 loss: 0.0437\n",
      "Epoch 119: Batch 110/116 loss: 0.0610\n",
      "Epoch 119: Batch 111/116 loss: 0.0528\n",
      "Epoch 119: Batch 112/116 loss: 0.0447\n",
      "Epoch 119: Batch 113/116 loss: 0.0411\n",
      "Epoch 119: Batch 114/116 loss: 0.0326\n",
      "Epoch 119: Batch 115/116 loss: 0.0372\n",
      "Epoch 119: Batch 116/116 loss: 0.0531\n",
      "Epoch 119 train loss: 0.0493 valid loss: 0.0516\n",
      "Epoch 120: Batch 1/116 loss: 0.0288\n",
      "Epoch 120: Batch 2/116 loss: 0.0403\n",
      "Epoch 120: Batch 3/116 loss: 0.0482\n",
      "Epoch 120: Batch 4/116 loss: 0.0587\n",
      "Epoch 120: Batch 5/116 loss: 0.0257\n",
      "Epoch 120: Batch 6/116 loss: 0.0407\n",
      "Epoch 120: Batch 7/116 loss: 0.0332\n",
      "Epoch 120: Batch 8/116 loss: 0.0842\n",
      "Epoch 120: Batch 9/116 loss: 0.0395\n",
      "Epoch 120: Batch 10/116 loss: 0.0436\n",
      "Epoch 120: Batch 11/116 loss: 0.0356\n",
      "Epoch 120: Batch 12/116 loss: 0.0349\n",
      "Epoch 120: Batch 13/116 loss: 0.0487\n",
      "Epoch 120: Batch 14/116 loss: 0.0579\n",
      "Epoch 120: Batch 15/116 loss: 0.0497\n",
      "Epoch 120: Batch 16/116 loss: 0.0588\n",
      "Epoch 120: Batch 17/116 loss: 0.0406\n",
      "Epoch 120: Batch 18/116 loss: 0.0646\n",
      "Epoch 120: Batch 19/116 loss: 0.0439\n",
      "Epoch 120: Batch 20/116 loss: 0.0443\n",
      "Epoch 120: Batch 21/116 loss: 0.0322\n",
      "Epoch 120: Batch 22/116 loss: 0.0551\n",
      "Epoch 120: Batch 23/116 loss: 0.0681\n",
      "Epoch 120: Batch 24/116 loss: 0.0546\n",
      "Epoch 120: Batch 25/116 loss: 0.0454\n",
      "Epoch 120: Batch 26/116 loss: 0.0491\n",
      "Epoch 120: Batch 27/116 loss: 0.0493\n",
      "Epoch 120: Batch 28/116 loss: 0.0411\n",
      "Epoch 120: Batch 29/116 loss: 0.0495\n",
      "Epoch 120: Batch 30/116 loss: 0.0457\n",
      "Epoch 120: Batch 31/116 loss: 0.0476\n",
      "Epoch 120: Batch 32/116 loss: 0.0484\n",
      "Epoch 120: Batch 33/116 loss: 0.0544\n",
      "Epoch 120: Batch 34/116 loss: 0.0580\n",
      "Epoch 120: Batch 35/116 loss: 0.0367\n",
      "Epoch 120: Batch 36/116 loss: 0.0560\n",
      "Epoch 120: Batch 37/116 loss: 0.0415\n",
      "Epoch 120: Batch 38/116 loss: 0.0482\n",
      "Epoch 120: Batch 39/116 loss: 0.0466\n",
      "Epoch 120: Batch 40/116 loss: 0.0571\n",
      "Epoch 120: Batch 41/116 loss: 0.0310\n",
      "Epoch 120: Batch 42/116 loss: 0.0580\n",
      "Epoch 120: Batch 43/116 loss: 0.0678\n",
      "Epoch 120: Batch 44/116 loss: 0.0445\n",
      "Epoch 120: Batch 45/116 loss: 0.0448\n",
      "Epoch 120: Batch 46/116 loss: 0.0582\n",
      "Epoch 120: Batch 47/116 loss: 0.0504\n",
      "Epoch 120: Batch 48/116 loss: 0.0472\n",
      "Epoch 120: Batch 49/116 loss: 0.0410\n",
      "Epoch 120: Batch 50/116 loss: 0.0478\n",
      "Epoch 120: Batch 51/116 loss: 0.0667\n",
      "Epoch 120: Batch 52/116 loss: 0.0316\n",
      "Epoch 120: Batch 53/116 loss: 0.0612\n",
      "Epoch 120: Batch 54/116 loss: 0.0505\n",
      "Epoch 120: Batch 55/116 loss: 0.0862\n",
      "Epoch 120: Batch 56/116 loss: 0.0478\n",
      "Epoch 120: Batch 57/116 loss: 0.0551\n",
      "Epoch 120: Batch 58/116 loss: 0.0656\n",
      "Epoch 120: Batch 59/116 loss: 0.0510\n",
      "Epoch 120: Batch 60/116 loss: 0.0333\n",
      "Epoch 120: Batch 61/116 loss: 0.0675\n",
      "Epoch 120: Batch 62/116 loss: 0.0396\n",
      "Epoch 120: Batch 63/116 loss: 0.0722\n",
      "Epoch 120: Batch 64/116 loss: 0.0482\n",
      "Epoch 120: Batch 65/116 loss: 0.0855\n",
      "Epoch 120: Batch 66/116 loss: 0.0771\n",
      "Epoch 120: Batch 67/116 loss: 0.0519\n",
      "Epoch 120: Batch 68/116 loss: 0.0359\n",
      "Epoch 120: Batch 69/116 loss: 0.0351\n",
      "Epoch 120: Batch 70/116 loss: 0.0459\n",
      "Epoch 120: Batch 71/116 loss: 0.0470\n",
      "Epoch 120: Batch 72/116 loss: 0.0397\n",
      "Epoch 120: Batch 73/116 loss: 0.0623\n",
      "Epoch 120: Batch 74/116 loss: 0.0517\n",
      "Epoch 120: Batch 75/116 loss: 0.0502\n",
      "Epoch 120: Batch 76/116 loss: 0.0541\n",
      "Epoch 120: Batch 77/116 loss: 0.0648\n",
      "Epoch 120: Batch 78/116 loss: 0.0619\n",
      "Epoch 120: Batch 79/116 loss: 0.0427\n",
      "Epoch 120: Batch 80/116 loss: 0.0679\n",
      "Epoch 120: Batch 81/116 loss: 0.0491\n",
      "Epoch 120: Batch 82/116 loss: 0.0429\n",
      "Epoch 120: Batch 83/116 loss: 0.0740\n",
      "Epoch 120: Batch 84/116 loss: 0.0669\n",
      "Epoch 120: Batch 85/116 loss: 0.0569\n",
      "Epoch 120: Batch 86/116 loss: 0.0556\n",
      "Epoch 120: Batch 87/116 loss: 0.0426\n",
      "Epoch 120: Batch 88/116 loss: 0.0532\n",
      "Epoch 120: Batch 89/116 loss: 0.0525\n",
      "Epoch 120: Batch 90/116 loss: 0.0456\n",
      "Epoch 120: Batch 91/116 loss: 0.0294\n",
      "Epoch 120: Batch 92/116 loss: 0.0496\n",
      "Epoch 120: Batch 93/116 loss: 0.0381\n",
      "Epoch 120: Batch 94/116 loss: 0.0476\n",
      "Epoch 120: Batch 95/116 loss: 0.0440\n",
      "Epoch 120: Batch 96/116 loss: 0.0440\n",
      "Epoch 120: Batch 97/116 loss: 0.0533\n",
      "Epoch 120: Batch 98/116 loss: 0.0521\n",
      "Epoch 120: Batch 99/116 loss: 0.0305\n",
      "Epoch 120: Batch 100/116 loss: 0.0449\n",
      "Epoch 120: Batch 101/116 loss: 0.0360\n",
      "Epoch 120: Batch 102/116 loss: 0.0454\n",
      "Epoch 120: Batch 103/116 loss: 0.0357\n",
      "Epoch 120: Batch 104/116 loss: 0.0259\n",
      "Epoch 120: Batch 105/116 loss: 0.0344\n",
      "Epoch 120: Batch 106/116 loss: 0.0656\n",
      "Epoch 120: Batch 107/116 loss: 0.0319\n",
      "Epoch 120: Batch 108/116 loss: 0.0419\n",
      "Epoch 120: Batch 109/116 loss: 0.0525\n",
      "Epoch 120: Batch 110/116 loss: 0.0585\n",
      "Epoch 120: Batch 111/116 loss: 0.0490\n",
      "Epoch 120: Batch 112/116 loss: 0.0419\n",
      "Epoch 120: Batch 113/116 loss: 0.0406\n",
      "Epoch 120: Batch 114/116 loss: 0.0560\n",
      "Epoch 120: Batch 115/116 loss: 0.0401\n",
      "Epoch 120: Batch 116/116 loss: 0.0445\n",
      "Epoch 120 train loss: 0.0493 valid loss: 0.0549\n",
      "performance reducing: 1\n",
      "Epoch 121: Batch 1/116 loss: 0.0395\n",
      "Epoch 121: Batch 2/116 loss: 0.0407\n",
      "Epoch 121: Batch 3/116 loss: 0.0293\n",
      "Epoch 121: Batch 4/116 loss: 0.0281\n",
      "Epoch 121: Batch 5/116 loss: 0.0368\n",
      "Epoch 121: Batch 6/116 loss: 0.0449\n",
      "Epoch 121: Batch 7/116 loss: 0.0361\n",
      "Epoch 121: Batch 8/116 loss: 0.0499\n",
      "Epoch 121: Batch 9/116 loss: 0.0679\n",
      "Epoch 121: Batch 10/116 loss: 0.0315\n",
      "Epoch 121: Batch 11/116 loss: 0.0477\n",
      "Epoch 121: Batch 12/116 loss: 0.0590\n",
      "Epoch 121: Batch 13/116 loss: 0.0584\n",
      "Epoch 121: Batch 14/116 loss: 0.0371\n",
      "Epoch 121: Batch 15/116 loss: 0.0473\n",
      "Epoch 121: Batch 16/116 loss: 0.0322\n",
      "Epoch 121: Batch 17/116 loss: 0.0499\n",
      "Epoch 121: Batch 18/116 loss: 0.0584\n",
      "Epoch 121: Batch 19/116 loss: 0.0305\n",
      "Epoch 121: Batch 20/116 loss: 0.0549\n",
      "Epoch 121: Batch 21/116 loss: 0.0506\n",
      "Epoch 121: Batch 22/116 loss: 0.0375\n",
      "Epoch 121: Batch 23/116 loss: 0.0337\n",
      "Epoch 121: Batch 24/116 loss: 0.0414\n",
      "Epoch 121: Batch 25/116 loss: 0.0575\n",
      "Epoch 121: Batch 26/116 loss: 0.0540\n",
      "Epoch 121: Batch 27/116 loss: 0.0403\n",
      "Epoch 121: Batch 28/116 loss: 0.0529\n",
      "Epoch 121: Batch 29/116 loss: 0.0304\n",
      "Epoch 121: Batch 30/116 loss: 0.0421\n",
      "Epoch 121: Batch 31/116 loss: 0.0581\n",
      "Epoch 121: Batch 32/116 loss: 0.0444\n",
      "Epoch 121: Batch 33/116 loss: 0.0474\n",
      "Epoch 121: Batch 34/116 loss: 0.0351\n",
      "Epoch 121: Batch 35/116 loss: 0.0531\n",
      "Epoch 121: Batch 36/116 loss: 0.0745\n",
      "Epoch 121: Batch 37/116 loss: 0.0359\n",
      "Epoch 121: Batch 38/116 loss: 0.0702\n",
      "Epoch 121: Batch 39/116 loss: 0.0784\n",
      "Epoch 121: Batch 40/116 loss: 0.0551\n",
      "Epoch 121: Batch 41/116 loss: 0.0572\n",
      "Epoch 121: Batch 42/116 loss: 0.0553\n",
      "Epoch 121: Batch 43/116 loss: 0.0562\n",
      "Epoch 121: Batch 44/116 loss: 0.0415\n",
      "Epoch 121: Batch 45/116 loss: 0.0605\n",
      "Epoch 121: Batch 46/116 loss: 0.0466\n",
      "Epoch 121: Batch 47/116 loss: 0.0447\n",
      "Epoch 121: Batch 48/116 loss: 0.0436\n",
      "Epoch 121: Batch 49/116 loss: 0.0470\n",
      "Epoch 121: Batch 50/116 loss: 0.0268\n",
      "Epoch 121: Batch 51/116 loss: 0.0454\n",
      "Epoch 121: Batch 52/116 loss: 0.0465\n",
      "Epoch 121: Batch 53/116 loss: 0.0390\n",
      "Epoch 121: Batch 54/116 loss: 0.0561\n",
      "Epoch 121: Batch 55/116 loss: 0.0459\n",
      "Epoch 121: Batch 56/116 loss: 0.0509\n",
      "Epoch 121: Batch 57/116 loss: 0.0351\n",
      "Epoch 121: Batch 58/116 loss: 0.0387\n",
      "Epoch 121: Batch 59/116 loss: 0.0294\n",
      "Epoch 121: Batch 60/116 loss: 0.0250\n",
      "Epoch 121: Batch 61/116 loss: 0.0306\n",
      "Epoch 121: Batch 62/116 loss: 0.0562\n",
      "Epoch 121: Batch 63/116 loss: 0.0286\n",
      "Epoch 121: Batch 64/116 loss: 0.0588\n",
      "Epoch 121: Batch 65/116 loss: 0.0322\n",
      "Epoch 121: Batch 66/116 loss: 0.0621\n",
      "Epoch 121: Batch 67/116 loss: 0.0521\n",
      "Epoch 121: Batch 68/116 loss: 0.0575\n",
      "Epoch 121: Batch 69/116 loss: 0.0464\n",
      "Epoch 121: Batch 70/116 loss: 0.0535\n",
      "Epoch 121: Batch 71/116 loss: 0.0560\n",
      "Epoch 121: Batch 72/116 loss: 0.0591\n",
      "Epoch 121: Batch 73/116 loss: 0.0575\n",
      "Epoch 121: Batch 74/116 loss: 0.0538\n",
      "Epoch 121: Batch 75/116 loss: 0.0462\n",
      "Epoch 121: Batch 76/116 loss: 0.0354\n",
      "Epoch 121: Batch 77/116 loss: 0.0356\n",
      "Epoch 121: Batch 78/116 loss: 0.0414\n",
      "Epoch 121: Batch 79/116 loss: 0.0425\n",
      "Epoch 121: Batch 80/116 loss: 0.0505\n",
      "Epoch 121: Batch 81/116 loss: 0.0668\n",
      "Epoch 121: Batch 82/116 loss: 0.0348\n",
      "Epoch 121: Batch 83/116 loss: 0.0581\n",
      "Epoch 121: Batch 84/116 loss: 0.0534\n",
      "Epoch 121: Batch 85/116 loss: 0.0467\n",
      "Epoch 121: Batch 86/116 loss: 0.0564\n",
      "Epoch 121: Batch 87/116 loss: 0.0441\n",
      "Epoch 121: Batch 88/116 loss: 0.0315\n",
      "Epoch 121: Batch 89/116 loss: 0.0583\n",
      "Epoch 121: Batch 90/116 loss: 0.0708\n",
      "Epoch 121: Batch 91/116 loss: 0.0432\n",
      "Epoch 121: Batch 92/116 loss: 0.0695\n",
      "Epoch 121: Batch 93/116 loss: 0.0486\n",
      "Epoch 121: Batch 94/116 loss: 0.0298\n",
      "Epoch 121: Batch 95/116 loss: 0.0615\n",
      "Epoch 121: Batch 96/116 loss: 0.0425\n",
      "Epoch 121: Batch 97/116 loss: 0.0518\n",
      "Epoch 121: Batch 98/116 loss: 0.0722\n",
      "Epoch 121: Batch 99/116 loss: 0.0605\n",
      "Epoch 121: Batch 100/116 loss: 0.0494\n",
      "Epoch 121: Batch 101/116 loss: 0.0655\n",
      "Epoch 121: Batch 102/116 loss: 0.0440\n",
      "Epoch 121: Batch 103/116 loss: 0.0615\n",
      "Epoch 121: Batch 104/116 loss: 0.0563\n",
      "Epoch 121: Batch 105/116 loss: 0.0436\n",
      "Epoch 121: Batch 106/116 loss: 0.0868\n",
      "Epoch 121: Batch 107/116 loss: 0.0516\n",
      "Epoch 121: Batch 108/116 loss: 0.0568\n",
      "Epoch 121: Batch 109/116 loss: 0.0480\n",
      "Epoch 121: Batch 110/116 loss: 0.0610\n",
      "Epoch 121: Batch 111/116 loss: 0.0518\n",
      "Epoch 121: Batch 112/116 loss: 0.0564\n",
      "Epoch 121: Batch 113/116 loss: 0.0484\n",
      "Epoch 121: Batch 114/116 loss: 0.0322\n",
      "Epoch 121: Batch 115/116 loss: 0.0517\n",
      "Epoch 121: Batch 116/116 loss: 0.0557\n",
      "Epoch 121 train loss: 0.0486 valid loss: 0.0608\n",
      "performance reducing: 2\n",
      "Epoch 122: Batch 1/116 loss: 0.0421\n",
      "Epoch 122: Batch 2/116 loss: 0.0434\n",
      "Epoch 122: Batch 3/116 loss: 0.0564\n",
      "Epoch 122: Batch 4/116 loss: 0.0448\n",
      "Epoch 122: Batch 5/116 loss: 0.0332\n",
      "Epoch 122: Batch 6/116 loss: 0.0608\n",
      "Epoch 122: Batch 7/116 loss: 0.0620\n",
      "Epoch 122: Batch 8/116 loss: 0.0607\n",
      "Epoch 122: Batch 9/116 loss: 0.0560\n",
      "Epoch 122: Batch 10/116 loss: 0.0433\n",
      "Epoch 122: Batch 11/116 loss: 0.0249\n",
      "Epoch 122: Batch 12/116 loss: 0.0458\n",
      "Epoch 122: Batch 13/116 loss: 0.0439\n",
      "Epoch 122: Batch 14/116 loss: 0.0509\n",
      "Epoch 122: Batch 15/116 loss: 0.0423\n",
      "Epoch 122: Batch 16/116 loss: 0.1169\n",
      "Epoch 122: Batch 17/116 loss: 0.0500\n",
      "Epoch 122: Batch 18/116 loss: 0.0481\n",
      "Epoch 122: Batch 19/116 loss: 0.0692\n",
      "Epoch 122: Batch 20/116 loss: 0.0560\n",
      "Epoch 122: Batch 21/116 loss: 0.0553\n",
      "Epoch 122: Batch 22/116 loss: 0.0425\n",
      "Epoch 122: Batch 23/116 loss: 0.0551\n",
      "Epoch 122: Batch 24/116 loss: 0.0676\n",
      "Epoch 122: Batch 25/116 loss: 0.0461\n",
      "Epoch 122: Batch 26/116 loss: 0.0445\n",
      "Epoch 122: Batch 27/116 loss: 0.0553\n",
      "Epoch 122: Batch 28/116 loss: 0.0470\n",
      "Epoch 122: Batch 29/116 loss: 0.0403\n",
      "Epoch 122: Batch 30/116 loss: 0.0587\n",
      "Epoch 122: Batch 31/116 loss: 0.0410\n",
      "Epoch 122: Batch 32/116 loss: 0.0382\n",
      "Epoch 122: Batch 33/116 loss: 0.0721\n",
      "Epoch 122: Batch 34/116 loss: 0.0480\n",
      "Epoch 122: Batch 35/116 loss: 0.0648\n",
      "Epoch 122: Batch 36/116 loss: 0.0529\n",
      "Epoch 122: Batch 37/116 loss: 0.0546\n",
      "Epoch 122: Batch 38/116 loss: 0.0450\n",
      "Epoch 122: Batch 39/116 loss: 0.0396\n",
      "Epoch 122: Batch 40/116 loss: 0.0511\n",
      "Epoch 122: Batch 41/116 loss: 0.0495\n",
      "Epoch 122: Batch 42/116 loss: 0.0245\n",
      "Epoch 122: Batch 43/116 loss: 0.0529\n",
      "Epoch 122: Batch 44/116 loss: 0.0368\n",
      "Epoch 122: Batch 45/116 loss: 0.0465\n",
      "Epoch 122: Batch 46/116 loss: 0.0553\n",
      "Epoch 122: Batch 47/116 loss: 0.0266\n",
      "Epoch 122: Batch 48/116 loss: 0.0373\n",
      "Epoch 122: Batch 49/116 loss: 0.0428\n",
      "Epoch 122: Batch 50/116 loss: 0.0535\n",
      "Epoch 122: Batch 51/116 loss: 0.0431\n",
      "Epoch 122: Batch 52/116 loss: 0.0286\n",
      "Epoch 122: Batch 53/116 loss: 0.0482\n",
      "Epoch 122: Batch 54/116 loss: 0.0627\n",
      "Epoch 122: Batch 55/116 loss: 0.0482\n",
      "Epoch 122: Batch 56/116 loss: 0.0359\n",
      "Epoch 122: Batch 57/116 loss: 0.0706\n",
      "Epoch 122: Batch 58/116 loss: 0.0573\n",
      "Epoch 122: Batch 59/116 loss: 0.0426\n",
      "Epoch 122: Batch 60/116 loss: 0.0571\n",
      "Epoch 122: Batch 61/116 loss: 0.0575\n",
      "Epoch 122: Batch 62/116 loss: 0.0622\n",
      "Epoch 122: Batch 63/116 loss: 0.0556\n",
      "Epoch 122: Batch 64/116 loss: 0.0411\n",
      "Epoch 122: Batch 65/116 loss: 0.0368\n",
      "Epoch 122: Batch 66/116 loss: 0.0324\n",
      "Epoch 122: Batch 67/116 loss: 0.0604\n",
      "Epoch 122: Batch 68/116 loss: 0.0221\n",
      "Epoch 122: Batch 69/116 loss: 0.0511\n",
      "Epoch 122: Batch 70/116 loss: 0.0499\n",
      "Epoch 122: Batch 71/116 loss: 0.0596\n",
      "Epoch 122: Batch 72/116 loss: 0.0344\n",
      "Epoch 122: Batch 73/116 loss: 0.0421\n",
      "Epoch 122: Batch 74/116 loss: 0.0733\n",
      "Epoch 122: Batch 75/116 loss: 0.0603\n",
      "Epoch 122: Batch 76/116 loss: 0.0558\n",
      "Epoch 122: Batch 77/116 loss: 0.0378\n",
      "Epoch 122: Batch 78/116 loss: 0.0723\n",
      "Epoch 122: Batch 79/116 loss: 0.0609\n",
      "Epoch 122: Batch 80/116 loss: 0.0369\n",
      "Epoch 122: Batch 81/116 loss: 0.0523\n",
      "Epoch 122: Batch 82/116 loss: 0.0593\n",
      "Epoch 122: Batch 83/116 loss: 0.0611\n",
      "Epoch 122: Batch 84/116 loss: 0.0561\n",
      "Epoch 122: Batch 85/116 loss: 0.0656\n",
      "Epoch 122: Batch 86/116 loss: 0.0638\n",
      "Epoch 122: Batch 87/116 loss: 0.0514\n",
      "Epoch 122: Batch 88/116 loss: 0.0417\n",
      "Epoch 122: Batch 89/116 loss: 0.0378\n",
      "Epoch 122: Batch 90/116 loss: 0.0263\n",
      "Epoch 122: Batch 91/116 loss: 0.0543\n",
      "Epoch 122: Batch 92/116 loss: 0.0603\n",
      "Epoch 122: Batch 93/116 loss: 0.0636\n",
      "Epoch 122: Batch 94/116 loss: 0.0435\n",
      "Epoch 122: Batch 95/116 loss: 0.0387\n",
      "Epoch 122: Batch 96/116 loss: 0.0593\n",
      "Epoch 122: Batch 97/116 loss: 0.0367\n",
      "Epoch 122: Batch 98/116 loss: 0.0472\n",
      "Epoch 122: Batch 99/116 loss: 0.0618\n",
      "Epoch 122: Batch 100/116 loss: 0.0430\n",
      "Epoch 122: Batch 101/116 loss: 0.0515\n",
      "Epoch 122: Batch 102/116 loss: 0.0408\n",
      "Epoch 122: Batch 103/116 loss: 0.0445\n",
      "Epoch 122: Batch 104/116 loss: 0.0433\n",
      "Epoch 122: Batch 105/116 loss: 0.0535\n",
      "Epoch 122: Batch 106/116 loss: 0.0487\n",
      "Epoch 122: Batch 107/116 loss: 0.0585\n",
      "Epoch 122: Batch 108/116 loss: 0.0486\n",
      "Epoch 122: Batch 109/116 loss: 0.0414\n",
      "Epoch 122: Batch 110/116 loss: 0.0324\n",
      "Epoch 122: Batch 111/116 loss: 0.0327\n",
      "Epoch 122: Batch 112/116 loss: 0.0375\n",
      "Epoch 122: Batch 113/116 loss: 0.0418\n",
      "Epoch 122: Batch 114/116 loss: 0.0531\n",
      "Epoch 122: Batch 115/116 loss: 0.0323\n",
      "Epoch 122: Batch 116/116 loss: 0.0326\n",
      "Epoch 122 train loss: 0.0492 valid loss: 0.0558\n",
      "performance reducing: 3\n",
      "Epoch 123: Batch 1/116 loss: 0.0665\n",
      "Epoch 123: Batch 2/116 loss: 0.0525\n",
      "Epoch 123: Batch 3/116 loss: 0.0384\n",
      "Epoch 123: Batch 4/116 loss: 0.0597\n",
      "Epoch 123: Batch 5/116 loss: 0.0379\n",
      "Epoch 123: Batch 6/116 loss: 0.0436\n",
      "Epoch 123: Batch 7/116 loss: 0.0400\n",
      "Epoch 123: Batch 8/116 loss: 0.0437\n",
      "Epoch 123: Batch 9/116 loss: 0.0440\n",
      "Epoch 123: Batch 10/116 loss: 0.0486\n",
      "Epoch 123: Batch 11/116 loss: 0.0552\n",
      "Epoch 123: Batch 12/116 loss: 0.0360\n",
      "Epoch 123: Batch 13/116 loss: 0.0580\n",
      "Epoch 123: Batch 14/116 loss: 0.0459\n",
      "Epoch 123: Batch 15/116 loss: 0.0745\n",
      "Epoch 123: Batch 16/116 loss: 0.0403\n",
      "Epoch 123: Batch 17/116 loss: 0.0499\n",
      "Epoch 123: Batch 18/116 loss: 0.0489\n",
      "Epoch 123: Batch 19/116 loss: 0.0535\n",
      "Epoch 123: Batch 20/116 loss: 0.0531\n",
      "Epoch 123: Batch 21/116 loss: 0.0463\n",
      "Epoch 123: Batch 22/116 loss: 0.0416\n",
      "Epoch 123: Batch 23/116 loss: 0.0510\n",
      "Epoch 123: Batch 24/116 loss: 0.1006\n",
      "Epoch 123: Batch 25/116 loss: 0.0463\n",
      "Epoch 123: Batch 26/116 loss: 0.0707\n",
      "Epoch 123: Batch 27/116 loss: 0.0599\n",
      "Epoch 123: Batch 28/116 loss: 0.0233\n",
      "Epoch 123: Batch 29/116 loss: 0.0457\n",
      "Epoch 123: Batch 30/116 loss: 0.0503\n",
      "Epoch 123: Batch 31/116 loss: 0.0555\n",
      "Epoch 123: Batch 32/116 loss: 0.0474\n",
      "Epoch 123: Batch 33/116 loss: 0.0693\n",
      "Epoch 123: Batch 34/116 loss: 0.0422\n",
      "Epoch 123: Batch 35/116 loss: 0.0545\n",
      "Epoch 123: Batch 36/116 loss: 0.0863\n",
      "Epoch 123: Batch 37/116 loss: 0.0628\n",
      "Epoch 123: Batch 38/116 loss: 0.0572\n",
      "Epoch 123: Batch 39/116 loss: 0.0467\n",
      "Epoch 123: Batch 40/116 loss: 0.0323\n",
      "Epoch 123: Batch 41/116 loss: 0.0420\n",
      "Epoch 123: Batch 42/116 loss: 0.0486\n",
      "Epoch 123: Batch 43/116 loss: 0.0507\n",
      "Epoch 123: Batch 44/116 loss: 0.0788\n",
      "Epoch 123: Batch 45/116 loss: 0.0421\n",
      "Epoch 123: Batch 46/116 loss: 0.0554\n",
      "Epoch 123: Batch 47/116 loss: 0.0447\n",
      "Epoch 123: Batch 48/116 loss: 0.0542\n",
      "Epoch 123: Batch 49/116 loss: 0.0530\n",
      "Epoch 123: Batch 50/116 loss: 0.0239\n",
      "Epoch 123: Batch 51/116 loss: 0.0592\n",
      "Epoch 123: Batch 52/116 loss: 0.0545\n",
      "Epoch 123: Batch 53/116 loss: 0.0564\n",
      "Epoch 123: Batch 54/116 loss: 0.0534\n",
      "Epoch 123: Batch 55/116 loss: 0.0657\n",
      "Epoch 123: Batch 56/116 loss: 0.0368\n",
      "Epoch 123: Batch 57/116 loss: 0.0527\n",
      "Epoch 123: Batch 58/116 loss: 0.0450\n",
      "Epoch 123: Batch 59/116 loss: 0.0639\n",
      "Epoch 123: Batch 60/116 loss: 0.0439\n",
      "Epoch 123: Batch 61/116 loss: 0.0559\n",
      "Epoch 123: Batch 62/116 loss: 0.0378\n",
      "Epoch 123: Batch 63/116 loss: 0.0423\n",
      "Epoch 123: Batch 64/116 loss: 0.0852\n",
      "Epoch 123: Batch 65/116 loss: 0.0465\n",
      "Epoch 123: Batch 66/116 loss: 0.0420\n",
      "Epoch 123: Batch 67/116 loss: 0.0383\n",
      "Epoch 123: Batch 68/116 loss: 0.0861\n",
      "Epoch 123: Batch 69/116 loss: 0.0612\n",
      "Epoch 123: Batch 70/116 loss: 0.0354\n",
      "Epoch 123: Batch 71/116 loss: 0.0495\n",
      "Epoch 123: Batch 72/116 loss: 0.0409\n",
      "Epoch 123: Batch 73/116 loss: 0.0566\n",
      "Epoch 123: Batch 74/116 loss: 0.0589\n",
      "Epoch 123: Batch 75/116 loss: 0.0460\n",
      "Epoch 123: Batch 76/116 loss: 0.0416\n",
      "Epoch 123: Batch 77/116 loss: 0.0314\n",
      "Epoch 123: Batch 78/116 loss: 0.0497\n",
      "Epoch 123: Batch 79/116 loss: 0.0652\n",
      "Epoch 123: Batch 80/116 loss: 0.0373\n",
      "Epoch 123: Batch 81/116 loss: 0.0424\n",
      "Epoch 123: Batch 82/116 loss: 0.0519\n",
      "Epoch 123: Batch 83/116 loss: 0.0407\n",
      "Epoch 123: Batch 84/116 loss: 0.0513\n",
      "Epoch 123: Batch 85/116 loss: 0.0511\n",
      "Epoch 123: Batch 86/116 loss: 0.0552\n",
      "Epoch 123: Batch 87/116 loss: 0.0853\n",
      "Epoch 123: Batch 88/116 loss: 0.0448\n",
      "Epoch 123: Batch 89/116 loss: 0.0394\n",
      "Epoch 123: Batch 90/116 loss: 0.0579\n",
      "Epoch 123: Batch 91/116 loss: 0.0258\n",
      "Epoch 123: Batch 92/116 loss: 0.0432\n",
      "Epoch 123: Batch 93/116 loss: 0.0427\n",
      "Epoch 123: Batch 94/116 loss: 0.0620\n",
      "Epoch 123: Batch 95/116 loss: 0.0595\n",
      "Epoch 123: Batch 96/116 loss: 0.0334\n",
      "Epoch 123: Batch 97/116 loss: 0.0569\n",
      "Epoch 123: Batch 98/116 loss: 0.0330\n",
      "Epoch 123: Batch 99/116 loss: 0.0589\n",
      "Epoch 123: Batch 100/116 loss: 0.0443\n",
      "Epoch 123: Batch 101/116 loss: 0.0301\n",
      "Epoch 123: Batch 102/116 loss: 0.0284\n",
      "Epoch 123: Batch 103/116 loss: 0.0432\n",
      "Epoch 123: Batch 104/116 loss: 0.0364\n",
      "Epoch 123: Batch 105/116 loss: 0.0332\n",
      "Epoch 123: Batch 106/116 loss: 0.0341\n",
      "Epoch 123: Batch 107/116 loss: 0.0494\n",
      "Epoch 123: Batch 108/116 loss: 0.0789\n",
      "Epoch 123: Batch 109/116 loss: 0.0350\n",
      "Epoch 123: Batch 110/116 loss: 0.0398\n",
      "Epoch 123: Batch 111/116 loss: 0.0298\n",
      "Epoch 123: Batch 112/116 loss: 0.0516\n",
      "Epoch 123: Batch 113/116 loss: 0.0363\n",
      "Epoch 123: Batch 114/116 loss: 0.0338\n",
      "Epoch 123: Batch 115/116 loss: 0.0431\n",
      "Epoch 123: Batch 116/116 loss: 0.0288\n",
      "Epoch 123 train loss: 0.0493 valid loss: 0.0607\n",
      "performance reducing: 4\n",
      "Epoch 124: Batch 1/116 loss: 0.0232\n",
      "Epoch 124: Batch 2/116 loss: 0.0407\n",
      "Epoch 124: Batch 3/116 loss: 0.0491\n",
      "Epoch 124: Batch 4/116 loss: 0.0566\n",
      "Epoch 124: Batch 5/116 loss: 0.0591\n",
      "Epoch 124: Batch 6/116 loss: 0.0502\n",
      "Epoch 124: Batch 7/116 loss: 0.0553\n",
      "Epoch 124: Batch 8/116 loss: 0.0233\n",
      "Epoch 124: Batch 9/116 loss: 0.0490\n",
      "Epoch 124: Batch 10/116 loss: 0.0376\n",
      "Epoch 124: Batch 11/116 loss: 0.0383\n",
      "Epoch 124: Batch 12/116 loss: 0.0470\n",
      "Epoch 124: Batch 13/116 loss: 0.0394\n",
      "Epoch 124: Batch 14/116 loss: 0.0407\n",
      "Epoch 124: Batch 15/116 loss: 0.0456\n",
      "Epoch 124: Batch 16/116 loss: 0.0392\n",
      "Epoch 124: Batch 17/116 loss: 0.0220\n",
      "Epoch 124: Batch 18/116 loss: 0.0438\n",
      "Epoch 124: Batch 19/116 loss: 0.0397\n",
      "Epoch 124: Batch 20/116 loss: 0.0584\n",
      "Epoch 124: Batch 21/116 loss: 0.0523\n",
      "Epoch 124: Batch 22/116 loss: 0.0353\n",
      "Epoch 124: Batch 23/116 loss: 0.0476\n",
      "Epoch 124: Batch 24/116 loss: 0.0409\n",
      "Epoch 124: Batch 25/116 loss: 0.0521\n",
      "Epoch 124: Batch 26/116 loss: 0.0710\n",
      "Epoch 124: Batch 27/116 loss: 0.0555\n",
      "Epoch 124: Batch 28/116 loss: 0.0451\n",
      "Epoch 124: Batch 29/116 loss: 0.0547\n",
      "Epoch 124: Batch 30/116 loss: 0.0522\n",
      "Epoch 124: Batch 31/116 loss: 0.0451\n",
      "Epoch 124: Batch 32/116 loss: 0.0424\n",
      "Epoch 124: Batch 33/116 loss: 0.0516\n",
      "Epoch 124: Batch 34/116 loss: 0.0631\n",
      "Epoch 124: Batch 35/116 loss: 0.0448\n",
      "Epoch 124: Batch 36/116 loss: 0.0439\n",
      "Epoch 124: Batch 37/116 loss: 0.0495\n",
      "Epoch 124: Batch 38/116 loss: 0.0345\n",
      "Epoch 124: Batch 39/116 loss: 0.0380\n",
      "Epoch 124: Batch 40/116 loss: 0.0237\n",
      "Epoch 124: Batch 41/116 loss: 0.0528\n",
      "Epoch 124: Batch 42/116 loss: 0.0632\n",
      "Epoch 124: Batch 43/116 loss: 0.0382\n",
      "Epoch 124: Batch 44/116 loss: 0.0437\n",
      "Epoch 124: Batch 45/116 loss: 0.0386\n",
      "Epoch 124: Batch 46/116 loss: 0.0575\n",
      "Epoch 124: Batch 47/116 loss: 0.0418\n",
      "Epoch 124: Batch 48/116 loss: 0.0411\n",
      "Epoch 124: Batch 49/116 loss: 0.0438\n",
      "Epoch 124: Batch 50/116 loss: 0.0412\n",
      "Epoch 124: Batch 51/116 loss: 0.0356\n",
      "Epoch 124: Batch 52/116 loss: 0.0522\n",
      "Epoch 124: Batch 53/116 loss: 0.0505\n",
      "Epoch 124: Batch 54/116 loss: 0.0464\n",
      "Epoch 124: Batch 55/116 loss: 0.0279\n",
      "Epoch 124: Batch 56/116 loss: 0.0558\n",
      "Epoch 124: Batch 57/116 loss: 0.1018\n",
      "Epoch 124: Batch 58/116 loss: 0.0471\n",
      "Epoch 124: Batch 59/116 loss: 0.0422\n",
      "Epoch 124: Batch 60/116 loss: 0.0415\n",
      "Epoch 124: Batch 61/116 loss: 0.0569\n",
      "Epoch 124: Batch 62/116 loss: 0.0519\n",
      "Epoch 124: Batch 63/116 loss: 0.0425\n",
      "Epoch 124: Batch 64/116 loss: 0.0647\n",
      "Epoch 124: Batch 65/116 loss: 0.0603\n",
      "Epoch 124: Batch 66/116 loss: 0.0453\n",
      "Epoch 124: Batch 67/116 loss: 0.0611\n",
      "Epoch 124: Batch 68/116 loss: 0.0543\n",
      "Epoch 124: Batch 69/116 loss: 0.0574\n",
      "Epoch 124: Batch 70/116 loss: 0.0555\n",
      "Epoch 124: Batch 71/116 loss: 0.0580\n",
      "Epoch 124: Batch 72/116 loss: 0.0273\n",
      "Epoch 124: Batch 73/116 loss: 0.0454\n",
      "Epoch 124: Batch 74/116 loss: 0.0389\n",
      "Epoch 124: Batch 75/116 loss: 0.0379\n",
      "Epoch 124: Batch 76/116 loss: 0.0472\n",
      "Epoch 124: Batch 77/116 loss: 0.0597\n",
      "Epoch 124: Batch 78/116 loss: 0.0406\n",
      "Epoch 124: Batch 79/116 loss: 0.0540\n",
      "Epoch 124: Batch 80/116 loss: 0.0475\n",
      "Epoch 124: Batch 81/116 loss: 0.0662\n",
      "Epoch 124: Batch 82/116 loss: 0.0328\n",
      "Epoch 124: Batch 83/116 loss: 0.0316\n",
      "Epoch 124: Batch 84/116 loss: 0.0336\n",
      "Epoch 124: Batch 85/116 loss: 0.0388\n",
      "Epoch 124: Batch 86/116 loss: 0.0652\n",
      "Epoch 124: Batch 87/116 loss: 0.0637\n",
      "Epoch 124: Batch 88/116 loss: 0.0399\n",
      "Epoch 124: Batch 89/116 loss: 0.0549\n",
      "Epoch 124: Batch 90/116 loss: 0.0607\n",
      "Epoch 124: Batch 91/116 loss: 0.0652\n",
      "Epoch 124: Batch 92/116 loss: 0.0640\n",
      "Epoch 124: Batch 93/116 loss: 0.0538\n",
      "Epoch 124: Batch 94/116 loss: 0.0305\n",
      "Epoch 124: Batch 95/116 loss: 0.0436\n",
      "Epoch 124: Batch 96/116 loss: 0.0443\n",
      "Epoch 124: Batch 97/116 loss: 0.0469\n",
      "Epoch 124: Batch 98/116 loss: 0.0535\n",
      "Epoch 124: Batch 99/116 loss: 0.0548\n",
      "Epoch 124: Batch 100/116 loss: 0.0816\n",
      "Epoch 124: Batch 101/116 loss: 0.0823\n",
      "Epoch 124: Batch 102/116 loss: 0.0561\n",
      "Epoch 124: Batch 103/116 loss: 0.0394\n",
      "Epoch 124: Batch 104/116 loss: 0.0428\n",
      "Epoch 124: Batch 105/116 loss: 0.0416\n",
      "Epoch 124: Batch 106/116 loss: 0.0531\n",
      "Epoch 124: Batch 107/116 loss: 0.0361\n",
      "Epoch 124: Batch 108/116 loss: 0.0429\n",
      "Epoch 124: Batch 109/116 loss: 0.0429\n",
      "Epoch 124: Batch 110/116 loss: 0.0773\n",
      "Epoch 124: Batch 111/116 loss: 0.0645\n",
      "Epoch 124: Batch 112/116 loss: 0.0583\n",
      "Epoch 124: Batch 113/116 loss: 0.0518\n",
      "Epoch 124: Batch 114/116 loss: 0.0333\n",
      "Epoch 124: Batch 115/116 loss: 0.0745\n",
      "Epoch 124: Batch 116/116 loss: 0.0705\n",
      "Epoch 124 train loss: 0.0488 valid loss: 0.0550\n",
      "performance reducing: 5\n",
      "Epoch 125: Batch 1/116 loss: 0.0464\n",
      "Epoch 125: Batch 2/116 loss: 0.0520\n",
      "Epoch 125: Batch 3/116 loss: 0.0410\n",
      "Epoch 125: Batch 4/116 loss: 0.0458\n",
      "Epoch 125: Batch 5/116 loss: 0.0390\n",
      "Epoch 125: Batch 6/116 loss: 0.0430\n",
      "Epoch 125: Batch 7/116 loss: 0.0599\n",
      "Epoch 125: Batch 8/116 loss: 0.0299\n",
      "Epoch 125: Batch 9/116 loss: 0.0715\n",
      "Epoch 125: Batch 10/116 loss: 0.0387\n",
      "Epoch 125: Batch 11/116 loss: 0.0566\n",
      "Epoch 125: Batch 12/116 loss: 0.0788\n",
      "Epoch 125: Batch 13/116 loss: 0.0421\n",
      "Epoch 125: Batch 14/116 loss: 0.0440\n",
      "Epoch 125: Batch 15/116 loss: 0.0559\n",
      "Epoch 125: Batch 16/116 loss: 0.0739\n",
      "Epoch 125: Batch 17/116 loss: 0.0402\n",
      "Epoch 125: Batch 18/116 loss: 0.0302\n",
      "Epoch 125: Batch 19/116 loss: 0.0524\n",
      "Epoch 125: Batch 20/116 loss: 0.0456\n",
      "Epoch 125: Batch 21/116 loss: 0.0451\n",
      "Epoch 125: Batch 22/116 loss: 0.0340\n",
      "Epoch 125: Batch 23/116 loss: 0.0414\n",
      "Epoch 125: Batch 24/116 loss: 0.0452\n",
      "Epoch 125: Batch 25/116 loss: 0.0582\n",
      "Epoch 125: Batch 26/116 loss: 0.0680\n",
      "Epoch 125: Batch 27/116 loss: 0.0395\n",
      "Epoch 125: Batch 28/116 loss: 0.0398\n",
      "Epoch 125: Batch 29/116 loss: 0.0458\n",
      "Epoch 125: Batch 30/116 loss: 0.0501\n",
      "Epoch 125: Batch 31/116 loss: 0.0435\n",
      "Epoch 125: Batch 32/116 loss: 0.0461\n",
      "Epoch 125: Batch 33/116 loss: 0.0708\n",
      "Epoch 125: Batch 34/116 loss: 0.0451\n",
      "Epoch 125: Batch 35/116 loss: 0.0621\n",
      "Epoch 125: Batch 36/116 loss: 0.0505\n",
      "Epoch 125: Batch 37/116 loss: 0.0665\n",
      "Epoch 125: Batch 38/116 loss: 0.0501\n",
      "Epoch 125: Batch 39/116 loss: 0.0495\n",
      "Epoch 125: Batch 40/116 loss: 0.0516\n",
      "Epoch 125: Batch 41/116 loss: 0.0467\n",
      "Epoch 125: Batch 42/116 loss: 0.0520\n",
      "Epoch 125: Batch 43/116 loss: 0.0463\n",
      "Epoch 125: Batch 44/116 loss: 0.0413\n",
      "Epoch 125: Batch 45/116 loss: 0.0418\n",
      "Epoch 125: Batch 46/116 loss: 0.0597\n",
      "Epoch 125: Batch 47/116 loss: 0.0244\n",
      "Epoch 125: Batch 48/116 loss: 0.0270\n",
      "Epoch 125: Batch 49/116 loss: 0.0433\n",
      "Epoch 125: Batch 50/116 loss: 0.0436\n",
      "Epoch 125: Batch 51/116 loss: 0.0413\n",
      "Epoch 125: Batch 52/116 loss: 0.0498\n",
      "Epoch 125: Batch 53/116 loss: 0.0572\n",
      "Epoch 125: Batch 54/116 loss: 0.0717\n",
      "Epoch 125: Batch 55/116 loss: 0.0526\n",
      "Epoch 125: Batch 56/116 loss: 0.0260\n",
      "Epoch 125: Batch 57/116 loss: 0.0368\n",
      "Epoch 125: Batch 58/116 loss: 0.0441\n",
      "Epoch 125: Batch 59/116 loss: 0.0574\n",
      "Epoch 125: Batch 60/116 loss: 0.0527\n",
      "Epoch 125: Batch 61/116 loss: 0.0521\n",
      "Epoch 125: Batch 62/116 loss: 0.0392\n",
      "Epoch 125: Batch 63/116 loss: 0.0535\n",
      "Epoch 125: Batch 64/116 loss: 0.0379\n",
      "Epoch 125: Batch 65/116 loss: 0.0316\n",
      "Epoch 125: Batch 66/116 loss: 0.0673\n",
      "Epoch 125: Batch 67/116 loss: 0.0742\n",
      "Epoch 125: Batch 68/116 loss: 0.0398\n",
      "Epoch 125: Batch 69/116 loss: 0.0449\n",
      "Epoch 125: Batch 70/116 loss: 0.0609\n",
      "Epoch 125: Batch 71/116 loss: 0.0534\n",
      "Epoch 125: Batch 72/116 loss: 0.0409\n",
      "Epoch 125: Batch 73/116 loss: 0.0415\n",
      "Epoch 125: Batch 74/116 loss: 0.0439\n",
      "Epoch 125: Batch 75/116 loss: 0.0497\n",
      "Epoch 125: Batch 76/116 loss: 0.0440\n",
      "Epoch 125: Batch 77/116 loss: 0.0354\n",
      "Epoch 125: Batch 78/116 loss: 0.0408\n",
      "Epoch 125: Batch 79/116 loss: 0.0736\n",
      "Epoch 125: Batch 80/116 loss: 0.0451\n",
      "Epoch 125: Batch 81/116 loss: 0.0581\n",
      "Epoch 125: Batch 82/116 loss: 0.0540\n",
      "Epoch 125: Batch 83/116 loss: 0.0519\n",
      "Epoch 125: Batch 84/116 loss: 0.0511\n",
      "Epoch 125: Batch 85/116 loss: 0.0811\n",
      "Epoch 125: Batch 86/116 loss: 0.0480\n",
      "Epoch 125: Batch 87/116 loss: 0.0274\n",
      "Epoch 125: Batch 88/116 loss: 0.0356\n",
      "Epoch 125: Batch 89/116 loss: 0.0465\n",
      "Epoch 125: Batch 90/116 loss: 0.0444\n",
      "Epoch 125: Batch 91/116 loss: 0.0472\n",
      "Epoch 125: Batch 92/116 loss: 0.0452\n",
      "Epoch 125: Batch 93/116 loss: 0.0599\n",
      "Epoch 125: Batch 94/116 loss: 0.0369\n",
      "Epoch 125: Batch 95/116 loss: 0.0432\n",
      "Epoch 125: Batch 96/116 loss: 0.0703\n",
      "Epoch 125: Batch 97/116 loss: 0.0538\n",
      "Epoch 125: Batch 98/116 loss: 0.0492\n",
      "Epoch 125: Batch 99/116 loss: 0.0398\n",
      "Epoch 125: Batch 100/116 loss: 0.0431\n",
      "Epoch 125: Batch 101/116 loss: 0.0675\n",
      "Epoch 125: Batch 102/116 loss: 0.0417\n",
      "Epoch 125: Batch 103/116 loss: 0.0485\n",
      "Epoch 125: Batch 104/116 loss: 0.0558\n",
      "Epoch 125: Batch 105/116 loss: 0.0299\n",
      "Epoch 125: Batch 106/116 loss: 0.0560\n",
      "Epoch 125: Batch 107/116 loss: 0.0671\n",
      "Epoch 125: Batch 108/116 loss: 0.0707\n",
      "Epoch 125: Batch 109/116 loss: 0.0370\n",
      "Epoch 125: Batch 110/116 loss: 0.0476\n",
      "Epoch 125: Batch 111/116 loss: 0.0391\n",
      "Epoch 125: Batch 112/116 loss: 0.0362\n",
      "Epoch 125: Batch 113/116 loss: 0.0472\n",
      "Epoch 125: Batch 114/116 loss: 0.0453\n",
      "Epoch 125: Batch 115/116 loss: 0.0360\n",
      "Epoch 125: Batch 116/116 loss: 0.0633\n",
      "Epoch 125 train loss: 0.0487 valid loss: 0.0510\n",
      "Epoch 126: Batch 1/116 loss: 0.0452\n",
      "Epoch 126: Batch 2/116 loss: 0.0480\n",
      "Epoch 126: Batch 3/116 loss: 0.0564\n",
      "Epoch 126: Batch 4/116 loss: 0.0342\n",
      "Epoch 126: Batch 5/116 loss: 0.0524\n",
      "Epoch 126: Batch 6/116 loss: 0.0461\n",
      "Epoch 126: Batch 7/116 loss: 0.0468\n",
      "Epoch 126: Batch 8/116 loss: 0.0317\n",
      "Epoch 126: Batch 9/116 loss: 0.0341\n",
      "Epoch 126: Batch 10/116 loss: 0.0642\n",
      "Epoch 126: Batch 11/116 loss: 0.0486\n",
      "Epoch 126: Batch 12/116 loss: 0.0347\n",
      "Epoch 126: Batch 13/116 loss: 0.0468\n",
      "Epoch 126: Batch 14/116 loss: 0.0555\n",
      "Epoch 126: Batch 15/116 loss: 0.0482\n",
      "Epoch 126: Batch 16/116 loss: 0.0499\n",
      "Epoch 126: Batch 17/116 loss: 0.1015\n",
      "Epoch 126: Batch 18/116 loss: 0.0305\n",
      "Epoch 126: Batch 19/116 loss: 0.0533\n",
      "Epoch 126: Batch 20/116 loss: 0.0470\n",
      "Epoch 126: Batch 21/116 loss: 0.0380\n",
      "Epoch 126: Batch 22/116 loss: 0.0566\n",
      "Epoch 126: Batch 23/116 loss: 0.0467\n",
      "Epoch 126: Batch 24/116 loss: 0.0623\n",
      "Epoch 126: Batch 25/116 loss: 0.0543\n",
      "Epoch 126: Batch 26/116 loss: 0.0344\n",
      "Epoch 126: Batch 27/116 loss: 0.0347\n",
      "Epoch 126: Batch 28/116 loss: 0.0546\n",
      "Epoch 126: Batch 29/116 loss: 0.0453\n",
      "Epoch 126: Batch 30/116 loss: 0.0680\n",
      "Epoch 126: Batch 31/116 loss: 0.0326\n",
      "Epoch 126: Batch 32/116 loss: 0.0444\n",
      "Epoch 126: Batch 33/116 loss: 0.0443\n",
      "Epoch 126: Batch 34/116 loss: 0.0497\n",
      "Epoch 126: Batch 35/116 loss: 0.0394\n",
      "Epoch 126: Batch 36/116 loss: 0.0603\n",
      "Epoch 126: Batch 37/116 loss: 0.0370\n",
      "Epoch 126: Batch 38/116 loss: 0.0506\n",
      "Epoch 126: Batch 39/116 loss: 0.0350\n",
      "Epoch 126: Batch 40/116 loss: 0.0414\n",
      "Epoch 126: Batch 41/116 loss: 0.0535\n",
      "Epoch 126: Batch 42/116 loss: 0.0468\n",
      "Epoch 126: Batch 43/116 loss: 0.0377\n",
      "Epoch 126: Batch 44/116 loss: 0.0732\n",
      "Epoch 126: Batch 45/116 loss: 0.0794\n",
      "Epoch 126: Batch 46/116 loss: 0.0556\n",
      "Epoch 126: Batch 47/116 loss: 0.0407\n",
      "Epoch 126: Batch 48/116 loss: 0.0557\n",
      "Epoch 126: Batch 49/116 loss: 0.0433\n",
      "Epoch 126: Batch 50/116 loss: 0.0487\n",
      "Epoch 126: Batch 51/116 loss: 0.0495\n",
      "Epoch 126: Batch 52/116 loss: 0.0496\n",
      "Epoch 126: Batch 53/116 loss: 0.0367\n",
      "Epoch 126: Batch 54/116 loss: 0.0428\n",
      "Epoch 126: Batch 55/116 loss: 0.0557\n",
      "Epoch 126: Batch 56/116 loss: 0.0504\n",
      "Epoch 126: Batch 57/116 loss: 0.0430\n",
      "Epoch 126: Batch 58/116 loss: 0.0502\n",
      "Epoch 126: Batch 59/116 loss: 0.0461\n",
      "Epoch 126: Batch 60/116 loss: 0.0483\n",
      "Epoch 126: Batch 61/116 loss: 0.0700\n",
      "Epoch 126: Batch 62/116 loss: 0.0381\n",
      "Epoch 126: Batch 63/116 loss: 0.0401\n",
      "Epoch 126: Batch 64/116 loss: 0.0417\n",
      "Epoch 126: Batch 65/116 loss: 0.0270\n",
      "Epoch 126: Batch 66/116 loss: 0.0398\n",
      "Epoch 126: Batch 67/116 loss: 0.0478\n",
      "Epoch 126: Batch 68/116 loss: 0.0440\n",
      "Epoch 126: Batch 69/116 loss: 0.0540\n",
      "Epoch 126: Batch 70/116 loss: 0.0457\n",
      "Epoch 126: Batch 71/116 loss: 0.0378\n",
      "Epoch 126: Batch 72/116 loss: 0.0398\n",
      "Epoch 126: Batch 73/116 loss: 0.0357\n",
      "Epoch 126: Batch 74/116 loss: 0.0294\n",
      "Epoch 126: Batch 75/116 loss: 0.0387\n",
      "Epoch 126: Batch 76/116 loss: 0.0498\n",
      "Epoch 126: Batch 77/116 loss: 0.0259\n",
      "Epoch 126: Batch 78/116 loss: 0.0438\n",
      "Epoch 126: Batch 79/116 loss: 0.0507\n",
      "Epoch 126: Batch 80/116 loss: 0.0363\n",
      "Epoch 126: Batch 81/116 loss: 0.0534\n",
      "Epoch 126: Batch 82/116 loss: 0.0800\n",
      "Epoch 126: Batch 83/116 loss: 0.0615\n",
      "Epoch 126: Batch 84/116 loss: 0.0456\n",
      "Epoch 126: Batch 85/116 loss: 0.0523\n",
      "Epoch 126: Batch 86/116 loss: 0.0305\n",
      "Epoch 126: Batch 87/116 loss: 0.0678\n",
      "Epoch 126: Batch 88/116 loss: 0.0479\n",
      "Epoch 126: Batch 89/116 loss: 0.0469\n",
      "Epoch 126: Batch 90/116 loss: 0.0562\n",
      "Epoch 126: Batch 91/116 loss: 0.0434\n",
      "Epoch 126: Batch 92/116 loss: 0.0471\n",
      "Epoch 126: Batch 93/116 loss: 0.0335\n",
      "Epoch 126: Batch 94/116 loss: 0.0331\n",
      "Epoch 126: Batch 95/116 loss: 0.0284\n",
      "Epoch 126: Batch 96/116 loss: 0.0436\n",
      "Epoch 126: Batch 97/116 loss: 0.0719\n",
      "Epoch 126: Batch 98/116 loss: 0.0566\n",
      "Epoch 126: Batch 99/116 loss: 0.0594\n",
      "Epoch 126: Batch 100/116 loss: 0.0659\n",
      "Epoch 126: Batch 101/116 loss: 0.0749\n",
      "Epoch 126: Batch 102/116 loss: 0.0528\n",
      "Epoch 126: Batch 103/116 loss: 0.0312\n",
      "Epoch 126: Batch 104/116 loss: 0.0585\n",
      "Epoch 126: Batch 105/116 loss: 0.0457\n",
      "Epoch 126: Batch 106/116 loss: 0.0485\n",
      "Epoch 126: Batch 107/116 loss: 0.0624\n",
      "Epoch 126: Batch 108/116 loss: 0.0459\n",
      "Epoch 126: Batch 109/116 loss: 0.0509\n",
      "Epoch 126: Batch 110/116 loss: 0.0577\n",
      "Epoch 126: Batch 111/116 loss: 0.0463\n",
      "Epoch 126: Batch 112/116 loss: 0.0410\n",
      "Epoch 126: Batch 113/116 loss: 0.0338\n",
      "Epoch 126: Batch 114/116 loss: 0.0569\n",
      "Epoch 126: Batch 115/116 loss: 0.0673\n",
      "Epoch 126: Batch 116/116 loss: 0.0335\n",
      "Epoch 126 train loss: 0.0481 valid loss: 0.0546\n",
      "performance reducing: 1\n",
      "Epoch 127: Batch 1/116 loss: 0.0375\n",
      "Epoch 127: Batch 2/116 loss: 0.0495\n",
      "Epoch 127: Batch 3/116 loss: 0.0737\n",
      "Epoch 127: Batch 4/116 loss: 0.0530\n",
      "Epoch 127: Batch 5/116 loss: 0.0776\n",
      "Epoch 127: Batch 6/116 loss: 0.0752\n",
      "Epoch 127: Batch 7/116 loss: 0.0407\n",
      "Epoch 127: Batch 8/116 loss: 0.0607\n",
      "Epoch 127: Batch 9/116 loss: 0.0471\n",
      "Epoch 127: Batch 10/116 loss: 0.0385\n",
      "Epoch 127: Batch 11/116 loss: 0.0383\n",
      "Epoch 127: Batch 12/116 loss: 0.0427\n",
      "Epoch 127: Batch 13/116 loss: 0.0406\n",
      "Epoch 127: Batch 14/116 loss: 0.0462\n",
      "Epoch 127: Batch 15/116 loss: 0.0481\n",
      "Epoch 127: Batch 16/116 loss: 0.0381\n",
      "Epoch 127: Batch 17/116 loss: 0.0514\n",
      "Epoch 127: Batch 18/116 loss: 0.0605\n",
      "Epoch 127: Batch 19/116 loss: 0.0211\n",
      "Epoch 127: Batch 20/116 loss: 0.0418\n",
      "Epoch 127: Batch 21/116 loss: 0.0464\n",
      "Epoch 127: Batch 22/116 loss: 0.0480\n",
      "Epoch 127: Batch 23/116 loss: 0.0383\n",
      "Epoch 127: Batch 24/116 loss: 0.0676\n",
      "Epoch 127: Batch 25/116 loss: 0.0362\n",
      "Epoch 127: Batch 26/116 loss: 0.0393\n",
      "Epoch 127: Batch 27/116 loss: 0.0349\n",
      "Epoch 127: Batch 28/116 loss: 0.0552\n",
      "Epoch 127: Batch 29/116 loss: 0.0629\n",
      "Epoch 127: Batch 30/116 loss: 0.0437\n",
      "Epoch 127: Batch 31/116 loss: 0.0675\n",
      "Epoch 127: Batch 32/116 loss: 0.0629\n",
      "Epoch 127: Batch 33/116 loss: 0.0503\n",
      "Epoch 127: Batch 34/116 loss: 0.0409\n",
      "Epoch 127: Batch 35/116 loss: 0.0712\n",
      "Epoch 127: Batch 36/116 loss: 0.0776\n",
      "Epoch 127: Batch 37/116 loss: 0.0574\n",
      "Epoch 127: Batch 38/116 loss: 0.0543\n",
      "Epoch 127: Batch 39/116 loss: 0.0325\n",
      "Epoch 127: Batch 40/116 loss: 0.0569\n",
      "Epoch 127: Batch 41/116 loss: 0.0465\n",
      "Epoch 127: Batch 42/116 loss: 0.0557\n",
      "Epoch 127: Batch 43/116 loss: 0.0362\n",
      "Epoch 127: Batch 44/116 loss: 0.0384\n",
      "Epoch 127: Batch 45/116 loss: 0.0390\n",
      "Epoch 127: Batch 46/116 loss: 0.0347\n",
      "Epoch 127: Batch 47/116 loss: 0.0412\n",
      "Epoch 127: Batch 48/116 loss: 0.0437\n",
      "Epoch 127: Batch 49/116 loss: 0.0639\n",
      "Epoch 127: Batch 50/116 loss: 0.0402\n",
      "Epoch 127: Batch 51/116 loss: 0.0337\n",
      "Epoch 127: Batch 52/116 loss: 0.0428\n",
      "Epoch 127: Batch 53/116 loss: 0.0459\n",
      "Epoch 127: Batch 54/116 loss: 0.0563\n",
      "Epoch 127: Batch 55/116 loss: 0.0261\n",
      "Epoch 127: Batch 56/116 loss: 0.0348\n",
      "Epoch 127: Batch 57/116 loss: 0.0405\n",
      "Epoch 127: Batch 58/116 loss: 0.0382\n",
      "Epoch 127: Batch 59/116 loss: 0.0481\n",
      "Epoch 127: Batch 60/116 loss: 0.0464\n",
      "Epoch 127: Batch 61/116 loss: 0.0474\n",
      "Epoch 127: Batch 62/116 loss: 0.0402\n",
      "Epoch 127: Batch 63/116 loss: 0.0535\n",
      "Epoch 127: Batch 64/116 loss: 0.0362\n",
      "Epoch 127: Batch 65/116 loss: 0.0590\n",
      "Epoch 127: Batch 66/116 loss: 0.0236\n",
      "Epoch 127: Batch 67/116 loss: 0.0443\n",
      "Epoch 127: Batch 68/116 loss: 0.0477\n",
      "Epoch 127: Batch 69/116 loss: 0.0407\n",
      "Epoch 127: Batch 70/116 loss: 0.0489\n",
      "Epoch 127: Batch 71/116 loss: 0.0900\n",
      "Epoch 127: Batch 72/116 loss: 0.0491\n",
      "Epoch 127: Batch 73/116 loss: 0.0643\n",
      "Epoch 127: Batch 74/116 loss: 0.0621\n",
      "Epoch 127: Batch 75/116 loss: 0.0455\n",
      "Epoch 127: Batch 76/116 loss: 0.0256\n",
      "Epoch 127: Batch 77/116 loss: 0.0360\n",
      "Epoch 127: Batch 78/116 loss: 0.0377\n",
      "Epoch 127: Batch 79/116 loss: 0.0474\n",
      "Epoch 127: Batch 80/116 loss: 0.0732\n",
      "Epoch 127: Batch 81/116 loss: 0.0528\n",
      "Epoch 127: Batch 82/116 loss: 0.0520\n",
      "Epoch 127: Batch 83/116 loss: 0.0331\n",
      "Epoch 127: Batch 84/116 loss: 0.0627\n",
      "Epoch 127: Batch 85/116 loss: 0.0444\n",
      "Epoch 127: Batch 86/116 loss: 0.0532\n",
      "Epoch 127: Batch 87/116 loss: 0.0554\n",
      "Epoch 127: Batch 88/116 loss: 0.0677\n",
      "Epoch 127: Batch 89/116 loss: 0.0418\n",
      "Epoch 127: Batch 90/116 loss: 0.0537\n",
      "Epoch 127: Batch 91/116 loss: 0.0602\n",
      "Epoch 127: Batch 92/116 loss: 0.0659\n",
      "Epoch 127: Batch 93/116 loss: 0.0687\n",
      "Epoch 127: Batch 94/116 loss: 0.0372\n",
      "Epoch 127: Batch 95/116 loss: 0.0320\n",
      "Epoch 127: Batch 96/116 loss: 0.0587\n",
      "Epoch 127: Batch 97/116 loss: 0.0461\n",
      "Epoch 127: Batch 98/116 loss: 0.0392\n",
      "Epoch 127: Batch 99/116 loss: 0.0453\n",
      "Epoch 127: Batch 100/116 loss: 0.0611\n",
      "Epoch 127: Batch 101/116 loss: 0.0474\n",
      "Epoch 127: Batch 102/116 loss: 0.0309\n",
      "Epoch 127: Batch 103/116 loss: 0.0539\n",
      "Epoch 127: Batch 104/116 loss: 0.0577\n",
      "Epoch 127: Batch 105/116 loss: 0.0494\n",
      "Epoch 127: Batch 106/116 loss: 0.0473\n",
      "Epoch 127: Batch 107/116 loss: 0.0489\n",
      "Epoch 127: Batch 108/116 loss: 0.0323\n",
      "Epoch 127: Batch 109/116 loss: 0.0532\n",
      "Epoch 127: Batch 110/116 loss: 0.0355\n",
      "Epoch 127: Batch 111/116 loss: 0.0444\n",
      "Epoch 127: Batch 112/116 loss: 0.0506\n",
      "Epoch 127: Batch 113/116 loss: 0.0613\n",
      "Epoch 127: Batch 114/116 loss: 0.0360\n",
      "Epoch 127: Batch 115/116 loss: 0.0511\n",
      "Epoch 127: Batch 116/116 loss: 0.0310\n",
      "Epoch 127 train loss: 0.0484 valid loss: 0.0536\n",
      "performance reducing: 2\n",
      "Epoch 128: Batch 1/116 loss: 0.0470\n",
      "Epoch 128: Batch 2/116 loss: 0.0442\n",
      "Epoch 128: Batch 3/116 loss: 0.0509\n",
      "Epoch 128: Batch 4/116 loss: 0.0531\n",
      "Epoch 128: Batch 5/116 loss: 0.0499\n",
      "Epoch 128: Batch 6/116 loss: 0.0378\n",
      "Epoch 128: Batch 7/116 loss: 0.0615\n",
      "Epoch 128: Batch 8/116 loss: 0.0345\n",
      "Epoch 128: Batch 9/116 loss: 0.0404\n",
      "Epoch 128: Batch 10/116 loss: 0.0686\n",
      "Epoch 128: Batch 11/116 loss: 0.0555\n",
      "Epoch 128: Batch 12/116 loss: 0.0338\n",
      "Epoch 128: Batch 13/116 loss: 0.0430\n",
      "Epoch 128: Batch 14/116 loss: 0.0548\n",
      "Epoch 128: Batch 15/116 loss: 0.0397\n",
      "Epoch 128: Batch 16/116 loss: 0.0536\n",
      "Epoch 128: Batch 17/116 loss: 0.0511\n",
      "Epoch 128: Batch 18/116 loss: 0.0397\n",
      "Epoch 128: Batch 19/116 loss: 0.0424\n",
      "Epoch 128: Batch 20/116 loss: 0.0652\n",
      "Epoch 128: Batch 21/116 loss: 0.0709\n",
      "Epoch 128: Batch 22/116 loss: 0.0443\n",
      "Epoch 128: Batch 23/116 loss: 0.0595\n",
      "Epoch 128: Batch 24/116 loss: 0.0414\n",
      "Epoch 128: Batch 25/116 loss: 0.0396\n",
      "Epoch 128: Batch 26/116 loss: 0.0386\n",
      "Epoch 128: Batch 27/116 loss: 0.0432\n",
      "Epoch 128: Batch 28/116 loss: 0.0362\n",
      "Epoch 128: Batch 29/116 loss: 0.0415\n",
      "Epoch 128: Batch 30/116 loss: 0.0546\n",
      "Epoch 128: Batch 31/116 loss: 0.0499\n",
      "Epoch 128: Batch 32/116 loss: 0.0397\n",
      "Epoch 128: Batch 33/116 loss: 0.0505\n",
      "Epoch 128: Batch 34/116 loss: 0.0321\n",
      "Epoch 128: Batch 35/116 loss: 0.0592\n",
      "Epoch 128: Batch 36/116 loss: 0.0471\n",
      "Epoch 128: Batch 37/116 loss: 0.0563\n",
      "Epoch 128: Batch 38/116 loss: 0.0585\n",
      "Epoch 128: Batch 39/116 loss: 0.0426\n",
      "Epoch 128: Batch 40/116 loss: 0.0625\n",
      "Epoch 128: Batch 41/116 loss: 0.0437\n",
      "Epoch 128: Batch 42/116 loss: 0.0514\n",
      "Epoch 128: Batch 43/116 loss: 0.0383\n",
      "Epoch 128: Batch 44/116 loss: 0.0571\n",
      "Epoch 128: Batch 45/116 loss: 0.0347\n",
      "Epoch 128: Batch 46/116 loss: 0.0468\n",
      "Epoch 128: Batch 47/116 loss: 0.0536\n",
      "Epoch 128: Batch 48/116 loss: 0.0744\n",
      "Epoch 128: Batch 49/116 loss: 0.0232\n",
      "Epoch 128: Batch 50/116 loss: 0.0551\n",
      "Epoch 128: Batch 51/116 loss: 0.0409\n",
      "Epoch 128: Batch 52/116 loss: 0.0504\n",
      "Epoch 128: Batch 53/116 loss: 0.0497\n",
      "Epoch 128: Batch 54/116 loss: 0.0352\n",
      "Epoch 128: Batch 55/116 loss: 0.0323\n",
      "Epoch 128: Batch 56/116 loss: 0.0349\n",
      "Epoch 128: Batch 57/116 loss: 0.0509\n",
      "Epoch 128: Batch 58/116 loss: 0.0254\n",
      "Epoch 128: Batch 59/116 loss: 0.0430\n",
      "Epoch 128: Batch 60/116 loss: 0.0493\n",
      "Epoch 128: Batch 61/116 loss: 0.0371\n",
      "Epoch 128: Batch 62/116 loss: 0.0319\n",
      "Epoch 128: Batch 63/116 loss: 0.0411\n",
      "Epoch 128: Batch 64/116 loss: 0.0447\n",
      "Epoch 128: Batch 65/116 loss: 0.0860\n",
      "Epoch 128: Batch 66/116 loss: 0.0634\n",
      "Epoch 128: Batch 67/116 loss: 0.0817\n",
      "Epoch 128: Batch 68/116 loss: 0.0556\n",
      "Epoch 128: Batch 69/116 loss: 0.0509\n",
      "Epoch 128: Batch 70/116 loss: 0.0513\n",
      "Epoch 128: Batch 71/116 loss: 0.0421\n",
      "Epoch 128: Batch 72/116 loss: 0.0560\n",
      "Epoch 128: Batch 73/116 loss: 0.0636\n",
      "Epoch 128: Batch 74/116 loss: 0.0312\n",
      "Epoch 128: Batch 75/116 loss: 0.0663\n",
      "Epoch 128: Batch 76/116 loss: 0.0361\n",
      "Epoch 128: Batch 77/116 loss: 0.0605\n",
      "Epoch 128: Batch 78/116 loss: 0.0506\n",
      "Epoch 128: Batch 79/116 loss: 0.0686\n",
      "Epoch 128: Batch 80/116 loss: 0.0363\n",
      "Epoch 128: Batch 81/116 loss: 0.0403\n",
      "Epoch 128: Batch 82/116 loss: 0.0675\n",
      "Epoch 128: Batch 83/116 loss: 0.0584\n",
      "Epoch 128: Batch 84/116 loss: 0.0541\n",
      "Epoch 128: Batch 85/116 loss: 0.0469\n",
      "Epoch 128: Batch 86/116 loss: 0.0642\n",
      "Epoch 128: Batch 87/116 loss: 0.0593\n",
      "Epoch 128: Batch 88/116 loss: 0.0395\n",
      "Epoch 128: Batch 89/116 loss: 0.0553\n",
      "Epoch 128: Batch 90/116 loss: 0.0533\n",
      "Epoch 128: Batch 91/116 loss: 0.0399\n",
      "Epoch 128: Batch 92/116 loss: 0.0480\n",
      "Epoch 128: Batch 93/116 loss: 0.0487\n",
      "Epoch 128: Batch 94/116 loss: 0.0397\n",
      "Epoch 128: Batch 95/116 loss: 0.0396\n",
      "Epoch 128: Batch 96/116 loss: 0.0541\n",
      "Epoch 128: Batch 97/116 loss: 0.0401\n",
      "Epoch 128: Batch 98/116 loss: 0.0388\n",
      "Epoch 128: Batch 99/116 loss: 0.0431\n",
      "Epoch 128: Batch 100/116 loss: 0.0449\n",
      "Epoch 128: Batch 101/116 loss: 0.0468\n",
      "Epoch 128: Batch 102/116 loss: 0.0478\n",
      "Epoch 128: Batch 103/116 loss: 0.0411\n",
      "Epoch 128: Batch 104/116 loss: 0.0318\n",
      "Epoch 128: Batch 105/116 loss: 0.0485\n",
      "Epoch 128: Batch 106/116 loss: 0.0332\n",
      "Epoch 128: Batch 107/116 loss: 0.0510\n",
      "Epoch 128: Batch 108/116 loss: 0.0391\n",
      "Epoch 128: Batch 109/116 loss: 0.0371\n",
      "Epoch 128: Batch 110/116 loss: 0.0440\n",
      "Epoch 128: Batch 111/116 loss: 0.0445\n",
      "Epoch 128: Batch 112/116 loss: 0.0527\n",
      "Epoch 128: Batch 113/116 loss: 0.0502\n",
      "Epoch 128: Batch 114/116 loss: 0.0421\n",
      "Epoch 128: Batch 115/116 loss: 0.0500\n",
      "Epoch 128: Batch 116/116 loss: 0.0737\n",
      "Epoch 128 train loss: 0.0482 valid loss: 0.0534\n",
      "performance reducing: 3\n",
      "Epoch 129: Batch 1/116 loss: 0.0361\n",
      "Epoch 129: Batch 2/116 loss: 0.0553\n",
      "Epoch 129: Batch 3/116 loss: 0.0490\n",
      "Epoch 129: Batch 4/116 loss: 0.0689\n",
      "Epoch 129: Batch 5/116 loss: 0.0748\n",
      "Epoch 129: Batch 6/116 loss: 0.0492\n",
      "Epoch 129: Batch 7/116 loss: 0.0413\n",
      "Epoch 129: Batch 8/116 loss: 0.0438\n",
      "Epoch 129: Batch 9/116 loss: 0.0467\n",
      "Epoch 129: Batch 10/116 loss: 0.0414\n",
      "Epoch 129: Batch 11/116 loss: 0.0475\n",
      "Epoch 129: Batch 12/116 loss: 0.0472\n",
      "Epoch 129: Batch 13/116 loss: 0.0623\n",
      "Epoch 129: Batch 14/116 loss: 0.0361\n",
      "Epoch 129: Batch 15/116 loss: 0.0501\n",
      "Epoch 129: Batch 16/116 loss: 0.0527\n",
      "Epoch 129: Batch 17/116 loss: 0.0556\n",
      "Epoch 129: Batch 18/116 loss: 0.0521\n",
      "Epoch 129: Batch 19/116 loss: 0.0377\n",
      "Epoch 129: Batch 20/116 loss: 0.0360\n",
      "Epoch 129: Batch 21/116 loss: 0.0363\n",
      "Epoch 129: Batch 22/116 loss: 0.0582\n",
      "Epoch 129: Batch 23/116 loss: 0.0380\n",
      "Epoch 129: Batch 24/116 loss: 0.0267\n",
      "Epoch 129: Batch 25/116 loss: 0.0480\n",
      "Epoch 129: Batch 26/116 loss: 0.0368\n",
      "Epoch 129: Batch 27/116 loss: 0.0448\n",
      "Epoch 129: Batch 28/116 loss: 0.0576\n",
      "Epoch 129: Batch 29/116 loss: 0.0400\n",
      "Epoch 129: Batch 30/116 loss: 0.0346\n",
      "Epoch 129: Batch 31/116 loss: 0.0599\n",
      "Epoch 129: Batch 32/116 loss: 0.0561\n",
      "Epoch 129: Batch 33/116 loss: 0.0475\n",
      "Epoch 129: Batch 34/116 loss: 0.0373\n",
      "Epoch 129: Batch 35/116 loss: 0.0514\n",
      "Epoch 129: Batch 36/116 loss: 0.0615\n",
      "Epoch 129: Batch 37/116 loss: 0.0410\n",
      "Epoch 129: Batch 38/116 loss: 0.0421\n",
      "Epoch 129: Batch 39/116 loss: 0.0603\n",
      "Epoch 129: Batch 40/116 loss: 0.0406\n",
      "Epoch 129: Batch 41/116 loss: 0.0363\n",
      "Epoch 129: Batch 42/116 loss: 0.0857\n",
      "Epoch 129: Batch 43/116 loss: 0.0569\n",
      "Epoch 129: Batch 44/116 loss: 0.0507\n",
      "Epoch 129: Batch 45/116 loss: 0.0335\n",
      "Epoch 129: Batch 46/116 loss: 0.0491\n",
      "Epoch 129: Batch 47/116 loss: 0.0433\n",
      "Epoch 129: Batch 48/116 loss: 0.0358\n",
      "Epoch 129: Batch 49/116 loss: 0.0466\n",
      "Epoch 129: Batch 50/116 loss: 0.0568\n",
      "Epoch 129: Batch 51/116 loss: 0.0503\n",
      "Epoch 129: Batch 52/116 loss: 0.0445\n",
      "Epoch 129: Batch 53/116 loss: 0.0560\n",
      "Epoch 129: Batch 54/116 loss: 0.0436\n",
      "Epoch 129: Batch 55/116 loss: 0.0576\n",
      "Epoch 129: Batch 56/116 loss: 0.0383\n",
      "Epoch 129: Batch 57/116 loss: 0.0388\n",
      "Epoch 129: Batch 58/116 loss: 0.0691\n",
      "Epoch 129: Batch 59/116 loss: 0.0391\n",
      "Epoch 129: Batch 60/116 loss: 0.0659\n",
      "Epoch 129: Batch 61/116 loss: 0.0481\n",
      "Epoch 129: Batch 62/116 loss: 0.0461\n",
      "Epoch 129: Batch 63/116 loss: 0.0558\n",
      "Epoch 129: Batch 64/116 loss: 0.0479\n",
      "Epoch 129: Batch 65/116 loss: 0.0407\n",
      "Epoch 129: Batch 66/116 loss: 0.0338\n",
      "Epoch 129: Batch 67/116 loss: 0.0398\n",
      "Epoch 129: Batch 68/116 loss: 0.0471\n",
      "Epoch 129: Batch 69/116 loss: 0.0361\n",
      "Epoch 129: Batch 70/116 loss: 0.0501\n",
      "Epoch 129: Batch 71/116 loss: 0.0734\n",
      "Epoch 129: Batch 72/116 loss: 0.0508\n",
      "Epoch 129: Batch 73/116 loss: 0.0570\n",
      "Epoch 129: Batch 74/116 loss: 0.0431\n",
      "Epoch 129: Batch 75/116 loss: 0.0417\n",
      "Epoch 129: Batch 76/116 loss: 0.0431\n",
      "Epoch 129: Batch 77/116 loss: 0.0473\n",
      "Epoch 129: Batch 78/116 loss: 0.0452\n",
      "Epoch 129: Batch 79/116 loss: 0.0468\n",
      "Epoch 129: Batch 80/116 loss: 0.0469\n",
      "Epoch 129: Batch 81/116 loss: 0.0729\n",
      "Epoch 129: Batch 82/116 loss: 0.0549\n",
      "Epoch 129: Batch 83/116 loss: 0.0523\n",
      "Epoch 129: Batch 84/116 loss: 0.0590\n",
      "Epoch 129: Batch 85/116 loss: 0.0369\n",
      "Epoch 129: Batch 86/116 loss: 0.0515\n",
      "Epoch 129: Batch 87/116 loss: 0.0540\n",
      "Epoch 129: Batch 88/116 loss: 0.0316\n",
      "Epoch 129: Batch 89/116 loss: 0.0413\n",
      "Epoch 129: Batch 90/116 loss: 0.0571\n",
      "Epoch 129: Batch 91/116 loss: 0.0368\n",
      "Epoch 129: Batch 92/116 loss: 0.0532\n",
      "Epoch 129: Batch 93/116 loss: 0.0360\n",
      "Epoch 129: Batch 94/116 loss: 0.0389\n",
      "Epoch 129: Batch 95/116 loss: 0.0538\n",
      "Epoch 129: Batch 96/116 loss: 0.0335\n",
      "Epoch 129: Batch 97/116 loss: 0.0398\n",
      "Epoch 129: Batch 98/116 loss: 0.0442\n",
      "Epoch 129: Batch 99/116 loss: 0.0344\n",
      "Epoch 129: Batch 100/116 loss: 0.0459\n",
      "Epoch 129: Batch 101/116 loss: 0.0482\n",
      "Epoch 129: Batch 102/116 loss: 0.0750\n",
      "Epoch 129: Batch 103/116 loss: 0.0470\n",
      "Epoch 129: Batch 104/116 loss: 0.0396\n",
      "Epoch 129: Batch 105/116 loss: 0.0273\n",
      "Epoch 129: Batch 106/116 loss: 0.0403\n",
      "Epoch 129: Batch 107/116 loss: 0.0371\n",
      "Epoch 129: Batch 108/116 loss: 0.0328\n",
      "Epoch 129: Batch 109/116 loss: 0.0351\n",
      "Epoch 129: Batch 110/116 loss: 0.0456\n",
      "Epoch 129: Batch 111/116 loss: 0.0341\n",
      "Epoch 129: Batch 112/116 loss: 0.0268\n",
      "Epoch 129: Batch 113/116 loss: 0.0373\n",
      "Epoch 129: Batch 114/116 loss: 0.0438\n",
      "Epoch 129: Batch 115/116 loss: 0.0614\n",
      "Epoch 129: Batch 116/116 loss: 0.0424\n",
      "Epoch 129 train loss: 0.0469 valid loss: 0.0520\n",
      "performance reducing: 4\n",
      "Epoch 130: Batch 1/116 loss: 0.0505\n",
      "Epoch 130: Batch 2/116 loss: 0.0368\n",
      "Epoch 130: Batch 3/116 loss: 0.0379\n",
      "Epoch 130: Batch 4/116 loss: 0.0699\n",
      "Epoch 130: Batch 5/116 loss: 0.0493\n",
      "Epoch 130: Batch 6/116 loss: 0.0457\n",
      "Epoch 130: Batch 7/116 loss: 0.0484\n",
      "Epoch 130: Batch 8/116 loss: 0.0539\n",
      "Epoch 130: Batch 9/116 loss: 0.0376\n",
      "Epoch 130: Batch 10/116 loss: 0.0489\n",
      "Epoch 130: Batch 11/116 loss: 0.0484\n",
      "Epoch 130: Batch 12/116 loss: 0.0533\n",
      "Epoch 130: Batch 13/116 loss: 0.0546\n",
      "Epoch 130: Batch 14/116 loss: 0.0626\n",
      "Epoch 130: Batch 15/116 loss: 0.0500\n",
      "Epoch 130: Batch 16/116 loss: 0.0454\n",
      "Epoch 130: Batch 17/116 loss: 0.0551\n",
      "Epoch 130: Batch 18/116 loss: 0.0592\n",
      "Epoch 130: Batch 19/116 loss: 0.0537\n",
      "Epoch 130: Batch 20/116 loss: 0.0519\n",
      "Epoch 130: Batch 21/116 loss: 0.0580\n",
      "Epoch 130: Batch 22/116 loss: 0.0457\n",
      "Epoch 130: Batch 23/116 loss: 0.0445\n",
      "Epoch 130: Batch 24/116 loss: 0.0370\n",
      "Epoch 130: Batch 25/116 loss: 0.0496\n",
      "Epoch 130: Batch 26/116 loss: 0.0484\n",
      "Epoch 130: Batch 27/116 loss: 0.0424\n",
      "Epoch 130: Batch 28/116 loss: 0.0462\n",
      "Epoch 130: Batch 29/116 loss: 0.0561\n",
      "Epoch 130: Batch 30/116 loss: 0.0350\n",
      "Epoch 130: Batch 31/116 loss: 0.0288\n",
      "Epoch 130: Batch 32/116 loss: 0.0243\n",
      "Epoch 130: Batch 33/116 loss: 0.0401\n",
      "Epoch 130: Batch 34/116 loss: 0.0506\n",
      "Epoch 130: Batch 35/116 loss: 0.0539\n",
      "Epoch 130: Batch 36/116 loss: 0.0460\n",
      "Epoch 130: Batch 37/116 loss: 0.0422\n",
      "Epoch 130: Batch 38/116 loss: 0.0518\n",
      "Epoch 130: Batch 39/116 loss: 0.0455\n",
      "Epoch 130: Batch 40/116 loss: 0.0352\n",
      "Epoch 130: Batch 41/116 loss: 0.0577\n",
      "Epoch 130: Batch 42/116 loss: 0.0495\n",
      "Epoch 130: Batch 43/116 loss: 0.0348\n",
      "Epoch 130: Batch 44/116 loss: 0.0421\n",
      "Epoch 130: Batch 45/116 loss: 0.0382\n",
      "Epoch 130: Batch 46/116 loss: 0.0675\n",
      "Epoch 130: Batch 47/116 loss: 0.0501\n",
      "Epoch 130: Batch 48/116 loss: 0.0351\n",
      "Epoch 130: Batch 49/116 loss: 0.0725\n",
      "Epoch 130: Batch 50/116 loss: 0.0473\n",
      "Epoch 130: Batch 51/116 loss: 0.0561\n",
      "Epoch 130: Batch 52/116 loss: 0.0531\n",
      "Epoch 130: Batch 53/116 loss: 0.0611\n",
      "Epoch 130: Batch 54/116 loss: 0.0418\n",
      "Epoch 130: Batch 55/116 loss: 0.0437\n",
      "Epoch 130: Batch 56/116 loss: 0.0727\n",
      "Epoch 130: Batch 57/116 loss: 0.0478\n",
      "Epoch 130: Batch 58/116 loss: 0.0793\n",
      "Epoch 130: Batch 59/116 loss: 0.0314\n",
      "Epoch 130: Batch 60/116 loss: 0.0534\n",
      "Epoch 130: Batch 61/116 loss: 0.0417\n",
      "Epoch 130: Batch 62/116 loss: 0.0466\n",
      "Epoch 130: Batch 63/116 loss: 0.0550\n",
      "Epoch 130: Batch 64/116 loss: 0.0260\n",
      "Epoch 130: Batch 65/116 loss: 0.0506\n",
      "Epoch 130: Batch 66/116 loss: 0.0485\n",
      "Epoch 130: Batch 67/116 loss: 0.0329\n",
      "Epoch 130: Batch 68/116 loss: 0.0330\n",
      "Epoch 130: Batch 69/116 loss: 0.0436\n",
      "Epoch 130: Batch 70/116 loss: 0.0398\n",
      "Epoch 130: Batch 71/116 loss: 0.0573\n",
      "Epoch 130: Batch 72/116 loss: 0.0386\n",
      "Epoch 130: Batch 73/116 loss: 0.0444\n",
      "Epoch 130: Batch 74/116 loss: 0.0457\n",
      "Epoch 130: Batch 75/116 loss: 0.0379\n",
      "Epoch 130: Batch 76/116 loss: 0.0486\n",
      "Epoch 130: Batch 77/116 loss: 0.0362\n",
      "Epoch 130: Batch 78/116 loss: 0.0419\n",
      "Epoch 130: Batch 79/116 loss: 0.0518\n",
      "Epoch 130: Batch 80/116 loss: 0.0462\n",
      "Epoch 130: Batch 81/116 loss: 0.0460\n",
      "Epoch 130: Batch 82/116 loss: 0.0424\n",
      "Epoch 130: Batch 83/116 loss: 0.0294\n",
      "Epoch 130: Batch 84/116 loss: 0.0545\n",
      "Epoch 130: Batch 85/116 loss: 0.0486\n",
      "Epoch 130: Batch 86/116 loss: 0.0601\n",
      "Epoch 130: Batch 87/116 loss: 0.0385\n",
      "Epoch 130: Batch 88/116 loss: 0.0443\n",
      "Epoch 130: Batch 89/116 loss: 0.0581\n",
      "Epoch 130: Batch 90/116 loss: 0.0322\n",
      "Epoch 130: Batch 91/116 loss: 0.0409\n",
      "Epoch 130: Batch 92/116 loss: 0.0552\n",
      "Epoch 130: Batch 93/116 loss: 0.0429\n",
      "Epoch 130: Batch 94/116 loss: 0.0409\n",
      "Epoch 130: Batch 95/116 loss: 0.0324\n",
      "Epoch 130: Batch 96/116 loss: 0.0609\n",
      "Epoch 130: Batch 97/116 loss: 0.0610\n",
      "Epoch 130: Batch 98/116 loss: 0.0388\n",
      "Epoch 130: Batch 99/116 loss: 0.0524\n",
      "Epoch 130: Batch 100/116 loss: 0.0744\n",
      "Epoch 130: Batch 101/116 loss: 0.0522\n",
      "Epoch 130: Batch 102/116 loss: 0.0396\n",
      "Epoch 130: Batch 103/116 loss: 0.0526\n",
      "Epoch 130: Batch 104/116 loss: 0.0669\n",
      "Epoch 130: Batch 105/116 loss: 0.0463\n",
      "Epoch 130: Batch 106/116 loss: 0.0481\n",
      "Epoch 130: Batch 107/116 loss: 0.0510\n",
      "Epoch 130: Batch 108/116 loss: 0.0458\n",
      "Epoch 130: Batch 109/116 loss: 0.0414\n",
      "Epoch 130: Batch 110/116 loss: 0.0392\n",
      "Epoch 130: Batch 111/116 loss: 0.0602\n",
      "Epoch 130: Batch 112/116 loss: 0.0399\n",
      "Epoch 130: Batch 113/116 loss: 0.0564\n",
      "Epoch 130: Batch 114/116 loss: 0.0330\n",
      "Epoch 130: Batch 115/116 loss: 0.0493\n",
      "Epoch 130: Batch 116/116 loss: 0.0490\n",
      "Epoch 130 train loss: 0.0476 valid loss: 0.0613\n",
      "performance reducing: 5\n",
      "Epoch 131: Batch 1/116 loss: 0.0486\n",
      "Epoch 131: Batch 2/116 loss: 0.0803\n",
      "Epoch 131: Batch 3/116 loss: 0.0272\n",
      "Epoch 131: Batch 4/116 loss: 0.0512\n",
      "Epoch 131: Batch 5/116 loss: 0.0224\n",
      "Epoch 131: Batch 6/116 loss: 0.0365\n",
      "Epoch 131: Batch 7/116 loss: 0.0527\n",
      "Epoch 131: Batch 8/116 loss: 0.0441\n",
      "Epoch 131: Batch 9/116 loss: 0.0568\n",
      "Epoch 131: Batch 10/116 loss: 0.0429\n",
      "Epoch 131: Batch 11/116 loss: 0.0340\n",
      "Epoch 131: Batch 12/116 loss: 0.1012\n",
      "Epoch 131: Batch 13/116 loss: 0.0588\n",
      "Epoch 131: Batch 14/116 loss: 0.0612\n",
      "Epoch 131: Batch 15/116 loss: 0.0594\n",
      "Epoch 131: Batch 16/116 loss: 0.0600\n",
      "Epoch 131: Batch 17/116 loss: 0.0577\n",
      "Epoch 131: Batch 18/116 loss: 0.0404\n",
      "Epoch 131: Batch 19/116 loss: 0.0419\n",
      "Epoch 131: Batch 20/116 loss: 0.0573\n",
      "Epoch 131: Batch 21/116 loss: 0.0371\n",
      "Epoch 131: Batch 22/116 loss: 0.0636\n",
      "Epoch 131: Batch 23/116 loss: 0.0450\n",
      "Epoch 131: Batch 24/116 loss: 0.0593\n",
      "Epoch 131: Batch 25/116 loss: 0.0416\n",
      "Epoch 131: Batch 26/116 loss: 0.0458\n",
      "Epoch 131: Batch 27/116 loss: 0.0542\n",
      "Epoch 131: Batch 28/116 loss: 0.0482\n",
      "Epoch 131: Batch 29/116 loss: 0.0468\n",
      "Epoch 131: Batch 30/116 loss: 0.0406\n",
      "Epoch 131: Batch 31/116 loss: 0.0509\n",
      "Epoch 131: Batch 32/116 loss: 0.0427\n",
      "Epoch 131: Batch 33/116 loss: 0.0402\n",
      "Epoch 131: Batch 34/116 loss: 0.0374\n",
      "Epoch 131: Batch 35/116 loss: 0.0445\n",
      "Epoch 131: Batch 36/116 loss: 0.0374\n",
      "Epoch 131: Batch 37/116 loss: 0.0481\n",
      "Epoch 131: Batch 38/116 loss: 0.0902\n",
      "Epoch 131: Batch 39/116 loss: 0.0724\n",
      "Epoch 131: Batch 40/116 loss: 0.0880\n",
      "Epoch 131: Batch 41/116 loss: 0.0574\n",
      "Epoch 131: Batch 42/116 loss: 0.0935\n",
      "Epoch 131: Batch 43/116 loss: 0.0589\n",
      "Epoch 131: Batch 44/116 loss: 0.0584\n",
      "Epoch 131: Batch 45/116 loss: 0.0414\n",
      "Epoch 131: Batch 46/116 loss: 0.0483\n",
      "Epoch 131: Batch 47/116 loss: 0.0511\n",
      "Epoch 131: Batch 48/116 loss: 0.0430\n",
      "Epoch 131: Batch 49/116 loss: 0.0356\n",
      "Epoch 131: Batch 50/116 loss: 0.0449\n",
      "Epoch 131: Batch 51/116 loss: 0.0355\n",
      "Epoch 131: Batch 52/116 loss: 0.0502\n",
      "Epoch 131: Batch 53/116 loss: 0.0464\n",
      "Epoch 131: Batch 54/116 loss: 0.0605\n",
      "Epoch 131: Batch 55/116 loss: 0.0511\n",
      "Epoch 131: Batch 56/116 loss: 0.0331\n",
      "Epoch 131: Batch 57/116 loss: 0.0510\n",
      "Epoch 131: Batch 58/116 loss: 0.0492\n",
      "Epoch 131: Batch 59/116 loss: 0.0459\n",
      "Epoch 131: Batch 60/116 loss: 0.0261\n",
      "Epoch 131: Batch 61/116 loss: 0.0245\n",
      "Epoch 131: Batch 62/116 loss: 0.0596\n",
      "Epoch 131: Batch 63/116 loss: 0.0526\n",
      "Epoch 131: Batch 64/116 loss: 0.0439\n",
      "Epoch 131: Batch 65/116 loss: 0.0465\n",
      "Epoch 131: Batch 66/116 loss: 0.0560\n",
      "Epoch 131: Batch 67/116 loss: 0.0686\n",
      "Epoch 131: Batch 68/116 loss: 0.0447\n",
      "Epoch 131: Batch 69/116 loss: 0.0491\n",
      "Epoch 131: Batch 70/116 loss: 0.0442\n",
      "Epoch 131: Batch 71/116 loss: 0.0477\n",
      "Epoch 131: Batch 72/116 loss: 0.0374\n",
      "Epoch 131: Batch 73/116 loss: 0.0498\n",
      "Epoch 131: Batch 74/116 loss: 0.0452\n",
      "Epoch 131: Batch 75/116 loss: 0.0575\n",
      "Epoch 131: Batch 76/116 loss: 0.0605\n",
      "Epoch 131: Batch 77/116 loss: 0.0365\n",
      "Epoch 131: Batch 78/116 loss: 0.0364\n",
      "Epoch 131: Batch 79/116 loss: 0.0458\n",
      "Epoch 131: Batch 80/116 loss: 0.0495\n",
      "Epoch 131: Batch 81/116 loss: 0.0605\n",
      "Epoch 131: Batch 82/116 loss: 0.0500\n",
      "Epoch 131: Batch 83/116 loss: 0.0473\n",
      "Epoch 131: Batch 84/116 loss: 0.0532\n",
      "Epoch 131: Batch 85/116 loss: 0.0500\n",
      "Epoch 131: Batch 86/116 loss: 0.0505\n",
      "Epoch 131: Batch 87/116 loss: 0.0406\n",
      "Epoch 131: Batch 88/116 loss: 0.0479\n",
      "Epoch 131: Batch 89/116 loss: 0.0392\n",
      "Epoch 131: Batch 90/116 loss: 0.0563\n",
      "Epoch 131: Batch 91/116 loss: 0.0461\n",
      "Epoch 131: Batch 92/116 loss: 0.0432\n",
      "Epoch 131: Batch 93/116 loss: 0.0486\n",
      "Epoch 131: Batch 94/116 loss: 0.0395\n",
      "Epoch 131: Batch 95/116 loss: 0.0283\n",
      "Epoch 131: Batch 96/116 loss: 0.0626\n",
      "Epoch 131: Batch 97/116 loss: 0.0278\n",
      "Epoch 131: Batch 98/116 loss: 0.0323\n",
      "Epoch 131: Batch 99/116 loss: 0.0251\n",
      "Epoch 131: Batch 100/116 loss: 0.0394\n",
      "Epoch 131: Batch 101/116 loss: 0.0552\n",
      "Epoch 131: Batch 102/116 loss: 0.0400\n",
      "Epoch 131: Batch 103/116 loss: 0.0283\n",
      "Epoch 131: Batch 104/116 loss: 0.0517\n",
      "Epoch 131: Batch 105/116 loss: 0.0279\n",
      "Epoch 131: Batch 106/116 loss: 0.0375\n",
      "Epoch 131: Batch 107/116 loss: 0.0445\n",
      "Epoch 131: Batch 108/116 loss: 0.0587\n",
      "Epoch 131: Batch 109/116 loss: 0.0602\n",
      "Epoch 131: Batch 110/116 loss: 0.0437\n",
      "Epoch 131: Batch 111/116 loss: 0.0322\n",
      "Epoch 131: Batch 112/116 loss: 0.0549\n",
      "Epoch 131: Batch 113/116 loss: 0.0454\n",
      "Epoch 131: Batch 114/116 loss: 0.0640\n",
      "Epoch 131: Batch 115/116 loss: 0.0228\n",
      "Epoch 131: Batch 116/116 loss: 0.0575\n",
      "Epoch 131 train loss: 0.0484 valid loss: 0.0544\n",
      "performance reducing: 6\n",
      "Epoch 132: Batch 1/116 loss: 0.0503\n",
      "Epoch 132: Batch 2/116 loss: 0.0531\n",
      "Epoch 132: Batch 3/116 loss: 0.0455\n",
      "Epoch 132: Batch 4/116 loss: 0.0467\n",
      "Epoch 132: Batch 5/116 loss: 0.0527\n",
      "Epoch 132: Batch 6/116 loss: 0.0443\n",
      "Epoch 132: Batch 7/116 loss: 0.0481\n",
      "Epoch 132: Batch 8/116 loss: 0.0531\n",
      "Epoch 132: Batch 9/116 loss: 0.0466\n",
      "Epoch 132: Batch 10/116 loss: 0.0474\n",
      "Epoch 132: Batch 11/116 loss: 0.0317\n",
      "Epoch 132: Batch 12/116 loss: 0.0335\n",
      "Epoch 132: Batch 13/116 loss: 0.0402\n",
      "Epoch 132: Batch 14/116 loss: 0.0464\n",
      "Epoch 132: Batch 15/116 loss: 0.0450\n",
      "Epoch 132: Batch 16/116 loss: 0.0491\n",
      "Epoch 132: Batch 17/116 loss: 0.0483\n",
      "Epoch 132: Batch 18/116 loss: 0.0475\n",
      "Epoch 132: Batch 19/116 loss: 0.0241\n",
      "Epoch 132: Batch 20/116 loss: 0.0459\n",
      "Epoch 132: Batch 21/116 loss: 0.0838\n",
      "Epoch 132: Batch 22/116 loss: 0.0396\n",
      "Epoch 132: Batch 23/116 loss: 0.0554\n",
      "Epoch 132: Batch 24/116 loss: 0.0486\n",
      "Epoch 132: Batch 25/116 loss: 0.0496\n",
      "Epoch 132: Batch 26/116 loss: 0.0349\n",
      "Epoch 132: Batch 27/116 loss: 0.0400\n",
      "Epoch 132: Batch 28/116 loss: 0.0463\n",
      "Epoch 132: Batch 29/116 loss: 0.0335\n",
      "Epoch 132: Batch 30/116 loss: 0.0364\n",
      "Epoch 132: Batch 31/116 loss: 0.0705\n",
      "Epoch 132: Batch 32/116 loss: 0.0705\n",
      "Epoch 132: Batch 33/116 loss: 0.0564\n",
      "Epoch 132: Batch 34/116 loss: 0.0354\n",
      "Epoch 132: Batch 35/116 loss: 0.0412\n",
      "Epoch 132: Batch 36/116 loss: 0.0522\n",
      "Epoch 132: Batch 37/116 loss: 0.0360\n",
      "Epoch 132: Batch 38/116 loss: 0.0457\n",
      "Epoch 132: Batch 39/116 loss: 0.0370\n",
      "Epoch 132: Batch 40/116 loss: 0.0560\n",
      "Epoch 132: Batch 41/116 loss: 0.0534\n",
      "Epoch 132: Batch 42/116 loss: 0.0473\n",
      "Epoch 132: Batch 43/116 loss: 0.0585\n",
      "Epoch 132: Batch 44/116 loss: 0.0572\n",
      "Epoch 132: Batch 45/116 loss: 0.0503\n",
      "Epoch 132: Batch 46/116 loss: 0.0525\n",
      "Epoch 132: Batch 47/116 loss: 0.0477\n",
      "Epoch 132: Batch 48/116 loss: 0.0524\n",
      "Epoch 132: Batch 49/116 loss: 0.0506\n",
      "Epoch 132: Batch 50/116 loss: 0.0668\n",
      "Epoch 132: Batch 51/116 loss: 0.0253\n",
      "Epoch 132: Batch 52/116 loss: 0.0475\n",
      "Epoch 132: Batch 53/116 loss: 0.0301\n",
      "Epoch 132: Batch 54/116 loss: 0.0403\n",
      "Epoch 132: Batch 55/116 loss: 0.0617\n",
      "Epoch 132: Batch 56/116 loss: 0.0480\n",
      "Epoch 132: Batch 57/116 loss: 0.0534\n",
      "Epoch 132: Batch 58/116 loss: 0.0529\n",
      "Epoch 132: Batch 59/116 loss: 0.0420\n",
      "Epoch 132: Batch 60/116 loss: 0.0576\n",
      "Epoch 132: Batch 61/116 loss: 0.0564\n",
      "Epoch 132: Batch 62/116 loss: 0.0605\n",
      "Epoch 132: Batch 63/116 loss: 0.0409\n",
      "Epoch 132: Batch 64/116 loss: 0.0478\n",
      "Epoch 132: Batch 65/116 loss: 0.0365\n",
      "Epoch 132: Batch 66/116 loss: 0.0598\n",
      "Epoch 132: Batch 67/116 loss: 0.0652\n",
      "Epoch 132: Batch 68/116 loss: 0.0206\n",
      "Epoch 132: Batch 69/116 loss: 0.0430\n",
      "Epoch 132: Batch 70/116 loss: 0.0342\n",
      "Epoch 132: Batch 71/116 loss: 0.0381\n",
      "Epoch 132: Batch 72/116 loss: 0.0614\n",
      "Epoch 132: Batch 73/116 loss: 0.0490\n",
      "Epoch 132: Batch 74/116 loss: 0.0527\n",
      "Epoch 132: Batch 75/116 loss: 0.0481\n",
      "Epoch 132: Batch 76/116 loss: 0.0458\n",
      "Epoch 132: Batch 77/116 loss: 0.0354\n",
      "Epoch 132: Batch 78/116 loss: 0.0503\n",
      "Epoch 132: Batch 79/116 loss: 0.0748\n",
      "Epoch 132: Batch 80/116 loss: 0.0355\n",
      "Epoch 132: Batch 81/116 loss: 0.0574\n",
      "Epoch 132: Batch 82/116 loss: 0.0276\n",
      "Epoch 132: Batch 83/116 loss: 0.0587\n",
      "Epoch 132: Batch 84/116 loss: 0.0291\n",
      "Epoch 132: Batch 85/116 loss: 0.0442\n",
      "Epoch 132: Batch 86/116 loss: 0.0249\n",
      "Epoch 132: Batch 87/116 loss: 0.0307\n",
      "Epoch 132: Batch 88/116 loss: 0.0371\n",
      "Epoch 132: Batch 89/116 loss: 0.0498\n",
      "Epoch 132: Batch 90/116 loss: 0.0565\n",
      "Epoch 132: Batch 91/116 loss: 0.0511\n",
      "Epoch 132: Batch 92/116 loss: 0.0525\n",
      "Epoch 132: Batch 93/116 loss: 0.0386\n",
      "Epoch 132: Batch 94/116 loss: 0.0603\n",
      "Epoch 132: Batch 95/116 loss: 0.0391\n",
      "Epoch 132: Batch 96/116 loss: 0.0409\n",
      "Epoch 132: Batch 97/116 loss: 0.0398\n",
      "Epoch 132: Batch 98/116 loss: 0.0503\n",
      "Epoch 132: Batch 99/116 loss: 0.0464\n",
      "Epoch 132: Batch 100/116 loss: 0.0507\n",
      "Epoch 132: Batch 101/116 loss: 0.0500\n",
      "Epoch 132: Batch 102/116 loss: 0.0395\n",
      "Epoch 132: Batch 103/116 loss: 0.0332\n",
      "Epoch 132: Batch 104/116 loss: 0.0641\n",
      "Epoch 132: Batch 105/116 loss: 0.0476\n",
      "Epoch 132: Batch 106/116 loss: 0.0749\n",
      "Epoch 132: Batch 107/116 loss: 0.0384\n",
      "Epoch 132: Batch 108/116 loss: 0.0568\n",
      "Epoch 132: Batch 109/116 loss: 0.0700\n",
      "Epoch 132: Batch 110/116 loss: 0.0489\n",
      "Epoch 132: Batch 111/116 loss: 0.0468\n",
      "Epoch 132: Batch 112/116 loss: 0.0504\n",
      "Epoch 132: Batch 113/116 loss: 0.0623\n",
      "Epoch 132: Batch 114/116 loss: 0.0704\n",
      "Epoch 132: Batch 115/116 loss: 0.0446\n",
      "Epoch 132: Batch 116/116 loss: 0.0616\n",
      "Epoch 132 train loss: 0.0480 valid loss: 0.0534\n",
      "performance reducing: 7\n",
      "Epoch 133: Batch 1/116 loss: 0.0405\n",
      "Epoch 133: Batch 2/116 loss: 0.0574\n",
      "Epoch 133: Batch 3/116 loss: 0.0487\n",
      "Epoch 133: Batch 4/116 loss: 0.0533\n",
      "Epoch 133: Batch 5/116 loss: 0.0329\n",
      "Epoch 133: Batch 6/116 loss: 0.0438\n",
      "Epoch 133: Batch 7/116 loss: 0.0512\n",
      "Epoch 133: Batch 8/116 loss: 0.0399\n",
      "Epoch 133: Batch 9/116 loss: 0.0523\n",
      "Epoch 133: Batch 10/116 loss: 0.0646\n",
      "Epoch 133: Batch 11/116 loss: 0.0420\n",
      "Epoch 133: Batch 12/116 loss: 0.0613\n",
      "Epoch 133: Batch 13/116 loss: 0.0456\n",
      "Epoch 133: Batch 14/116 loss: 0.0289\n",
      "Epoch 133: Batch 15/116 loss: 0.0709\n",
      "Epoch 133: Batch 16/116 loss: 0.0662\n",
      "Epoch 133: Batch 17/116 loss: 0.0349\n",
      "Epoch 133: Batch 18/116 loss: 0.0494\n",
      "Epoch 133: Batch 19/116 loss: 0.0508\n",
      "Epoch 133: Batch 20/116 loss: 0.0460\n",
      "Epoch 133: Batch 21/116 loss: 0.0488\n",
      "Epoch 133: Batch 22/116 loss: 0.0442\n",
      "Epoch 133: Batch 23/116 loss: 0.0303\n",
      "Epoch 133: Batch 24/116 loss: 0.0377\n",
      "Epoch 133: Batch 25/116 loss: 0.0373\n",
      "Epoch 133: Batch 26/116 loss: 0.0495\n",
      "Epoch 133: Batch 27/116 loss: 0.0418\n",
      "Epoch 133: Batch 28/116 loss: 0.0571\n",
      "Epoch 133: Batch 29/116 loss: 0.0346\n",
      "Epoch 133: Batch 30/116 loss: 0.0370\n",
      "Epoch 133: Batch 31/116 loss: 0.0678\n",
      "Epoch 133: Batch 32/116 loss: 0.0375\n",
      "Epoch 133: Batch 33/116 loss: 0.0496\n",
      "Epoch 133: Batch 34/116 loss: 0.0430\n",
      "Epoch 133: Batch 35/116 loss: 0.0430\n",
      "Epoch 133: Batch 36/116 loss: 0.0442\n",
      "Epoch 133: Batch 37/116 loss: 0.0473\n",
      "Epoch 133: Batch 38/116 loss: 0.0330\n",
      "Epoch 133: Batch 39/116 loss: 0.0470\n",
      "Epoch 133: Batch 40/116 loss: 0.0540\n",
      "Epoch 133: Batch 41/116 loss: 0.0540\n",
      "Epoch 133: Batch 42/116 loss: 0.0479\n",
      "Epoch 133: Batch 43/116 loss: 0.0511\n",
      "Epoch 133: Batch 44/116 loss: 0.0423\n",
      "Epoch 133: Batch 45/116 loss: 0.0502\n",
      "Epoch 133: Batch 46/116 loss: 0.0494\n",
      "Epoch 133: Batch 47/116 loss: 0.0470\n",
      "Epoch 133: Batch 48/116 loss: 0.0359\n",
      "Epoch 133: Batch 49/116 loss: 0.0498\n",
      "Epoch 133: Batch 50/116 loss: 0.0585\n",
      "Epoch 133: Batch 51/116 loss: 0.0362\n",
      "Epoch 133: Batch 52/116 loss: 0.0680\n",
      "Epoch 133: Batch 53/116 loss: 0.0281\n",
      "Epoch 133: Batch 54/116 loss: 0.0500\n",
      "Epoch 133: Batch 55/116 loss: 0.0542\n",
      "Epoch 133: Batch 56/116 loss: 0.0490\n",
      "Epoch 133: Batch 57/116 loss: 0.0526\n",
      "Epoch 133: Batch 58/116 loss: 0.0518\n",
      "Epoch 133: Batch 59/116 loss: 0.0620\n",
      "Epoch 133: Batch 60/116 loss: 0.0466\n",
      "Epoch 133: Batch 61/116 loss: 0.0453\n",
      "Epoch 133: Batch 62/116 loss: 0.0464\n",
      "Epoch 133: Batch 63/116 loss: 0.0399\n",
      "Epoch 133: Batch 64/116 loss: 0.0446\n",
      "Epoch 133: Batch 65/116 loss: 0.0260\n",
      "Epoch 133: Batch 66/116 loss: 0.0458\n",
      "Epoch 133: Batch 67/116 loss: 0.0375\n",
      "Epoch 133: Batch 68/116 loss: 0.0432\n",
      "Epoch 133: Batch 69/116 loss: 0.0343\n",
      "Epoch 133: Batch 70/116 loss: 0.0550\n",
      "Epoch 133: Batch 71/116 loss: 0.0308\n",
      "Epoch 133: Batch 72/116 loss: 0.1018\n",
      "Epoch 133: Batch 73/116 loss: 0.0643\n",
      "Epoch 133: Batch 74/116 loss: 0.0384\n",
      "Epoch 133: Batch 75/116 loss: 0.0532\n",
      "Epoch 133: Batch 76/116 loss: 0.0378\n",
      "Epoch 133: Batch 77/116 loss: 0.0535\n",
      "Epoch 133: Batch 78/116 loss: 0.0506\n",
      "Epoch 133: Batch 79/116 loss: 0.0642\n",
      "Epoch 133: Batch 80/116 loss: 0.0350\n",
      "Epoch 133: Batch 81/116 loss: 0.0528\n",
      "Epoch 133: Batch 82/116 loss: 0.0401\n",
      "Epoch 133: Batch 83/116 loss: 0.0583\n",
      "Epoch 133: Batch 84/116 loss: 0.0467\n",
      "Epoch 133: Batch 85/116 loss: 0.0366\n",
      "Epoch 133: Batch 86/116 loss: 0.0473\n",
      "Epoch 133: Batch 87/116 loss: 0.0319\n",
      "Epoch 133: Batch 88/116 loss: 0.0468\n",
      "Epoch 133: Batch 89/116 loss: 0.0487\n",
      "Epoch 133: Batch 90/116 loss: 0.0502\n",
      "Epoch 133: Batch 91/116 loss: 0.0522\n",
      "Epoch 133: Batch 92/116 loss: 0.0299\n",
      "Epoch 133: Batch 93/116 loss: 0.0361\n",
      "Epoch 133: Batch 94/116 loss: 0.0612\n",
      "Epoch 133: Batch 95/116 loss: 0.0441\n",
      "Epoch 133: Batch 96/116 loss: 0.0387\n",
      "Epoch 133: Batch 97/116 loss: 0.0381\n",
      "Epoch 133: Batch 98/116 loss: 0.0437\n",
      "Epoch 133: Batch 99/116 loss: 0.0373\n",
      "Epoch 133: Batch 100/116 loss: 0.0481\n",
      "Epoch 133: Batch 101/116 loss: 0.0462\n",
      "Epoch 133: Batch 102/116 loss: 0.0375\n",
      "Epoch 133: Batch 103/116 loss: 0.0622\n",
      "Epoch 133: Batch 104/116 loss: 0.0429\n",
      "Epoch 133: Batch 105/116 loss: 0.0334\n",
      "Epoch 133: Batch 106/116 loss: 0.0573\n",
      "Epoch 133: Batch 107/116 loss: 0.0516\n",
      "Epoch 133: Batch 108/116 loss: 0.0458\n",
      "Epoch 133: Batch 109/116 loss: 0.0559\n",
      "Epoch 133: Batch 110/116 loss: 0.0545\n",
      "Epoch 133: Batch 111/116 loss: 0.0552\n",
      "Epoch 133: Batch 112/116 loss: 0.0504\n",
      "Epoch 133: Batch 113/116 loss: 0.0582\n",
      "Epoch 133: Batch 114/116 loss: 0.0534\n",
      "Epoch 133: Batch 115/116 loss: 0.0461\n",
      "Epoch 133: Batch 116/116 loss: 0.0633\n",
      "Epoch 133 train loss: 0.0474 valid loss: 0.0573\n",
      "performance reducing: 8\n",
      "Epoch 134: Batch 1/116 loss: 0.0515\n",
      "Epoch 134: Batch 2/116 loss: 0.0472\n",
      "Epoch 134: Batch 3/116 loss: 0.0710\n",
      "Epoch 134: Batch 4/116 loss: 0.0429\n",
      "Epoch 134: Batch 5/116 loss: 0.0686\n",
      "Epoch 134: Batch 6/116 loss: 0.0367\n",
      "Epoch 134: Batch 7/116 loss: 0.0429\n",
      "Epoch 134: Batch 8/116 loss: 0.0535\n",
      "Epoch 134: Batch 9/116 loss: 0.0636\n",
      "Epoch 134: Batch 10/116 loss: 0.0383\n",
      "Epoch 134: Batch 11/116 loss: 0.0173\n",
      "Epoch 134: Batch 12/116 loss: 0.0489\n",
      "Epoch 134: Batch 13/116 loss: 0.0535\n",
      "Epoch 134: Batch 14/116 loss: 0.0547\n",
      "Epoch 134: Batch 15/116 loss: 0.0390\n",
      "Epoch 134: Batch 16/116 loss: 0.0564\n",
      "Epoch 134: Batch 17/116 loss: 0.0552\n",
      "Epoch 134: Batch 18/116 loss: 0.0480\n",
      "Epoch 134: Batch 19/116 loss: 0.0635\n",
      "Epoch 134: Batch 20/116 loss: 0.0499\n",
      "Epoch 134: Batch 21/116 loss: 0.0500\n",
      "Epoch 134: Batch 22/116 loss: 0.0354\n",
      "Epoch 134: Batch 23/116 loss: 0.0474\n",
      "Epoch 134: Batch 24/116 loss: 0.0626\n",
      "Epoch 134: Batch 25/116 loss: 0.0312\n",
      "Epoch 134: Batch 26/116 loss: 0.0563\n",
      "Epoch 134: Batch 27/116 loss: 0.0364\n",
      "Epoch 134: Batch 28/116 loss: 0.0579\n",
      "Epoch 134: Batch 29/116 loss: 0.0481\n",
      "Epoch 134: Batch 30/116 loss: 0.0492\n",
      "Epoch 134: Batch 31/116 loss: 0.0434\n",
      "Epoch 134: Batch 32/116 loss: 0.0657\n",
      "Epoch 134: Batch 33/116 loss: 0.0332\n",
      "Epoch 134: Batch 34/116 loss: 0.0334\n",
      "Epoch 134: Batch 35/116 loss: 0.0422\n",
      "Epoch 134: Batch 36/116 loss: 0.0727\n",
      "Epoch 134: Batch 37/116 loss: 0.0449\n",
      "Epoch 134: Batch 38/116 loss: 0.0554\n",
      "Epoch 134: Batch 39/116 loss: 0.0497\n",
      "Epoch 134: Batch 40/116 loss: 0.0381\n",
      "Epoch 134: Batch 41/116 loss: 0.0561\n",
      "Epoch 134: Batch 42/116 loss: 0.0393\n",
      "Epoch 134: Batch 43/116 loss: 0.0522\n",
      "Epoch 134: Batch 44/116 loss: 0.0404\n",
      "Epoch 134: Batch 45/116 loss: 0.0507\n",
      "Epoch 134: Batch 46/116 loss: 0.0529\n",
      "Epoch 134: Batch 47/116 loss: 0.0298\n",
      "Epoch 134: Batch 48/116 loss: 0.0300\n",
      "Epoch 134: Batch 49/116 loss: 0.0458\n",
      "Epoch 134: Batch 50/116 loss: 0.0363\n",
      "Epoch 134: Batch 51/116 loss: 0.0695\n",
      "Epoch 134: Batch 52/116 loss: 0.0446\n",
      "Epoch 134: Batch 53/116 loss: 0.0383\n",
      "Epoch 134: Batch 54/116 loss: 0.0283\n",
      "Epoch 134: Batch 55/116 loss: 0.0476\n",
      "Epoch 134: Batch 56/116 loss: 0.0541\n",
      "Epoch 134: Batch 57/116 loss: 0.0287\n",
      "Epoch 134: Batch 58/116 loss: 0.0557\n",
      "Epoch 134: Batch 59/116 loss: 0.0412\n",
      "Epoch 134: Batch 60/116 loss: 0.0579\n",
      "Epoch 134: Batch 61/116 loss: 0.0454\n",
      "Epoch 134: Batch 62/116 loss: 0.0444\n",
      "Epoch 134: Batch 63/116 loss: 0.0539\n",
      "Epoch 134: Batch 64/116 loss: 0.0403\n",
      "Epoch 134: Batch 65/116 loss: 0.0556\n",
      "Epoch 134: Batch 66/116 loss: 0.0631\n",
      "Epoch 134: Batch 67/116 loss: 0.0461\n",
      "Epoch 134: Batch 68/116 loss: 0.0311\n",
      "Epoch 134: Batch 69/116 loss: 0.0563\n",
      "Epoch 134: Batch 70/116 loss: 0.1118\n",
      "Epoch 134: Batch 71/116 loss: 0.0515\n",
      "Epoch 134: Batch 72/116 loss: 0.0381\n",
      "Epoch 134: Batch 73/116 loss: 0.0505\n",
      "Epoch 134: Batch 74/116 loss: 0.0493\n",
      "Epoch 134: Batch 75/116 loss: 0.0502\n",
      "Epoch 134: Batch 76/116 loss: 0.0544\n",
      "Epoch 134: Batch 77/116 loss: 0.0566\n",
      "Epoch 134: Batch 78/116 loss: 0.0545\n",
      "Epoch 134: Batch 79/116 loss: 0.0610\n",
      "Epoch 134: Batch 80/116 loss: 0.0415\n",
      "Epoch 134: Batch 81/116 loss: 0.0439\n",
      "Epoch 134: Batch 82/116 loss: 0.0439\n",
      "Epoch 134: Batch 83/116 loss: 0.0406\n",
      "Epoch 134: Batch 84/116 loss: 0.0529\n",
      "Epoch 134: Batch 85/116 loss: 0.0280\n",
      "Epoch 134: Batch 86/116 loss: 0.0443\n",
      "Epoch 134: Batch 87/116 loss: 0.0402\n",
      "Epoch 134: Batch 88/116 loss: 0.0396\n",
      "Epoch 134: Batch 89/116 loss: 0.0468\n",
      "Epoch 134: Batch 90/116 loss: 0.0415\n",
      "Epoch 134: Batch 91/116 loss: 0.0573\n",
      "Epoch 134: Batch 92/116 loss: 0.0326\n",
      "Epoch 134: Batch 93/116 loss: 0.0755\n",
      "Epoch 134: Batch 94/116 loss: 0.0706\n",
      "Epoch 134: Batch 95/116 loss: 0.0593\n",
      "Epoch 134: Batch 96/116 loss: 0.0357\n",
      "Epoch 134: Batch 97/116 loss: 0.0373\n",
      "Epoch 134: Batch 98/116 loss: 0.0671\n",
      "Epoch 134: Batch 99/116 loss: 0.0335\n",
      "Epoch 134: Batch 100/116 loss: 0.0379\n",
      "Epoch 134: Batch 101/116 loss: 0.0440\n",
      "Epoch 134: Batch 102/116 loss: 0.0306\n",
      "Epoch 134: Batch 103/116 loss: 0.0345\n",
      "Epoch 134: Batch 104/116 loss: 0.0511\n",
      "Epoch 134: Batch 105/116 loss: 0.0429\n",
      "Epoch 134: Batch 106/116 loss: 0.0419\n",
      "Epoch 134: Batch 107/116 loss: 0.0318\n",
      "Epoch 134: Batch 108/116 loss: 0.0366\n",
      "Epoch 134: Batch 109/116 loss: 0.0686\n",
      "Epoch 134: Batch 110/116 loss: 0.0445\n",
      "Epoch 134: Batch 111/116 loss: 0.0415\n",
      "Epoch 134: Batch 112/116 loss: 0.0696\n",
      "Epoch 134: Batch 113/116 loss: 0.0681\n",
      "Epoch 134: Batch 114/116 loss: 0.0499\n",
      "Epoch 134: Batch 115/116 loss: 0.0398\n",
      "Epoch 134: Batch 116/116 loss: 0.0433\n",
      "Epoch 134 train loss: 0.0481 valid loss: 0.0581\n",
      "performance reducing: 9\n",
      "Epoch 135: Batch 1/116 loss: 0.0514\n",
      "Epoch 135: Batch 2/116 loss: 0.0485\n",
      "Epoch 135: Batch 3/116 loss: 0.0527\n",
      "Epoch 135: Batch 4/116 loss: 0.0471\n",
      "Epoch 135: Batch 5/116 loss: 0.0501\n",
      "Epoch 135: Batch 6/116 loss: 0.0302\n",
      "Epoch 135: Batch 7/116 loss: 0.0515\n",
      "Epoch 135: Batch 8/116 loss: 0.0467\n",
      "Epoch 135: Batch 9/116 loss: 0.0523\n",
      "Epoch 135: Batch 10/116 loss: 0.0561\n",
      "Epoch 135: Batch 11/116 loss: 0.0297\n",
      "Epoch 135: Batch 12/116 loss: 0.0492\n",
      "Epoch 135: Batch 13/116 loss: 0.0322\n",
      "Epoch 135: Batch 14/116 loss: 0.0422\n",
      "Epoch 135: Batch 15/116 loss: 0.0424\n",
      "Epoch 135: Batch 16/116 loss: 0.0497\n",
      "Epoch 135: Batch 17/116 loss: 0.0423\n",
      "Epoch 135: Batch 18/116 loss: 0.0378\n",
      "Epoch 135: Batch 19/116 loss: 0.0762\n",
      "Epoch 135: Batch 20/116 loss: 0.0438\n",
      "Epoch 135: Batch 21/116 loss: 0.0621\n",
      "Epoch 135: Batch 22/116 loss: 0.0354\n",
      "Epoch 135: Batch 23/116 loss: 0.0521\n",
      "Epoch 135: Batch 24/116 loss: 0.0620\n",
      "Epoch 135: Batch 25/116 loss: 0.0400\n",
      "Epoch 135: Batch 26/116 loss: 0.0391\n",
      "Epoch 135: Batch 27/116 loss: 0.0566\n",
      "Epoch 135: Batch 28/116 loss: 0.0414\n",
      "Epoch 135: Batch 29/116 loss: 0.0536\n",
      "Epoch 135: Batch 30/116 loss: 0.0388\n",
      "Epoch 135: Batch 31/116 loss: 0.0324\n",
      "Epoch 135: Batch 32/116 loss: 0.0505\n",
      "Epoch 135: Batch 33/116 loss: 0.0364\n",
      "Epoch 135: Batch 34/116 loss: 0.0260\n",
      "Epoch 135: Batch 35/116 loss: 0.0323\n",
      "Epoch 135: Batch 36/116 loss: 0.0665\n",
      "Epoch 135: Batch 37/116 loss: 0.0449\n",
      "Epoch 135: Batch 38/116 loss: 0.0429\n",
      "Epoch 135: Batch 39/116 loss: 0.0411\n",
      "Epoch 135: Batch 40/116 loss: 0.0658\n",
      "Epoch 135: Batch 41/116 loss: 0.0433\n",
      "Epoch 135: Batch 42/116 loss: 0.0466\n",
      "Epoch 135: Batch 43/116 loss: 0.0421\n",
      "Epoch 135: Batch 44/116 loss: 0.0603\n",
      "Epoch 135: Batch 45/116 loss: 0.0688\n",
      "Epoch 135: Batch 46/116 loss: 0.0452\n",
      "Epoch 135: Batch 47/116 loss: 0.0409\n",
      "Epoch 135: Batch 48/116 loss: 0.0422\n",
      "Epoch 135: Batch 49/116 loss: 0.0482\n",
      "Epoch 135: Batch 50/116 loss: 0.0481\n",
      "Epoch 135: Batch 51/116 loss: 0.0737\n",
      "Epoch 135: Batch 52/116 loss: 0.0294\n",
      "Epoch 135: Batch 53/116 loss: 0.0481\n",
      "Epoch 135: Batch 54/116 loss: 0.0340\n",
      "Epoch 135: Batch 55/116 loss: 0.0613\n",
      "Epoch 135: Batch 56/116 loss: 0.0445\n",
      "Epoch 135: Batch 57/116 loss: 0.0420\n",
      "Epoch 135: Batch 58/116 loss: 0.0703\n",
      "Epoch 135: Batch 59/116 loss: 0.0543\n",
      "Epoch 135: Batch 60/116 loss: 0.0360\n",
      "Epoch 135: Batch 61/116 loss: 0.0398\n",
      "Epoch 135: Batch 62/116 loss: 0.0478\n",
      "Epoch 135: Batch 63/116 loss: 0.0437\n",
      "Epoch 135: Batch 64/116 loss: 0.0631\n",
      "Epoch 135: Batch 65/116 loss: 0.0642\n",
      "Epoch 135: Batch 66/116 loss: 0.0432\n",
      "Epoch 135: Batch 67/116 loss: 0.0300\n",
      "Epoch 135: Batch 68/116 loss: 0.0499\n",
      "Epoch 135: Batch 69/116 loss: 0.0280\n",
      "Epoch 135: Batch 70/116 loss: 0.0487\n",
      "Epoch 135: Batch 71/116 loss: 0.0479\n",
      "Epoch 135: Batch 72/116 loss: 0.0496\n",
      "Epoch 135: Batch 73/116 loss: 0.0492\n",
      "Epoch 135: Batch 74/116 loss: 0.0552\n",
      "Epoch 135: Batch 75/116 loss: 0.0434\n",
      "Epoch 135: Batch 76/116 loss: 0.0427\n",
      "Epoch 135: Batch 77/116 loss: 0.0528\n",
      "Epoch 135: Batch 78/116 loss: 0.0506\n",
      "Epoch 135: Batch 79/116 loss: 0.0217\n",
      "Epoch 135: Batch 80/116 loss: 0.0623\n",
      "Epoch 135: Batch 81/116 loss: 0.0442\n",
      "Epoch 135: Batch 82/116 loss: 0.0375\n",
      "Epoch 135: Batch 83/116 loss: 0.0442\n",
      "Epoch 135: Batch 84/116 loss: 0.0450\n",
      "Epoch 135: Batch 85/116 loss: 0.0522\n",
      "Epoch 135: Batch 86/116 loss: 0.0424\n",
      "Epoch 135: Batch 87/116 loss: 0.0449\n",
      "Epoch 135: Batch 88/116 loss: 0.0291\n",
      "Epoch 135: Batch 89/116 loss: 0.0922\n",
      "Epoch 135: Batch 90/116 loss: 0.0375\n",
      "Epoch 135: Batch 91/116 loss: 0.0695\n",
      "Epoch 135: Batch 92/116 loss: 0.0467\n",
      "Epoch 135: Batch 93/116 loss: 0.0329\n",
      "Epoch 135: Batch 94/116 loss: 0.0437\n",
      "Epoch 135: Batch 95/116 loss: 0.0506\n",
      "Epoch 135: Batch 96/116 loss: 0.0545\n",
      "Epoch 135: Batch 97/116 loss: 0.0228\n",
      "Epoch 135: Batch 98/116 loss: 0.0616\n",
      "Epoch 135: Batch 99/116 loss: 0.0454\n",
      "Epoch 135: Batch 100/116 loss: 0.0440\n",
      "Epoch 135: Batch 101/116 loss: 0.0600\n",
      "Epoch 135: Batch 102/116 loss: 0.0484\n",
      "Epoch 135: Batch 103/116 loss: 0.0393\n",
      "Epoch 135: Batch 104/116 loss: 0.0395\n",
      "Epoch 135: Batch 105/116 loss: 0.0524\n",
      "Epoch 135: Batch 106/116 loss: 0.0605\n",
      "Epoch 135: Batch 107/116 loss: 0.0462\n",
      "Epoch 135: Batch 108/116 loss: 0.0398\n",
      "Epoch 135: Batch 109/116 loss: 0.0399\n",
      "Epoch 135: Batch 110/116 loss: 0.0474\n",
      "Epoch 135: Batch 111/116 loss: 0.0504\n",
      "Epoch 135: Batch 112/116 loss: 0.0401\n",
      "Epoch 135: Batch 113/116 loss: 0.0356\n",
      "Epoch 135: Batch 114/116 loss: 0.0460\n",
      "Epoch 135: Batch 115/116 loss: 0.0407\n",
      "Epoch 135: Batch 116/116 loss: 0.0849\n",
      "Epoch 135 train loss: 0.0472 valid loss: 0.0529\n",
      "performance reducing: 10\n",
      "Epoch 136: Batch 1/116 loss: 0.0475\n",
      "Epoch 136: Batch 2/116 loss: 0.0555\n",
      "Epoch 136: Batch 3/116 loss: 0.0622\n",
      "Epoch 136: Batch 4/116 loss: 0.0452\n",
      "Epoch 136: Batch 5/116 loss: 0.0476\n",
      "Epoch 136: Batch 6/116 loss: 0.0467\n",
      "Epoch 136: Batch 7/116 loss: 0.0494\n",
      "Epoch 136: Batch 8/116 loss: 0.0506\n",
      "Epoch 136: Batch 9/116 loss: 0.0406\n",
      "Epoch 136: Batch 10/116 loss: 0.0361\n",
      "Epoch 136: Batch 11/116 loss: 0.0592\n",
      "Epoch 136: Batch 12/116 loss: 0.0366\n",
      "Epoch 136: Batch 13/116 loss: 0.0603\n",
      "Epoch 136: Batch 14/116 loss: 0.0604\n",
      "Epoch 136: Batch 15/116 loss: 0.0632\n",
      "Epoch 136: Batch 16/116 loss: 0.0352\n",
      "Epoch 136: Batch 17/116 loss: 0.0716\n",
      "Epoch 136: Batch 18/116 loss: 0.0409\n",
      "Epoch 136: Batch 19/116 loss: 0.0478\n",
      "Epoch 136: Batch 20/116 loss: 0.0509\n",
      "Epoch 136: Batch 21/116 loss: 0.0584\n",
      "Epoch 136: Batch 22/116 loss: 0.0306\n",
      "Epoch 136: Batch 23/116 loss: 0.0501\n",
      "Epoch 136: Batch 24/116 loss: 0.0459\n",
      "Epoch 136: Batch 25/116 loss: 0.0245\n",
      "Epoch 136: Batch 26/116 loss: 0.0458\n",
      "Epoch 136: Batch 27/116 loss: 0.0222\n",
      "Epoch 136: Batch 28/116 loss: 0.0475\n",
      "Epoch 136: Batch 29/116 loss: 0.0519\n",
      "Epoch 136: Batch 30/116 loss: 0.0729\n",
      "Epoch 136: Batch 31/116 loss: 0.0431\n",
      "Epoch 136: Batch 32/116 loss: 0.0495\n",
      "Epoch 136: Batch 33/116 loss: 0.0407\n",
      "Epoch 136: Batch 34/116 loss: 0.0356\n",
      "Epoch 136: Batch 35/116 loss: 0.0469\n",
      "Epoch 136: Batch 36/116 loss: 0.0505\n",
      "Epoch 136: Batch 37/116 loss: 0.0378\n",
      "Epoch 136: Batch 38/116 loss: 0.0504\n",
      "Epoch 136: Batch 39/116 loss: 0.0526\n",
      "Epoch 136: Batch 40/116 loss: 0.0494\n",
      "Epoch 136: Batch 41/116 loss: 0.0555\n",
      "Epoch 136: Batch 42/116 loss: 0.0531\n",
      "Epoch 136: Batch 43/116 loss: 0.0453\n",
      "Epoch 136: Batch 44/116 loss: 0.0719\n",
      "Epoch 136: Batch 45/116 loss: 0.0594\n",
      "Epoch 136: Batch 46/116 loss: 0.0319\n",
      "Epoch 136: Batch 47/116 loss: 0.0478\n",
      "Epoch 136: Batch 48/116 loss: 0.0490\n",
      "Epoch 136: Batch 49/116 loss: 0.0761\n",
      "Epoch 136: Batch 50/116 loss: 0.0596\n",
      "Epoch 136: Batch 51/116 loss: 0.0613\n",
      "Epoch 136: Batch 52/116 loss: 0.0439\n",
      "Epoch 136: Batch 53/116 loss: 0.0473\n",
      "Epoch 136: Batch 54/116 loss: 0.0601\n",
      "Epoch 136: Batch 55/116 loss: 0.0427\n",
      "Epoch 136: Batch 56/116 loss: 0.0280\n",
      "Epoch 136: Batch 57/116 loss: 0.0493\n",
      "Epoch 136: Batch 58/116 loss: 0.0265\n",
      "Epoch 136: Batch 59/116 loss: 0.0269\n",
      "Epoch 136: Batch 60/116 loss: 0.0446\n",
      "Epoch 136: Batch 61/116 loss: 0.0511\n",
      "Epoch 136: Batch 62/116 loss: 0.0535\n",
      "Epoch 136: Batch 63/116 loss: 0.0347\n",
      "Epoch 136: Batch 64/116 loss: 0.0352\n",
      "Epoch 136: Batch 65/116 loss: 0.0796\n",
      "Epoch 136: Batch 66/116 loss: 0.0417\n",
      "Epoch 136: Batch 67/116 loss: 0.0379\n",
      "Epoch 136: Batch 68/116 loss: 0.0375\n",
      "Epoch 136: Batch 69/116 loss: 0.0216\n",
      "Epoch 136: Batch 70/116 loss: 0.0559\n",
      "Epoch 136: Batch 71/116 loss: 0.0374\n",
      "Epoch 136: Batch 72/116 loss: 0.0599\n",
      "Epoch 136: Batch 73/116 loss: 0.0496\n",
      "Epoch 136: Batch 74/116 loss: 0.0401\n",
      "Epoch 136: Batch 75/116 loss: 0.0596\n",
      "Epoch 136: Batch 76/116 loss: 0.0360\n",
      "Epoch 136: Batch 77/116 loss: 0.0555\n",
      "Epoch 136: Batch 78/116 loss: 0.0621\n",
      "Epoch 136: Batch 79/116 loss: 0.0481\n",
      "Epoch 136: Batch 80/116 loss: 0.0392\n",
      "Epoch 136: Batch 81/116 loss: 0.0530\n",
      "Epoch 136: Batch 82/116 loss: 0.0358\n",
      "Epoch 136: Batch 83/116 loss: 0.0384\n",
      "Epoch 136: Batch 84/116 loss: 0.0488\n",
      "Epoch 136: Batch 85/116 loss: 0.0267\n",
      "Epoch 136: Batch 86/116 loss: 0.0487\n",
      "Epoch 136: Batch 87/116 loss: 0.0239\n",
      "Epoch 136: Batch 88/116 loss: 0.0403\n",
      "Epoch 136: Batch 89/116 loss: 0.0451\n",
      "Epoch 136: Batch 90/116 loss: 0.0455\n",
      "Epoch 136: Batch 91/116 loss: 0.0547\n",
      "Epoch 136: Batch 92/116 loss: 0.0284\n",
      "Epoch 136: Batch 93/116 loss: 0.0554\n",
      "Epoch 136: Batch 94/116 loss: 0.0562\n",
      "Epoch 136: Batch 95/116 loss: 0.0530\n",
      "Epoch 136: Batch 96/116 loss: 0.0430\n",
      "Epoch 136: Batch 97/116 loss: 0.0351\n",
      "Epoch 136: Batch 98/116 loss: 0.0371\n",
      "Epoch 136: Batch 99/116 loss: 0.0602\n",
      "Epoch 136: Batch 100/116 loss: 0.0277\n",
      "Epoch 136: Batch 101/116 loss: 0.0377\n",
      "Epoch 136: Batch 102/116 loss: 0.0572\n",
      "Epoch 136: Batch 103/116 loss: 0.0496\n",
      "Epoch 136: Batch 104/116 loss: 0.0320\n",
      "Epoch 136: Batch 105/116 loss: 0.0501\n",
      "Epoch 136: Batch 106/116 loss: 0.0475\n",
      "Epoch 136: Batch 107/116 loss: 0.0521\n",
      "Epoch 136: Batch 108/116 loss: 0.0410\n",
      "Epoch 136: Batch 109/116 loss: 0.0506\n",
      "Epoch 136: Batch 110/116 loss: 0.0451\n",
      "Epoch 136: Batch 111/116 loss: 0.0404\n",
      "Epoch 136: Batch 112/116 loss: 0.0516\n",
      "Epoch 136: Batch 113/116 loss: 0.0345\n",
      "Epoch 136: Batch 114/116 loss: 0.0566\n",
      "Epoch 136: Batch 115/116 loss: 0.0559\n",
      "Epoch 136: Batch 116/116 loss: 0.0559\n",
      "Epoch 136 train loss: 0.0469 valid loss: 0.0540\n",
      "performance reducing: 11\n",
      "Epoch 137: Batch 1/116 loss: 0.0681\n",
      "Epoch 137: Batch 2/116 loss: 0.0434\n",
      "Epoch 137: Batch 3/116 loss: 0.0555\n",
      "Epoch 137: Batch 4/116 loss: 0.0395\n",
      "Epoch 137: Batch 5/116 loss: 0.0374\n",
      "Epoch 137: Batch 6/116 loss: 0.0405\n",
      "Epoch 137: Batch 7/116 loss: 0.0468\n",
      "Epoch 137: Batch 8/116 loss: 0.0268\n",
      "Epoch 137: Batch 9/116 loss: 0.0738\n",
      "Epoch 137: Batch 10/116 loss: 0.0589\n",
      "Epoch 137: Batch 11/116 loss: 0.0454\n",
      "Epoch 137: Batch 12/116 loss: 0.0543\n",
      "Epoch 137: Batch 13/116 loss: 0.0520\n",
      "Epoch 137: Batch 14/116 loss: 0.0587\n",
      "Epoch 137: Batch 15/116 loss: 0.0373\n",
      "Epoch 137: Batch 16/116 loss: 0.0404\n",
      "Epoch 137: Batch 17/116 loss: 0.0490\n",
      "Epoch 137: Batch 18/116 loss: 0.0483\n",
      "Epoch 137: Batch 19/116 loss: 0.0410\n",
      "Epoch 137: Batch 20/116 loss: 0.0622\n",
      "Epoch 137: Batch 21/116 loss: 0.0369\n",
      "Epoch 137: Batch 22/116 loss: 0.0527\n",
      "Epoch 137: Batch 23/116 loss: 0.0593\n",
      "Epoch 137: Batch 24/116 loss: 0.0427\n",
      "Epoch 137: Batch 25/116 loss: 0.0429\n",
      "Epoch 137: Batch 26/116 loss: 0.0603\n",
      "Epoch 137: Batch 27/116 loss: 0.0431\n",
      "Epoch 137: Batch 28/116 loss: 0.0507\n",
      "Epoch 137: Batch 29/116 loss: 0.0286\n",
      "Epoch 137: Batch 30/116 loss: 0.0403\n",
      "Epoch 137: Batch 31/116 loss: 0.0677\n",
      "Epoch 137: Batch 32/116 loss: 0.0445\n",
      "Epoch 137: Batch 33/116 loss: 0.0313\n",
      "Epoch 137: Batch 34/116 loss: 0.0615\n",
      "Epoch 137: Batch 35/116 loss: 0.0656\n",
      "Epoch 137: Batch 36/116 loss: 0.0576\n",
      "Epoch 137: Batch 37/116 loss: 0.0272\n",
      "Epoch 137: Batch 38/116 loss: 0.0262\n",
      "Epoch 137: Batch 39/116 loss: 0.0423\n",
      "Epoch 137: Batch 40/116 loss: 0.0630\n",
      "Epoch 137: Batch 41/116 loss: 0.0495\n",
      "Epoch 137: Batch 42/116 loss: 0.0441\n",
      "Epoch 137: Batch 43/116 loss: 0.0409\n",
      "Epoch 137: Batch 44/116 loss: 0.0312\n",
      "Epoch 137: Batch 45/116 loss: 0.0594\n",
      "Epoch 137: Batch 46/116 loss: 0.0324\n",
      "Epoch 137: Batch 47/116 loss: 0.0479\n",
      "Epoch 137: Batch 48/116 loss: 0.0461\n",
      "Epoch 137: Batch 49/116 loss: 0.0406\n",
      "Epoch 137: Batch 50/116 loss: 0.0531\n",
      "Epoch 137: Batch 51/116 loss: 0.0368\n",
      "Epoch 137: Batch 52/116 loss: 0.0497\n",
      "Epoch 137: Batch 53/116 loss: 0.0441\n",
      "Epoch 137: Batch 54/116 loss: 0.0514\n",
      "Epoch 137: Batch 55/116 loss: 0.0658\n",
      "Epoch 137: Batch 56/116 loss: 0.0468\n",
      "Epoch 137: Batch 57/116 loss: 0.0403\n",
      "Epoch 137: Batch 58/116 loss: 0.0356\n",
      "Epoch 137: Batch 59/116 loss: 0.0427\n",
      "Epoch 137: Batch 60/116 loss: 0.0450\n",
      "Epoch 137: Batch 61/116 loss: 0.0344\n",
      "Epoch 137: Batch 62/116 loss: 0.0321\n",
      "Epoch 137: Batch 63/116 loss: 0.0392\n",
      "Epoch 137: Batch 64/116 loss: 0.0350\n",
      "Epoch 137: Batch 65/116 loss: 0.0453\n",
      "Epoch 137: Batch 66/116 loss: 0.0549\n",
      "Epoch 137: Batch 67/116 loss: 0.0250\n",
      "Epoch 137: Batch 68/116 loss: 0.0522\n",
      "Epoch 137: Batch 69/116 loss: 0.0353\n",
      "Epoch 137: Batch 70/116 loss: 0.0463\n",
      "Epoch 137: Batch 71/116 loss: 0.0525\n",
      "Epoch 137: Batch 72/116 loss: 0.0480\n",
      "Epoch 137: Batch 73/116 loss: 0.0438\n",
      "Epoch 137: Batch 74/116 loss: 0.0437\n",
      "Epoch 137: Batch 75/116 loss: 0.0461\n",
      "Epoch 137: Batch 76/116 loss: 0.0415\n",
      "Epoch 137: Batch 77/116 loss: 0.0419\n",
      "Epoch 137: Batch 78/116 loss: 0.0552\n",
      "Epoch 137: Batch 79/116 loss: 0.0372\n",
      "Epoch 137: Batch 80/116 loss: 0.0319\n",
      "Epoch 137: Batch 81/116 loss: 0.0525\n",
      "Epoch 137: Batch 82/116 loss: 0.0448\n",
      "Epoch 137: Batch 83/116 loss: 0.0691\n",
      "Epoch 137: Batch 84/116 loss: 0.0374\n",
      "Epoch 137: Batch 85/116 loss: 0.0490\n",
      "Epoch 137: Batch 86/116 loss: 0.0467\n",
      "Epoch 137: Batch 87/116 loss: 0.0441\n",
      "Epoch 137: Batch 88/116 loss: 0.0309\n",
      "Epoch 137: Batch 89/116 loss: 0.0642\n",
      "Epoch 137: Batch 90/116 loss: 0.0676\n",
      "Epoch 137: Batch 91/116 loss: 0.0472\n",
      "Epoch 137: Batch 92/116 loss: 0.0451\n",
      "Epoch 137: Batch 93/116 loss: 0.0422\n",
      "Epoch 137: Batch 94/116 loss: 0.0580\n",
      "Epoch 137: Batch 95/116 loss: 0.0676\n",
      "Epoch 137: Batch 96/116 loss: 0.0367\n",
      "Epoch 137: Batch 97/116 loss: 0.0420\n",
      "Epoch 137: Batch 98/116 loss: 0.0427\n",
      "Epoch 137: Batch 99/116 loss: 0.0436\n",
      "Epoch 137: Batch 100/116 loss: 0.0411\n",
      "Epoch 137: Batch 101/116 loss: 0.0369\n",
      "Epoch 137: Batch 102/116 loss: 0.0294\n",
      "Epoch 137: Batch 103/116 loss: 0.0575\n",
      "Epoch 137: Batch 104/116 loss: 0.0517\n",
      "Epoch 137: Batch 105/116 loss: 0.0487\n",
      "Epoch 137: Batch 106/116 loss: 0.0451\n",
      "Epoch 137: Batch 107/116 loss: 0.0533\n",
      "Epoch 137: Batch 108/116 loss: 0.0465\n",
      "Epoch 137: Batch 109/116 loss: 0.0647\n",
      "Epoch 137: Batch 110/116 loss: 0.0636\n",
      "Epoch 137: Batch 111/116 loss: 0.0474\n",
      "Epoch 137: Batch 112/116 loss: 0.0515\n",
      "Epoch 137: Batch 113/116 loss: 0.0332\n",
      "Epoch 137: Batch 114/116 loss: 0.0459\n",
      "Epoch 137: Batch 115/116 loss: 0.0363\n",
      "Epoch 137: Batch 116/116 loss: 0.0426\n",
      "Epoch 137 train loss: 0.0465 valid loss: 0.0567\n",
      "performance reducing: 12\n",
      "Epoch 138: Batch 1/116 loss: 0.0657\n",
      "Epoch 138: Batch 2/116 loss: 0.0491\n",
      "Epoch 138: Batch 3/116 loss: 0.0428\n",
      "Epoch 138: Batch 4/116 loss: 0.0244\n",
      "Epoch 138: Batch 5/116 loss: 0.0355\n",
      "Epoch 138: Batch 6/116 loss: 0.0414\n",
      "Epoch 138: Batch 7/116 loss: 0.0494\n",
      "Epoch 138: Batch 8/116 loss: 0.0524\n",
      "Epoch 138: Batch 9/116 loss: 0.0400\n",
      "Epoch 138: Batch 10/116 loss: 0.0475\n",
      "Epoch 138: Batch 11/116 loss: 0.0541\n",
      "Epoch 138: Batch 12/116 loss: 0.0610\n",
      "Epoch 138: Batch 13/116 loss: 0.0381\n",
      "Epoch 138: Batch 14/116 loss: 0.0319\n",
      "Epoch 138: Batch 15/116 loss: 0.0360\n",
      "Epoch 138: Batch 16/116 loss: 0.0564\n",
      "Epoch 138: Batch 17/116 loss: 0.0413\n",
      "Epoch 138: Batch 18/116 loss: 0.0380\n",
      "Epoch 138: Batch 19/116 loss: 0.0374\n",
      "Epoch 138: Batch 20/116 loss: 0.0468\n",
      "Epoch 138: Batch 21/116 loss: 0.0431\n",
      "Epoch 138: Batch 22/116 loss: 0.0399\n",
      "Epoch 138: Batch 23/116 loss: 0.0457\n",
      "Epoch 138: Batch 24/116 loss: 0.0531\n",
      "Epoch 138: Batch 25/116 loss: 0.0492\n",
      "Epoch 138: Batch 26/116 loss: 0.0299\n",
      "Epoch 138: Batch 27/116 loss: 0.0476\n",
      "Epoch 138: Batch 28/116 loss: 0.0547\n",
      "Epoch 138: Batch 29/116 loss: 0.0447\n",
      "Epoch 138: Batch 30/116 loss: 0.0329\n",
      "Epoch 138: Batch 31/116 loss: 0.0487\n",
      "Epoch 138: Batch 32/116 loss: 0.0396\n",
      "Epoch 138: Batch 33/116 loss: 0.0387\n",
      "Epoch 138: Batch 34/116 loss: 0.0418\n",
      "Epoch 138: Batch 35/116 loss: 0.0784\n",
      "Epoch 138: Batch 36/116 loss: 0.0694\n",
      "Epoch 138: Batch 37/116 loss: 0.0693\n",
      "Epoch 138: Batch 38/116 loss: 0.1002\n",
      "Epoch 138: Batch 39/116 loss: 0.0677\n",
      "Epoch 138: Batch 40/116 loss: 0.0901\n",
      "Epoch 138: Batch 41/116 loss: 0.0559\n",
      "Epoch 138: Batch 42/116 loss: 0.0623\n",
      "Epoch 138: Batch 43/116 loss: 0.1001\n",
      "Epoch 138: Batch 44/116 loss: 0.0545\n",
      "Epoch 138: Batch 45/116 loss: 0.0586\n",
      "Epoch 138: Batch 46/116 loss: 0.0363\n",
      "Epoch 138: Batch 47/116 loss: 0.0610\n",
      "Epoch 138: Batch 48/116 loss: 0.0484\n",
      "Epoch 138: Batch 49/116 loss: 0.0518\n",
      "Epoch 138: Batch 50/116 loss: 0.0503\n",
      "Epoch 138: Batch 51/116 loss: 0.0387\n",
      "Epoch 138: Batch 52/116 loss: 0.0325\n",
      "Epoch 138: Batch 53/116 loss: 0.0442\n",
      "Epoch 138: Batch 54/116 loss: 0.0665\n",
      "Epoch 138: Batch 55/116 loss: 0.0568\n",
      "Epoch 138: Batch 56/116 loss: 0.0585\n",
      "Epoch 138: Batch 57/116 loss: 0.0396\n",
      "Epoch 138: Batch 58/116 loss: 0.0461\n",
      "Epoch 138: Batch 59/116 loss: 0.0528\n",
      "Epoch 138: Batch 60/116 loss: 0.0767\n",
      "Epoch 138: Batch 61/116 loss: 0.0396\n",
      "Epoch 138: Batch 62/116 loss: 0.0525\n",
      "Epoch 138: Batch 63/116 loss: 0.0490\n",
      "Epoch 138: Batch 64/116 loss: 0.0638\n",
      "Epoch 138: Batch 65/116 loss: 0.0538\n",
      "Epoch 138: Batch 66/116 loss: 0.0574\n",
      "Epoch 138: Batch 67/116 loss: 0.0447\n",
      "Epoch 138: Batch 68/116 loss: 0.0555\n",
      "Epoch 138: Batch 69/116 loss: 0.0729\n",
      "Epoch 138: Batch 70/116 loss: 0.0587\n",
      "Epoch 138: Batch 71/116 loss: 0.0514\n",
      "Epoch 138: Batch 72/116 loss: 0.0393\n",
      "Epoch 138: Batch 73/116 loss: 0.0441\n",
      "Epoch 138: Batch 74/116 loss: 0.0683\n",
      "Epoch 138: Batch 75/116 loss: 0.0351\n",
      "Epoch 138: Batch 76/116 loss: 0.0512\n",
      "Epoch 138: Batch 77/116 loss: 0.0595\n",
      "Epoch 138: Batch 78/116 loss: 0.0652\n",
      "Epoch 138: Batch 79/116 loss: 0.0517\n",
      "Epoch 138: Batch 80/116 loss: 0.0525\n",
      "Epoch 138: Batch 81/116 loss: 0.0592\n",
      "Epoch 138: Batch 82/116 loss: 0.0535\n",
      "Epoch 138: Batch 83/116 loss: 0.0536\n",
      "Epoch 138: Batch 84/116 loss: 0.0361\n",
      "Epoch 138: Batch 85/116 loss: 0.0513\n",
      "Epoch 138: Batch 86/116 loss: 0.0561\n",
      "Epoch 138: Batch 87/116 loss: 0.0395\n",
      "Epoch 138: Batch 88/116 loss: 0.0391\n",
      "Epoch 138: Batch 89/116 loss: 0.0452\n",
      "Epoch 138: Batch 90/116 loss: 0.0468\n",
      "Epoch 138: Batch 91/116 loss: 0.0492\n",
      "Epoch 138: Batch 92/116 loss: 0.0365\n",
      "Epoch 138: Batch 93/116 loss: 0.0415\n",
      "Epoch 138: Batch 94/116 loss: 0.0697\n",
      "Epoch 138: Batch 95/116 loss: 0.0459\n",
      "Epoch 138: Batch 96/116 loss: 0.0543\n",
      "Epoch 138: Batch 97/116 loss: 0.0778\n",
      "Epoch 138: Batch 98/116 loss: 0.0558\n",
      "Epoch 138: Batch 99/116 loss: 0.0405\n",
      "Epoch 138: Batch 100/116 loss: 0.0393\n",
      "Epoch 138: Batch 101/116 loss: 0.0358\n",
      "Epoch 138: Batch 102/116 loss: 0.0370\n",
      "Epoch 138: Batch 103/116 loss: 0.0351\n",
      "Epoch 138: Batch 104/116 loss: 0.0434\n",
      "Epoch 138: Batch 105/116 loss: 0.0533\n",
      "Epoch 138: Batch 106/116 loss: 0.0367\n",
      "Epoch 138: Batch 107/116 loss: 0.0506\n",
      "Epoch 138: Batch 108/116 loss: 0.0471\n",
      "Epoch 138: Batch 109/116 loss: 0.0251\n",
      "Epoch 138: Batch 110/116 loss: 0.0537\n",
      "Epoch 138: Batch 111/116 loss: 0.0322\n",
      "Epoch 138: Batch 112/116 loss: 0.0512\n",
      "Epoch 138: Batch 113/116 loss: 0.0507\n",
      "Epoch 138: Batch 114/116 loss: 0.0611\n",
      "Epoch 138: Batch 115/116 loss: 0.0535\n",
      "Epoch 138: Batch 116/116 loss: 0.0397\n",
      "Epoch 138 train loss: 0.0503 valid loss: 0.0547\n",
      "performance reducing: 13\n",
      "Epoch 139: Batch 1/116 loss: 0.0496\n",
      "Epoch 139: Batch 2/116 loss: 0.0480\n",
      "Epoch 139: Batch 3/116 loss: 0.0410\n",
      "Epoch 139: Batch 4/116 loss: 0.0357\n",
      "Epoch 139: Batch 5/116 loss: 0.0338\n",
      "Epoch 139: Batch 6/116 loss: 0.0384\n",
      "Epoch 139: Batch 7/116 loss: 0.0504\n",
      "Epoch 139: Batch 8/116 loss: 0.0531\n",
      "Epoch 139: Batch 9/116 loss: 0.0529\n",
      "Epoch 139: Batch 10/116 loss: 0.0408\n",
      "Epoch 139: Batch 11/116 loss: 0.0299\n",
      "Epoch 139: Batch 12/116 loss: 0.0597\n",
      "Epoch 139: Batch 13/116 loss: 0.0343\n",
      "Epoch 139: Batch 14/116 loss: 0.0573\n",
      "Epoch 139: Batch 15/116 loss: 0.0523\n",
      "Epoch 139: Batch 16/116 loss: 0.0337\n",
      "Epoch 139: Batch 17/116 loss: 0.0377\n",
      "Epoch 139: Batch 18/116 loss: 0.0475\n",
      "Epoch 139: Batch 19/116 loss: 0.0514\n",
      "Epoch 139: Batch 20/116 loss: 0.0508\n",
      "Epoch 139: Batch 21/116 loss: 0.0460\n",
      "Epoch 139: Batch 22/116 loss: 0.0314\n",
      "Epoch 139: Batch 23/116 loss: 0.0530\n",
      "Epoch 139: Batch 24/116 loss: 0.0392\n",
      "Epoch 139: Batch 25/116 loss: 0.0299\n",
      "Epoch 139: Batch 26/116 loss: 0.0480\n",
      "Epoch 139: Batch 27/116 loss: 0.0605\n",
      "Epoch 139: Batch 28/116 loss: 0.0639\n",
      "Epoch 139: Batch 29/116 loss: 0.0501\n",
      "Epoch 139: Batch 30/116 loss: 0.0522\n",
      "Epoch 139: Batch 31/116 loss: 0.0919\n",
      "Epoch 139: Batch 32/116 loss: 0.0441\n",
      "Epoch 139: Batch 33/116 loss: 0.0532\n",
      "Epoch 139: Batch 34/116 loss: 0.0452\n",
      "Epoch 139: Batch 35/116 loss: 0.0455\n",
      "Epoch 139: Batch 36/116 loss: 0.0429\n",
      "Epoch 139: Batch 37/116 loss: 0.0385\n",
      "Epoch 139: Batch 38/116 loss: 0.0510\n",
      "Epoch 139: Batch 39/116 loss: 0.0637\n",
      "Epoch 139: Batch 40/116 loss: 0.0391\n",
      "Epoch 139: Batch 41/116 loss: 0.0638\n",
      "Epoch 139: Batch 42/116 loss: 0.0521\n",
      "Epoch 139: Batch 43/116 loss: 0.0377\n",
      "Epoch 139: Batch 44/116 loss: 0.0344\n",
      "Epoch 139: Batch 45/116 loss: 0.0515\n",
      "Epoch 139: Batch 46/116 loss: 0.0544\n",
      "Epoch 139: Batch 47/116 loss: 0.0501\n",
      "Epoch 139: Batch 48/116 loss: 0.0372\n",
      "Epoch 139: Batch 49/116 loss: 0.0525\n",
      "Epoch 139: Batch 50/116 loss: 0.0459\n",
      "Epoch 139: Batch 51/116 loss: 0.0464\n",
      "Epoch 139: Batch 52/116 loss: 0.0580\n",
      "Epoch 139: Batch 53/116 loss: 0.0530\n",
      "Epoch 139: Batch 54/116 loss: 0.0378\n",
      "Epoch 139: Batch 55/116 loss: 0.0548\n",
      "Epoch 139: Batch 56/116 loss: 0.0429\n",
      "Epoch 139: Batch 57/116 loss: 0.0479\n",
      "Epoch 139: Batch 58/116 loss: 0.0539\n",
      "Epoch 139: Batch 59/116 loss: 0.0756\n",
      "Epoch 139: Batch 60/116 loss: 0.0452\n",
      "Epoch 139: Batch 61/116 loss: 0.0488\n",
      "Epoch 139: Batch 62/116 loss: 0.0357\n",
      "Epoch 139: Batch 63/116 loss: 0.0305\n",
      "Epoch 139: Batch 64/116 loss: 0.0403\n",
      "Epoch 139: Batch 65/116 loss: 0.0325\n",
      "Epoch 139: Batch 66/116 loss: 0.0686\n",
      "Epoch 139: Batch 67/116 loss: 0.0472\n",
      "Epoch 139: Batch 68/116 loss: 0.0311\n",
      "Epoch 139: Batch 69/116 loss: 0.0637\n",
      "Epoch 139: Batch 70/116 loss: 0.0612\n",
      "Epoch 139: Batch 71/116 loss: 0.0386\n",
      "Epoch 139: Batch 72/116 loss: 0.0392\n",
      "Epoch 139: Batch 73/116 loss: 0.0380\n",
      "Epoch 139: Batch 74/116 loss: 0.0352\n",
      "Epoch 139: Batch 75/116 loss: 0.0259\n",
      "Epoch 139: Batch 76/116 loss: 0.0656\n",
      "Epoch 139: Batch 77/116 loss: 0.0432\n",
      "Epoch 139: Batch 78/116 loss: 0.0287\n",
      "Epoch 139: Batch 79/116 loss: 0.0531\n",
      "Epoch 139: Batch 80/116 loss: 0.0784\n",
      "Epoch 139: Batch 81/116 loss: 0.0407\n",
      "Epoch 139: Batch 82/116 loss: 0.0374\n",
      "Epoch 139: Batch 83/116 loss: 0.0416\n",
      "Epoch 139: Batch 84/116 loss: 0.0564\n",
      "Epoch 139: Batch 85/116 loss: 0.0239\n",
      "Epoch 139: Batch 86/116 loss: 0.0549\n",
      "Epoch 139: Batch 87/116 loss: 0.0571\n",
      "Epoch 139: Batch 88/116 loss: 0.0337\n",
      "Epoch 139: Batch 89/116 loss: 0.0364\n",
      "Epoch 139: Batch 90/116 loss: 0.0617\n",
      "Epoch 139: Batch 91/116 loss: 0.0369\n",
      "Epoch 139: Batch 92/116 loss: 0.0425\n",
      "Epoch 139: Batch 93/116 loss: 0.0373\n",
      "Epoch 139: Batch 94/116 loss: 0.0412\n",
      "Epoch 139: Batch 95/116 loss: 0.0592\n",
      "Epoch 139: Batch 96/116 loss: 0.0584\n",
      "Epoch 139: Batch 97/116 loss: 0.0494\n",
      "Epoch 139: Batch 98/116 loss: 0.0552\n",
      "Epoch 139: Batch 99/116 loss: 0.0416\n",
      "Epoch 139: Batch 100/116 loss: 0.0504\n",
      "Epoch 139: Batch 101/116 loss: 0.0512\n",
      "Epoch 139: Batch 102/116 loss: 0.0457\n",
      "Epoch 139: Batch 103/116 loss: 0.0385\n",
      "Epoch 139: Batch 104/116 loss: 0.0506\n",
      "Epoch 139: Batch 105/116 loss: 0.0539\n",
      "Epoch 139: Batch 106/116 loss: 0.0535\n",
      "Epoch 139: Batch 107/116 loss: 0.0606\n",
      "Epoch 139: Batch 108/116 loss: 0.0272\n",
      "Epoch 139: Batch 109/116 loss: 0.0457\n",
      "Epoch 139: Batch 110/116 loss: 0.0564\n",
      "Epoch 139: Batch 111/116 loss: 0.0318\n",
      "Epoch 139: Batch 112/116 loss: 0.0555\n",
      "Epoch 139: Batch 113/116 loss: 0.0371\n",
      "Epoch 139: Batch 114/116 loss: 0.0368\n",
      "Epoch 139: Batch 115/116 loss: 0.0478\n",
      "Epoch 139: Batch 116/116 loss: 0.0328\n",
      "Epoch 139 train loss: 0.0468 valid loss: 0.0550\n",
      "performance reducing: 14\n",
      "Epoch 140: Batch 1/116 loss: 0.0583\n",
      "Epoch 140: Batch 2/116 loss: 0.0475\n",
      "Epoch 140: Batch 3/116 loss: 0.0457\n",
      "Epoch 140: Batch 4/116 loss: 0.0417\n",
      "Epoch 140: Batch 5/116 loss: 0.0621\n",
      "Epoch 140: Batch 6/116 loss: 0.0490\n",
      "Epoch 140: Batch 7/116 loss: 0.0612\n",
      "Epoch 140: Batch 8/116 loss: 0.0359\n",
      "Epoch 140: Batch 9/116 loss: 0.0382\n",
      "Epoch 140: Batch 10/116 loss: 0.0521\n",
      "Epoch 140: Batch 11/116 loss: 0.0292\n",
      "Epoch 140: Batch 12/116 loss: 0.0618\n",
      "Epoch 140: Batch 13/116 loss: 0.0457\n",
      "Epoch 140: Batch 14/116 loss: 0.0588\n",
      "Epoch 140: Batch 15/116 loss: 0.0568\n",
      "Epoch 140: Batch 16/116 loss: 0.0637\n",
      "Epoch 140: Batch 17/116 loss: 0.0429\n",
      "Epoch 140: Batch 18/116 loss: 0.0575\n",
      "Epoch 140: Batch 19/116 loss: 0.0460\n",
      "Epoch 140: Batch 20/116 loss: 0.0480\n",
      "Epoch 140: Batch 21/116 loss: 0.0573\n",
      "Epoch 140: Batch 22/116 loss: 0.0445\n",
      "Epoch 140: Batch 23/116 loss: 0.0355\n",
      "Epoch 140: Batch 24/116 loss: 0.0487\n",
      "Epoch 140: Batch 25/116 loss: 0.0576\n",
      "Epoch 140: Batch 26/116 loss: 0.0485\n",
      "Epoch 140: Batch 27/116 loss: 0.0481\n",
      "Epoch 140: Batch 28/116 loss: 0.0505\n",
      "Epoch 140: Batch 29/116 loss: 0.0406\n",
      "Epoch 140: Batch 30/116 loss: 0.0522\n",
      "Epoch 140: Batch 31/116 loss: 0.0531\n",
      "Epoch 140: Batch 32/116 loss: 0.0640\n",
      "Epoch 140: Batch 33/116 loss: 0.0481\n",
      "Epoch 140: Batch 34/116 loss: 0.0373\n",
      "Epoch 140: Batch 35/116 loss: 0.0620\n",
      "Epoch 140: Batch 36/116 loss: 0.0632\n",
      "Epoch 140: Batch 37/116 loss: 0.0402\n",
      "Epoch 140: Batch 38/116 loss: 0.0389\n",
      "Epoch 140: Batch 39/116 loss: 0.0243\n",
      "Epoch 140: Batch 40/116 loss: 0.0482\n",
      "Epoch 140: Batch 41/116 loss: 0.0350\n",
      "Epoch 140: Batch 42/116 loss: 0.0520\n",
      "Epoch 140: Batch 43/116 loss: 0.0497\n",
      "Epoch 140: Batch 44/116 loss: 0.0308\n",
      "Epoch 140: Batch 45/116 loss: 0.0538\n",
      "Epoch 140: Batch 46/116 loss: 0.0548\n",
      "Epoch 140: Batch 47/116 loss: 0.0676\n",
      "Epoch 140: Batch 48/116 loss: 0.0467\n",
      "Epoch 140: Batch 49/116 loss: 0.0369\n",
      "Epoch 140: Batch 50/116 loss: 0.0507\n",
      "Epoch 140: Batch 51/116 loss: 0.0773\n",
      "Epoch 140: Batch 52/116 loss: 0.0490\n",
      "Epoch 140: Batch 53/116 loss: 0.0504\n",
      "Epoch 140: Batch 54/116 loss: 0.0367\n",
      "Epoch 140: Batch 55/116 loss: 0.0523\n",
      "Epoch 140: Batch 56/116 loss: 0.0359\n",
      "Epoch 140: Batch 57/116 loss: 0.0443\n",
      "Epoch 140: Batch 58/116 loss: 0.0397\n",
      "Epoch 140: Batch 59/116 loss: 0.0581\n",
      "Epoch 140: Batch 60/116 loss: 0.0415\n",
      "Epoch 140: Batch 61/116 loss: 0.0526\n",
      "Epoch 140: Batch 62/116 loss: 0.0472\n",
      "Epoch 140: Batch 63/116 loss: 0.0608\n",
      "Epoch 140: Batch 64/116 loss: 0.0393\n",
      "Epoch 140: Batch 65/116 loss: 0.0542\n",
      "Epoch 140: Batch 66/116 loss: 0.0580\n",
      "Epoch 140: Batch 67/116 loss: 0.0509\n",
      "Epoch 140: Batch 68/116 loss: 0.0323\n",
      "Epoch 140: Batch 69/116 loss: 0.0676\n",
      "Epoch 140: Batch 70/116 loss: 0.0438\n",
      "Epoch 140: Batch 71/116 loss: 0.0523\n",
      "Epoch 140: Batch 72/116 loss: 0.0397\n",
      "Epoch 140: Batch 73/116 loss: 0.0326\n",
      "Epoch 140: Batch 74/116 loss: 0.0544\n",
      "Epoch 140: Batch 75/116 loss: 0.0398\n",
      "Epoch 140: Batch 76/116 loss: 0.0363\n",
      "Epoch 140: Batch 77/116 loss: 0.0556\n",
      "Epoch 140: Batch 78/116 loss: 0.0450\n",
      "Epoch 140: Batch 79/116 loss: 0.0522\n",
      "Epoch 140: Batch 80/116 loss: 0.0447\n",
      "Epoch 140: Batch 81/116 loss: 0.0460\n",
      "Epoch 140: Batch 82/116 loss: 0.0302\n",
      "Epoch 140: Batch 83/116 loss: 0.0451\n",
      "Epoch 140: Batch 84/116 loss: 0.0358\n",
      "Epoch 140: Batch 85/116 loss: 0.0398\n",
      "Epoch 140: Batch 86/116 loss: 0.0259\n",
      "Epoch 140: Batch 87/116 loss: 0.0339\n",
      "Epoch 140: Batch 88/116 loss: 0.0430\n",
      "Epoch 140: Batch 89/116 loss: 0.0539\n",
      "Epoch 140: Batch 90/116 loss: 0.0597\n",
      "Epoch 140: Batch 91/116 loss: 0.0460\n",
      "Epoch 140: Batch 92/116 loss: 0.0415\n",
      "Epoch 140: Batch 93/116 loss: 0.0476\n",
      "Epoch 140: Batch 94/116 loss: 0.0402\n",
      "Epoch 140: Batch 95/116 loss: 0.0601\n",
      "Epoch 140: Batch 96/116 loss: 0.0406\n",
      "Epoch 140: Batch 97/116 loss: 0.0491\n",
      "Epoch 140: Batch 98/116 loss: 0.0404\n",
      "Epoch 140: Batch 99/116 loss: 0.0581\n",
      "Epoch 140: Batch 100/116 loss: 0.0508\n",
      "Epoch 140: Batch 101/116 loss: 0.0572\n",
      "Epoch 140: Batch 102/116 loss: 0.0351\n",
      "Epoch 140: Batch 103/116 loss: 0.0405\n",
      "Epoch 140: Batch 104/116 loss: 0.0403\n",
      "Epoch 140: Batch 105/116 loss: 0.0344\n",
      "Epoch 140: Batch 106/116 loss: 0.0311\n",
      "Epoch 140: Batch 107/116 loss: 0.0487\n",
      "Epoch 140: Batch 108/116 loss: 0.0290\n",
      "Epoch 140: Batch 109/116 loss: 0.0556\n",
      "Epoch 140: Batch 110/116 loss: 0.0248\n",
      "Epoch 140: Batch 111/116 loss: 0.0460\n",
      "Epoch 140: Batch 112/116 loss: 0.0514\n",
      "Epoch 140: Batch 113/116 loss: 0.0494\n",
      "Epoch 140: Batch 114/116 loss: 0.0708\n",
      "Epoch 140: Batch 115/116 loss: 0.0313\n",
      "Epoch 140: Batch 116/116 loss: 0.0400\n",
      "Epoch 140 train loss: 0.0471 valid loss: 0.0545\n",
      "performance reducing: 15\n",
      "Epoch 141: Batch 1/116 loss: 0.0438\n",
      "Epoch 141: Batch 2/116 loss: 0.0209\n",
      "Epoch 141: Batch 3/116 loss: 0.0516\n",
      "Epoch 141: Batch 4/116 loss: 0.0606\n",
      "Epoch 141: Batch 5/116 loss: 0.0447\n",
      "Epoch 141: Batch 6/116 loss: 0.0724\n",
      "Epoch 141: Batch 7/116 loss: 0.0393\n",
      "Epoch 141: Batch 8/116 loss: 0.0521\n",
      "Epoch 141: Batch 9/116 loss: 0.0412\n",
      "Epoch 141: Batch 10/116 loss: 0.0470\n",
      "Epoch 141: Batch 11/116 loss: 0.0493\n",
      "Epoch 141: Batch 12/116 loss: 0.0552\n",
      "Epoch 141: Batch 13/116 loss: 0.0644\n",
      "Epoch 141: Batch 14/116 loss: 0.0483\n",
      "Epoch 141: Batch 15/116 loss: 0.0337\n",
      "Epoch 141: Batch 16/116 loss: 0.0478\n",
      "Epoch 141: Batch 17/116 loss: 0.0587\n",
      "Epoch 141: Batch 18/116 loss: 0.0448\n",
      "Epoch 141: Batch 19/116 loss: 0.0426\n",
      "Epoch 141: Batch 20/116 loss: 0.0569\n",
      "Epoch 141: Batch 21/116 loss: 0.0316\n",
      "Epoch 141: Batch 22/116 loss: 0.0488\n",
      "Epoch 141: Batch 23/116 loss: 0.0417\n",
      "Epoch 141: Batch 24/116 loss: 0.0671\n",
      "Epoch 141: Batch 25/116 loss: 0.0442\n",
      "Epoch 141: Batch 26/116 loss: 0.0315\n",
      "Epoch 141: Batch 27/116 loss: 0.0596\n",
      "Epoch 141: Batch 28/116 loss: 0.0394\n",
      "Epoch 141: Batch 29/116 loss: 0.0279\n",
      "Epoch 141: Batch 30/116 loss: 0.0432\n",
      "Epoch 141: Batch 31/116 loss: 0.0390\n",
      "Epoch 141: Batch 32/116 loss: 0.0510\n",
      "Epoch 141: Batch 33/116 loss: 0.0360\n",
      "Epoch 141: Batch 34/116 loss: 0.0575\n",
      "Epoch 141: Batch 35/116 loss: 0.0611\n",
      "Epoch 141: Batch 36/116 loss: 0.0423\n",
      "Epoch 141: Batch 37/116 loss: 0.0363\n",
      "Epoch 141: Batch 38/116 loss: 0.0541\n",
      "Epoch 141: Batch 39/116 loss: 0.0389\n",
      "Epoch 141: Batch 40/116 loss: 0.0372\n",
      "Epoch 141: Batch 41/116 loss: 0.0414\n",
      "Epoch 141: Batch 42/116 loss: 0.0518\n",
      "Epoch 141: Batch 43/116 loss: 0.0616\n",
      "Epoch 141: Batch 44/116 loss: 0.0340\n",
      "Epoch 141: Batch 45/116 loss: 0.0423\n",
      "Epoch 141: Batch 46/116 loss: 0.0596\n",
      "Epoch 141: Batch 47/116 loss: 0.0406\n",
      "Epoch 141: Batch 48/116 loss: 0.0541\n",
      "Epoch 141: Batch 49/116 loss: 0.0407\n",
      "Epoch 141: Batch 50/116 loss: 0.0303\n",
      "Epoch 141: Batch 51/116 loss: 0.0489\n",
      "Epoch 141: Batch 52/116 loss: 0.0508\n",
      "Epoch 141: Batch 53/116 loss: 0.0311\n",
      "Epoch 141: Batch 54/116 loss: 0.0437\n",
      "Epoch 141: Batch 55/116 loss: 0.0402\n",
      "Epoch 141: Batch 56/116 loss: 0.0443\n",
      "Epoch 141: Batch 57/116 loss: 0.0523\n",
      "Epoch 141: Batch 58/116 loss: 0.0644\n",
      "Epoch 141: Batch 59/116 loss: 0.0600\n",
      "Epoch 141: Batch 60/116 loss: 0.0568\n",
      "Epoch 141: Batch 61/116 loss: 0.0445\n",
      "Epoch 141: Batch 62/116 loss: 0.0661\n",
      "Epoch 141: Batch 63/116 loss: 0.0522\n",
      "Epoch 141: Batch 64/116 loss: 0.0467\n",
      "Epoch 141: Batch 65/116 loss: 0.0460\n",
      "Epoch 141: Batch 66/116 loss: 0.0606\n",
      "Epoch 141: Batch 67/116 loss: 0.0338\n",
      "Epoch 141: Batch 68/116 loss: 0.0399\n",
      "Epoch 141: Batch 69/116 loss: 0.0457\n",
      "Epoch 141: Batch 70/116 loss: 0.0599\n",
      "Epoch 141: Batch 71/116 loss: 0.0469\n",
      "Epoch 141: Batch 72/116 loss: 0.0547\n",
      "Epoch 141: Batch 73/116 loss: 0.0441\n",
      "Epoch 141: Batch 74/116 loss: 0.0410\n",
      "Epoch 141: Batch 75/116 loss: 0.0310\n",
      "Epoch 141: Batch 76/116 loss: 0.0412\n",
      "Epoch 141: Batch 77/116 loss: 0.0466\n",
      "Epoch 141: Batch 78/116 loss: 0.0503\n",
      "Epoch 141: Batch 79/116 loss: 0.0517\n",
      "Epoch 141: Batch 80/116 loss: 0.0554\n",
      "Epoch 141: Batch 81/116 loss: 0.0358\n",
      "Epoch 141: Batch 82/116 loss: 0.0578\n",
      "Epoch 141: Batch 83/116 loss: 0.0440\n",
      "Epoch 141: Batch 84/116 loss: 0.0521\n",
      "Epoch 141: Batch 85/116 loss: 0.0540\n",
      "Epoch 141: Batch 86/116 loss: 0.0508\n",
      "Epoch 141: Batch 87/116 loss: 0.0536\n",
      "Epoch 141: Batch 88/116 loss: 0.0578\n",
      "Epoch 141: Batch 89/116 loss: 0.0448\n",
      "Epoch 141: Batch 90/116 loss: 0.0416\n",
      "Epoch 141: Batch 91/116 loss: 0.0514\n",
      "Epoch 141: Batch 92/116 loss: 0.0416\n",
      "Epoch 141: Batch 93/116 loss: 0.0455\n",
      "Epoch 141: Batch 94/116 loss: 0.0360\n",
      "Epoch 141: Batch 95/116 loss: 0.0493\n",
      "Epoch 141: Batch 96/116 loss: 0.0520\n",
      "Epoch 141: Batch 97/116 loss: 0.0224\n",
      "Epoch 141: Batch 98/116 loss: 0.0426\n",
      "Epoch 141: Batch 99/116 loss: 0.0456\n",
      "Epoch 141: Batch 100/116 loss: 0.0295\n",
      "Epoch 141: Batch 101/116 loss: 0.0576\n",
      "Epoch 141: Batch 102/116 loss: 0.0505\n",
      "Epoch 141: Batch 103/116 loss: 0.0312\n",
      "Epoch 141: Batch 104/116 loss: 0.0454\n",
      "Epoch 141: Batch 105/116 loss: 0.0508\n",
      "Epoch 141: Batch 106/116 loss: 0.0317\n",
      "Epoch 141: Batch 107/116 loss: 0.0174\n",
      "Epoch 141: Batch 108/116 loss: 0.0400\n",
      "Epoch 141: Batch 109/116 loss: 0.0334\n",
      "Epoch 141: Batch 110/116 loss: 0.0384\n",
      "Epoch 141: Batch 111/116 loss: 0.0544\n",
      "Epoch 141: Batch 112/116 loss: 0.0301\n",
      "Epoch 141: Batch 113/116 loss: 0.0365\n",
      "Epoch 141: Batch 114/116 loss: 0.0452\n",
      "Epoch 141: Batch 115/116 loss: 0.0329\n",
      "Epoch 141: Batch 116/116 loss: 0.0507\n",
      "Epoch 141 train loss: 0.0459 valid loss: 0.0548\n",
      "performance reducing: 16\n",
      "Epoch 142: Batch 1/116 loss: 0.0372\n",
      "Epoch 142: Batch 2/116 loss: 0.0453\n",
      "Epoch 142: Batch 3/116 loss: 0.0619\n",
      "Epoch 142: Batch 4/116 loss: 0.0452\n",
      "Epoch 142: Batch 5/116 loss: 0.0457\n",
      "Epoch 142: Batch 6/116 loss: 0.0259\n",
      "Epoch 142: Batch 7/116 loss: 0.0377\n",
      "Epoch 142: Batch 8/116 loss: 0.0391\n",
      "Epoch 142: Batch 9/116 loss: 0.0406\n",
      "Epoch 142: Batch 10/116 loss: 0.0436\n",
      "Epoch 142: Batch 11/116 loss: 0.0476\n",
      "Epoch 142: Batch 12/116 loss: 0.0413\n",
      "Epoch 142: Batch 13/116 loss: 0.0364\n",
      "Epoch 142: Batch 14/116 loss: 0.0444\n",
      "Epoch 142: Batch 15/116 loss: 0.0413\n",
      "Epoch 142: Batch 16/116 loss: 0.0330\n",
      "Epoch 142: Batch 17/116 loss: 0.0334\n",
      "Epoch 142: Batch 18/116 loss: 0.0561\n",
      "Epoch 142: Batch 19/116 loss: 0.0372\n",
      "Epoch 142: Batch 20/116 loss: 0.0543\n",
      "Epoch 142: Batch 21/116 loss: 0.0314\n",
      "Epoch 142: Batch 22/116 loss: 0.0595\n",
      "Epoch 142: Batch 23/116 loss: 0.0408\n",
      "Epoch 142: Batch 24/116 loss: 0.0726\n",
      "Epoch 142: Batch 25/116 loss: 0.0418\n",
      "Epoch 142: Batch 26/116 loss: 0.0447\n",
      "Epoch 142: Batch 27/116 loss: 0.0387\n",
      "Epoch 142: Batch 28/116 loss: 0.0446\n",
      "Epoch 142: Batch 29/116 loss: 0.0453\n",
      "Epoch 142: Batch 30/116 loss: 0.0476\n",
      "Epoch 142: Batch 31/116 loss: 0.0473\n",
      "Epoch 142: Batch 32/116 loss: 0.0422\n",
      "Epoch 142: Batch 33/116 loss: 0.0413\n",
      "Epoch 142: Batch 34/116 loss: 0.0339\n",
      "Epoch 142: Batch 35/116 loss: 0.0510\n",
      "Epoch 142: Batch 36/116 loss: 0.0384\n",
      "Epoch 142: Batch 37/116 loss: 0.0462\n",
      "Epoch 142: Batch 38/116 loss: 0.0451\n",
      "Epoch 142: Batch 39/116 loss: 0.0552\n",
      "Epoch 142: Batch 40/116 loss: 0.0417\n",
      "Epoch 142: Batch 41/116 loss: 0.0383\n",
      "Epoch 142: Batch 42/116 loss: 0.0367\n",
      "Epoch 142: Batch 43/116 loss: 0.0441\n",
      "Epoch 142: Batch 44/116 loss: 0.0555\n",
      "Epoch 142: Batch 45/116 loss: 0.0383\n",
      "Epoch 142: Batch 46/116 loss: 0.0386\n",
      "Epoch 142: Batch 47/116 loss: 0.0467\n",
      "Epoch 142: Batch 48/116 loss: 0.0494\n",
      "Epoch 142: Batch 49/116 loss: 0.0799\n",
      "Epoch 142: Batch 50/116 loss: 0.0234\n",
      "Epoch 142: Batch 51/116 loss: 0.0594\n",
      "Epoch 142: Batch 52/116 loss: 0.0374\n",
      "Epoch 142: Batch 53/116 loss: 0.0505\n",
      "Epoch 142: Batch 54/116 loss: 0.0776\n",
      "Epoch 142: Batch 55/116 loss: 0.0500\n",
      "Epoch 142: Batch 56/116 loss: 0.0299\n",
      "Epoch 142: Batch 57/116 loss: 0.0278\n",
      "Epoch 142: Batch 58/116 loss: 0.0652\n",
      "Epoch 142: Batch 59/116 loss: 0.0742\n",
      "Epoch 142: Batch 60/116 loss: 0.0430\n",
      "Epoch 142: Batch 61/116 loss: 0.0515\n",
      "Epoch 142: Batch 62/116 loss: 0.0387\n",
      "Epoch 142: Batch 63/116 loss: 0.0320\n",
      "Epoch 142: Batch 64/116 loss: 0.0437\n",
      "Epoch 142: Batch 65/116 loss: 0.0413\n",
      "Epoch 142: Batch 66/116 loss: 0.0550\n",
      "Epoch 142: Batch 67/116 loss: 0.0415\n",
      "Epoch 142: Batch 68/116 loss: 0.0380\n",
      "Epoch 142: Batch 69/116 loss: 0.0457\n",
      "Epoch 142: Batch 70/116 loss: 0.0672\n",
      "Epoch 142: Batch 71/116 loss: 0.0580\n",
      "Epoch 142: Batch 72/116 loss: 0.0391\n",
      "Epoch 142: Batch 73/116 loss: 0.0630\n",
      "Epoch 142: Batch 74/116 loss: 0.0596\n",
      "Epoch 142: Batch 75/116 loss: 0.0522\n",
      "Epoch 142: Batch 76/116 loss: 0.0320\n",
      "Epoch 142: Batch 77/116 loss: 0.0564\n",
      "Epoch 142: Batch 78/116 loss: 0.0408\n",
      "Epoch 142: Batch 79/116 loss: 0.0492\n",
      "Epoch 142: Batch 80/116 loss: 0.0432\n",
      "Epoch 142: Batch 81/116 loss: 0.0659\n",
      "Epoch 142: Batch 82/116 loss: 0.0360\n",
      "Epoch 142: Batch 83/116 loss: 0.0467\n",
      "Epoch 142: Batch 84/116 loss: 0.0387\n",
      "Epoch 142: Batch 85/116 loss: 0.0390\n",
      "Epoch 142: Batch 86/116 loss: 0.0474\n",
      "Epoch 142: Batch 87/116 loss: 0.0446\n",
      "Epoch 142: Batch 88/116 loss: 0.0398\n",
      "Epoch 142: Batch 89/116 loss: 0.0276\n",
      "Epoch 142: Batch 90/116 loss: 0.0432\n",
      "Epoch 142: Batch 91/116 loss: 0.0252\n",
      "Epoch 142: Batch 92/116 loss: 0.0561\n",
      "Epoch 142: Batch 93/116 loss: 0.0654\n",
      "Epoch 142: Batch 94/116 loss: 0.0349\n",
      "Epoch 142: Batch 95/116 loss: 0.0582\n",
      "Epoch 142: Batch 96/116 loss: 0.0599\n",
      "Epoch 142: Batch 97/116 loss: 0.0367\n",
      "Epoch 142: Batch 98/116 loss: 0.0575\n",
      "Epoch 142: Batch 99/116 loss: 0.0382\n",
      "Epoch 142: Batch 100/116 loss: 0.0386\n",
      "Epoch 142: Batch 101/116 loss: 0.0420\n",
      "Epoch 142: Batch 102/116 loss: 0.0454\n",
      "Epoch 142: Batch 103/116 loss: 0.0569\n",
      "Epoch 142: Batch 104/116 loss: 0.0326\n",
      "Epoch 142: Batch 105/116 loss: 0.0420\n",
      "Epoch 142: Batch 106/116 loss: 0.0430\n",
      "Epoch 142: Batch 107/116 loss: 0.0601\n",
      "Epoch 142: Batch 108/116 loss: 0.0401\n",
      "Epoch 142: Batch 109/116 loss: 0.0565\n",
      "Epoch 142: Batch 110/116 loss: 0.0326\n",
      "Epoch 142: Batch 111/116 loss: 0.0473\n",
      "Epoch 142: Batch 112/116 loss: 0.0572\n",
      "Epoch 142: Batch 113/116 loss: 0.0604\n",
      "Epoch 142: Batch 114/116 loss: 0.0455\n",
      "Epoch 142: Batch 115/116 loss: 0.0491\n",
      "Epoch 142: Batch 116/116 loss: 0.0648\n",
      "Epoch 142 train loss: 0.0461 valid loss: 0.0524\n",
      "performance reducing: 17\n",
      "Epoch 143: Batch 1/116 loss: 0.0472\n",
      "Epoch 143: Batch 2/116 loss: 0.0559\n",
      "Epoch 143: Batch 3/116 loss: 0.0494\n",
      "Epoch 143: Batch 4/116 loss: 0.0575\n",
      "Epoch 143: Batch 5/116 loss: 0.0350\n",
      "Epoch 143: Batch 6/116 loss: 0.0331\n",
      "Epoch 143: Batch 7/116 loss: 0.0353\n",
      "Epoch 143: Batch 8/116 loss: 0.0544\n",
      "Epoch 143: Batch 9/116 loss: 0.0457\n",
      "Epoch 143: Batch 10/116 loss: 0.0627\n",
      "Epoch 143: Batch 11/116 loss: 0.0624\n",
      "Epoch 143: Batch 12/116 loss: 0.0543\n",
      "Epoch 143: Batch 13/116 loss: 0.0549\n",
      "Epoch 143: Batch 14/116 loss: 0.0450\n",
      "Epoch 143: Batch 15/116 loss: 0.0607\n",
      "Epoch 143: Batch 16/116 loss: 0.0575\n",
      "Epoch 143: Batch 17/116 loss: 0.0512\n",
      "Epoch 143: Batch 18/116 loss: 0.0564\n",
      "Epoch 143: Batch 19/116 loss: 0.0506\n",
      "Epoch 143: Batch 20/116 loss: 0.1436\n",
      "Epoch 143: Batch 21/116 loss: 0.0771\n",
      "Epoch 143: Batch 22/116 loss: 0.0707\n",
      "Epoch 143: Batch 23/116 loss: 0.0469\n",
      "Epoch 143: Batch 24/116 loss: 0.0296\n",
      "Epoch 143: Batch 25/116 loss: 0.0644\n",
      "Epoch 143: Batch 26/116 loss: 0.0493\n",
      "Epoch 143: Batch 27/116 loss: 0.0445\n",
      "Epoch 143: Batch 28/116 loss: 0.0486\n",
      "Epoch 143: Batch 29/116 loss: 0.0321\n",
      "Epoch 143: Batch 30/116 loss: 0.0538\n",
      "Epoch 143: Batch 31/116 loss: 0.0807\n",
      "Epoch 143: Batch 32/116 loss: 0.0574\n",
      "Epoch 143: Batch 33/116 loss: 0.0488\n",
      "Epoch 143: Batch 34/116 loss: 0.0313\n",
      "Epoch 143: Batch 35/116 loss: 0.0601\n",
      "Epoch 143: Batch 36/116 loss: 0.0304\n",
      "Epoch 143: Batch 37/116 loss: 0.0312\n",
      "Epoch 143: Batch 38/116 loss: 0.0459\n",
      "Epoch 143: Batch 39/116 loss: 0.0476\n",
      "Epoch 143: Batch 40/116 loss: 0.0367\n",
      "Epoch 143: Batch 41/116 loss: 0.0558\n",
      "Epoch 143: Batch 42/116 loss: 0.0460\n",
      "Epoch 143: Batch 43/116 loss: 0.0263\n",
      "Epoch 143: Batch 44/116 loss: 0.0656\n",
      "Epoch 143: Batch 45/116 loss: 0.0528\n",
      "Epoch 143: Batch 46/116 loss: 0.0741\n",
      "Epoch 143: Batch 47/116 loss: 0.0346\n",
      "Epoch 143: Batch 48/116 loss: 0.0419\n",
      "Epoch 143: Batch 49/116 loss: 0.0553\n",
      "Epoch 143: Batch 50/116 loss: 0.0399\n",
      "Epoch 143: Batch 51/116 loss: 0.0478\n",
      "Epoch 143: Batch 52/116 loss: 0.0460\n",
      "Epoch 143: Batch 53/116 loss: 0.0271\n",
      "Epoch 143: Batch 54/116 loss: 0.0592\n",
      "Epoch 143: Batch 55/116 loss: 0.0485\n",
      "Epoch 143: Batch 56/116 loss: 0.0688\n",
      "Epoch 143: Batch 57/116 loss: 0.0388\n",
      "Epoch 143: Batch 58/116 loss: 0.0504\n",
      "Epoch 143: Batch 59/116 loss: 0.0598\n",
      "Epoch 143: Batch 60/116 loss: 0.0386\n",
      "Epoch 143: Batch 61/116 loss: 0.0413\n",
      "Epoch 143: Batch 62/116 loss: 0.0610\n",
      "Epoch 143: Batch 63/116 loss: 0.0476\n",
      "Epoch 143: Batch 64/116 loss: 0.0395\n",
      "Epoch 143: Batch 65/116 loss: 0.0376\n",
      "Epoch 143: Batch 66/116 loss: 0.0302\n",
      "Epoch 143: Batch 67/116 loss: 0.0346\n",
      "Epoch 143: Batch 68/116 loss: 0.0422\n",
      "Epoch 143: Batch 69/116 loss: 0.0653\n",
      "Epoch 143: Batch 70/116 loss: 0.0327\n",
      "Epoch 143: Batch 71/116 loss: 0.0343\n",
      "Epoch 143: Batch 72/116 loss: 0.0452\n",
      "Epoch 143: Batch 73/116 loss: 0.0581\n",
      "Epoch 143: Batch 74/116 loss: 0.0549\n",
      "Epoch 143: Batch 75/116 loss: 0.0541\n",
      "Epoch 143: Batch 76/116 loss: 0.0390\n",
      "Epoch 143: Batch 77/116 loss: 0.0649\n",
      "Epoch 143: Batch 78/116 loss: 0.0449\n",
      "Epoch 143: Batch 79/116 loss: 0.0323\n",
      "Epoch 143: Batch 80/116 loss: 0.0383\n",
      "Epoch 143: Batch 81/116 loss: 0.0386\n",
      "Epoch 143: Batch 82/116 loss: 0.0480\n",
      "Epoch 143: Batch 83/116 loss: 0.0374\n",
      "Epoch 143: Batch 84/116 loss: 0.0322\n",
      "Epoch 143: Batch 85/116 loss: 0.0397\n",
      "Epoch 143: Batch 86/116 loss: 0.0465\n",
      "Epoch 143: Batch 87/116 loss: 0.0568\n",
      "Epoch 143: Batch 88/116 loss: 0.0445\n",
      "Epoch 143: Batch 89/116 loss: 0.0434\n",
      "Epoch 143: Batch 90/116 loss: 0.0549\n",
      "Epoch 143: Batch 91/116 loss: 0.0425\n",
      "Epoch 143: Batch 92/116 loss: 0.0332\n",
      "Epoch 143: Batch 93/116 loss: 0.0587\n",
      "Epoch 143: Batch 94/116 loss: 0.0249\n",
      "Epoch 143: Batch 95/116 loss: 0.0542\n",
      "Epoch 143: Batch 96/116 loss: 0.0501\n",
      "Epoch 143: Batch 97/116 loss: 0.0257\n",
      "Epoch 143: Batch 98/116 loss: 0.0558\n",
      "Epoch 143: Batch 99/116 loss: 0.0428\n",
      "Epoch 143: Batch 100/116 loss: 0.0331\n",
      "Epoch 143: Batch 101/116 loss: 0.0606\n",
      "Epoch 143: Batch 102/116 loss: 0.0465\n",
      "Epoch 143: Batch 103/116 loss: 0.0468\n",
      "Epoch 143: Batch 104/116 loss: 0.0370\n",
      "Epoch 143: Batch 105/116 loss: 0.0279\n",
      "Epoch 143: Batch 106/116 loss: 0.0562\n",
      "Epoch 143: Batch 107/116 loss: 0.0407\n",
      "Epoch 143: Batch 108/116 loss: 0.0296\n",
      "Epoch 143: Batch 109/116 loss: 0.0329\n",
      "Epoch 143: Batch 110/116 loss: 0.0294\n",
      "Epoch 143: Batch 111/116 loss: 0.0576\n",
      "Epoch 143: Batch 112/116 loss: 0.0405\n",
      "Epoch 143: Batch 113/116 loss: 0.0412\n",
      "Epoch 143: Batch 114/116 loss: 0.0311\n",
      "Epoch 143: Batch 115/116 loss: 0.0264\n",
      "Epoch 143: Batch 116/116 loss: 0.0605\n",
      "Epoch 143 train loss: 0.0474 valid loss: 0.0509\n",
      "Epoch 144: Batch 1/116 loss: 0.0373\n",
      "Epoch 144: Batch 2/116 loss: 0.0355\n",
      "Epoch 144: Batch 3/116 loss: 0.0384\n",
      "Epoch 144: Batch 4/116 loss: 0.0521\n",
      "Epoch 144: Batch 5/116 loss: 0.0453\n",
      "Epoch 144: Batch 6/116 loss: 0.0454\n",
      "Epoch 144: Batch 7/116 loss: 0.0387\n",
      "Epoch 144: Batch 8/116 loss: 0.0323\n",
      "Epoch 144: Batch 9/116 loss: 0.0382\n",
      "Epoch 144: Batch 10/116 loss: 0.0376\n",
      "Epoch 144: Batch 11/116 loss: 0.0281\n",
      "Epoch 144: Batch 12/116 loss: 0.0454\n",
      "Epoch 144: Batch 13/116 loss: 0.0538\n",
      "Epoch 144: Batch 14/116 loss: 0.0359\n",
      "Epoch 144: Batch 15/116 loss: 0.0468\n",
      "Epoch 144: Batch 16/116 loss: 0.0611\n",
      "Epoch 144: Batch 17/116 loss: 0.0255\n",
      "Epoch 144: Batch 18/116 loss: 0.0582\n",
      "Epoch 144: Batch 19/116 loss: 0.0366\n",
      "Epoch 144: Batch 20/116 loss: 0.0448\n",
      "Epoch 144: Batch 21/116 loss: 0.0337\n",
      "Epoch 144: Batch 22/116 loss: 0.0746\n",
      "Epoch 144: Batch 23/116 loss: 0.0415\n",
      "Epoch 144: Batch 24/116 loss: 0.0421\n",
      "Epoch 144: Batch 25/116 loss: 0.0496\n",
      "Epoch 144: Batch 26/116 loss: 0.0482\n",
      "Epoch 144: Batch 27/116 loss: 0.0587\n",
      "Epoch 144: Batch 28/116 loss: 0.0465\n",
      "Epoch 144: Batch 29/116 loss: 0.0427\n",
      "Epoch 144: Batch 30/116 loss: 0.0286\n",
      "Epoch 144: Batch 31/116 loss: 0.0448\n",
      "Epoch 144: Batch 32/116 loss: 0.0520\n",
      "Epoch 144: Batch 33/116 loss: 0.0453\n",
      "Epoch 144: Batch 34/116 loss: 0.0552\n",
      "Epoch 144: Batch 35/116 loss: 0.0420\n",
      "Epoch 144: Batch 36/116 loss: 0.0469\n",
      "Epoch 144: Batch 37/116 loss: 0.0390\n",
      "Epoch 144: Batch 38/116 loss: 0.0611\n",
      "Epoch 144: Batch 39/116 loss: 0.0542\n",
      "Epoch 144: Batch 40/116 loss: 0.0434\n",
      "Epoch 144: Batch 41/116 loss: 0.0460\n",
      "Epoch 144: Batch 42/116 loss: 0.0404\n",
      "Epoch 144: Batch 43/116 loss: 0.0650\n",
      "Epoch 144: Batch 44/116 loss: 0.0483\n",
      "Epoch 144: Batch 45/116 loss: 0.0467\n",
      "Epoch 144: Batch 46/116 loss: 0.0782\n",
      "Epoch 144: Batch 47/116 loss: 0.0322\n",
      "Epoch 144: Batch 48/116 loss: 0.0352\n",
      "Epoch 144: Batch 49/116 loss: 0.0468\n",
      "Epoch 144: Batch 50/116 loss: 0.0393\n",
      "Epoch 144: Batch 51/116 loss: 0.0498\n",
      "Epoch 144: Batch 52/116 loss: 0.0557\n",
      "Epoch 144: Batch 53/116 loss: 0.0567\n",
      "Epoch 144: Batch 54/116 loss: 0.0448\n",
      "Epoch 144: Batch 55/116 loss: 0.0389\n",
      "Epoch 144: Batch 56/116 loss: 0.0581\n",
      "Epoch 144: Batch 57/116 loss: 0.0397\n",
      "Epoch 144: Batch 58/116 loss: 0.0276\n",
      "Epoch 144: Batch 59/116 loss: 0.0621\n",
      "Epoch 144: Batch 60/116 loss: 0.0428\n",
      "Epoch 144: Batch 61/116 loss: 0.0488\n",
      "Epoch 144: Batch 62/116 loss: 0.0432\n",
      "Epoch 144: Batch 63/116 loss: 0.0345\n",
      "Epoch 144: Batch 64/116 loss: 0.0284\n",
      "Epoch 144: Batch 65/116 loss: 0.0493\n",
      "Epoch 144: Batch 66/116 loss: 0.0502\n",
      "Epoch 144: Batch 67/116 loss: 0.0381\n",
      "Epoch 144: Batch 68/116 loss: 0.0573\n",
      "Epoch 144: Batch 69/116 loss: 0.0442\n",
      "Epoch 144: Batch 70/116 loss: 0.0366\n",
      "Epoch 144: Batch 71/116 loss: 0.0371\n",
      "Epoch 144: Batch 72/116 loss: 0.0415\n",
      "Epoch 144: Batch 73/116 loss: 0.0352\n",
      "Epoch 144: Batch 74/116 loss: 0.0497\n",
      "Epoch 144: Batch 75/116 loss: 0.0378\n",
      "Epoch 144: Batch 76/116 loss: 0.0416\n",
      "Epoch 144: Batch 77/116 loss: 0.0416\n",
      "Epoch 144: Batch 78/116 loss: 0.0451\n",
      "Epoch 144: Batch 79/116 loss: 0.0511\n",
      "Epoch 144: Batch 80/116 loss: 0.0457\n",
      "Epoch 144: Batch 81/116 loss: 0.0439\n",
      "Epoch 144: Batch 82/116 loss: 0.0491\n",
      "Epoch 144: Batch 83/116 loss: 0.0263\n",
      "Epoch 144: Batch 84/116 loss: 0.0396\n",
      "Epoch 144: Batch 85/116 loss: 0.0687\n",
      "Epoch 144: Batch 86/116 loss: 0.0271\n",
      "Epoch 144: Batch 87/116 loss: 0.0504\n",
      "Epoch 144: Batch 88/116 loss: 0.0332\n",
      "Epoch 144: Batch 89/116 loss: 0.0364\n",
      "Epoch 144: Batch 90/116 loss: 0.0528\n",
      "Epoch 144: Batch 91/116 loss: 0.0447\n",
      "Epoch 144: Batch 92/116 loss: 0.0356\n",
      "Epoch 144: Batch 93/116 loss: 0.0487\n",
      "Epoch 144: Batch 94/116 loss: 0.0452\n",
      "Epoch 144: Batch 95/116 loss: 0.0305\n",
      "Epoch 144: Batch 96/116 loss: 0.0365\n",
      "Epoch 144: Batch 97/116 loss: 0.0493\n",
      "Epoch 144: Batch 98/116 loss: 0.0557\n",
      "Epoch 144: Batch 99/116 loss: 0.0502\n",
      "Epoch 144: Batch 100/116 loss: 0.0529\n",
      "Epoch 144: Batch 101/116 loss: 0.0481\n",
      "Epoch 144: Batch 102/116 loss: 0.0485\n",
      "Epoch 144: Batch 103/116 loss: 0.0495\n",
      "Epoch 144: Batch 104/116 loss: 0.0453\n",
      "Epoch 144: Batch 105/116 loss: 0.0669\n",
      "Epoch 144: Batch 106/116 loss: 0.0332\n",
      "Epoch 144: Batch 107/116 loss: 0.0588\n",
      "Epoch 144: Batch 108/116 loss: 0.0677\n",
      "Epoch 144: Batch 109/116 loss: 0.0577\n",
      "Epoch 144: Batch 110/116 loss: 0.0442\n",
      "Epoch 144: Batch 111/116 loss: 0.0603\n",
      "Epoch 144: Batch 112/116 loss: 0.0476\n",
      "Epoch 144: Batch 113/116 loss: 0.0491\n",
      "Epoch 144: Batch 114/116 loss: 0.0392\n",
      "Epoch 144: Batch 115/116 loss: 0.0350\n",
      "Epoch 144: Batch 116/116 loss: 0.0359\n",
      "Epoch 144 train loss: 0.0454 valid loss: 0.0499\n",
      "Epoch 145: Batch 1/116 loss: 0.0426\n",
      "Epoch 145: Batch 2/116 loss: 0.0424\n",
      "Epoch 145: Batch 3/116 loss: 0.0462\n",
      "Epoch 145: Batch 4/116 loss: 0.0455\n",
      "Epoch 145: Batch 5/116 loss: 0.0315\n",
      "Epoch 145: Batch 6/116 loss: 0.0550\n",
      "Epoch 145: Batch 7/116 loss: 0.0527\n",
      "Epoch 145: Batch 8/116 loss: 0.0450\n",
      "Epoch 145: Batch 9/116 loss: 0.0350\n",
      "Epoch 145: Batch 10/116 loss: 0.0481\n",
      "Epoch 145: Batch 11/116 loss: 0.0554\n",
      "Epoch 145: Batch 12/116 loss: 0.0401\n",
      "Epoch 145: Batch 13/116 loss: 0.0598\n",
      "Epoch 145: Batch 14/116 loss: 0.0296\n",
      "Epoch 145: Batch 15/116 loss: 0.0360\n",
      "Epoch 145: Batch 16/116 loss: 0.0452\n",
      "Epoch 145: Batch 17/116 loss: 0.0411\n",
      "Epoch 145: Batch 18/116 loss: 0.0388\n",
      "Epoch 145: Batch 19/116 loss: 0.0515\n",
      "Epoch 145: Batch 20/116 loss: 0.0479\n",
      "Epoch 145: Batch 21/116 loss: 0.0549\n",
      "Epoch 145: Batch 22/116 loss: 0.0547\n",
      "Epoch 145: Batch 23/116 loss: 0.0440\n",
      "Epoch 145: Batch 24/116 loss: 0.0414\n",
      "Epoch 145: Batch 25/116 loss: 0.0492\n",
      "Epoch 145: Batch 26/116 loss: 0.0391\n",
      "Epoch 145: Batch 27/116 loss: 0.0453\n",
      "Epoch 145: Batch 28/116 loss: 0.0406\n",
      "Epoch 145: Batch 29/116 loss: 0.0642\n",
      "Epoch 145: Batch 30/116 loss: 0.0585\n",
      "Epoch 145: Batch 31/116 loss: 0.0311\n",
      "Epoch 145: Batch 32/116 loss: 0.0489\n",
      "Epoch 145: Batch 33/116 loss: 0.0685\n",
      "Epoch 145: Batch 34/116 loss: 0.0539\n",
      "Epoch 145: Batch 35/116 loss: 0.0411\n",
      "Epoch 145: Batch 36/116 loss: 0.0452\n",
      "Epoch 145: Batch 37/116 loss: 0.0423\n",
      "Epoch 145: Batch 38/116 loss: 0.0609\n",
      "Epoch 145: Batch 39/116 loss: 0.0395\n",
      "Epoch 145: Batch 40/116 loss: 0.0366\n",
      "Epoch 145: Batch 41/116 loss: 0.0350\n",
      "Epoch 145: Batch 42/116 loss: 0.0346\n",
      "Epoch 145: Batch 43/116 loss: 0.0477\n",
      "Epoch 145: Batch 44/116 loss: 0.0402\n",
      "Epoch 145: Batch 45/116 loss: 0.0520\n",
      "Epoch 145: Batch 46/116 loss: 0.0344\n",
      "Epoch 145: Batch 47/116 loss: 0.0564\n",
      "Epoch 145: Batch 48/116 loss: 0.0470\n",
      "Epoch 145: Batch 49/116 loss: 0.0455\n",
      "Epoch 145: Batch 50/116 loss: 0.0360\n",
      "Epoch 145: Batch 51/116 loss: 0.0413\n",
      "Epoch 145: Batch 52/116 loss: 0.0464\n",
      "Epoch 145: Batch 53/116 loss: 0.0268\n",
      "Epoch 145: Batch 54/116 loss: 0.0442\n",
      "Epoch 145: Batch 55/116 loss: 0.0403\n",
      "Epoch 145: Batch 56/116 loss: 0.0526\n",
      "Epoch 145: Batch 57/116 loss: 0.0426\n",
      "Epoch 145: Batch 58/116 loss: 0.0481\n",
      "Epoch 145: Batch 59/116 loss: 0.0547\n",
      "Epoch 145: Batch 60/116 loss: 0.0576\n",
      "Epoch 145: Batch 61/116 loss: 0.0495\n",
      "Epoch 145: Batch 62/116 loss: 0.0426\n",
      "Epoch 145: Batch 63/116 loss: 0.0635\n",
      "Epoch 145: Batch 64/116 loss: 0.0623\n",
      "Epoch 145: Batch 65/116 loss: 0.0535\n",
      "Epoch 145: Batch 66/116 loss: 0.0441\n",
      "Epoch 145: Batch 67/116 loss: 0.0425\n",
      "Epoch 145: Batch 68/116 loss: 0.0396\n",
      "Epoch 145: Batch 69/116 loss: 0.0381\n",
      "Epoch 145: Batch 70/116 loss: 0.0562\n",
      "Epoch 145: Batch 71/116 loss: 0.0378\n",
      "Epoch 145: Batch 72/116 loss: 0.0342\n",
      "Epoch 145: Batch 73/116 loss: 0.0350\n",
      "Epoch 145: Batch 74/116 loss: 0.0262\n",
      "Epoch 145: Batch 75/116 loss: 0.0439\n",
      "Epoch 145: Batch 76/116 loss: 0.0442\n",
      "Epoch 145: Batch 77/116 loss: 0.0249\n",
      "Epoch 145: Batch 78/116 loss: 0.0472\n",
      "Epoch 145: Batch 79/116 loss: 0.0391\n",
      "Epoch 145: Batch 80/116 loss: 0.0397\n",
      "Epoch 145: Batch 81/116 loss: 0.0336\n",
      "Epoch 145: Batch 82/116 loss: 0.0635\n",
      "Epoch 145: Batch 83/116 loss: 0.0382\n",
      "Epoch 145: Batch 84/116 loss: 0.0265\n",
      "Epoch 145: Batch 85/116 loss: 0.0381\n",
      "Epoch 145: Batch 86/116 loss: 0.0293\n",
      "Epoch 145: Batch 87/116 loss: 0.0337\n",
      "Epoch 145: Batch 88/116 loss: 0.0486\n",
      "Epoch 145: Batch 89/116 loss: 0.0402\n",
      "Epoch 145: Batch 90/116 loss: 0.0353\n",
      "Epoch 145: Batch 91/116 loss: 0.0557\n",
      "Epoch 145: Batch 92/116 loss: 0.0425\n",
      "Epoch 145: Batch 93/116 loss: 0.0333\n",
      "Epoch 145: Batch 94/116 loss: 0.0486\n",
      "Epoch 145: Batch 95/116 loss: 0.0558\n",
      "Epoch 145: Batch 96/116 loss: 0.0786\n",
      "Epoch 145: Batch 97/116 loss: 0.0466\n",
      "Epoch 145: Batch 98/116 loss: 0.0413\n",
      "Epoch 145: Batch 99/116 loss: 0.0387\n",
      "Epoch 145: Batch 100/116 loss: 0.0347\n",
      "Epoch 145: Batch 101/116 loss: 0.0605\n",
      "Epoch 145: Batch 102/116 loss: 0.0493\n",
      "Epoch 145: Batch 103/116 loss: 0.0531\n",
      "Epoch 145: Batch 104/116 loss: 0.0516\n",
      "Epoch 145: Batch 105/116 loss: 0.0359\n",
      "Epoch 145: Batch 106/116 loss: 0.0268\n",
      "Epoch 145: Batch 107/116 loss: 0.0331\n",
      "Epoch 145: Batch 108/116 loss: 0.0493\n",
      "Epoch 145: Batch 109/116 loss: 0.0687\n",
      "Epoch 145: Batch 110/116 loss: 0.0306\n",
      "Epoch 145: Batch 111/116 loss: 0.0540\n",
      "Epoch 145: Batch 112/116 loss: 0.0455\n",
      "Epoch 145: Batch 113/116 loss: 0.0480\n",
      "Epoch 145: Batch 114/116 loss: 0.0515\n",
      "Epoch 145: Batch 115/116 loss: 0.0378\n",
      "Epoch 145: Batch 116/116 loss: 0.0526\n",
      "Epoch 145 train loss: 0.0449 valid loss: 0.0738\n",
      "performance reducing: 1\n",
      "Epoch 146: Batch 1/116 loss: 0.0620\n",
      "Epoch 146: Batch 2/116 loss: 0.0531\n",
      "Epoch 146: Batch 3/116 loss: 0.0555\n",
      "Epoch 146: Batch 4/116 loss: 0.0558\n",
      "Epoch 146: Batch 5/116 loss: 0.0665\n",
      "Epoch 146: Batch 6/116 loss: 0.0487\n",
      "Epoch 146: Batch 7/116 loss: 0.0413\n",
      "Epoch 146: Batch 8/116 loss: 0.0519\n",
      "Epoch 146: Batch 9/116 loss: 0.0372\n",
      "Epoch 146: Batch 10/116 loss: 0.0401\n",
      "Epoch 146: Batch 11/116 loss: 0.0353\n",
      "Epoch 146: Batch 12/116 loss: 0.0409\n",
      "Epoch 146: Batch 13/116 loss: 0.0437\n",
      "Epoch 146: Batch 14/116 loss: 0.0506\n",
      "Epoch 146: Batch 15/116 loss: 0.0445\n",
      "Epoch 146: Batch 16/116 loss: 0.0472\n",
      "Epoch 146: Batch 17/116 loss: 0.0538\n",
      "Epoch 146: Batch 18/116 loss: 0.0506\n",
      "Epoch 146: Batch 19/116 loss: 0.0642\n",
      "Epoch 146: Batch 20/116 loss: 0.0501\n",
      "Epoch 146: Batch 21/116 loss: 0.0487\n",
      "Epoch 146: Batch 22/116 loss: 0.0408\n",
      "Epoch 146: Batch 23/116 loss: 0.0460\n",
      "Epoch 146: Batch 24/116 loss: 0.0544\n",
      "Epoch 146: Batch 25/116 loss: 0.0330\n",
      "Epoch 146: Batch 26/116 loss: 0.0371\n",
      "Epoch 146: Batch 27/116 loss: 0.0379\n",
      "Epoch 146: Batch 28/116 loss: 0.0555\n",
      "Epoch 146: Batch 29/116 loss: 0.0369\n",
      "Epoch 146: Batch 30/116 loss: 0.0518\n",
      "Epoch 146: Batch 31/116 loss: 0.0455\n",
      "Epoch 146: Batch 32/116 loss: 0.0559\n",
      "Epoch 146: Batch 33/116 loss: 0.0275\n",
      "Epoch 146: Batch 34/116 loss: 0.0404\n",
      "Epoch 146: Batch 35/116 loss: 0.0356\n",
      "Epoch 146: Batch 36/116 loss: 0.0323\n",
      "Epoch 146: Batch 37/116 loss: 0.0434\n",
      "Epoch 146: Batch 38/116 loss: 0.0418\n",
      "Epoch 146: Batch 39/116 loss: 0.0617\n",
      "Epoch 146: Batch 40/116 loss: 0.0483\n",
      "Epoch 146: Batch 41/116 loss: 0.0383\n",
      "Epoch 146: Batch 42/116 loss: 0.0564\n",
      "Epoch 146: Batch 43/116 loss: 0.0618\n",
      "Epoch 146: Batch 44/116 loss: 0.0387\n",
      "Epoch 146: Batch 45/116 loss: 0.0454\n",
      "Epoch 146: Batch 46/116 loss: 0.0550\n",
      "Epoch 146: Batch 47/116 loss: 0.0282\n",
      "Epoch 146: Batch 48/116 loss: 0.0358\n",
      "Epoch 146: Batch 49/116 loss: 0.0417\n",
      "Epoch 146: Batch 50/116 loss: 0.0402\n",
      "Epoch 146: Batch 51/116 loss: 0.0519\n",
      "Epoch 146: Batch 52/116 loss: 0.0626\n",
      "Epoch 146: Batch 53/116 loss: 0.0611\n",
      "Epoch 146: Batch 54/116 loss: 0.0500\n",
      "Epoch 146: Batch 55/116 loss: 0.0419\n",
      "Epoch 146: Batch 56/116 loss: 0.0490\n",
      "Epoch 146: Batch 57/116 loss: 0.0371\n",
      "Epoch 146: Batch 58/116 loss: 0.0517\n",
      "Epoch 146: Batch 59/116 loss: 0.0451\n",
      "Epoch 146: Batch 60/116 loss: 0.0383\n",
      "Epoch 146: Batch 61/116 loss: 0.0328\n",
      "Epoch 146: Batch 62/116 loss: 0.0519\n",
      "Epoch 146: Batch 63/116 loss: 0.0463\n",
      "Epoch 146: Batch 64/116 loss: 0.0702\n",
      "Epoch 146: Batch 65/116 loss: 0.0435\n",
      "Epoch 146: Batch 66/116 loss: 0.0377\n",
      "Epoch 146: Batch 67/116 loss: 0.0381\n",
      "Epoch 146: Batch 68/116 loss: 0.0453\n",
      "Epoch 146: Batch 69/116 loss: 0.0448\n",
      "Epoch 146: Batch 70/116 loss: 0.0473\n",
      "Epoch 146: Batch 71/116 loss: 0.0428\n",
      "Epoch 146: Batch 72/116 loss: 0.0526\n",
      "Epoch 146: Batch 73/116 loss: 0.0465\n",
      "Epoch 146: Batch 74/116 loss: 0.0357\n",
      "Epoch 146: Batch 75/116 loss: 0.0402\n",
      "Epoch 146: Batch 76/116 loss: 0.0449\n",
      "Epoch 146: Batch 77/116 loss: 0.0713\n",
      "Epoch 146: Batch 78/116 loss: 0.0299\n",
      "Epoch 146: Batch 79/116 loss: 0.0429\n",
      "Epoch 146: Batch 80/116 loss: 0.0422\n",
      "Epoch 146: Batch 81/116 loss: 0.0359\n",
      "Epoch 146: Batch 82/116 loss: 0.0347\n",
      "Epoch 146: Batch 83/116 loss: 0.0448\n",
      "Epoch 146: Batch 84/116 loss: 0.0367\n",
      "Epoch 146: Batch 85/116 loss: 0.0570\n",
      "Epoch 146: Batch 86/116 loss: 0.0465\n",
      "Epoch 146: Batch 87/116 loss: 0.0598\n",
      "Epoch 146: Batch 88/116 loss: 0.0299\n",
      "Epoch 146: Batch 89/116 loss: 0.0295\n",
      "Epoch 146: Batch 90/116 loss: 0.0672\n",
      "Epoch 146: Batch 91/116 loss: 0.0425\n",
      "Epoch 146: Batch 92/116 loss: 0.0621\n",
      "Epoch 146: Batch 93/116 loss: 0.0517\n",
      "Epoch 146: Batch 94/116 loss: 0.0514\n",
      "Epoch 146: Batch 95/116 loss: 0.0429\n",
      "Epoch 146: Batch 96/116 loss: 0.0499\n",
      "Epoch 146: Batch 97/116 loss: 0.0448\n",
      "Epoch 146: Batch 98/116 loss: 0.0390\n",
      "Epoch 146: Batch 99/116 loss: 0.0561\n",
      "Epoch 146: Batch 100/116 loss: 0.0595\n",
      "Epoch 146: Batch 101/116 loss: 0.0216\n",
      "Epoch 146: Batch 102/116 loss: 0.0471\n",
      "Epoch 146: Batch 103/116 loss: 0.0487\n",
      "Epoch 146: Batch 104/116 loss: 0.0499\n",
      "Epoch 146: Batch 105/116 loss: 0.0442\n",
      "Epoch 146: Batch 106/116 loss: 0.0396\n",
      "Epoch 146: Batch 107/116 loss: 0.0417\n",
      "Epoch 146: Batch 108/116 loss: 0.0395\n",
      "Epoch 146: Batch 109/116 loss: 0.0426\n",
      "Epoch 146: Batch 110/116 loss: 0.0473\n",
      "Epoch 146: Batch 111/116 loss: 0.0365\n",
      "Epoch 146: Batch 112/116 loss: 0.0317\n",
      "Epoch 146: Batch 113/116 loss: 0.0441\n",
      "Epoch 146: Batch 114/116 loss: 0.0312\n",
      "Epoch 146: Batch 115/116 loss: 0.0505\n",
      "Epoch 146: Batch 116/116 loss: 0.0655\n",
      "Epoch 146 train loss: 0.0459 valid loss: 0.0539\n",
      "performance reducing: 2\n",
      "Epoch 147: Batch 1/116 loss: 0.0313\n",
      "Epoch 147: Batch 2/116 loss: 0.0313\n",
      "Epoch 147: Batch 3/116 loss: 0.0468\n",
      "Epoch 147: Batch 4/116 loss: 0.0472\n",
      "Epoch 147: Batch 5/116 loss: 0.0380\n",
      "Epoch 147: Batch 6/116 loss: 0.0448\n",
      "Epoch 147: Batch 7/116 loss: 0.0453\n",
      "Epoch 147: Batch 8/116 loss: 0.0476\n",
      "Epoch 147: Batch 9/116 loss: 0.0314\n",
      "Epoch 147: Batch 10/116 loss: 0.0460\n",
      "Epoch 147: Batch 11/116 loss: 0.0376\n",
      "Epoch 147: Batch 12/116 loss: 0.0554\n",
      "Epoch 147: Batch 13/116 loss: 0.0610\n",
      "Epoch 147: Batch 14/116 loss: 0.0332\n",
      "Epoch 147: Batch 15/116 loss: 0.0544\n",
      "Epoch 147: Batch 16/116 loss: 0.0369\n",
      "Epoch 147: Batch 17/116 loss: 0.0506\n",
      "Epoch 147: Batch 18/116 loss: 0.0541\n",
      "Epoch 147: Batch 19/116 loss: 0.0271\n",
      "Epoch 147: Batch 20/116 loss: 0.0342\n",
      "Epoch 147: Batch 21/116 loss: 0.0543\n",
      "Epoch 147: Batch 22/116 loss: 0.0656\n",
      "Epoch 147: Batch 23/116 loss: 0.0400\n",
      "Epoch 147: Batch 24/116 loss: 0.0387\n",
      "Epoch 147: Batch 25/116 loss: 0.0393\n",
      "Epoch 147: Batch 26/116 loss: 0.0650\n",
      "Epoch 147: Batch 27/116 loss: 0.0387\n",
      "Epoch 147: Batch 28/116 loss: 0.0487\n",
      "Epoch 147: Batch 29/116 loss: 0.0418\n",
      "Epoch 147: Batch 30/116 loss: 0.0642\n",
      "Epoch 147: Batch 31/116 loss: 0.0294\n",
      "Epoch 147: Batch 32/116 loss: 0.0345\n",
      "Epoch 147: Batch 33/116 loss: 0.0362\n",
      "Epoch 147: Batch 34/116 loss: 0.0393\n",
      "Epoch 147: Batch 35/116 loss: 0.0578\n",
      "Epoch 147: Batch 36/116 loss: 0.0487\n",
      "Epoch 147: Batch 37/116 loss: 0.0531\n",
      "Epoch 147: Batch 38/116 loss: 0.0420\n",
      "Epoch 147: Batch 39/116 loss: 0.0287\n",
      "Epoch 147: Batch 40/116 loss: 0.0318\n",
      "Epoch 147: Batch 41/116 loss: 0.0312\n",
      "Epoch 147: Batch 42/116 loss: 0.0615\n",
      "Epoch 147: Batch 43/116 loss: 0.0422\n",
      "Epoch 147: Batch 44/116 loss: 0.0626\n",
      "Epoch 147: Batch 45/116 loss: 0.0568\n",
      "Epoch 147: Batch 46/116 loss: 0.0302\n",
      "Epoch 147: Batch 47/116 loss: 0.0368\n",
      "Epoch 147: Batch 48/116 loss: 0.0504\n",
      "Epoch 147: Batch 49/116 loss: 0.0444\n",
      "Epoch 147: Batch 50/116 loss: 0.0400\n",
      "Epoch 147: Batch 51/116 loss: 0.0437\n",
      "Epoch 147: Batch 52/116 loss: 0.0383\n",
      "Epoch 147: Batch 53/116 loss: 0.0453\n",
      "Epoch 147: Batch 54/116 loss: 0.0474\n",
      "Epoch 147: Batch 55/116 loss: 0.0479\n",
      "Epoch 147: Batch 56/116 loss: 0.0334\n",
      "Epoch 147: Batch 57/116 loss: 0.0451\n",
      "Epoch 147: Batch 58/116 loss: 0.0600\n",
      "Epoch 147: Batch 59/116 loss: 0.0304\n",
      "Epoch 147: Batch 60/116 loss: 0.0554\n",
      "Epoch 147: Batch 61/116 loss: 0.0456\n",
      "Epoch 147: Batch 62/116 loss: 0.0324\n",
      "Epoch 147: Batch 63/116 loss: 0.0530\n",
      "Epoch 147: Batch 64/116 loss: 0.0411\n",
      "Epoch 147: Batch 65/116 loss: 0.0495\n",
      "Epoch 147: Batch 66/116 loss: 0.0382\n",
      "Epoch 147: Batch 67/116 loss: 0.0621\n",
      "Epoch 147: Batch 68/116 loss: 0.0440\n",
      "Epoch 147: Batch 69/116 loss: 0.0492\n",
      "Epoch 147: Batch 70/116 loss: 0.0429\n",
      "Epoch 147: Batch 71/116 loss: 0.0354\n",
      "Epoch 147: Batch 72/116 loss: 0.0428\n",
      "Epoch 147: Batch 73/116 loss: 0.0354\n",
      "Epoch 147: Batch 74/116 loss: 0.0631\n",
      "Epoch 147: Batch 75/116 loss: 0.0422\n",
      "Epoch 147: Batch 76/116 loss: 0.0517\n",
      "Epoch 147: Batch 77/116 loss: 0.0359\n",
      "Epoch 147: Batch 78/116 loss: 0.0523\n",
      "Epoch 147: Batch 79/116 loss: 0.0728\n",
      "Epoch 147: Batch 80/116 loss: 0.0544\n",
      "Epoch 147: Batch 81/116 loss: 0.0374\n",
      "Epoch 147: Batch 82/116 loss: 0.0538\n",
      "Epoch 147: Batch 83/116 loss: 0.0455\n",
      "Epoch 147: Batch 84/116 loss: 0.0312\n",
      "Epoch 147: Batch 85/116 loss: 0.0276\n",
      "Epoch 147: Batch 86/116 loss: 0.0554\n",
      "Epoch 147: Batch 87/116 loss: 0.0568\n",
      "Epoch 147: Batch 88/116 loss: 0.0444\n",
      "Epoch 147: Batch 89/116 loss: 0.0686\n",
      "Epoch 147: Batch 90/116 loss: 0.0341\n",
      "Epoch 147: Batch 91/116 loss: 0.0516\n",
      "Epoch 147: Batch 92/116 loss: 0.0392\n",
      "Epoch 147: Batch 93/116 loss: 0.0424\n",
      "Epoch 147: Batch 94/116 loss: 0.0426\n",
      "Epoch 147: Batch 95/116 loss: 0.0282\n",
      "Epoch 147: Batch 96/116 loss: 0.0240\n",
      "Epoch 147: Batch 97/116 loss: 0.0441\n",
      "Epoch 147: Batch 98/116 loss: 0.0473\n",
      "Epoch 147: Batch 99/116 loss: 0.0638\n",
      "Epoch 147: Batch 100/116 loss: 0.0503\n",
      "Epoch 147: Batch 101/116 loss: 0.0358\n",
      "Epoch 147: Batch 102/116 loss: 0.0478\n",
      "Epoch 147: Batch 103/116 loss: 0.0498\n",
      "Epoch 147: Batch 104/116 loss: 0.0442\n",
      "Epoch 147: Batch 105/116 loss: 0.0264\n",
      "Epoch 147: Batch 106/116 loss: 0.0469\n",
      "Epoch 147: Batch 107/116 loss: 0.0423\n",
      "Epoch 147: Batch 108/116 loss: 0.0483\n",
      "Epoch 147: Batch 109/116 loss: 0.0348\n",
      "Epoch 147: Batch 110/116 loss: 0.0528\n",
      "Epoch 147: Batch 111/116 loss: 0.0450\n",
      "Epoch 147: Batch 112/116 loss: 0.0468\n",
      "Epoch 147: Batch 113/116 loss: 0.0601\n",
      "Epoch 147: Batch 114/116 loss: 0.0452\n",
      "Epoch 147: Batch 115/116 loss: 0.0403\n",
      "Epoch 147: Batch 116/116 loss: 0.0417\n",
      "Epoch 147 train loss: 0.0448 valid loss: 0.0523\n",
      "performance reducing: 3\n",
      "Epoch 148: Batch 1/116 loss: 0.0575\n",
      "Epoch 148: Batch 2/116 loss: 0.0345\n",
      "Epoch 148: Batch 3/116 loss: 0.0412\n",
      "Epoch 148: Batch 4/116 loss: 0.0508\n",
      "Epoch 148: Batch 5/116 loss: 0.0527\n",
      "Epoch 148: Batch 6/116 loss: 0.0409\n",
      "Epoch 148: Batch 7/116 loss: 0.0608\n",
      "Epoch 148: Batch 8/116 loss: 0.0612\n",
      "Epoch 148: Batch 9/116 loss: 0.0455\n",
      "Epoch 148: Batch 10/116 loss: 0.0411\n",
      "Epoch 148: Batch 11/116 loss: 0.0371\n",
      "Epoch 148: Batch 12/116 loss: 0.0353\n",
      "Epoch 148: Batch 13/116 loss: 0.0472\n",
      "Epoch 148: Batch 14/116 loss: 0.0380\n",
      "Epoch 148: Batch 15/116 loss: 0.0413\n",
      "Epoch 148: Batch 16/116 loss: 0.0392\n",
      "Epoch 148: Batch 17/116 loss: 0.0586\n",
      "Epoch 148: Batch 18/116 loss: 0.0338\n",
      "Epoch 148: Batch 19/116 loss: 0.0438\n",
      "Epoch 148: Batch 20/116 loss: 0.0367\n",
      "Epoch 148: Batch 21/116 loss: 0.0271\n",
      "Epoch 148: Batch 22/116 loss: 0.0546\n",
      "Epoch 148: Batch 23/116 loss: 0.0898\n",
      "Epoch 148: Batch 24/116 loss: 0.0473\n",
      "Epoch 148: Batch 25/116 loss: 0.0358\n",
      "Epoch 148: Batch 26/116 loss: 0.0351\n",
      "Epoch 148: Batch 27/116 loss: 0.0418\n",
      "Epoch 148: Batch 28/116 loss: 0.0602\n",
      "Epoch 148: Batch 29/116 loss: 0.0661\n",
      "Epoch 148: Batch 30/116 loss: 0.0407\n",
      "Epoch 148: Batch 31/116 loss: 0.0492\n",
      "Epoch 148: Batch 32/116 loss: 0.0310\n",
      "Epoch 148: Batch 33/116 loss: 0.0588\n",
      "Epoch 148: Batch 34/116 loss: 0.0465\n",
      "Epoch 148: Batch 35/116 loss: 0.0389\n",
      "Epoch 148: Batch 36/116 loss: 0.0428\n",
      "Epoch 148: Batch 37/116 loss: 0.0273\n",
      "Epoch 148: Batch 38/116 loss: 0.0378\n",
      "Epoch 148: Batch 39/116 loss: 0.0451\n",
      "Epoch 148: Batch 40/116 loss: 0.0401\n",
      "Epoch 148: Batch 41/116 loss: 0.0413\n",
      "Epoch 148: Batch 42/116 loss: 0.0408\n",
      "Epoch 148: Batch 43/116 loss: 0.0556\n",
      "Epoch 148: Batch 44/116 loss: 0.0416\n",
      "Epoch 148: Batch 45/116 loss: 0.0433\n",
      "Epoch 148: Batch 46/116 loss: 0.0293\n",
      "Epoch 148: Batch 47/116 loss: 0.0441\n",
      "Epoch 148: Batch 48/116 loss: 0.0298\n",
      "Epoch 148: Batch 49/116 loss: 0.0357\n",
      "Epoch 148: Batch 50/116 loss: 0.0564\n",
      "Epoch 148: Batch 51/116 loss: 0.0394\n",
      "Epoch 148: Batch 52/116 loss: 0.0366\n",
      "Epoch 148: Batch 53/116 loss: 0.0259\n",
      "Epoch 148: Batch 54/116 loss: 0.0414\n",
      "Epoch 148: Batch 55/116 loss: 0.0352\n",
      "Epoch 148: Batch 56/116 loss: 0.0513\n",
      "Epoch 148: Batch 57/116 loss: 0.0302\n",
      "Epoch 148: Batch 58/116 loss: 0.0326\n",
      "Epoch 148: Batch 59/116 loss: 0.0524\n",
      "Epoch 148: Batch 60/116 loss: 0.0686\n",
      "Epoch 148: Batch 61/116 loss: 0.0442\n",
      "Epoch 148: Batch 62/116 loss: 0.0475\n",
      "Epoch 148: Batch 63/116 loss: 0.0362\n",
      "Epoch 148: Batch 64/116 loss: 0.0224\n",
      "Epoch 148: Batch 65/116 loss: 0.0374\n",
      "Epoch 148: Batch 66/116 loss: 0.0369\n",
      "Epoch 148: Batch 67/116 loss: 0.0556\n",
      "Epoch 148: Batch 68/116 loss: 0.0409\n",
      "Epoch 148: Batch 69/116 loss: 0.0288\n",
      "Epoch 148: Batch 70/116 loss: 0.0395\n",
      "Epoch 148: Batch 71/116 loss: 0.0376\n",
      "Epoch 148: Batch 72/116 loss: 0.0469\n",
      "Epoch 148: Batch 73/116 loss: 0.0395\n",
      "Epoch 148: Batch 74/116 loss: 0.0428\n",
      "Epoch 148: Batch 75/116 loss: 0.0398\n",
      "Epoch 148: Batch 76/116 loss: 0.0560\n",
      "Epoch 148: Batch 77/116 loss: 0.0390\n",
      "Epoch 148: Batch 78/116 loss: 0.0219\n",
      "Epoch 148: Batch 79/116 loss: 0.0360\n",
      "Epoch 148: Batch 80/116 loss: 0.0370\n",
      "Epoch 148: Batch 81/116 loss: 0.0476\n",
      "Epoch 148: Batch 82/116 loss: 0.0381\n",
      "Epoch 148: Batch 83/116 loss: 0.0424\n",
      "Epoch 148: Batch 84/116 loss: 0.0269\n",
      "Epoch 148: Batch 85/116 loss: 0.0310\n",
      "Epoch 148: Batch 86/116 loss: 0.0343\n",
      "Epoch 148: Batch 87/116 loss: 0.0478\n",
      "Epoch 148: Batch 88/116 loss: 0.0558\n",
      "Epoch 148: Batch 89/116 loss: 0.0412\n",
      "Epoch 148: Batch 90/116 loss: 0.0720\n",
      "Epoch 148: Batch 91/116 loss: 0.0530\n",
      "Epoch 148: Batch 92/116 loss: 0.0446\n",
      "Epoch 148: Batch 93/116 loss: 0.0383\n",
      "Epoch 148: Batch 94/116 loss: 0.0414\n",
      "Epoch 148: Batch 95/116 loss: 0.0350\n",
      "Epoch 148: Batch 96/116 loss: 0.0425\n",
      "Epoch 148: Batch 97/116 loss: 0.0991\n",
      "Epoch 148: Batch 98/116 loss: 0.0426\n",
      "Epoch 148: Batch 99/116 loss: 0.0406\n",
      "Epoch 148: Batch 100/116 loss: 0.0498\n",
      "Epoch 148: Batch 101/116 loss: 0.0437\n",
      "Epoch 148: Batch 102/116 loss: 0.0543\n",
      "Epoch 148: Batch 103/116 loss: 0.0430\n",
      "Epoch 148: Batch 104/116 loss: 0.0522\n",
      "Epoch 148: Batch 105/116 loss: 0.0497\n",
      "Epoch 148: Batch 106/116 loss: 0.0326\n",
      "Epoch 148: Batch 107/116 loss: 0.0487\n",
      "Epoch 148: Batch 108/116 loss: 0.0386\n",
      "Epoch 148: Batch 109/116 loss: 0.0475\n",
      "Epoch 148: Batch 110/116 loss: 0.0393\n",
      "Epoch 148: Batch 111/116 loss: 0.0392\n",
      "Epoch 148: Batch 112/116 loss: 0.0498\n",
      "Epoch 148: Batch 113/116 loss: 0.0476\n",
      "Epoch 148: Batch 114/116 loss: 0.0553\n",
      "Epoch 148: Batch 115/116 loss: 0.0662\n",
      "Epoch 148: Batch 116/116 loss: 0.0574\n",
      "Epoch 148 train loss: 0.0441 valid loss: 0.0515\n",
      "performance reducing: 4\n",
      "Epoch 149: Batch 1/116 loss: 0.0487\n",
      "Epoch 149: Batch 2/116 loss: 0.0593\n",
      "Epoch 149: Batch 3/116 loss: 0.0444\n",
      "Epoch 149: Batch 4/116 loss: 0.0413\n",
      "Epoch 149: Batch 5/116 loss: 0.0331\n",
      "Epoch 149: Batch 6/116 loss: 0.0489\n",
      "Epoch 149: Batch 7/116 loss: 0.0263\n",
      "Epoch 149: Batch 8/116 loss: 0.0402\n",
      "Epoch 149: Batch 9/116 loss: 0.0365\n",
      "Epoch 149: Batch 10/116 loss: 0.0481\n",
      "Epoch 149: Batch 11/116 loss: 0.0351\n",
      "Epoch 149: Batch 12/116 loss: 0.0260\n",
      "Epoch 149: Batch 13/116 loss: 0.0514\n",
      "Epoch 149: Batch 14/116 loss: 0.0433\n",
      "Epoch 149: Batch 15/116 loss: 0.0347\n",
      "Epoch 149: Batch 16/116 loss: 0.0363\n",
      "Epoch 149: Batch 17/116 loss: 0.0429\n",
      "Epoch 149: Batch 18/116 loss: 0.0335\n",
      "Epoch 149: Batch 19/116 loss: 0.0474\n",
      "Epoch 149: Batch 20/116 loss: 0.0489\n",
      "Epoch 149: Batch 21/116 loss: 0.0567\n",
      "Epoch 149: Batch 22/116 loss: 0.0553\n",
      "Epoch 149: Batch 23/116 loss: 0.0488\n",
      "Epoch 149: Batch 24/116 loss: 0.0588\n",
      "Epoch 149: Batch 25/116 loss: 0.0384\n",
      "Epoch 149: Batch 26/116 loss: 0.0442\n",
      "Epoch 149: Batch 27/116 loss: 0.0416\n",
      "Epoch 149: Batch 28/116 loss: 0.0390\n",
      "Epoch 149: Batch 29/116 loss: 0.1051\n",
      "Epoch 149: Batch 30/116 loss: 0.0570\n",
      "Epoch 149: Batch 31/116 loss: 0.0448\n",
      "Epoch 149: Batch 32/116 loss: 0.0612\n",
      "Epoch 149: Batch 33/116 loss: 0.0457\n",
      "Epoch 149: Batch 34/116 loss: 0.0404\n",
      "Epoch 149: Batch 35/116 loss: 0.0397\n",
      "Epoch 149: Batch 36/116 loss: 0.0409\n",
      "Epoch 149: Batch 37/116 loss: 0.0422\n",
      "Epoch 149: Batch 38/116 loss: 0.0216\n",
      "Epoch 149: Batch 39/116 loss: 0.0255\n",
      "Epoch 149: Batch 40/116 loss: 0.0366\n",
      "Epoch 149: Batch 41/116 loss: 0.0255\n",
      "Epoch 149: Batch 42/116 loss: 0.0540\n",
      "Epoch 149: Batch 43/116 loss: 0.0493\n",
      "Epoch 149: Batch 44/116 loss: 0.0582\n",
      "Epoch 149: Batch 45/116 loss: 0.0331\n",
      "Epoch 149: Batch 46/116 loss: 0.0487\n",
      "Epoch 149: Batch 47/116 loss: 0.0512\n",
      "Epoch 149: Batch 48/116 loss: 0.0508\n",
      "Epoch 149: Batch 49/116 loss: 0.0301\n",
      "Epoch 149: Batch 50/116 loss: 0.0671\n",
      "Epoch 149: Batch 51/116 loss: 0.0329\n",
      "Epoch 149: Batch 52/116 loss: 0.0615\n",
      "Epoch 149: Batch 53/116 loss: 0.0374\n",
      "Epoch 149: Batch 54/116 loss: 0.0422\n",
      "Epoch 149: Batch 55/116 loss: 0.0349\n",
      "Epoch 149: Batch 56/116 loss: 0.0414\n",
      "Epoch 149: Batch 57/116 loss: 0.0336\n",
      "Epoch 149: Batch 58/116 loss: 0.0469\n",
      "Epoch 149: Batch 59/116 loss: 0.0473\n",
      "Epoch 149: Batch 60/116 loss: 0.0304\n",
      "Epoch 149: Batch 61/116 loss: 0.0544\n",
      "Epoch 149: Batch 62/116 loss: 0.0648\n",
      "Epoch 149: Batch 63/116 loss: 0.0443\n",
      "Epoch 149: Batch 64/116 loss: 0.0522\n",
      "Epoch 149: Batch 65/116 loss: 0.0478\n",
      "Epoch 149: Batch 66/116 loss: 0.0569\n",
      "Epoch 149: Batch 67/116 loss: 0.0347\n",
      "Epoch 149: Batch 68/116 loss: 0.0486\n",
      "Epoch 149: Batch 69/116 loss: 0.0445\n",
      "Epoch 149: Batch 70/116 loss: 0.0389\n",
      "Epoch 149: Batch 71/116 loss: 0.0551\n",
      "Epoch 149: Batch 72/116 loss: 0.0358\n",
      "Epoch 149: Batch 73/116 loss: 0.0346\n",
      "Epoch 149: Batch 74/116 loss: 0.0487\n",
      "Epoch 149: Batch 75/116 loss: 0.0430\n",
      "Epoch 149: Batch 76/116 loss: 0.0399\n",
      "Epoch 149: Batch 77/116 loss: 0.0505\n",
      "Epoch 149: Batch 78/116 loss: 0.0478\n",
      "Epoch 149: Batch 79/116 loss: 0.0315\n",
      "Epoch 149: Batch 80/116 loss: 0.0284\n",
      "Epoch 149: Batch 81/116 loss: 0.0564\n",
      "Epoch 149: Batch 82/116 loss: 0.0465\n",
      "Epoch 149: Batch 83/116 loss: 0.0482\n",
      "Epoch 149: Batch 84/116 loss: 0.0528\n",
      "Epoch 149: Batch 85/116 loss: 0.0398\n",
      "Epoch 149: Batch 86/116 loss: 0.0373\n",
      "Epoch 149: Batch 87/116 loss: 0.0574\n",
      "Epoch 149: Batch 88/116 loss: 0.0431\n",
      "Epoch 149: Batch 89/116 loss: 0.0491\n",
      "Epoch 149: Batch 90/116 loss: 0.0457\n",
      "Epoch 149: Batch 91/116 loss: 0.0418\n",
      "Epoch 149: Batch 92/116 loss: 0.0323\n",
      "Epoch 149: Batch 93/116 loss: 0.0438\n",
      "Epoch 149: Batch 94/116 loss: 0.0427\n",
      "Epoch 149: Batch 95/116 loss: 0.0376\n",
      "Epoch 149: Batch 96/116 loss: 0.0436\n",
      "Epoch 149: Batch 97/116 loss: 0.0481\n",
      "Epoch 149: Batch 98/116 loss: 0.0375\n",
      "Epoch 149: Batch 99/116 loss: 0.0330\n",
      "Epoch 149: Batch 100/116 loss: 0.0429\n",
      "Epoch 149: Batch 101/116 loss: 0.0668\n",
      "Epoch 149: Batch 102/116 loss: 0.0530\n",
      "Epoch 149: Batch 103/116 loss: 0.0506\n",
      "Epoch 149: Batch 104/116 loss: 0.0373\n",
      "Epoch 149: Batch 105/116 loss: 0.0647\n",
      "Epoch 149: Batch 106/116 loss: 0.0676\n",
      "Epoch 149: Batch 107/116 loss: 0.0424\n",
      "Epoch 149: Batch 108/116 loss: 0.0539\n",
      "Epoch 149: Batch 109/116 loss: 0.0468\n",
      "Epoch 149: Batch 110/116 loss: 0.0447\n",
      "Epoch 149: Batch 111/116 loss: 0.0452\n",
      "Epoch 149: Batch 112/116 loss: 0.0581\n",
      "Epoch 149: Batch 113/116 loss: 0.0361\n",
      "Epoch 149: Batch 114/116 loss: 0.0472\n",
      "Epoch 149: Batch 115/116 loss: 0.0562\n",
      "Epoch 149: Batch 116/116 loss: 0.0665\n",
      "Epoch 149 train loss: 0.0454 valid loss: 0.0590\n",
      "performance reducing: 5\n",
      "Epoch 150: Batch 1/116 loss: 0.0408\n",
      "Epoch 150: Batch 2/116 loss: 0.0523\n",
      "Epoch 150: Batch 3/116 loss: 0.0633\n",
      "Epoch 150: Batch 4/116 loss: 0.0296\n",
      "Epoch 150: Batch 5/116 loss: 0.0530\n",
      "Epoch 150: Batch 6/116 loss: 0.0573\n",
      "Epoch 150: Batch 7/116 loss: 0.0465\n",
      "Epoch 150: Batch 8/116 loss: 0.0648\n",
      "Epoch 150: Batch 9/116 loss: 0.0356\n",
      "Epoch 150: Batch 10/116 loss: 0.0643\n",
      "Epoch 150: Batch 11/116 loss: 0.0305\n",
      "Epoch 150: Batch 12/116 loss: 0.0469\n",
      "Epoch 150: Batch 13/116 loss: 0.0545\n",
      "Epoch 150: Batch 14/116 loss: 0.0416\n",
      "Epoch 150: Batch 15/116 loss: 0.0510\n",
      "Epoch 150: Batch 16/116 loss: 0.0530\n",
      "Epoch 150: Batch 17/116 loss: 0.0422\n",
      "Epoch 150: Batch 18/116 loss: 0.0435\n",
      "Epoch 150: Batch 19/116 loss: 0.0524\n",
      "Epoch 150: Batch 20/116 loss: 0.0392\n",
      "Epoch 150: Batch 21/116 loss: 0.0406\n",
      "Epoch 150: Batch 22/116 loss: 0.0495\n",
      "Epoch 150: Batch 23/116 loss: 0.0499\n",
      "Epoch 150: Batch 24/116 loss: 0.0235\n",
      "Epoch 150: Batch 25/116 loss: 0.0351\n",
      "Epoch 150: Batch 26/116 loss: 0.0373\n",
      "Epoch 150: Batch 27/116 loss: 0.0558\n",
      "Epoch 150: Batch 28/116 loss: 0.0284\n",
      "Epoch 150: Batch 29/116 loss: 0.0413\n",
      "Epoch 150: Batch 30/116 loss: 0.0402\n",
      "Epoch 150: Batch 31/116 loss: 0.0461\n",
      "Epoch 150: Batch 32/116 loss: 0.0435\n",
      "Epoch 150: Batch 33/116 loss: 0.0455\n",
      "Epoch 150: Batch 34/116 loss: 0.0474\n",
      "Epoch 150: Batch 35/116 loss: 0.0491\n",
      "Epoch 150: Batch 36/116 loss: 0.0422\n",
      "Epoch 150: Batch 37/116 loss: 0.0492\n",
      "Epoch 150: Batch 38/116 loss: 0.0433\n",
      "Epoch 150: Batch 39/116 loss: 0.0503\n",
      "Epoch 150: Batch 40/116 loss: 0.0358\n",
      "Epoch 150: Batch 41/116 loss: 0.0615\n",
      "Epoch 150: Batch 42/116 loss: 0.0311\n",
      "Epoch 150: Batch 43/116 loss: 0.0532\n",
      "Epoch 150: Batch 44/116 loss: 0.0431\n",
      "Epoch 150: Batch 45/116 loss: 0.0500\n",
      "Epoch 150: Batch 46/116 loss: 0.0484\n",
      "Epoch 150: Batch 47/116 loss: 0.0654\n",
      "Epoch 150: Batch 48/116 loss: 0.0364\n",
      "Epoch 150: Batch 49/116 loss: 0.0623\n",
      "Epoch 150: Batch 50/116 loss: 0.0561\n",
      "Epoch 150: Batch 51/116 loss: 0.0513\n",
      "Epoch 150: Batch 52/116 loss: 0.0405\n",
      "Epoch 150: Batch 53/116 loss: 0.0478\n",
      "Epoch 150: Batch 54/116 loss: 0.0402\n",
      "Epoch 150: Batch 55/116 loss: 0.0561\n",
      "Epoch 150: Batch 56/116 loss: 0.0331\n",
      "Epoch 150: Batch 57/116 loss: 0.0327\n",
      "Epoch 150: Batch 58/116 loss: 0.0629\n",
      "Epoch 150: Batch 59/116 loss: 0.0474\n",
      "Epoch 150: Batch 60/116 loss: 0.0429\n",
      "Epoch 150: Batch 61/116 loss: 0.0445\n",
      "Epoch 150: Batch 62/116 loss: 0.0453\n",
      "Epoch 150: Batch 63/116 loss: 0.0298\n",
      "Epoch 150: Batch 64/116 loss: 0.0472\n",
      "Epoch 150: Batch 65/116 loss: 0.0531\n",
      "Epoch 150: Batch 66/116 loss: 0.0481\n",
      "Epoch 150: Batch 67/116 loss: 0.0299\n",
      "Epoch 150: Batch 68/116 loss: 0.0402\n",
      "Epoch 150: Batch 69/116 loss: 0.0275\n",
      "Epoch 150: Batch 70/116 loss: 0.0511\n",
      "Epoch 150: Batch 71/116 loss: 0.0346\n",
      "Epoch 150: Batch 72/116 loss: 0.0615\n",
      "Epoch 150: Batch 73/116 loss: 0.0671\n",
      "Epoch 150: Batch 74/116 loss: 0.0479\n",
      "Epoch 150: Batch 75/116 loss: 0.0430\n",
      "Epoch 150: Batch 76/116 loss: 0.0531\n",
      "Epoch 150: Batch 77/116 loss: 0.0274\n",
      "Epoch 150: Batch 78/116 loss: 0.0347\n",
      "Epoch 150: Batch 79/116 loss: 0.0322\n",
      "Epoch 150: Batch 80/116 loss: 0.0532\n",
      "Epoch 150: Batch 81/116 loss: 0.0611\n",
      "Epoch 150: Batch 82/116 loss: 0.0467\n",
      "Epoch 150: Batch 83/116 loss: 0.0477\n",
      "Epoch 150: Batch 84/116 loss: 0.0366\n",
      "Epoch 150: Batch 85/116 loss: 0.0355\n",
      "Epoch 150: Batch 86/116 loss: 0.0428\n",
      "Epoch 150: Batch 87/116 loss: 0.0294\n",
      "Epoch 150: Batch 88/116 loss: 0.0500\n",
      "Epoch 150: Batch 89/116 loss: 0.0329\n",
      "Epoch 150: Batch 90/116 loss: 0.0317\n",
      "Epoch 150: Batch 91/116 loss: 0.0641\n",
      "Epoch 150: Batch 92/116 loss: 0.0600\n",
      "Epoch 150: Batch 93/116 loss: 0.0506\n",
      "Epoch 150: Batch 94/116 loss: 0.0367\n",
      "Epoch 150: Batch 95/116 loss: 0.0454\n",
      "Epoch 150: Batch 96/116 loss: 0.0486\n",
      "Epoch 150: Batch 97/116 loss: 0.0379\n",
      "Epoch 150: Batch 98/116 loss: 0.0454\n",
      "Epoch 150: Batch 99/116 loss: 0.0637\n",
      "Epoch 150: Batch 100/116 loss: 0.0428\n",
      "Epoch 150: Batch 101/116 loss: 0.0219\n",
      "Epoch 150: Batch 102/116 loss: 0.0846\n",
      "Epoch 150: Batch 103/116 loss: 0.0486\n",
      "Epoch 150: Batch 104/116 loss: 0.0359\n",
      "Epoch 150: Batch 105/116 loss: 0.0665\n",
      "Epoch 150: Batch 106/116 loss: 0.0327\n",
      "Epoch 150: Batch 107/116 loss: 0.0321\n",
      "Epoch 150: Batch 108/116 loss: 0.0504\n",
      "Epoch 150: Batch 109/116 loss: 0.0528\n",
      "Epoch 150: Batch 110/116 loss: 0.0384\n",
      "Epoch 150: Batch 111/116 loss: 0.0507\n",
      "Epoch 150: Batch 112/116 loss: 0.0510\n",
      "Epoch 150: Batch 113/116 loss: 0.0454\n",
      "Epoch 150: Batch 114/116 loss: 0.0359\n",
      "Epoch 150: Batch 115/116 loss: 0.0366\n",
      "Epoch 150: Batch 116/116 loss: 0.0440\n",
      "Epoch 150 train loss: 0.0456 valid loss: 0.0598\n",
      "performance reducing: 6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lr = 1e-3\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # 后面修改为Dice损失函数看看效果\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/images/*')\n",
    "    train_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/masks/*')\n",
    "    \n",
    "    val_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/images/*')\n",
    "    val_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/masks/*')\n",
    "\n",
    "    train_transforms = Compose([\n",
    "        # todo 先Resize试一下\n",
    "        ToPILImage(),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomRotation(degrees=(0, 180)),\n",
    "#         ColorJitter(), # 这个有问题，但是是什么问题？\n",
    "#         GrayScale(), # 这个貌似也有问题，问题更大\n",
    "        RandomCrop(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    valid_transforms = Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = load_data(train_image_path, train_mask_path, batch_size=32, drop_last=True,\n",
    "                             transforms=train_transforms)\n",
    "    valid_loader = load_data(val_image_path, val_mask_path, batch_size=32, transforms=valid_transforms)\n",
    "    \n",
    "    continue_train = True\n",
    "    \n",
    "    if not continue_train:\n",
    "        epoch = 50\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, epoch, device=device)\n",
    "    else:\n",
    "        total_epoch = 150\n",
    "        pretrain_params = torch.load('../input/covid-xray-unet/last_model.pth')\n",
    "        model.load_state_dict(pretrain_params['last_model_state_dict'])\n",
    "        optimizer.load_state_dict(pretrain_params['last_optimizer_state_dict'])\n",
    "        current_epoch = pretrain_params['epoch']\n",
    "        model.train()\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9533.710462,
   "end_time": "2022-08-11T04:42:19.061473",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-11T02:03:25.351011",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
