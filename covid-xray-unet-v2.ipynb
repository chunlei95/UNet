{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5535019",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:02.691304Z",
     "iopub.status.busy": "2022-08-10T13:39:02.690758Z",
     "iopub.status.idle": "2022-08-10T13:39:02.701574Z",
     "shell.execute_reply": "2022-08-10T13:39:02.700733Z"
    },
    "papermill": {
     "duration": 0.01806,
     "end_time": "2022-08-10T13:39:02.703570",
     "exception": false,
     "start_time": "2022-08-10T13:39:02.685510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297c1c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:02.710052Z",
     "iopub.status.busy": "2022-08-10T13:39:02.709796Z",
     "iopub.status.idle": "2022-08-10T13:39:04.808492Z",
     "shell.execute_reply": "2022-08-10T13:39:04.807537Z"
    },
    "papermill": {
     "duration": 2.104758,
     "end_time": "2022-08-10T13:39:04.810855",
     "exception": false,
     "start_time": "2022-08-10T13:39:02.706097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "import torchvision.transforms.functional as Func\n",
    "\n",
    "\n",
    "class ToPILImage(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_pil_image(image)\n",
    "        if target is not None:\n",
    "            target = Func.to_pil_image(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        seed = np.random.randint(65536)\n",
    "        torch.manual_seed(seed)\n",
    "        crop = torchvision.transforms.RandomCrop(self.size)\n",
    "        image = crop(image)\n",
    "        if target is not None:\n",
    "            target = crop(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        super(Resize, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.resize(image, self.size)\n",
    "        if target is not None:\n",
    "            target = Func.resize(target, self.size, interpolation=Func.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = Func.hflip(image)\n",
    "            if target is not None:\n",
    "                target = Func.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __call__(self, image, target):\n",
    "        color_jitter = torchvision.transforms.ColorJitter()\n",
    "        image = color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class GrayScale(object):\n",
    "    def __call__(self, image, target):\n",
    "        gray_scale = torchvision.transforms.Grayscale()\n",
    "        image = gray_scale(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, degrees):\n",
    "        super(RandomRotation, self).__init__()\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        degree = random.randint(self.degrees[0], self.degrees[1])\n",
    "        image = Func.rotate(image, degree)\n",
    "        if target is not None:\n",
    "            target = Func.rotate(target, degree)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = Func.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target=None):\n",
    "        image = Func.to_tensor(image)\n",
    "        if target is not None:\n",
    "            target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af52273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:04.817253Z",
     "iopub.status.busy": "2022-08-10T13:39:04.816793Z",
     "iopub.status.idle": "2022-08-10T13:39:04.826821Z",
     "shell.execute_reply": "2022-08-10T13:39:04.825986Z"
    },
    "papermill": {
     "duration": 0.015412,
     "end_time": "2022-08-10T13:39:04.828858",
     "exception": false,
     "start_time": "2022-08-10T13:39:04.813446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, target_path, transforms=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data_paths = data_path\n",
    "        self.target_paths = target_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        image = plt.imread(self.data_paths[item])\n",
    "        target = plt.imread(self.target_paths[item])\n",
    "\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # image = torch.from_numpy(image)\n",
    "        # target = torch.from_numpy(target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "\n",
    "def load_data(data_path, target_path, batch_size, drop_last=False, transforms=None):\n",
    "    datas = CustomDataset(data_path=data_path, target_path=target_path, transforms=transforms)\n",
    "    data_loader = DataLoader(datas, shuffle=True, batch_size=batch_size, drop_last=drop_last)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c423f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:04.835067Z",
     "iopub.status.busy": "2022-08-10T13:39:04.834798Z",
     "iopub.status.idle": "2022-08-10T13:39:04.857119Z",
     "shell.execute_reply": "2022-08-10T13:39:04.856100Z"
    },
    "papermill": {
     "duration": 0.028333,
     "end_time": "2022-08-10T13:39:04.859478",
     "exception": false,
     "start_time": "2022-08-10T13:39:04.831145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net模型的pytorch实现。\n",
    "    论文地址：https://arxiv.org/abs/1505.04597\n",
    "    模型的总体结构: 编码器 -> 一个ConvBlock -> 解码器 -> 一个Conv 1 * 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.eb1 = EncoderBlock(1, 64, 64, kernel_size=2)\n",
    "        self.eb2 = EncoderBlock(64, 128, 128, kernel_size=2)\n",
    "        self.eb3 = EncoderBlock(128, 256, 256, kernel_size=2)\n",
    "        self.eb4 = EncoderBlock(256, 512, 512, kernel_size=2)\n",
    "        # 编码器与解码器之间有一个ConvBlock\n",
    "        self.cb = ConvBlock(512, 1024, 1024)\n",
    "        # 解码器部分\n",
    "        self.db1 = DecoderBlock(1024, 512, 512)\n",
    "        self.db2 = DecoderBlock(512, 512, 256)\n",
    "        self.db3 = DecoderBlock(256, 128, 128)\n",
    "        self.db4 = DecoderBlock(128, 64, 64)\n",
    "        # 一个Conv 1 * 1, 二分类，结果为两个通道\n",
    "        self.conv1x1 = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ex1, skip_x1 = self.eb1(x)\n",
    "        ex2, skip_x2 = self.eb2(ex1)\n",
    "        ex3, skip_x3 = self.eb3(ex2)\n",
    "        ex4, skip_x4 = self.eb4(ex3)\n",
    "        cbx = self.cb(ex4)\n",
    "        dx1 = self.db1(cbx, skip_x4)\n",
    "        dx2 = self.db2(dx1, skip_x3)\n",
    "        dx3 = self.db3(dx2, skip_x2)\n",
    "        dx4 = self.db4(dx3, skip_x1)\n",
    "        crop = transforms.CenterCrop(size=(x.shape[-1], x.shape[-2]))\n",
    "        # normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        return self.sigmoid(self.conv1x1(crop(dx4)))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"一个Conv2d卷积后跟一个Relu激活函数，卷积核大小为3 * 3\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_relu_list = [nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(mid_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(out_channels),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "        self.conv_relu = nn.Sequential(*conv_relu_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_relu(x)\n",
    "\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    \"\"\"下采样，使用max pool方法执行，核大小为 2 * 2，用在编码器的ConvBlock后面\n",
    "\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    \"\"\"上采样，用在解码器的ConvBlock前面，使用转置卷积，同时通道数减半，\n",
    "\n",
    "    C_out = out_channels\n",
    "    H_out = (H_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    W_out = (W_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "\n",
    "    :param in_channels: 转置卷积的输入通道数\n",
    "    :param out_channels: 转置卷积的输出通道数\n",
    "    :param kernel_size: 转置卷积的卷积核大小，默认为2\n",
    "    :param stride: 转置卷积的步幅，默认为2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, dilation=1, padding=0, output_padding=1):\n",
    "        super(UpSampling, self).__init__()\n",
    "        # self.up_sample = nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "        # stride=2, kernel_size=2相当于宽高翻倍\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            dilation=dilation, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_sample(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"编码器中的一个层次块\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "        self.down_sample = DownSampling(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block(x)\n",
    "        return self.down_sample(x1), x1\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    \"\"\"跳跃连接，在通道维上连接\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 将从编码器传过来的特征图裁剪到与输入相同尺寸\n",
    "        x1 = functional.center_crop(skip_x, [x.shape[-2], x.shape[-1]])\n",
    "        if x1.shape != x.shape:\n",
    "            raise Exception('要连接的两个特征图尺寸不一致，skip_x.shape={}，x.shape={}'.format(skip_x.shape, x.shape))\n",
    "        # 通道维连接\n",
    "        return torch.cat([x, x1], dim=1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的层次块，每个层次块都是UpSampling -> Concat -> ConvBlock\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up_sample = UpSampling(in_channels, out_channels)\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x1 = self.up_sample(x)\n",
    "        concat = ConcatLayer()\n",
    "        x2 = concat(x1, skip_x)\n",
    "        return self.conv_block(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be667f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:04.865384Z",
     "iopub.status.busy": "2022-08-10T13:39:04.865110Z",
     "iopub.status.idle": "2022-08-10T13:39:04.882237Z",
     "shell.execute_reply": "2022-08-10T13:39:04.881137Z"
    },
    "papermill": {
     "duration": 0.022728,
     "end_time": "2022-08-10T13:39:04.884491",
     "exception": false,
     "start_time": "2022-08-10T13:39:04.861763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames,SpellCheckingInspection\n",
    "def train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch=0, num_classes=2,\n",
    "          device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    loss_change_list = []\n",
    "    valid_loss_change_list = []\n",
    "    saved_last = {}\n",
    "    saved_best = {}\n",
    "    loss_change = {}\n",
    "\n",
    "    search_best = SearchBest()\n",
    "\n",
    "    for i in range(current_epoch, total_epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_onehot = convert_to_one_hot(y, num_classes=num_classes)\n",
    "            predict = model(x)\n",
    "\n",
    "            loss_value = criterion(predict, y_onehot)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "            print('Epoch {}: Batch {}/{} loss: {:.4f}'.format(i + 1, index + 1, len(train_loader), loss_value.item()))\n",
    "\n",
    "        loss_change_list.append(total_loss / len(train_loader))\n",
    "        valid_avg_loss = valid(model, criterion, valid_loader, num_classes, device)\n",
    "        valid_loss_change_list.append(valid_avg_loss)\n",
    "        print('Epoch {} train loss: {:.4f} valid loss: {:.4f}'.format(i + 1, total_loss / len(train_loader),\n",
    "                                                                      valid_avg_loss))\n",
    "        search_best(valid_avg_loss)\n",
    "        if search_best.counter == 0:\n",
    "            # save the relevant params of the best model state in the current time.\n",
    "            saved_best['best_model_state_dict'] = model.state_dict()\n",
    "            saved_best['best_optimizer_state_dict'] = optimizer.state_dict()\n",
    "            saved_best['epoch'] = i + 1\n",
    "    loss_change['train_loss_change_history'] = loss_change_list\n",
    "    loss_change['valid_loss_change_history'] = valid_loss_change_list\n",
    "    saved_last['last_model_state_dict'] = model.state_dict()\n",
    "    saved_last['last_optimizer_state_dict'] = optimizer.state_dict()\n",
    "    saved_last['epoch'] = total_epoch\n",
    "    torch.save(saved_best, './best_model.pth')\n",
    "    torch.save(saved_last, './last_model.pth')\n",
    "    torch.save(loss_change, './loss_change.pth')\n",
    "\n",
    "\n",
    "def convert_to_one_hot(data, num_classes):\n",
    "    if type(data) is not torch.Tensor:\n",
    "        raise RuntimeError('data must be a torch.Tensor')\n",
    "    if data.dtype is not torch.int64:\n",
    "        data = data.to(torch.int64)\n",
    "    data = F.one_hot(data, num_classes=num_classes).permute((0, -1, 1, 2))\n",
    "    return data.to(torch.float32)\n",
    "\n",
    "\n",
    "class SearchBest(object):\n",
    "    def __init__(self, min_delta=0, verbose=True):\n",
    "        super(SearchBest, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, valid_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = valid_loss\n",
    "        elif self.best_score - valid_loss >= self.min_delta:\n",
    "            self.best_score = valid_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print('performance reducing: {}'.format(self.counter))\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def valid(model, criterion, valid_loader, num_classes, device):\n",
    "    \"\"\"\n",
    "    :return: validate loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    valid_total_loss = 0.0\n",
    "    for index, (x, y) in enumerate(valid_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_onehot = convert_to_one_hot(y, num_classes)\n",
    "        with torch.no_grad():\n",
    "            predict = model(x)\n",
    "            valid_loss = criterion(predict, y_onehot)\n",
    "            valid_total_loss += valid_loss.item()\n",
    "    return valid_total_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92a6d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T13:39:04.891809Z",
     "iopub.status.busy": "2022-08-10T13:39:04.890317Z",
     "iopub.status.idle": "2022-08-10T16:17:11.972395Z",
     "shell.execute_reply": "2022-08-10T16:17:11.971362Z"
    },
    "papermill": {
     "duration": 9487.088219,
     "end_time": "2022-08-10T16:17:11.974997",
     "exception": false,
     "start_time": "2022-08-10T13:39:04.886778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Batch 1/116 loss: 0.0711\n",
      "Epoch 51: Batch 2/116 loss: 0.0507\n",
      "Epoch 51: Batch 3/116 loss: 0.0742\n",
      "Epoch 51: Batch 4/116 loss: 0.0515\n",
      "Epoch 51: Batch 5/116 loss: 0.1000\n",
      "Epoch 51: Batch 6/116 loss: 0.1119\n",
      "Epoch 51: Batch 7/116 loss: 0.0639\n",
      "Epoch 51: Batch 8/116 loss: 0.0523\n",
      "Epoch 51: Batch 9/116 loss: 0.0648\n",
      "Epoch 51: Batch 10/116 loss: 0.0616\n",
      "Epoch 51: Batch 11/116 loss: 0.0599\n",
      "Epoch 51: Batch 12/116 loss: 0.0777\n",
      "Epoch 51: Batch 13/116 loss: 0.0648\n",
      "Epoch 51: Batch 14/116 loss: 0.0630\n",
      "Epoch 51: Batch 15/116 loss: 0.0885\n",
      "Epoch 51: Batch 16/116 loss: 0.0701\n",
      "Epoch 51: Batch 17/116 loss: 0.0713\n",
      "Epoch 51: Batch 18/116 loss: 0.0725\n",
      "Epoch 51: Batch 19/116 loss: 0.0652\n",
      "Epoch 51: Batch 20/116 loss: 0.0627\n",
      "Epoch 51: Batch 21/116 loss: 0.0503\n",
      "Epoch 51: Batch 22/116 loss: 0.0421\n",
      "Epoch 51: Batch 23/116 loss: 0.0764\n",
      "Epoch 51: Batch 24/116 loss: 0.0427\n",
      "Epoch 51: Batch 25/116 loss: 0.0652\n",
      "Epoch 51: Batch 26/116 loss: 0.0753\n",
      "Epoch 51: Batch 27/116 loss: 0.0653\n",
      "Epoch 51: Batch 28/116 loss: 0.0632\n",
      "Epoch 51: Batch 29/116 loss: 0.0779\n",
      "Epoch 51: Batch 30/116 loss: 0.0754\n",
      "Epoch 51: Batch 31/116 loss: 0.0747\n",
      "Epoch 51: Batch 32/116 loss: 0.0561\n",
      "Epoch 51: Batch 33/116 loss: 0.0639\n",
      "Epoch 51: Batch 34/116 loss: 0.0859\n",
      "Epoch 51: Batch 35/116 loss: 0.0712\n",
      "Epoch 51: Batch 36/116 loss: 0.0672\n",
      "Epoch 51: Batch 37/116 loss: 0.0699\n",
      "Epoch 51: Batch 38/116 loss: 0.0518\n",
      "Epoch 51: Batch 39/116 loss: 0.0774\n",
      "Epoch 51: Batch 40/116 loss: 0.0604\n",
      "Epoch 51: Batch 41/116 loss: 0.0681\n",
      "Epoch 51: Batch 42/116 loss: 0.0480\n",
      "Epoch 51: Batch 43/116 loss: 0.0735\n",
      "Epoch 51: Batch 44/116 loss: 0.0686\n",
      "Epoch 51: Batch 45/116 loss: 0.0695\n",
      "Epoch 51: Batch 46/116 loss: 0.0677\n",
      "Epoch 51: Batch 47/116 loss: 0.0815\n",
      "Epoch 51: Batch 48/116 loss: 0.0571\n",
      "Epoch 51: Batch 49/116 loss: 0.0814\n",
      "Epoch 51: Batch 50/116 loss: 0.0575\n",
      "Epoch 51: Batch 51/116 loss: 0.0963\n",
      "Epoch 51: Batch 52/116 loss: 0.0722\n",
      "Epoch 51: Batch 53/116 loss: 0.0536\n",
      "Epoch 51: Batch 54/116 loss: 0.0421\n",
      "Epoch 51: Batch 55/116 loss: 0.0602\n",
      "Epoch 51: Batch 56/116 loss: 0.1007\n",
      "Epoch 51: Batch 57/116 loss: 0.0710\n",
      "Epoch 51: Batch 58/116 loss: 0.0703\n",
      "Epoch 51: Batch 59/116 loss: 0.0846\n",
      "Epoch 51: Batch 60/116 loss: 0.0806\n",
      "Epoch 51: Batch 61/116 loss: 0.0599\n",
      "Epoch 51: Batch 62/116 loss: 0.0497\n",
      "Epoch 51: Batch 63/116 loss: 0.0604\n",
      "Epoch 51: Batch 64/116 loss: 0.0808\n",
      "Epoch 51: Batch 65/116 loss: 0.0489\n",
      "Epoch 51: Batch 66/116 loss: 0.0824\n",
      "Epoch 51: Batch 67/116 loss: 0.0562\n",
      "Epoch 51: Batch 68/116 loss: 0.0510\n",
      "Epoch 51: Batch 69/116 loss: 0.0612\n",
      "Epoch 51: Batch 70/116 loss: 0.0726\n",
      "Epoch 51: Batch 71/116 loss: 0.0640\n",
      "Epoch 51: Batch 72/116 loss: 0.0741\n",
      "Epoch 51: Batch 73/116 loss: 0.0588\n",
      "Epoch 51: Batch 74/116 loss: 0.1085\n",
      "Epoch 51: Batch 75/116 loss: 0.0630\n",
      "Epoch 51: Batch 76/116 loss: 0.0699\n",
      "Epoch 51: Batch 77/116 loss: 0.0648\n",
      "Epoch 51: Batch 78/116 loss: 0.0623\n",
      "Epoch 51: Batch 79/116 loss: 0.0605\n",
      "Epoch 51: Batch 80/116 loss: 0.0649\n",
      "Epoch 51: Batch 81/116 loss: 0.0499\n",
      "Epoch 51: Batch 82/116 loss: 0.0532\n",
      "Epoch 51: Batch 83/116 loss: 0.0561\n",
      "Epoch 51: Batch 84/116 loss: 0.0786\n",
      "Epoch 51: Batch 85/116 loss: 0.0507\n",
      "Epoch 51: Batch 86/116 loss: 0.0601\n",
      "Epoch 51: Batch 87/116 loss: 0.0631\n",
      "Epoch 51: Batch 88/116 loss: 0.0573\n",
      "Epoch 51: Batch 89/116 loss: 0.0699\n",
      "Epoch 51: Batch 90/116 loss: 0.0522\n",
      "Epoch 51: Batch 91/116 loss: 0.0459\n",
      "Epoch 51: Batch 92/116 loss: 0.0531\n",
      "Epoch 51: Batch 93/116 loss: 0.0644\n",
      "Epoch 51: Batch 94/116 loss: 0.0483\n",
      "Epoch 51: Batch 95/116 loss: 0.0953\n",
      "Epoch 51: Batch 96/116 loss: 0.0722\n",
      "Epoch 51: Batch 97/116 loss: 0.0522\n",
      "Epoch 51: Batch 98/116 loss: 0.0515\n",
      "Epoch 51: Batch 99/116 loss: 0.0756\n",
      "Epoch 51: Batch 100/116 loss: 0.0536\n",
      "Epoch 51: Batch 101/116 loss: 0.0746\n",
      "Epoch 51: Batch 102/116 loss: 0.0817\n",
      "Epoch 51: Batch 103/116 loss: 0.1044\n",
      "Epoch 51: Batch 104/116 loss: 0.0694\n",
      "Epoch 51: Batch 105/116 loss: 0.0548\n",
      "Epoch 51: Batch 106/116 loss: 0.0596\n",
      "Epoch 51: Batch 107/116 loss: 0.0510\n",
      "Epoch 51: Batch 108/116 loss: 0.0617\n",
      "Epoch 51: Batch 109/116 loss: 0.1013\n",
      "Epoch 51: Batch 110/116 loss: 0.0648\n",
      "Epoch 51: Batch 111/116 loss: 0.0724\n",
      "Epoch 51: Batch 112/116 loss: 0.0521\n",
      "Epoch 51: Batch 113/116 loss: 0.0579\n",
      "Epoch 51: Batch 114/116 loss: 0.0774\n",
      "Epoch 51: Batch 115/116 loss: 0.0683\n",
      "Epoch 51: Batch 116/116 loss: 0.0546\n",
      "Epoch 51 train loss: 0.0667 valid loss: 0.0871\n",
      "Epoch 52: Batch 1/116 loss: 0.0637\n",
      "Epoch 52: Batch 2/116 loss: 0.0755\n",
      "Epoch 52: Batch 3/116 loss: 0.0455\n",
      "Epoch 52: Batch 4/116 loss: 0.0661\n",
      "Epoch 52: Batch 5/116 loss: 0.0652\n",
      "Epoch 52: Batch 6/116 loss: 0.0609\n",
      "Epoch 52: Batch 7/116 loss: 0.0737\n",
      "Epoch 52: Batch 8/116 loss: 0.0517\n",
      "Epoch 52: Batch 9/116 loss: 0.0601\n",
      "Epoch 52: Batch 10/116 loss: 0.0728\n",
      "Epoch 52: Batch 11/116 loss: 0.0565\n",
      "Epoch 52: Batch 12/116 loss: 0.0558\n",
      "Epoch 52: Batch 13/116 loss: 0.0918\n",
      "Epoch 52: Batch 14/116 loss: 0.0467\n",
      "Epoch 52: Batch 15/116 loss: 0.0537\n",
      "Epoch 52: Batch 16/116 loss: 0.0886\n",
      "Epoch 52: Batch 17/116 loss: 0.0500\n",
      "Epoch 52: Batch 18/116 loss: 0.0476\n",
      "Epoch 52: Batch 19/116 loss: 0.0576\n",
      "Epoch 52: Batch 20/116 loss: 0.0788\n",
      "Epoch 52: Batch 21/116 loss: 0.0515\n",
      "Epoch 52: Batch 22/116 loss: 0.0629\n",
      "Epoch 52: Batch 23/116 loss: 0.0582\n",
      "Epoch 52: Batch 24/116 loss: 0.0513\n",
      "Epoch 52: Batch 25/116 loss: 0.0604\n",
      "Epoch 52: Batch 26/116 loss: 0.0657\n",
      "Epoch 52: Batch 27/116 loss: 0.0718\n",
      "Epoch 52: Batch 28/116 loss: 0.1143\n",
      "Epoch 52: Batch 29/116 loss: 0.0538\n",
      "Epoch 52: Batch 30/116 loss: 0.0730\n",
      "Epoch 52: Batch 31/116 loss: 0.0892\n",
      "Epoch 52: Batch 32/116 loss: 0.0593\n",
      "Epoch 52: Batch 33/116 loss: 0.0710\n",
      "Epoch 52: Batch 34/116 loss: 0.0642\n",
      "Epoch 52: Batch 35/116 loss: 0.0560\n",
      "Epoch 52: Batch 36/116 loss: 0.0958\n",
      "Epoch 52: Batch 37/116 loss: 0.0542\n",
      "Epoch 52: Batch 38/116 loss: 0.0524\n",
      "Epoch 52: Batch 39/116 loss: 0.0604\n",
      "Epoch 52: Batch 40/116 loss: 0.0641\n",
      "Epoch 52: Batch 41/116 loss: 0.0610\n",
      "Epoch 52: Batch 42/116 loss: 0.0581\n",
      "Epoch 52: Batch 43/116 loss: 0.0768\n",
      "Epoch 52: Batch 44/116 loss: 0.0596\n",
      "Epoch 52: Batch 45/116 loss: 0.0670\n",
      "Epoch 52: Batch 46/116 loss: 0.0712\n",
      "Epoch 52: Batch 47/116 loss: 0.0738\n",
      "Epoch 52: Batch 48/116 loss: 0.0583\n",
      "Epoch 52: Batch 49/116 loss: 0.0806\n",
      "Epoch 52: Batch 50/116 loss: 0.0722\n",
      "Epoch 52: Batch 51/116 loss: 0.0575\n",
      "Epoch 52: Batch 52/116 loss: 0.0550\n",
      "Epoch 52: Batch 53/116 loss: 0.0632\n",
      "Epoch 52: Batch 54/116 loss: 0.0842\n",
      "Epoch 52: Batch 55/116 loss: 0.0541\n",
      "Epoch 52: Batch 56/116 loss: 0.0726\n",
      "Epoch 52: Batch 57/116 loss: 0.0737\n",
      "Epoch 52: Batch 58/116 loss: 0.0432\n",
      "Epoch 52: Batch 59/116 loss: 0.0705\n",
      "Epoch 52: Batch 60/116 loss: 0.0497\n",
      "Epoch 52: Batch 61/116 loss: 0.0673\n",
      "Epoch 52: Batch 62/116 loss: 0.0676\n",
      "Epoch 52: Batch 63/116 loss: 0.0494\n",
      "Epoch 52: Batch 64/116 loss: 0.0672\n",
      "Epoch 52: Batch 65/116 loss: 0.0756\n",
      "Epoch 52: Batch 66/116 loss: 0.0732\n",
      "Epoch 52: Batch 67/116 loss: 0.0616\n",
      "Epoch 52: Batch 68/116 loss: 0.0600\n",
      "Epoch 52: Batch 69/116 loss: 0.0741\n",
      "Epoch 52: Batch 70/116 loss: 0.1091\n",
      "Epoch 52: Batch 71/116 loss: 0.0574\n",
      "Epoch 52: Batch 72/116 loss: 0.0774\n",
      "Epoch 52: Batch 73/116 loss: 0.0816\n",
      "Epoch 52: Batch 74/116 loss: 0.0616\n",
      "Epoch 52: Batch 75/116 loss: 0.0543\n",
      "Epoch 52: Batch 76/116 loss: 0.0622\n",
      "Epoch 52: Batch 77/116 loss: 0.0810\n",
      "Epoch 52: Batch 78/116 loss: 0.0524\n",
      "Epoch 52: Batch 79/116 loss: 0.0519\n",
      "Epoch 52: Batch 80/116 loss: 0.0450\n",
      "Epoch 52: Batch 81/116 loss: 0.0572\n",
      "Epoch 52: Batch 82/116 loss: 0.0802\n",
      "Epoch 52: Batch 83/116 loss: 0.0697\n",
      "Epoch 52: Batch 84/116 loss: 0.0936\n",
      "Epoch 52: Batch 85/116 loss: 0.0690\n",
      "Epoch 52: Batch 86/116 loss: 0.0870\n",
      "Epoch 52: Batch 87/116 loss: 0.0658\n",
      "Epoch 52: Batch 88/116 loss: 0.0641\n",
      "Epoch 52: Batch 89/116 loss: 0.0858\n",
      "Epoch 52: Batch 90/116 loss: 0.0587\n",
      "Epoch 52: Batch 91/116 loss: 0.0674\n",
      "Epoch 52: Batch 92/116 loss: 0.0543\n",
      "Epoch 52: Batch 93/116 loss: 0.0885\n",
      "Epoch 52: Batch 94/116 loss: 0.0756\n",
      "Epoch 52: Batch 95/116 loss: 0.0538\n",
      "Epoch 52: Batch 96/116 loss: 0.0572\n",
      "Epoch 52: Batch 97/116 loss: 0.0766\n",
      "Epoch 52: Batch 98/116 loss: 0.0796\n",
      "Epoch 52: Batch 99/116 loss: 0.0643\n",
      "Epoch 52: Batch 100/116 loss: 0.0530\n",
      "Epoch 52: Batch 101/116 loss: 0.0539\n",
      "Epoch 52: Batch 102/116 loss: 0.0563\n",
      "Epoch 52: Batch 103/116 loss: 0.0598\n",
      "Epoch 52: Batch 104/116 loss: 0.0698\n",
      "Epoch 52: Batch 105/116 loss: 0.0690\n",
      "Epoch 52: Batch 106/116 loss: 0.0596\n",
      "Epoch 52: Batch 107/116 loss: 0.0752\n",
      "Epoch 52: Batch 108/116 loss: 0.0507\n",
      "Epoch 52: Batch 109/116 loss: 0.0631\n",
      "Epoch 52: Batch 110/116 loss: 0.0740\n",
      "Epoch 52: Batch 111/116 loss: 0.0600\n",
      "Epoch 52: Batch 112/116 loss: 0.0805\n",
      "Epoch 52: Batch 113/116 loss: 0.0616\n",
      "Epoch 52: Batch 114/116 loss: 0.0537\n",
      "Epoch 52: Batch 115/116 loss: 0.0718\n",
      "Epoch 52: Batch 116/116 loss: 0.0447\n",
      "Epoch 52 train loss: 0.0658 valid loss: 0.0665\n",
      "Epoch 53: Batch 1/116 loss: 0.0825\n",
      "Epoch 53: Batch 2/116 loss: 0.0795\n",
      "Epoch 53: Batch 3/116 loss: 0.0618\n",
      "Epoch 53: Batch 4/116 loss: 0.0647\n",
      "Epoch 53: Batch 5/116 loss: 0.0723\n",
      "Epoch 53: Batch 6/116 loss: 0.0448\n",
      "Epoch 53: Batch 7/116 loss: 0.0472\n",
      "Epoch 53: Batch 8/116 loss: 0.0575\n",
      "Epoch 53: Batch 9/116 loss: 0.0519\n",
      "Epoch 53: Batch 10/116 loss: 0.0941\n",
      "Epoch 53: Batch 11/116 loss: 0.0399\n",
      "Epoch 53: Batch 12/116 loss: 0.0747\n",
      "Epoch 53: Batch 13/116 loss: 0.0555\n",
      "Epoch 53: Batch 14/116 loss: 0.0596\n",
      "Epoch 53: Batch 15/116 loss: 0.0657\n",
      "Epoch 53: Batch 16/116 loss: 0.0670\n",
      "Epoch 53: Batch 17/116 loss: 0.0607\n",
      "Epoch 53: Batch 18/116 loss: 0.0855\n",
      "Epoch 53: Batch 19/116 loss: 0.0671\n",
      "Epoch 53: Batch 20/116 loss: 0.0742\n",
      "Epoch 53: Batch 21/116 loss: 0.0671\n",
      "Epoch 53: Batch 22/116 loss: 0.0653\n",
      "Epoch 53: Batch 23/116 loss: 0.0525\n",
      "Epoch 53: Batch 24/116 loss: 0.0600\n",
      "Epoch 53: Batch 25/116 loss: 0.0517\n",
      "Epoch 53: Batch 26/116 loss: 0.0462\n",
      "Epoch 53: Batch 27/116 loss: 0.0804\n",
      "Epoch 53: Batch 28/116 loss: 0.0789\n",
      "Epoch 53: Batch 29/116 loss: 0.0716\n",
      "Epoch 53: Batch 30/116 loss: 0.0705\n",
      "Epoch 53: Batch 31/116 loss: 0.0745\n",
      "Epoch 53: Batch 32/116 loss: 0.0834\n",
      "Epoch 53: Batch 33/116 loss: 0.0536\n",
      "Epoch 53: Batch 34/116 loss: 0.0513\n",
      "Epoch 53: Batch 35/116 loss: 0.0701\n",
      "Epoch 53: Batch 36/116 loss: 0.0698\n",
      "Epoch 53: Batch 37/116 loss: 0.0432\n",
      "Epoch 53: Batch 38/116 loss: 0.0720\n",
      "Epoch 53: Batch 39/116 loss: 0.0590\n",
      "Epoch 53: Batch 40/116 loss: 0.0742\n",
      "Epoch 53: Batch 41/116 loss: 0.0574\n",
      "Epoch 53: Batch 42/116 loss: 0.0637\n",
      "Epoch 53: Batch 43/116 loss: 0.0666\n",
      "Epoch 53: Batch 44/116 loss: 0.0586\n",
      "Epoch 53: Batch 45/116 loss: 0.0515\n",
      "Epoch 53: Batch 46/116 loss: 0.0648\n",
      "Epoch 53: Batch 47/116 loss: 0.0759\n",
      "Epoch 53: Batch 48/116 loss: 0.0629\n",
      "Epoch 53: Batch 49/116 loss: 0.0534\n",
      "Epoch 53: Batch 50/116 loss: 0.0676\n",
      "Epoch 53: Batch 51/116 loss: 0.0881\n",
      "Epoch 53: Batch 52/116 loss: 0.0623\n",
      "Epoch 53: Batch 53/116 loss: 0.0444\n",
      "Epoch 53: Batch 54/116 loss: 0.0687\n",
      "Epoch 53: Batch 55/116 loss: 0.0704\n",
      "Epoch 53: Batch 56/116 loss: 0.0563\n",
      "Epoch 53: Batch 57/116 loss: 0.0552\n",
      "Epoch 53: Batch 58/116 loss: 0.0691\n",
      "Epoch 53: Batch 59/116 loss: 0.0951\n",
      "Epoch 53: Batch 60/116 loss: 0.0852\n",
      "Epoch 53: Batch 61/116 loss: 0.0623\n",
      "Epoch 53: Batch 62/116 loss: 0.0675\n",
      "Epoch 53: Batch 63/116 loss: 0.0907\n",
      "Epoch 53: Batch 64/116 loss: 0.0585\n",
      "Epoch 53: Batch 65/116 loss: 0.0539\n",
      "Epoch 53: Batch 66/116 loss: 0.0759\n",
      "Epoch 53: Batch 67/116 loss: 0.0915\n",
      "Epoch 53: Batch 68/116 loss: 0.0720\n",
      "Epoch 53: Batch 69/116 loss: 0.0513\n",
      "Epoch 53: Batch 70/116 loss: 0.0642\n",
      "Epoch 53: Batch 71/116 loss: 0.0743\n",
      "Epoch 53: Batch 72/116 loss: 0.0563\n",
      "Epoch 53: Batch 73/116 loss: 0.0725\n",
      "Epoch 53: Batch 74/116 loss: 0.0903\n",
      "Epoch 53: Batch 75/116 loss: 0.0541\n",
      "Epoch 53: Batch 76/116 loss: 0.0953\n",
      "Epoch 53: Batch 77/116 loss: 0.0617\n",
      "Epoch 53: Batch 78/116 loss: 0.0771\n",
      "Epoch 53: Batch 79/116 loss: 0.0579\n",
      "Epoch 53: Batch 80/116 loss: 0.0705\n",
      "Epoch 53: Batch 81/116 loss: 0.0534\n",
      "Epoch 53: Batch 82/116 loss: 0.0516\n",
      "Epoch 53: Batch 83/116 loss: 0.0493\n",
      "Epoch 53: Batch 84/116 loss: 0.0638\n",
      "Epoch 53: Batch 85/116 loss: 0.0721\n",
      "Epoch 53: Batch 86/116 loss: 0.0748\n",
      "Epoch 53: Batch 87/116 loss: 0.0749\n",
      "Epoch 53: Batch 88/116 loss: 0.0553\n",
      "Epoch 53: Batch 89/116 loss: 0.0592\n",
      "Epoch 53: Batch 90/116 loss: 0.0513\n",
      "Epoch 53: Batch 91/116 loss: 0.0724\n",
      "Epoch 53: Batch 92/116 loss: 0.0518\n",
      "Epoch 53: Batch 93/116 loss: 0.0799\n",
      "Epoch 53: Batch 94/116 loss: 0.0706\n",
      "Epoch 53: Batch 95/116 loss: 0.0609\n",
      "Epoch 53: Batch 96/116 loss: 0.0658\n",
      "Epoch 53: Batch 97/116 loss: 0.0790\n",
      "Epoch 53: Batch 98/116 loss: 0.0537\n",
      "Epoch 53: Batch 99/116 loss: 0.0611\n",
      "Epoch 53: Batch 100/116 loss: 0.0644\n",
      "Epoch 53: Batch 101/116 loss: 0.0733\n",
      "Epoch 53: Batch 102/116 loss: 0.0607\n",
      "Epoch 53: Batch 103/116 loss: 0.0602\n",
      "Epoch 53: Batch 104/116 loss: 0.0640\n",
      "Epoch 53: Batch 105/116 loss: 0.0583\n",
      "Epoch 53: Batch 106/116 loss: 0.0429\n",
      "Epoch 53: Batch 107/116 loss: 0.0765\n",
      "Epoch 53: Batch 108/116 loss: 0.0625\n",
      "Epoch 53: Batch 109/116 loss: 0.1070\n",
      "Epoch 53: Batch 110/116 loss: 0.0667\n",
      "Epoch 53: Batch 111/116 loss: 0.0512\n",
      "Epoch 53: Batch 112/116 loss: 0.0871\n",
      "Epoch 53: Batch 113/116 loss: 0.0927\n",
      "Epoch 53: Batch 114/116 loss: 0.0870\n",
      "Epoch 53: Batch 115/116 loss: 0.0804\n",
      "Epoch 53: Batch 116/116 loss: 0.0804\n",
      "Epoch 53 train loss: 0.0667 valid loss: 0.0746\n",
      "performance reducing: 1\n",
      "Epoch 54: Batch 1/116 loss: 0.0757\n",
      "Epoch 54: Batch 2/116 loss: 0.0814\n",
      "Epoch 54: Batch 3/116 loss: 0.0644\n",
      "Epoch 54: Batch 4/116 loss: 0.0623\n",
      "Epoch 54: Batch 5/116 loss: 0.0451\n",
      "Epoch 54: Batch 6/116 loss: 0.0652\n",
      "Epoch 54: Batch 7/116 loss: 0.0698\n",
      "Epoch 54: Batch 8/116 loss: 0.0910\n",
      "Epoch 54: Batch 9/116 loss: 0.0745\n",
      "Epoch 54: Batch 10/116 loss: 0.0661\n",
      "Epoch 54: Batch 11/116 loss: 0.0578\n",
      "Epoch 54: Batch 12/116 loss: 0.0729\n",
      "Epoch 54: Batch 13/116 loss: 0.0842\n",
      "Epoch 54: Batch 14/116 loss: 0.0496\n",
      "Epoch 54: Batch 15/116 loss: 0.0550\n",
      "Epoch 54: Batch 16/116 loss: 0.0598\n",
      "Epoch 54: Batch 17/116 loss: 0.0726\n",
      "Epoch 54: Batch 18/116 loss: 0.0928\n",
      "Epoch 54: Batch 19/116 loss: 0.0779\n",
      "Epoch 54: Batch 20/116 loss: 0.0571\n",
      "Epoch 54: Batch 21/116 loss: 0.0534\n",
      "Epoch 54: Batch 22/116 loss: 0.0627\n",
      "Epoch 54: Batch 23/116 loss: 0.0640\n",
      "Epoch 54: Batch 24/116 loss: 0.0700\n",
      "Epoch 54: Batch 25/116 loss: 0.0660\n",
      "Epoch 54: Batch 26/116 loss: 0.0890\n",
      "Epoch 54: Batch 27/116 loss: 0.0797\n",
      "Epoch 54: Batch 28/116 loss: 0.0589\n",
      "Epoch 54: Batch 29/116 loss: 0.0398\n",
      "Epoch 54: Batch 30/116 loss: 0.0582\n",
      "Epoch 54: Batch 31/116 loss: 0.0781\n",
      "Epoch 54: Batch 32/116 loss: 0.0607\n",
      "Epoch 54: Batch 33/116 loss: 0.0694\n",
      "Epoch 54: Batch 34/116 loss: 0.0586\n",
      "Epoch 54: Batch 35/116 loss: 0.0550\n",
      "Epoch 54: Batch 36/116 loss: 0.0533\n",
      "Epoch 54: Batch 37/116 loss: 0.0686\n",
      "Epoch 54: Batch 38/116 loss: 0.0699\n",
      "Epoch 54: Batch 39/116 loss: 0.0516\n",
      "Epoch 54: Batch 40/116 loss: 0.0605\n",
      "Epoch 54: Batch 41/116 loss: 0.0673\n",
      "Epoch 54: Batch 42/116 loss: 0.0668\n",
      "Epoch 54: Batch 43/116 loss: 0.0540\n",
      "Epoch 54: Batch 44/116 loss: 0.0801\n",
      "Epoch 54: Batch 45/116 loss: 0.0614\n",
      "Epoch 54: Batch 46/116 loss: 0.0369\n",
      "Epoch 54: Batch 47/116 loss: 0.0513\n",
      "Epoch 54: Batch 48/116 loss: 0.0519\n",
      "Epoch 54: Batch 49/116 loss: 0.1110\n",
      "Epoch 54: Batch 50/116 loss: 0.0717\n",
      "Epoch 54: Batch 51/116 loss: 0.0682\n",
      "Epoch 54: Batch 52/116 loss: 0.0540\n",
      "Epoch 54: Batch 53/116 loss: 0.0795\n",
      "Epoch 54: Batch 54/116 loss: 0.0670\n",
      "Epoch 54: Batch 55/116 loss: 0.0682\n",
      "Epoch 54: Batch 56/116 loss: 0.0522\n",
      "Epoch 54: Batch 57/116 loss: 0.0442\n",
      "Epoch 54: Batch 58/116 loss: 0.0886\n",
      "Epoch 54: Batch 59/116 loss: 0.0650\n",
      "Epoch 54: Batch 60/116 loss: 0.0870\n",
      "Epoch 54: Batch 61/116 loss: 0.0480\n",
      "Epoch 54: Batch 62/116 loss: 0.0826\n",
      "Epoch 54: Batch 63/116 loss: 0.0505\n",
      "Epoch 54: Batch 64/116 loss: 0.0481\n",
      "Epoch 54: Batch 65/116 loss: 0.0598\n",
      "Epoch 54: Batch 66/116 loss: 0.0885\n",
      "Epoch 54: Batch 67/116 loss: 0.1223\n",
      "Epoch 54: Batch 68/116 loss: 0.0468\n",
      "Epoch 54: Batch 69/116 loss: 0.0735\n",
      "Epoch 54: Batch 70/116 loss: 0.0589\n",
      "Epoch 54: Batch 71/116 loss: 0.0766\n",
      "Epoch 54: Batch 72/116 loss: 0.0978\n",
      "Epoch 54: Batch 73/116 loss: 0.0813\n",
      "Epoch 54: Batch 74/116 loss: 0.0630\n",
      "Epoch 54: Batch 75/116 loss: 0.0541\n",
      "Epoch 54: Batch 76/116 loss: 0.0802\n",
      "Epoch 54: Batch 77/116 loss: 0.0673\n",
      "Epoch 54: Batch 78/116 loss: 0.0743\n",
      "Epoch 54: Batch 79/116 loss: 0.0628\n",
      "Epoch 54: Batch 80/116 loss: 0.0771\n",
      "Epoch 54: Batch 81/116 loss: 0.0556\n",
      "Epoch 54: Batch 82/116 loss: 0.0596\n",
      "Epoch 54: Batch 83/116 loss: 0.0603\n",
      "Epoch 54: Batch 84/116 loss: 0.0550\n",
      "Epoch 54: Batch 85/116 loss: 0.0834\n",
      "Epoch 54: Batch 86/116 loss: 0.0585\n",
      "Epoch 54: Batch 87/116 loss: 0.0691\n",
      "Epoch 54: Batch 88/116 loss: 0.0645\n",
      "Epoch 54: Batch 89/116 loss: 0.0677\n",
      "Epoch 54: Batch 90/116 loss: 0.0575\n",
      "Epoch 54: Batch 91/116 loss: 0.0678\n",
      "Epoch 54: Batch 92/116 loss: 0.0629\n",
      "Epoch 54: Batch 93/116 loss: 0.0433\n",
      "Epoch 54: Batch 94/116 loss: 0.0644\n",
      "Epoch 54: Batch 95/116 loss: 0.0396\n",
      "Epoch 54: Batch 96/116 loss: 0.0658\n",
      "Epoch 54: Batch 97/116 loss: 0.0503\n",
      "Epoch 54: Batch 98/116 loss: 0.0543\n",
      "Epoch 54: Batch 99/116 loss: 0.0583\n",
      "Epoch 54: Batch 100/116 loss: 0.0478\n",
      "Epoch 54: Batch 101/116 loss: 0.0857\n",
      "Epoch 54: Batch 102/116 loss: 0.1544\n",
      "Epoch 54: Batch 103/116 loss: 0.0550\n",
      "Epoch 54: Batch 104/116 loss: 0.0917\n",
      "Epoch 54: Batch 105/116 loss: 0.0782\n",
      "Epoch 54: Batch 106/116 loss: 0.0774\n",
      "Epoch 54: Batch 107/116 loss: 0.0699\n",
      "Epoch 54: Batch 108/116 loss: 0.0648\n",
      "Epoch 54: Batch 109/116 loss: 0.0635\n",
      "Epoch 54: Batch 110/116 loss: 0.0436\n",
      "Epoch 54: Batch 111/116 loss: 0.1005\n",
      "Epoch 54: Batch 112/116 loss: 0.0780\n",
      "Epoch 54: Batch 113/116 loss: 0.0773\n",
      "Epoch 54: Batch 114/116 loss: 0.0849\n",
      "Epoch 54: Batch 115/116 loss: 0.1324\n",
      "Epoch 54: Batch 116/116 loss: 0.0521\n",
      "Epoch 54 train loss: 0.0679 valid loss: 0.1365\n",
      "performance reducing: 2\n",
      "Epoch 55: Batch 1/116 loss: 0.0516\n",
      "Epoch 55: Batch 2/116 loss: 0.0702\n",
      "Epoch 55: Batch 3/116 loss: 0.0602\n",
      "Epoch 55: Batch 4/116 loss: 0.0547\n",
      "Epoch 55: Batch 5/116 loss: 0.0652\n",
      "Epoch 55: Batch 6/116 loss: 0.0670\n",
      "Epoch 55: Batch 7/116 loss: 0.0615\n",
      "Epoch 55: Batch 8/116 loss: 0.0770\n",
      "Epoch 55: Batch 9/116 loss: 0.0659\n",
      "Epoch 55: Batch 10/116 loss: 0.0460\n",
      "Epoch 55: Batch 11/116 loss: 0.0568\n",
      "Epoch 55: Batch 12/116 loss: 0.0503\n",
      "Epoch 55: Batch 13/116 loss: 0.0547\n",
      "Epoch 55: Batch 14/116 loss: 0.0579\n",
      "Epoch 55: Batch 15/116 loss: 0.0549\n",
      "Epoch 55: Batch 16/116 loss: 0.0486\n",
      "Epoch 55: Batch 17/116 loss: 0.0796\n",
      "Epoch 55: Batch 18/116 loss: 0.0758\n",
      "Epoch 55: Batch 19/116 loss: 0.0566\n",
      "Epoch 55: Batch 20/116 loss: 0.0475\n",
      "Epoch 55: Batch 21/116 loss: 0.0486\n",
      "Epoch 55: Batch 22/116 loss: 0.0582\n",
      "Epoch 55: Batch 23/116 loss: 0.1023\n",
      "Epoch 55: Batch 24/116 loss: 0.0715\n",
      "Epoch 55: Batch 25/116 loss: 0.0569\n",
      "Epoch 55: Batch 26/116 loss: 0.0757\n",
      "Epoch 55: Batch 27/116 loss: 0.0662\n",
      "Epoch 55: Batch 28/116 loss: 0.0702\n",
      "Epoch 55: Batch 29/116 loss: 0.0520\n",
      "Epoch 55: Batch 30/116 loss: 0.0667\n",
      "Epoch 55: Batch 31/116 loss: 0.0672\n",
      "Epoch 55: Batch 32/116 loss: 0.0522\n",
      "Epoch 55: Batch 33/116 loss: 0.0592\n",
      "Epoch 55: Batch 34/116 loss: 0.0521\n",
      "Epoch 55: Batch 35/116 loss: 0.0597\n",
      "Epoch 55: Batch 36/116 loss: 0.0502\n",
      "Epoch 55: Batch 37/116 loss: 0.0749\n",
      "Epoch 55: Batch 38/116 loss: 0.0638\n",
      "Epoch 55: Batch 39/116 loss: 0.0513\n",
      "Epoch 55: Batch 40/116 loss: 0.0687\n",
      "Epoch 55: Batch 41/116 loss: 0.0697\n",
      "Epoch 55: Batch 42/116 loss: 0.0774\n",
      "Epoch 55: Batch 43/116 loss: 0.1013\n",
      "Epoch 55: Batch 44/116 loss: 0.0734\n",
      "Epoch 55: Batch 45/116 loss: 0.0566\n",
      "Epoch 55: Batch 46/116 loss: 0.0577\n",
      "Epoch 55: Batch 47/116 loss: 0.0640\n",
      "Epoch 55: Batch 48/116 loss: 0.0671\n",
      "Epoch 55: Batch 49/116 loss: 0.0745\n",
      "Epoch 55: Batch 50/116 loss: 0.0820\n",
      "Epoch 55: Batch 51/116 loss: 0.0742\n",
      "Epoch 55: Batch 52/116 loss: 0.0533\n",
      "Epoch 55: Batch 53/116 loss: 0.0806\n",
      "Epoch 55: Batch 54/116 loss: 0.0770\n",
      "Epoch 55: Batch 55/116 loss: 0.0745\n",
      "Epoch 55: Batch 56/116 loss: 0.0733\n",
      "Epoch 55: Batch 57/116 loss: 0.0604\n",
      "Epoch 55: Batch 58/116 loss: 0.0566\n",
      "Epoch 55: Batch 59/116 loss: 0.0617\n",
      "Epoch 55: Batch 60/116 loss: 0.0681\n",
      "Epoch 55: Batch 61/116 loss: 0.0729\n",
      "Epoch 55: Batch 62/116 loss: 0.0730\n",
      "Epoch 55: Batch 63/116 loss: 0.0561\n",
      "Epoch 55: Batch 64/116 loss: 0.0681\n",
      "Epoch 55: Batch 65/116 loss: 0.0703\n",
      "Epoch 55: Batch 66/116 loss: 0.0711\n",
      "Epoch 55: Batch 67/116 loss: 0.0416\n",
      "Epoch 55: Batch 68/116 loss: 0.0480\n",
      "Epoch 55: Batch 69/116 loss: 0.0769\n",
      "Epoch 55: Batch 70/116 loss: 0.0434\n",
      "Epoch 55: Batch 71/116 loss: 0.0384\n",
      "Epoch 55: Batch 72/116 loss: 0.0638\n",
      "Epoch 55: Batch 73/116 loss: 0.0735\n",
      "Epoch 55: Batch 74/116 loss: 0.0556\n",
      "Epoch 55: Batch 75/116 loss: 0.0588\n",
      "Epoch 55: Batch 76/116 loss: 0.0851\n",
      "Epoch 55: Batch 77/116 loss: 0.0569\n",
      "Epoch 55: Batch 78/116 loss: 0.0884\n",
      "Epoch 55: Batch 79/116 loss: 0.0673\n",
      "Epoch 55: Batch 80/116 loss: 0.1053\n",
      "Epoch 55: Batch 81/116 loss: 0.0828\n",
      "Epoch 55: Batch 82/116 loss: 0.0620\n",
      "Epoch 55: Batch 83/116 loss: 0.0680\n",
      "Epoch 55: Batch 84/116 loss: 0.0804\n",
      "Epoch 55: Batch 85/116 loss: 0.0735\n",
      "Epoch 55: Batch 86/116 loss: 0.0631\n",
      "Epoch 55: Batch 87/116 loss: 0.0687\n",
      "Epoch 55: Batch 88/116 loss: 0.0614\n",
      "Epoch 55: Batch 89/116 loss: 0.0923\n",
      "Epoch 55: Batch 90/116 loss: 0.0788\n",
      "Epoch 55: Batch 91/116 loss: 0.0658\n",
      "Epoch 55: Batch 92/116 loss: 0.0683\n",
      "Epoch 55: Batch 93/116 loss: 0.0738\n",
      "Epoch 55: Batch 94/116 loss: 0.0595\n",
      "Epoch 55: Batch 95/116 loss: 0.0780\n",
      "Epoch 55: Batch 96/116 loss: 0.0466\n",
      "Epoch 55: Batch 97/116 loss: 0.1417\n",
      "Epoch 55: Batch 98/116 loss: 0.0647\n",
      "Epoch 55: Batch 99/116 loss: 0.0549\n",
      "Epoch 55: Batch 100/116 loss: 0.0875\n",
      "Epoch 55: Batch 101/116 loss: 0.0978\n",
      "Epoch 55: Batch 102/116 loss: 0.0520\n",
      "Epoch 55: Batch 103/116 loss: 0.0559\n",
      "Epoch 55: Batch 104/116 loss: 0.0581\n",
      "Epoch 55: Batch 105/116 loss: 0.0711\n",
      "Epoch 55: Batch 106/116 loss: 0.0627\n",
      "Epoch 55: Batch 107/116 loss: 0.0299\n",
      "Epoch 55: Batch 108/116 loss: 0.0558\n",
      "Epoch 55: Batch 109/116 loss: 0.0617\n",
      "Epoch 55: Batch 110/116 loss: 0.0577\n",
      "Epoch 55: Batch 111/116 loss: 0.0586\n",
      "Epoch 55: Batch 112/116 loss: 0.0964\n",
      "Epoch 55: Batch 113/116 loss: 0.0802\n",
      "Epoch 55: Batch 114/116 loss: 0.0531\n",
      "Epoch 55: Batch 115/116 loss: 0.0879\n",
      "Epoch 55: Batch 116/116 loss: 0.0833\n",
      "Epoch 55 train loss: 0.0666 valid loss: 0.0864\n",
      "performance reducing: 3\n",
      "Epoch 56: Batch 1/116 loss: 0.0734\n",
      "Epoch 56: Batch 2/116 loss: 0.0564\n",
      "Epoch 56: Batch 3/116 loss: 0.0541\n",
      "Epoch 56: Batch 4/116 loss: 0.0653\n",
      "Epoch 56: Batch 5/116 loss: 0.0644\n",
      "Epoch 56: Batch 6/116 loss: 0.1033\n",
      "Epoch 56: Batch 7/116 loss: 0.0717\n",
      "Epoch 56: Batch 8/116 loss: 0.0663\n",
      "Epoch 56: Batch 9/116 loss: 0.0678\n",
      "Epoch 56: Batch 10/116 loss: 0.0662\n",
      "Epoch 56: Batch 11/116 loss: 0.0698\n",
      "Epoch 56: Batch 12/116 loss: 0.0753\n",
      "Epoch 56: Batch 13/116 loss: 0.0877\n",
      "Epoch 56: Batch 14/116 loss: 0.0640\n",
      "Epoch 56: Batch 15/116 loss: 0.0566\n",
      "Epoch 56: Batch 16/116 loss: 0.0513\n",
      "Epoch 56: Batch 17/116 loss: 0.0647\n",
      "Epoch 56: Batch 18/116 loss: 0.0886\n",
      "Epoch 56: Batch 19/116 loss: 0.0674\n",
      "Epoch 56: Batch 20/116 loss: 0.0592\n",
      "Epoch 56: Batch 21/116 loss: 0.0906\n",
      "Epoch 56: Batch 22/116 loss: 0.0552\n",
      "Epoch 56: Batch 23/116 loss: 0.0576\n",
      "Epoch 56: Batch 24/116 loss: 0.0832\n",
      "Epoch 56: Batch 25/116 loss: 0.0628\n",
      "Epoch 56: Batch 26/116 loss: 0.0538\n",
      "Epoch 56: Batch 27/116 loss: 0.0555\n",
      "Epoch 56: Batch 28/116 loss: 0.0560\n",
      "Epoch 56: Batch 29/116 loss: 0.0723\n",
      "Epoch 56: Batch 30/116 loss: 0.0785\n",
      "Epoch 56: Batch 31/116 loss: 0.0452\n",
      "Epoch 56: Batch 32/116 loss: 0.0526\n",
      "Epoch 56: Batch 33/116 loss: 0.0535\n",
      "Epoch 56: Batch 34/116 loss: 0.0755\n",
      "Epoch 56: Batch 35/116 loss: 0.0719\n",
      "Epoch 56: Batch 36/116 loss: 0.0527\n",
      "Epoch 56: Batch 37/116 loss: 0.0521\n",
      "Epoch 56: Batch 38/116 loss: 0.0538\n",
      "Epoch 56: Batch 39/116 loss: 0.0516\n",
      "Epoch 56: Batch 40/116 loss: 0.0473\n",
      "Epoch 56: Batch 41/116 loss: 0.0730\n",
      "Epoch 56: Batch 42/116 loss: 0.0637\n",
      "Epoch 56: Batch 43/116 loss: 0.0520\n",
      "Epoch 56: Batch 44/116 loss: 0.0550\n",
      "Epoch 56: Batch 45/116 loss: 0.0492\n",
      "Epoch 56: Batch 46/116 loss: 0.0662\n",
      "Epoch 56: Batch 47/116 loss: 0.0583\n",
      "Epoch 56: Batch 48/116 loss: 0.0504\n",
      "Epoch 56: Batch 49/116 loss: 0.0394\n",
      "Epoch 56: Batch 50/116 loss: 0.0451\n",
      "Epoch 56: Batch 51/116 loss: 0.0553\n",
      "Epoch 56: Batch 52/116 loss: 0.0451\n",
      "Epoch 56: Batch 53/116 loss: 0.0697\n",
      "Epoch 56: Batch 54/116 loss: 0.0520\n",
      "Epoch 56: Batch 55/116 loss: 0.0571\n",
      "Epoch 56: Batch 56/116 loss: 0.0571\n",
      "Epoch 56: Batch 57/116 loss: 0.0513\n",
      "Epoch 56: Batch 58/116 loss: 0.0545\n",
      "Epoch 56: Batch 59/116 loss: 0.0907\n",
      "Epoch 56: Batch 60/116 loss: 0.0634\n",
      "Epoch 56: Batch 61/116 loss: 0.0694\n",
      "Epoch 56: Batch 62/116 loss: 0.0777\n",
      "Epoch 56: Batch 63/116 loss: 0.0806\n",
      "Epoch 56: Batch 64/116 loss: 0.0736\n",
      "Epoch 56: Batch 65/116 loss: 0.0506\n",
      "Epoch 56: Batch 66/116 loss: 0.0685\n",
      "Epoch 56: Batch 67/116 loss: 0.0785\n",
      "Epoch 56: Batch 68/116 loss: 0.0698\n",
      "Epoch 56: Batch 69/116 loss: 0.0917\n",
      "Epoch 56: Batch 70/116 loss: 0.0560\n",
      "Epoch 56: Batch 71/116 loss: 0.0741\n",
      "Epoch 56: Batch 72/116 loss: 0.0563\n",
      "Epoch 56: Batch 73/116 loss: 0.0808\n",
      "Epoch 56: Batch 74/116 loss: 0.0564\n",
      "Epoch 56: Batch 75/116 loss: 0.0530\n",
      "Epoch 56: Batch 76/116 loss: 0.0549\n",
      "Epoch 56: Batch 77/116 loss: 0.0770\n",
      "Epoch 56: Batch 78/116 loss: 0.0682\n",
      "Epoch 56: Batch 79/116 loss: 0.0702\n",
      "Epoch 56: Batch 80/116 loss: 0.0752\n",
      "Epoch 56: Batch 81/116 loss: 0.0574\n",
      "Epoch 56: Batch 82/116 loss: 0.0612\n",
      "Epoch 56: Batch 83/116 loss: 0.0687\n",
      "Epoch 56: Batch 84/116 loss: 0.0541\n",
      "Epoch 56: Batch 85/116 loss: 0.0637\n",
      "Epoch 56: Batch 86/116 loss: 0.0639\n",
      "Epoch 56: Batch 87/116 loss: 0.0358\n",
      "Epoch 56: Batch 88/116 loss: 0.0463\n",
      "Epoch 56: Batch 89/116 loss: 0.0556\n",
      "Epoch 56: Batch 90/116 loss: 0.0598\n",
      "Epoch 56: Batch 91/116 loss: 0.0796\n",
      "Epoch 56: Batch 92/116 loss: 0.0733\n",
      "Epoch 56: Batch 93/116 loss: 0.0943\n",
      "Epoch 56: Batch 94/116 loss: 0.0900\n",
      "Epoch 56: Batch 95/116 loss: 0.0643\n",
      "Epoch 56: Batch 96/116 loss: 0.0574\n",
      "Epoch 56: Batch 97/116 loss: 0.0614\n",
      "Epoch 56: Batch 98/116 loss: 0.0902\n",
      "Epoch 56: Batch 99/116 loss: 0.0493\n",
      "Epoch 56: Batch 100/116 loss: 0.0747\n",
      "Epoch 56: Batch 101/116 loss: 0.0638\n",
      "Epoch 56: Batch 102/116 loss: 0.0668\n",
      "Epoch 56: Batch 103/116 loss: 0.0525\n",
      "Epoch 56: Batch 104/116 loss: 0.0472\n",
      "Epoch 56: Batch 105/116 loss: 0.0854\n",
      "Epoch 56: Batch 106/116 loss: 0.0505\n",
      "Epoch 56: Batch 107/116 loss: 0.0791\n",
      "Epoch 56: Batch 108/116 loss: 0.0580\n",
      "Epoch 56: Batch 109/116 loss: 0.0602\n",
      "Epoch 56: Batch 110/116 loss: 0.0629\n",
      "Epoch 56: Batch 111/116 loss: 0.1319\n",
      "Epoch 56: Batch 112/116 loss: 0.0564\n",
      "Epoch 56: Batch 113/116 loss: 0.0520\n",
      "Epoch 56: Batch 114/116 loss: 0.0485\n",
      "Epoch 56: Batch 115/116 loss: 0.0637\n",
      "Epoch 56: Batch 116/116 loss: 0.0712\n",
      "Epoch 56 train loss: 0.0645 valid loss: 0.0652\n",
      "Epoch 57: Batch 1/116 loss: 0.0845\n",
      "Epoch 57: Batch 2/116 loss: 0.0616\n",
      "Epoch 57: Batch 3/116 loss: 0.0643\n",
      "Epoch 57: Batch 4/116 loss: 0.0499\n",
      "Epoch 57: Batch 5/116 loss: 0.0752\n",
      "Epoch 57: Batch 6/116 loss: 0.0691\n",
      "Epoch 57: Batch 7/116 loss: 0.0596\n",
      "Epoch 57: Batch 8/116 loss: 0.0658\n",
      "Epoch 57: Batch 9/116 loss: 0.0595\n",
      "Epoch 57: Batch 10/116 loss: 0.0700\n",
      "Epoch 57: Batch 11/116 loss: 0.0513\n",
      "Epoch 57: Batch 12/116 loss: 0.0662\n",
      "Epoch 57: Batch 13/116 loss: 0.0385\n",
      "Epoch 57: Batch 14/116 loss: 0.0623\n",
      "Epoch 57: Batch 15/116 loss: 0.0736\n",
      "Epoch 57: Batch 16/116 loss: 0.0442\n",
      "Epoch 57: Batch 17/116 loss: 0.0528\n",
      "Epoch 57: Batch 18/116 loss: 0.0620\n",
      "Epoch 57: Batch 19/116 loss: 0.0441\n",
      "Epoch 57: Batch 20/116 loss: 0.0656\n",
      "Epoch 57: Batch 21/116 loss: 0.0886\n",
      "Epoch 57: Batch 22/116 loss: 0.0646\n",
      "Epoch 57: Batch 23/116 loss: 0.0456\n",
      "Epoch 57: Batch 24/116 loss: 0.0562\n",
      "Epoch 57: Batch 25/116 loss: 0.0769\n",
      "Epoch 57: Batch 26/116 loss: 0.0711\n",
      "Epoch 57: Batch 27/116 loss: 0.0792\n",
      "Epoch 57: Batch 28/116 loss: 0.0501\n",
      "Epoch 57: Batch 29/116 loss: 0.0507\n",
      "Epoch 57: Batch 30/116 loss: 0.0733\n",
      "Epoch 57: Batch 31/116 loss: 0.0654\n",
      "Epoch 57: Batch 32/116 loss: 0.0783\n",
      "Epoch 57: Batch 33/116 loss: 0.0571\n",
      "Epoch 57: Batch 34/116 loss: 0.0559\n",
      "Epoch 57: Batch 35/116 loss: 0.0609\n",
      "Epoch 57: Batch 36/116 loss: 0.0579\n",
      "Epoch 57: Batch 37/116 loss: 0.0638\n",
      "Epoch 57: Batch 38/116 loss: 0.0537\n",
      "Epoch 57: Batch 39/116 loss: 0.0665\n",
      "Epoch 57: Batch 40/116 loss: 0.0371\n",
      "Epoch 57: Batch 41/116 loss: 0.0441\n",
      "Epoch 57: Batch 42/116 loss: 0.0634\n",
      "Epoch 57: Batch 43/116 loss: 0.0589\n",
      "Epoch 57: Batch 44/116 loss: 0.0595\n",
      "Epoch 57: Batch 45/116 loss: 0.0508\n",
      "Epoch 57: Batch 46/116 loss: 0.0611\n",
      "Epoch 57: Batch 47/116 loss: 0.0445\n",
      "Epoch 57: Batch 48/116 loss: 0.0538\n",
      "Epoch 57: Batch 49/116 loss: 0.0545\n",
      "Epoch 57: Batch 50/116 loss: 0.0680\n",
      "Epoch 57: Batch 51/116 loss: 0.0704\n",
      "Epoch 57: Batch 52/116 loss: 0.0474\n",
      "Epoch 57: Batch 53/116 loss: 0.0547\n",
      "Epoch 57: Batch 54/116 loss: 0.0757\n",
      "Epoch 57: Batch 55/116 loss: 0.0361\n",
      "Epoch 57: Batch 56/116 loss: 0.0896\n",
      "Epoch 57: Batch 57/116 loss: 0.0611\n",
      "Epoch 57: Batch 58/116 loss: 0.0540\n",
      "Epoch 57: Batch 59/116 loss: 0.0680\n",
      "Epoch 57: Batch 60/116 loss: 0.0595\n",
      "Epoch 57: Batch 61/116 loss: 0.0749\n",
      "Epoch 57: Batch 62/116 loss: 0.0563\n",
      "Epoch 57: Batch 63/116 loss: 0.0770\n",
      "Epoch 57: Batch 64/116 loss: 0.0636\n",
      "Epoch 57: Batch 65/116 loss: 0.0701\n",
      "Epoch 57: Batch 66/116 loss: 0.0730\n",
      "Epoch 57: Batch 67/116 loss: 0.0692\n",
      "Epoch 57: Batch 68/116 loss: 0.0650\n",
      "Epoch 57: Batch 69/116 loss: 0.0555\n",
      "Epoch 57: Batch 70/116 loss: 0.0582\n",
      "Epoch 57: Batch 71/116 loss: 0.0444\n",
      "Epoch 57: Batch 72/116 loss: 0.0741\n",
      "Epoch 57: Batch 73/116 loss: 0.0528\n",
      "Epoch 57: Batch 74/116 loss: 0.0683\n",
      "Epoch 57: Batch 75/116 loss: 0.0537\n",
      "Epoch 57: Batch 76/116 loss: 0.0564\n",
      "Epoch 57: Batch 77/116 loss: 0.0549\n",
      "Epoch 57: Batch 78/116 loss: 0.0766\n",
      "Epoch 57: Batch 79/116 loss: 0.0612\n",
      "Epoch 57: Batch 80/116 loss: 0.0797\n",
      "Epoch 57: Batch 81/116 loss: 0.0727\n",
      "Epoch 57: Batch 82/116 loss: 0.0627\n",
      "Epoch 57: Batch 83/116 loss: 0.1040\n",
      "Epoch 57: Batch 84/116 loss: 0.0780\n",
      "Epoch 57: Batch 85/116 loss: 0.0788\n",
      "Epoch 57: Batch 86/116 loss: 0.0749\n",
      "Epoch 57: Batch 87/116 loss: 0.0717\n",
      "Epoch 57: Batch 88/116 loss: 0.0535\n",
      "Epoch 57: Batch 89/116 loss: 0.0555\n",
      "Epoch 57: Batch 90/116 loss: 0.0451\n",
      "Epoch 57: Batch 91/116 loss: 0.0803\n",
      "Epoch 57: Batch 92/116 loss: 0.0640\n",
      "Epoch 57: Batch 93/116 loss: 0.0463\n",
      "Epoch 57: Batch 94/116 loss: 0.0497\n",
      "Epoch 57: Batch 95/116 loss: 0.0549\n",
      "Epoch 57: Batch 96/116 loss: 0.0746\n",
      "Epoch 57: Batch 97/116 loss: 0.0569\n",
      "Epoch 57: Batch 98/116 loss: 0.0888\n",
      "Epoch 57: Batch 99/116 loss: 0.1165\n",
      "Epoch 57: Batch 100/116 loss: 0.0697\n",
      "Epoch 57: Batch 101/116 loss: 0.0722\n",
      "Epoch 57: Batch 102/116 loss: 0.0633\n",
      "Epoch 57: Batch 103/116 loss: 0.0674\n",
      "Epoch 57: Batch 104/116 loss: 0.0515\n",
      "Epoch 57: Batch 105/116 loss: 0.0530\n",
      "Epoch 57: Batch 106/116 loss: 0.0615\n",
      "Epoch 57: Batch 107/116 loss: 0.0716\n",
      "Epoch 57: Batch 108/116 loss: 0.0723\n",
      "Epoch 57: Batch 109/116 loss: 0.0545\n",
      "Epoch 57: Batch 110/116 loss: 0.0538\n",
      "Epoch 57: Batch 111/116 loss: 0.0526\n",
      "Epoch 57: Batch 112/116 loss: 0.0592\n",
      "Epoch 57: Batch 113/116 loss: 0.0812\n",
      "Epoch 57: Batch 114/116 loss: 0.0694\n",
      "Epoch 57: Batch 115/116 loss: 0.0581\n",
      "Epoch 57: Batch 116/116 loss: 0.0724\n",
      "Epoch 57 train loss: 0.0633 valid loss: 0.0674\n",
      "performance reducing: 1\n",
      "Epoch 58: Batch 1/116 loss: 0.0989\n",
      "Epoch 58: Batch 2/116 loss: 0.0562\n",
      "Epoch 58: Batch 3/116 loss: 0.0556\n",
      "Epoch 58: Batch 4/116 loss: 0.0781\n",
      "Epoch 58: Batch 5/116 loss: 0.0477\n",
      "Epoch 58: Batch 6/116 loss: 0.0566\n",
      "Epoch 58: Batch 7/116 loss: 0.0616\n",
      "Epoch 58: Batch 8/116 loss: 0.0729\n",
      "Epoch 58: Batch 9/116 loss: 0.0434\n",
      "Epoch 58: Batch 10/116 loss: 0.0410\n",
      "Epoch 58: Batch 11/116 loss: 0.0582\n",
      "Epoch 58: Batch 12/116 loss: 0.0899\n",
      "Epoch 58: Batch 13/116 loss: 0.0526\n",
      "Epoch 58: Batch 14/116 loss: 0.0628\n",
      "Epoch 58: Batch 15/116 loss: 0.0724\n",
      "Epoch 58: Batch 16/116 loss: 0.0545\n",
      "Epoch 58: Batch 17/116 loss: 0.0797\n",
      "Epoch 58: Batch 18/116 loss: 0.0732\n",
      "Epoch 58: Batch 19/116 loss: 0.0958\n",
      "Epoch 58: Batch 20/116 loss: 0.0577\n",
      "Epoch 58: Batch 21/116 loss: 0.0747\n",
      "Epoch 58: Batch 22/116 loss: 0.0958\n",
      "Epoch 58: Batch 23/116 loss: 0.0679\n",
      "Epoch 58: Batch 24/116 loss: 0.0481\n",
      "Epoch 58: Batch 25/116 loss: 0.0935\n",
      "Epoch 58: Batch 26/116 loss: 0.0720\n",
      "Epoch 58: Batch 27/116 loss: 0.0565\n",
      "Epoch 58: Batch 28/116 loss: 0.0617\n",
      "Epoch 58: Batch 29/116 loss: 0.0767\n",
      "Epoch 58: Batch 30/116 loss: 0.0526\n",
      "Epoch 58: Batch 31/116 loss: 0.0508\n",
      "Epoch 58: Batch 32/116 loss: 0.0595\n",
      "Epoch 58: Batch 33/116 loss: 0.0592\n",
      "Epoch 58: Batch 34/116 loss: 0.0669\n",
      "Epoch 58: Batch 35/116 loss: 0.0721\n",
      "Epoch 58: Batch 36/116 loss: 0.0489\n",
      "Epoch 58: Batch 37/116 loss: 0.0489\n",
      "Epoch 58: Batch 38/116 loss: 0.0751\n",
      "Epoch 58: Batch 39/116 loss: 0.0542\n",
      "Epoch 58: Batch 40/116 loss: 0.0451\n",
      "Epoch 58: Batch 41/116 loss: 0.0650\n",
      "Epoch 58: Batch 42/116 loss: 0.0774\n",
      "Epoch 58: Batch 43/116 loss: 0.0552\n",
      "Epoch 58: Batch 44/116 loss: 0.0595\n",
      "Epoch 58: Batch 45/116 loss: 0.0748\n",
      "Epoch 58: Batch 46/116 loss: 0.0617\n",
      "Epoch 58: Batch 47/116 loss: 0.0775\n",
      "Epoch 58: Batch 48/116 loss: 0.0546\n",
      "Epoch 58: Batch 49/116 loss: 0.0533\n",
      "Epoch 58: Batch 50/116 loss: 0.0813\n",
      "Epoch 58: Batch 51/116 loss: 0.0586\n",
      "Epoch 58: Batch 52/116 loss: 0.0648\n",
      "Epoch 58: Batch 53/116 loss: 0.0676\n",
      "Epoch 58: Batch 54/116 loss: 0.0698\n",
      "Epoch 58: Batch 55/116 loss: 0.0699\n",
      "Epoch 58: Batch 56/116 loss: 0.0594\n",
      "Epoch 58: Batch 57/116 loss: 0.0397\n",
      "Epoch 58: Batch 58/116 loss: 0.0695\n",
      "Epoch 58: Batch 59/116 loss: 0.0697\n",
      "Epoch 58: Batch 60/116 loss: 0.0663\n",
      "Epoch 58: Batch 61/116 loss: 0.0559\n",
      "Epoch 58: Batch 62/116 loss: 0.0469\n",
      "Epoch 58: Batch 63/116 loss: 0.0536\n",
      "Epoch 58: Batch 64/116 loss: 0.0587\n",
      "Epoch 58: Batch 65/116 loss: 0.0783\n",
      "Epoch 58: Batch 66/116 loss: 0.0494\n",
      "Epoch 58: Batch 67/116 loss: 0.1011\n",
      "Epoch 58: Batch 68/116 loss: 0.0758\n",
      "Epoch 58: Batch 69/116 loss: 0.0519\n",
      "Epoch 58: Batch 70/116 loss: 0.1007\n",
      "Epoch 58: Batch 71/116 loss: 0.0615\n",
      "Epoch 58: Batch 72/116 loss: 0.0604\n",
      "Epoch 58: Batch 73/116 loss: 0.0618\n",
      "Epoch 58: Batch 74/116 loss: 0.0669\n",
      "Epoch 58: Batch 75/116 loss: 0.0646\n",
      "Epoch 58: Batch 76/116 loss: 0.0955\n",
      "Epoch 58: Batch 77/116 loss: 0.0681\n",
      "Epoch 58: Batch 78/116 loss: 0.0813\n",
      "Epoch 58: Batch 79/116 loss: 0.0582\n",
      "Epoch 58: Batch 80/116 loss: 0.0582\n",
      "Epoch 58: Batch 81/116 loss: 0.0604\n",
      "Epoch 58: Batch 82/116 loss: 0.0492\n",
      "Epoch 58: Batch 83/116 loss: 0.0417\n",
      "Epoch 58: Batch 84/116 loss: 0.0921\n",
      "Epoch 58: Batch 85/116 loss: 0.0580\n",
      "Epoch 58: Batch 86/116 loss: 0.0554\n",
      "Epoch 58: Batch 87/116 loss: 0.0738\n",
      "Epoch 58: Batch 88/116 loss: 0.0446\n",
      "Epoch 58: Batch 89/116 loss: 0.0582\n",
      "Epoch 58: Batch 90/116 loss: 0.0533\n",
      "Epoch 58: Batch 91/116 loss: 0.0779\n",
      "Epoch 58: Batch 92/116 loss: 0.0532\n",
      "Epoch 58: Batch 93/116 loss: 0.1037\n",
      "Epoch 58: Batch 94/116 loss: 0.0862\n",
      "Epoch 58: Batch 95/116 loss: 0.0731\n",
      "Epoch 58: Batch 96/116 loss: 0.0646\n",
      "Epoch 58: Batch 97/116 loss: 0.0851\n",
      "Epoch 58: Batch 98/116 loss: 0.0823\n",
      "Epoch 58: Batch 99/116 loss: 0.0636\n",
      "Epoch 58: Batch 100/116 loss: 0.0647\n",
      "Epoch 58: Batch 101/116 loss: 0.0540\n",
      "Epoch 58: Batch 102/116 loss: 0.0592\n",
      "Epoch 58: Batch 103/116 loss: 0.0727\n",
      "Epoch 58: Batch 104/116 loss: 0.0695\n",
      "Epoch 58: Batch 105/116 loss: 0.0559\n",
      "Epoch 58: Batch 106/116 loss: 0.0668\n",
      "Epoch 58: Batch 107/116 loss: 0.0731\n",
      "Epoch 58: Batch 108/116 loss: 0.0631\n",
      "Epoch 58: Batch 109/116 loss: 0.0515\n",
      "Epoch 58: Batch 110/116 loss: 0.0735\n",
      "Epoch 58: Batch 111/116 loss: 0.0572\n",
      "Epoch 58: Batch 112/116 loss: 0.0654\n",
      "Epoch 58: Batch 113/116 loss: 0.0624\n",
      "Epoch 58: Batch 114/116 loss: 0.0739\n",
      "Epoch 58: Batch 115/116 loss: 0.0639\n",
      "Epoch 58: Batch 116/116 loss: 0.0707\n",
      "Epoch 58 train loss: 0.0656 valid loss: 0.0631\n",
      "Epoch 59: Batch 1/116 loss: 0.0499\n",
      "Epoch 59: Batch 2/116 loss: 0.0742\n",
      "Epoch 59: Batch 3/116 loss: 0.0604\n",
      "Epoch 59: Batch 4/116 loss: 0.0900\n",
      "Epoch 59: Batch 5/116 loss: 0.0477\n",
      "Epoch 59: Batch 6/116 loss: 0.0712\n",
      "Epoch 59: Batch 7/116 loss: 0.0623\n",
      "Epoch 59: Batch 8/116 loss: 0.0600\n",
      "Epoch 59: Batch 9/116 loss: 0.0498\n",
      "Epoch 59: Batch 10/116 loss: 0.0632\n",
      "Epoch 59: Batch 11/116 loss: 0.0938\n",
      "Epoch 59: Batch 12/116 loss: 0.0610\n",
      "Epoch 59: Batch 13/116 loss: 0.0415\n",
      "Epoch 59: Batch 14/116 loss: 0.0668\n",
      "Epoch 59: Batch 15/116 loss: 0.0947\n",
      "Epoch 59: Batch 16/116 loss: 0.0759\n",
      "Epoch 59: Batch 17/116 loss: 0.0544\n",
      "Epoch 59: Batch 18/116 loss: 0.0642\n",
      "Epoch 59: Batch 19/116 loss: 0.0562\n",
      "Epoch 59: Batch 20/116 loss: 0.0535\n",
      "Epoch 59: Batch 21/116 loss: 0.0553\n",
      "Epoch 59: Batch 22/116 loss: 0.0587\n",
      "Epoch 59: Batch 23/116 loss: 0.0689\n",
      "Epoch 59: Batch 24/116 loss: 0.0444\n",
      "Epoch 59: Batch 25/116 loss: 0.0438\n",
      "Epoch 59: Batch 26/116 loss: 0.0655\n",
      "Epoch 59: Batch 27/116 loss: 0.0638\n",
      "Epoch 59: Batch 28/116 loss: 0.0945\n",
      "Epoch 59: Batch 29/116 loss: 0.0747\n",
      "Epoch 59: Batch 30/116 loss: 0.0698\n",
      "Epoch 59: Batch 31/116 loss: 0.0536\n",
      "Epoch 59: Batch 32/116 loss: 0.0584\n",
      "Epoch 59: Batch 33/116 loss: 0.0594\n",
      "Epoch 59: Batch 34/116 loss: 0.0406\n",
      "Epoch 59: Batch 35/116 loss: 0.1030\n",
      "Epoch 59: Batch 36/116 loss: 0.0659\n",
      "Epoch 59: Batch 37/116 loss: 0.0525\n",
      "Epoch 59: Batch 38/116 loss: 0.0437\n",
      "Epoch 59: Batch 39/116 loss: 0.0582\n",
      "Epoch 59: Batch 40/116 loss: 0.0563\n",
      "Epoch 59: Batch 41/116 loss: 0.0628\n",
      "Epoch 59: Batch 42/116 loss: 0.0625\n",
      "Epoch 59: Batch 43/116 loss: 0.0501\n",
      "Epoch 59: Batch 44/116 loss: 0.0666\n",
      "Epoch 59: Batch 45/116 loss: 0.0570\n",
      "Epoch 59: Batch 46/116 loss: 0.0742\n",
      "Epoch 59: Batch 47/116 loss: 0.0671\n",
      "Epoch 59: Batch 48/116 loss: 0.0562\n",
      "Epoch 59: Batch 49/116 loss: 0.0666\n",
      "Epoch 59: Batch 50/116 loss: 0.0678\n",
      "Epoch 59: Batch 51/116 loss: 0.0730\n",
      "Epoch 59: Batch 52/116 loss: 0.0833\n",
      "Epoch 59: Batch 53/116 loss: 0.0636\n",
      "Epoch 59: Batch 54/116 loss: 0.0814\n",
      "Epoch 59: Batch 55/116 loss: 0.0630\n",
      "Epoch 59: Batch 56/116 loss: 0.0757\n",
      "Epoch 59: Batch 57/116 loss: 0.0635\n",
      "Epoch 59: Batch 58/116 loss: 0.0554\n",
      "Epoch 59: Batch 59/116 loss: 0.0719\n",
      "Epoch 59: Batch 60/116 loss: 0.0756\n",
      "Epoch 59: Batch 61/116 loss: 0.0422\n",
      "Epoch 59: Batch 62/116 loss: 0.0661\n",
      "Epoch 59: Batch 63/116 loss: 0.0601\n",
      "Epoch 59: Batch 64/116 loss: 0.0693\n",
      "Epoch 59: Batch 65/116 loss: 0.0830\n",
      "Epoch 59: Batch 66/116 loss: 0.0603\n",
      "Epoch 59: Batch 67/116 loss: 0.0609\n",
      "Epoch 59: Batch 68/116 loss: 0.0775\n",
      "Epoch 59: Batch 69/116 loss: 0.0695\n",
      "Epoch 59: Batch 70/116 loss: 0.0761\n",
      "Epoch 59: Batch 71/116 loss: 0.0690\n",
      "Epoch 59: Batch 72/116 loss: 0.0608\n",
      "Epoch 59: Batch 73/116 loss: 0.0503\n",
      "Epoch 59: Batch 74/116 loss: 0.0502\n",
      "Epoch 59: Batch 75/116 loss: 0.0657\n",
      "Epoch 59: Batch 76/116 loss: 0.0568\n",
      "Epoch 59: Batch 77/116 loss: 0.0419\n",
      "Epoch 59: Batch 78/116 loss: 0.0409\n",
      "Epoch 59: Batch 79/116 loss: 0.0894\n",
      "Epoch 59: Batch 80/116 loss: 0.0536\n",
      "Epoch 59: Batch 81/116 loss: 0.0578\n",
      "Epoch 59: Batch 82/116 loss: 0.0447\n",
      "Epoch 59: Batch 83/116 loss: 0.0447\n",
      "Epoch 59: Batch 84/116 loss: 0.0602\n",
      "Epoch 59: Batch 85/116 loss: 0.0527\n",
      "Epoch 59: Batch 86/116 loss: 0.0542\n",
      "Epoch 59: Batch 87/116 loss: 0.0587\n",
      "Epoch 59: Batch 88/116 loss: 0.0559\n",
      "Epoch 59: Batch 89/116 loss: 0.0411\n",
      "Epoch 59: Batch 90/116 loss: 0.0519\n",
      "Epoch 59: Batch 91/116 loss: 0.0454\n",
      "Epoch 59: Batch 92/116 loss: 0.0641\n",
      "Epoch 59: Batch 93/116 loss: 0.0742\n",
      "Epoch 59: Batch 94/116 loss: 0.0710\n",
      "Epoch 59: Batch 95/116 loss: 0.0436\n",
      "Epoch 59: Batch 96/116 loss: 0.0908\n",
      "Epoch 59: Batch 97/116 loss: 0.0636\n",
      "Epoch 59: Batch 98/116 loss: 0.0558\n",
      "Epoch 59: Batch 99/116 loss: 0.0689\n",
      "Epoch 59: Batch 100/116 loss: 0.0459\n",
      "Epoch 59: Batch 101/116 loss: 0.0420\n",
      "Epoch 59: Batch 102/116 loss: 0.0616\n",
      "Epoch 59: Batch 103/116 loss: 0.0456\n",
      "Epoch 59: Batch 104/116 loss: 0.0538\n",
      "Epoch 59: Batch 105/116 loss: 0.0661\n",
      "Epoch 59: Batch 106/116 loss: 0.0498\n",
      "Epoch 59: Batch 107/116 loss: 0.0698\n",
      "Epoch 59: Batch 108/116 loss: 0.0617\n",
      "Epoch 59: Batch 109/116 loss: 0.0482\n",
      "Epoch 59: Batch 110/116 loss: 0.0696\n",
      "Epoch 59: Batch 111/116 loss: 0.0745\n",
      "Epoch 59: Batch 112/116 loss: 0.0905\n",
      "Epoch 59: Batch 113/116 loss: 0.0617\n",
      "Epoch 59: Batch 114/116 loss: 0.0697\n",
      "Epoch 59: Batch 115/116 loss: 0.0635\n",
      "Epoch 59: Batch 116/116 loss: 0.0584\n",
      "Epoch 59 train loss: 0.0624 valid loss: 0.0677\n",
      "performance reducing: 1\n",
      "Epoch 60: Batch 1/116 loss: 0.0584\n",
      "Epoch 60: Batch 2/116 loss: 0.0687\n",
      "Epoch 60: Batch 3/116 loss: 0.0492\n",
      "Epoch 60: Batch 4/116 loss: 0.0490\n",
      "Epoch 60: Batch 5/116 loss: 0.0596\n",
      "Epoch 60: Batch 6/116 loss: 0.0539\n",
      "Epoch 60: Batch 7/116 loss: 0.0432\n",
      "Epoch 60: Batch 8/116 loss: 0.0458\n",
      "Epoch 60: Batch 9/116 loss: 0.0638\n",
      "Epoch 60: Batch 10/116 loss: 0.0686\n",
      "Epoch 60: Batch 11/116 loss: 0.0541\n",
      "Epoch 60: Batch 12/116 loss: 0.0692\n",
      "Epoch 60: Batch 13/116 loss: 0.0506\n",
      "Epoch 60: Batch 14/116 loss: 0.0588\n",
      "Epoch 60: Batch 15/116 loss: 0.0435\n",
      "Epoch 60: Batch 16/116 loss: 0.0454\n",
      "Epoch 60: Batch 17/116 loss: 0.0604\n",
      "Epoch 60: Batch 18/116 loss: 0.0456\n",
      "Epoch 60: Batch 19/116 loss: 0.0549\n",
      "Epoch 60: Batch 20/116 loss: 0.0735\n",
      "Epoch 60: Batch 21/116 loss: 0.0612\n",
      "Epoch 60: Batch 22/116 loss: 0.0503\n",
      "Epoch 60: Batch 23/116 loss: 0.0537\n",
      "Epoch 60: Batch 24/116 loss: 0.0580\n",
      "Epoch 60: Batch 25/116 loss: 0.0385\n",
      "Epoch 60: Batch 26/116 loss: 0.0658\n",
      "Epoch 60: Batch 27/116 loss: 0.0422\n",
      "Epoch 60: Batch 28/116 loss: 0.0842\n",
      "Epoch 60: Batch 29/116 loss: 0.0843\n",
      "Epoch 60: Batch 30/116 loss: 0.0706\n",
      "Epoch 60: Batch 31/116 loss: 0.0598\n",
      "Epoch 60: Batch 32/116 loss: 0.0655\n",
      "Epoch 60: Batch 33/116 loss: 0.0503\n",
      "Epoch 60: Batch 34/116 loss: 0.0567\n",
      "Epoch 60: Batch 35/116 loss: 0.0666\n",
      "Epoch 60: Batch 36/116 loss: 0.0425\n",
      "Epoch 60: Batch 37/116 loss: 0.0674\n",
      "Epoch 60: Batch 38/116 loss: 0.0977\n",
      "Epoch 60: Batch 39/116 loss: 0.0496\n",
      "Epoch 60: Batch 40/116 loss: 0.0532\n",
      "Epoch 60: Batch 41/116 loss: 0.0443\n",
      "Epoch 60: Batch 42/116 loss: 0.0894\n",
      "Epoch 60: Batch 43/116 loss: 0.0477\n",
      "Epoch 60: Batch 44/116 loss: 0.0581\n",
      "Epoch 60: Batch 45/116 loss: 0.0476\n",
      "Epoch 60: Batch 46/116 loss: 0.0786\n",
      "Epoch 60: Batch 47/116 loss: 0.0477\n",
      "Epoch 60: Batch 48/116 loss: 0.0492\n",
      "Epoch 60: Batch 49/116 loss: 0.0763\n",
      "Epoch 60: Batch 50/116 loss: 0.0379\n",
      "Epoch 60: Batch 51/116 loss: 0.0765\n",
      "Epoch 60: Batch 52/116 loss: 0.0788\n",
      "Epoch 60: Batch 53/116 loss: 0.0461\n",
      "Epoch 60: Batch 54/116 loss: 0.0827\n",
      "Epoch 60: Batch 55/116 loss: 0.0633\n",
      "Epoch 60: Batch 56/116 loss: 0.0824\n",
      "Epoch 60: Batch 57/116 loss: 0.0855\n",
      "Epoch 60: Batch 58/116 loss: 0.0732\n",
      "Epoch 60: Batch 59/116 loss: 0.0474\n",
      "Epoch 60: Batch 60/116 loss: 0.0605\n",
      "Epoch 60: Batch 61/116 loss: 0.0710\n",
      "Epoch 60: Batch 62/116 loss: 0.0763\n",
      "Epoch 60: Batch 63/116 loss: 0.0583\n",
      "Epoch 60: Batch 64/116 loss: 0.0553\n",
      "Epoch 60: Batch 65/116 loss: 0.0674\n",
      "Epoch 60: Batch 66/116 loss: 0.0627\n",
      "Epoch 60: Batch 67/116 loss: 0.0383\n",
      "Epoch 60: Batch 68/116 loss: 0.0938\n",
      "Epoch 60: Batch 69/116 loss: 0.0813\n",
      "Epoch 60: Batch 70/116 loss: 0.0503\n",
      "Epoch 60: Batch 71/116 loss: 0.0710\n",
      "Epoch 60: Batch 72/116 loss: 0.0746\n",
      "Epoch 60: Batch 73/116 loss: 0.0560\n",
      "Epoch 60: Batch 74/116 loss: 0.0728\n",
      "Epoch 60: Batch 75/116 loss: 0.0611\n",
      "Epoch 60: Batch 76/116 loss: 0.0883\n",
      "Epoch 60: Batch 77/116 loss: 0.0829\n",
      "Epoch 60: Batch 78/116 loss: 0.0671\n",
      "Epoch 60: Batch 79/116 loss: 0.0677\n",
      "Epoch 60: Batch 80/116 loss: 0.0636\n",
      "Epoch 60: Batch 81/116 loss: 0.0640\n",
      "Epoch 60: Batch 82/116 loss: 0.0558\n",
      "Epoch 60: Batch 83/116 loss: 0.0501\n",
      "Epoch 60: Batch 84/116 loss: 0.0633\n",
      "Epoch 60: Batch 85/116 loss: 0.0583\n",
      "Epoch 60: Batch 86/116 loss: 0.0574\n",
      "Epoch 60: Batch 87/116 loss: 0.0827\n",
      "Epoch 60: Batch 88/116 loss: 0.0696\n",
      "Epoch 60: Batch 89/116 loss: 0.0586\n",
      "Epoch 60: Batch 90/116 loss: 0.0468\n",
      "Epoch 60: Batch 91/116 loss: 0.0836\n",
      "Epoch 60: Batch 92/116 loss: 0.0595\n",
      "Epoch 60: Batch 93/116 loss: 0.0645\n",
      "Epoch 60: Batch 94/116 loss: 0.0478\n",
      "Epoch 60: Batch 95/116 loss: 0.1103\n",
      "Epoch 60: Batch 96/116 loss: 0.0647\n",
      "Epoch 60: Batch 97/116 loss: 0.0870\n",
      "Epoch 60: Batch 98/116 loss: 0.0576\n",
      "Epoch 60: Batch 99/116 loss: 0.0608\n",
      "Epoch 60: Batch 100/116 loss: 0.0929\n",
      "Epoch 60: Batch 101/116 loss: 0.0629\n",
      "Epoch 60: Batch 102/116 loss: 0.0707\n",
      "Epoch 60: Batch 103/116 loss: 0.0776\n",
      "Epoch 60: Batch 104/116 loss: 0.1017\n",
      "Epoch 60: Batch 105/116 loss: 0.0721\n",
      "Epoch 60: Batch 106/116 loss: 0.0753\n",
      "Epoch 60: Batch 107/116 loss: 0.0567\n",
      "Epoch 60: Batch 108/116 loss: 0.0519\n",
      "Epoch 60: Batch 109/116 loss: 0.0673\n",
      "Epoch 60: Batch 110/116 loss: 0.0443\n",
      "Epoch 60: Batch 111/116 loss: 0.0622\n",
      "Epoch 60: Batch 112/116 loss: 0.0546\n",
      "Epoch 60: Batch 113/116 loss: 0.0635\n",
      "Epoch 60: Batch 114/116 loss: 0.0573\n",
      "Epoch 60: Batch 115/116 loss: 0.0568\n",
      "Epoch 60: Batch 116/116 loss: 0.0533\n",
      "Epoch 60 train loss: 0.0632 valid loss: 0.0631\n",
      "Epoch 61: Batch 1/116 loss: 0.0452\n",
      "Epoch 61: Batch 2/116 loss: 0.0625\n",
      "Epoch 61: Batch 3/116 loss: 0.0564\n",
      "Epoch 61: Batch 4/116 loss: 0.0337\n",
      "Epoch 61: Batch 5/116 loss: 0.0455\n",
      "Epoch 61: Batch 6/116 loss: 0.0633\n",
      "Epoch 61: Batch 7/116 loss: 0.0613\n",
      "Epoch 61: Batch 8/116 loss: 0.0608\n",
      "Epoch 61: Batch 9/116 loss: 0.0575\n",
      "Epoch 61: Batch 10/116 loss: 0.0543\n",
      "Epoch 61: Batch 11/116 loss: 0.0656\n",
      "Epoch 61: Batch 12/116 loss: 0.0847\n",
      "Epoch 61: Batch 13/116 loss: 0.0427\n",
      "Epoch 61: Batch 14/116 loss: 0.0552\n",
      "Epoch 61: Batch 15/116 loss: 0.0759\n",
      "Epoch 61: Batch 16/116 loss: 0.0666\n",
      "Epoch 61: Batch 17/116 loss: 0.0523\n",
      "Epoch 61: Batch 18/116 loss: 0.0568\n",
      "Epoch 61: Batch 19/116 loss: 0.0609\n",
      "Epoch 61: Batch 20/116 loss: 0.0766\n",
      "Epoch 61: Batch 21/116 loss: 0.0738\n",
      "Epoch 61: Batch 22/116 loss: 0.0461\n",
      "Epoch 61: Batch 23/116 loss: 0.0578\n",
      "Epoch 61: Batch 24/116 loss: 0.0587\n",
      "Epoch 61: Batch 25/116 loss: 0.0643\n",
      "Epoch 61: Batch 26/116 loss: 0.0557\n",
      "Epoch 61: Batch 27/116 loss: 0.0706\n",
      "Epoch 61: Batch 28/116 loss: 0.0614\n",
      "Epoch 61: Batch 29/116 loss: 0.0647\n",
      "Epoch 61: Batch 30/116 loss: 0.0735\n",
      "Epoch 61: Batch 31/116 loss: 0.0359\n",
      "Epoch 61: Batch 32/116 loss: 0.0620\n",
      "Epoch 61: Batch 33/116 loss: 0.0910\n",
      "Epoch 61: Batch 34/116 loss: 0.0585\n",
      "Epoch 61: Batch 35/116 loss: 0.0595\n",
      "Epoch 61: Batch 36/116 loss: 0.0464\n",
      "Epoch 61: Batch 37/116 loss: 0.0464\n",
      "Epoch 61: Batch 38/116 loss: 0.0638\n",
      "Epoch 61: Batch 39/116 loss: 0.0693\n",
      "Epoch 61: Batch 40/116 loss: 0.0597\n",
      "Epoch 61: Batch 41/116 loss: 0.0534\n",
      "Epoch 61: Batch 42/116 loss: 0.0636\n",
      "Epoch 61: Batch 43/116 loss: 0.0474\n",
      "Epoch 61: Batch 44/116 loss: 0.0745\n",
      "Epoch 61: Batch 45/116 loss: 0.0509\n",
      "Epoch 61: Batch 46/116 loss: 0.0468\n",
      "Epoch 61: Batch 47/116 loss: 0.0595\n",
      "Epoch 61: Batch 48/116 loss: 0.0714\n",
      "Epoch 61: Batch 49/116 loss: 0.0767\n",
      "Epoch 61: Batch 50/116 loss: 0.0669\n",
      "Epoch 61: Batch 51/116 loss: 0.0541\n",
      "Epoch 61: Batch 52/116 loss: 0.0752\n",
      "Epoch 61: Batch 53/116 loss: 0.0689\n",
      "Epoch 61: Batch 54/116 loss: 0.0512\n",
      "Epoch 61: Batch 55/116 loss: 0.0563\n",
      "Epoch 61: Batch 56/116 loss: 0.0733\n",
      "Epoch 61: Batch 57/116 loss: 0.0713\n",
      "Epoch 61: Batch 58/116 loss: 0.0583\n",
      "Epoch 61: Batch 59/116 loss: 0.0564\n",
      "Epoch 61: Batch 60/116 loss: 0.0647\n",
      "Epoch 61: Batch 61/116 loss: 0.0724\n",
      "Epoch 61: Batch 62/116 loss: 0.0529\n",
      "Epoch 61: Batch 63/116 loss: 0.0427\n",
      "Epoch 61: Batch 64/116 loss: 0.0582\n",
      "Epoch 61: Batch 65/116 loss: 0.0721\n",
      "Epoch 61: Batch 66/116 loss: 0.0527\n",
      "Epoch 61: Batch 67/116 loss: 0.0437\n",
      "Epoch 61: Batch 68/116 loss: 0.0536\n",
      "Epoch 61: Batch 69/116 loss: 0.0636\n",
      "Epoch 61: Batch 70/116 loss: 0.0634\n",
      "Epoch 61: Batch 71/116 loss: 0.0668\n",
      "Epoch 61: Batch 72/116 loss: 0.0678\n",
      "Epoch 61: Batch 73/116 loss: 0.0553\n",
      "Epoch 61: Batch 74/116 loss: 0.0790\n",
      "Epoch 61: Batch 75/116 loss: 0.0428\n",
      "Epoch 61: Batch 76/116 loss: 0.0689\n",
      "Epoch 61: Batch 77/116 loss: 0.0599\n",
      "Epoch 61: Batch 78/116 loss: 0.0647\n",
      "Epoch 61: Batch 79/116 loss: 0.0888\n",
      "Epoch 61: Batch 80/116 loss: 0.0581\n",
      "Epoch 61: Batch 81/116 loss: 0.0629\n",
      "Epoch 61: Batch 82/116 loss: 0.0699\n",
      "Epoch 61: Batch 83/116 loss: 0.0594\n",
      "Epoch 61: Batch 84/116 loss: 0.0761\n",
      "Epoch 61: Batch 85/116 loss: 0.0810\n",
      "Epoch 61: Batch 86/116 loss: 0.0682\n",
      "Epoch 61: Batch 87/116 loss: 0.0572\n",
      "Epoch 61: Batch 88/116 loss: 0.0697\n",
      "Epoch 61: Batch 89/116 loss: 0.0655\n",
      "Epoch 61: Batch 90/116 loss: 0.0814\n",
      "Epoch 61: Batch 91/116 loss: 0.0531\n",
      "Epoch 61: Batch 92/116 loss: 0.0832\n",
      "Epoch 61: Batch 93/116 loss: 0.0662\n",
      "Epoch 61: Batch 94/116 loss: 0.0664\n",
      "Epoch 61: Batch 95/116 loss: 0.0549\n",
      "Epoch 61: Batch 96/116 loss: 0.1257\n",
      "Epoch 61: Batch 97/116 loss: 0.1053\n",
      "Epoch 61: Batch 98/116 loss: 0.0616\n",
      "Epoch 61: Batch 99/116 loss: 0.1086\n",
      "Epoch 61: Batch 100/116 loss: 0.0966\n",
      "Epoch 61: Batch 101/116 loss: 0.1087\n",
      "Epoch 61: Batch 102/116 loss: 0.1218\n",
      "Epoch 61: Batch 103/116 loss: 0.1030\n",
      "Epoch 61: Batch 104/116 loss: 0.0728\n",
      "Epoch 61: Batch 105/116 loss: 0.1058\n",
      "Epoch 61: Batch 106/116 loss: 0.0785\n",
      "Epoch 61: Batch 107/116 loss: 0.2037\n",
      "Epoch 61: Batch 108/116 loss: 0.0864\n",
      "Epoch 61: Batch 109/116 loss: 0.0837\n",
      "Epoch 61: Batch 110/116 loss: 0.0693\n",
      "Epoch 61: Batch 111/116 loss: 0.0612\n",
      "Epoch 61: Batch 112/116 loss: 0.0709\n",
      "Epoch 61: Batch 113/116 loss: 0.0626\n",
      "Epoch 61: Batch 114/116 loss: 0.0620\n",
      "Epoch 61: Batch 115/116 loss: 0.0848\n",
      "Epoch 61: Batch 116/116 loss: 0.0541\n",
      "Epoch 61 train loss: 0.0673 valid loss: 0.0675\n",
      "performance reducing: 1\n",
      "Epoch 62: Batch 1/116 loss: 0.0669\n",
      "Epoch 62: Batch 2/116 loss: 0.0473\n",
      "Epoch 62: Batch 3/116 loss: 0.0692\n",
      "Epoch 62: Batch 4/116 loss: 0.0404\n",
      "Epoch 62: Batch 5/116 loss: 0.1165\n",
      "Epoch 62: Batch 6/116 loss: 0.0708\n",
      "Epoch 62: Batch 7/116 loss: 0.0656\n",
      "Epoch 62: Batch 8/116 loss: 0.0579\n",
      "Epoch 62: Batch 9/116 loss: 0.0642\n",
      "Epoch 62: Batch 10/116 loss: 0.0656\n",
      "Epoch 62: Batch 11/116 loss: 0.0548\n",
      "Epoch 62: Batch 12/116 loss: 0.0640\n",
      "Epoch 62: Batch 13/116 loss: 0.0485\n",
      "Epoch 62: Batch 14/116 loss: 0.0585\n",
      "Epoch 62: Batch 15/116 loss: 0.0815\n",
      "Epoch 62: Batch 16/116 loss: 0.0585\n",
      "Epoch 62: Batch 17/116 loss: 0.0604\n",
      "Epoch 62: Batch 18/116 loss: 0.0852\n",
      "Epoch 62: Batch 19/116 loss: 0.1051\n",
      "Epoch 62: Batch 20/116 loss: 0.0655\n",
      "Epoch 62: Batch 21/116 loss: 0.0747\n",
      "Epoch 62: Batch 22/116 loss: 0.0618\n",
      "Epoch 62: Batch 23/116 loss: 0.0651\n",
      "Epoch 62: Batch 24/116 loss: 0.0597\n",
      "Epoch 62: Batch 25/116 loss: 0.0817\n",
      "Epoch 62: Batch 26/116 loss: 0.0653\n",
      "Epoch 62: Batch 27/116 loss: 0.0801\n",
      "Epoch 62: Batch 28/116 loss: 0.0683\n",
      "Epoch 62: Batch 29/116 loss: 0.0734\n",
      "Epoch 62: Batch 30/116 loss: 0.0537\n",
      "Epoch 62: Batch 31/116 loss: 0.0712\n",
      "Epoch 62: Batch 32/116 loss: 0.0705\n",
      "Epoch 62: Batch 33/116 loss: 0.0927\n",
      "Epoch 62: Batch 34/116 loss: 0.0521\n",
      "Epoch 62: Batch 35/116 loss: 0.0582\n",
      "Epoch 62: Batch 36/116 loss: 0.0539\n",
      "Epoch 62: Batch 37/116 loss: 0.0838\n",
      "Epoch 62: Batch 38/116 loss: 0.0534\n",
      "Epoch 62: Batch 39/116 loss: 0.0571\n",
      "Epoch 62: Batch 40/116 loss: 0.0596\n",
      "Epoch 62: Batch 41/116 loss: 0.0705\n",
      "Epoch 62: Batch 42/116 loss: 0.0486\n",
      "Epoch 62: Batch 43/116 loss: 0.0630\n",
      "Epoch 62: Batch 44/116 loss: 0.0566\n",
      "Epoch 62: Batch 45/116 loss: 0.0780\n",
      "Epoch 62: Batch 46/116 loss: 0.0663\n",
      "Epoch 62: Batch 47/116 loss: 0.0521\n",
      "Epoch 62: Batch 48/116 loss: 0.0591\n",
      "Epoch 62: Batch 49/116 loss: 0.0403\n",
      "Epoch 62: Batch 50/116 loss: 0.0809\n",
      "Epoch 62: Batch 51/116 loss: 0.0523\n",
      "Epoch 62: Batch 52/116 loss: 0.0572\n",
      "Epoch 62: Batch 53/116 loss: 0.0574\n",
      "Epoch 62: Batch 54/116 loss: 0.0618\n",
      "Epoch 62: Batch 55/116 loss: 0.0700\n",
      "Epoch 62: Batch 56/116 loss: 0.0462\n",
      "Epoch 62: Batch 57/116 loss: 0.0616\n",
      "Epoch 62: Batch 58/116 loss: 0.0684\n",
      "Epoch 62: Batch 59/116 loss: 0.0556\n",
      "Epoch 62: Batch 60/116 loss: 0.0597\n",
      "Epoch 62: Batch 61/116 loss: 0.0382\n",
      "Epoch 62: Batch 62/116 loss: 0.0789\n",
      "Epoch 62: Batch 63/116 loss: 0.0508\n",
      "Epoch 62: Batch 64/116 loss: 0.0558\n",
      "Epoch 62: Batch 65/116 loss: 0.0544\n",
      "Epoch 62: Batch 66/116 loss: 0.0836\n",
      "Epoch 62: Batch 67/116 loss: 0.0406\n",
      "Epoch 62: Batch 68/116 loss: 0.0685\n",
      "Epoch 62: Batch 69/116 loss: 0.0498\n",
      "Epoch 62: Batch 70/116 loss: 0.0708\n",
      "Epoch 62: Batch 71/116 loss: 0.0849\n",
      "Epoch 62: Batch 72/116 loss: 0.0576\n",
      "Epoch 62: Batch 73/116 loss: 0.0752\n",
      "Epoch 62: Batch 74/116 loss: 0.0475\n",
      "Epoch 62: Batch 75/116 loss: 0.0537\n",
      "Epoch 62: Batch 76/116 loss: 0.0634\n",
      "Epoch 62: Batch 77/116 loss: 0.0537\n",
      "Epoch 62: Batch 78/116 loss: 0.0385\n",
      "Epoch 62: Batch 79/116 loss: 0.0853\n",
      "Epoch 62: Batch 80/116 loss: 0.0726\n",
      "Epoch 62: Batch 81/116 loss: 0.0532\n",
      "Epoch 62: Batch 82/116 loss: 0.0772\n",
      "Epoch 62: Batch 83/116 loss: 0.0469\n",
      "Epoch 62: Batch 84/116 loss: 0.0512\n",
      "Epoch 62: Batch 85/116 loss: 0.0434\n",
      "Epoch 62: Batch 86/116 loss: 0.0533\n",
      "Epoch 62: Batch 87/116 loss: 0.0713\n",
      "Epoch 62: Batch 88/116 loss: 0.0557\n",
      "Epoch 62: Batch 89/116 loss: 0.0585\n",
      "Epoch 62: Batch 90/116 loss: 0.0828\n",
      "Epoch 62: Batch 91/116 loss: 0.0670\n",
      "Epoch 62: Batch 92/116 loss: 0.0734\n",
      "Epoch 62: Batch 93/116 loss: 0.0678\n",
      "Epoch 62: Batch 94/116 loss: 0.0589\n",
      "Epoch 62: Batch 95/116 loss: 0.0456\n",
      "Epoch 62: Batch 96/116 loss: 0.1202\n",
      "Epoch 62: Batch 97/116 loss: 0.0556\n",
      "Epoch 62: Batch 98/116 loss: 0.0518\n",
      "Epoch 62: Batch 99/116 loss: 0.0683\n",
      "Epoch 62: Batch 100/116 loss: 0.0646\n",
      "Epoch 62: Batch 101/116 loss: 0.0802\n",
      "Epoch 62: Batch 102/116 loss: 0.0499\n",
      "Epoch 62: Batch 103/116 loss: 0.0673\n",
      "Epoch 62: Batch 104/116 loss: 0.0705\n",
      "Epoch 62: Batch 105/116 loss: 0.0764\n",
      "Epoch 62: Batch 106/116 loss: 0.0585\n",
      "Epoch 62: Batch 107/116 loss: 0.0485\n",
      "Epoch 62: Batch 108/116 loss: 0.0411\n",
      "Epoch 62: Batch 109/116 loss: 0.0671\n",
      "Epoch 62: Batch 110/116 loss: 0.0808\n",
      "Epoch 62: Batch 111/116 loss: 0.0591\n",
      "Epoch 62: Batch 112/116 loss: 0.0540\n",
      "Epoch 62: Batch 113/116 loss: 0.0550\n",
      "Epoch 62: Batch 114/116 loss: 0.0678\n",
      "Epoch 62: Batch 115/116 loss: 0.0555\n",
      "Epoch 62: Batch 116/116 loss: 0.0831\n",
      "Epoch 62 train loss: 0.0637 valid loss: 0.0628\n",
      "Epoch 63: Batch 1/116 loss: 0.0686\n",
      "Epoch 63: Batch 2/116 loss: 0.0530\n",
      "Epoch 63: Batch 3/116 loss: 0.0607\n",
      "Epoch 63: Batch 4/116 loss: 0.0711\n",
      "Epoch 63: Batch 5/116 loss: 0.0485\n",
      "Epoch 63: Batch 6/116 loss: 0.0618\n",
      "Epoch 63: Batch 7/116 loss: 0.0688\n",
      "Epoch 63: Batch 8/116 loss: 0.0534\n",
      "Epoch 63: Batch 9/116 loss: 0.0460\n",
      "Epoch 63: Batch 10/116 loss: 0.0468\n",
      "Epoch 63: Batch 11/116 loss: 0.0771\n",
      "Epoch 63: Batch 12/116 loss: 0.0630\n",
      "Epoch 63: Batch 13/116 loss: 0.0691\n",
      "Epoch 63: Batch 14/116 loss: 0.0630\n",
      "Epoch 63: Batch 15/116 loss: 0.0691\n",
      "Epoch 63: Batch 16/116 loss: 0.0614\n",
      "Epoch 63: Batch 17/116 loss: 0.0488\n",
      "Epoch 63: Batch 18/116 loss: 0.0671\n",
      "Epoch 63: Batch 19/116 loss: 0.0399\n",
      "Epoch 63: Batch 20/116 loss: 0.0704\n",
      "Epoch 63: Batch 21/116 loss: 0.1020\n",
      "Epoch 63: Batch 22/116 loss: 0.0673\n",
      "Epoch 63: Batch 23/116 loss: 0.0496\n",
      "Epoch 63: Batch 24/116 loss: 0.0516\n",
      "Epoch 63: Batch 25/116 loss: 0.0600\n",
      "Epoch 63: Batch 26/116 loss: 0.0518\n",
      "Epoch 63: Batch 27/116 loss: 0.0463\n",
      "Epoch 63: Batch 28/116 loss: 0.0507\n",
      "Epoch 63: Batch 29/116 loss: 0.0348\n",
      "Epoch 63: Batch 30/116 loss: 0.0668\n",
      "Epoch 63: Batch 31/116 loss: 0.0636\n",
      "Epoch 63: Batch 32/116 loss: 0.0375\n",
      "Epoch 63: Batch 33/116 loss: 0.0553\n",
      "Epoch 63: Batch 34/116 loss: 0.0901\n",
      "Epoch 63: Batch 35/116 loss: 0.0776\n",
      "Epoch 63: Batch 36/116 loss: 0.0617\n",
      "Epoch 63: Batch 37/116 loss: 0.0667\n",
      "Epoch 63: Batch 38/116 loss: 0.0529\n",
      "Epoch 63: Batch 39/116 loss: 0.0694\n",
      "Epoch 63: Batch 40/116 loss: 0.0713\n",
      "Epoch 63: Batch 41/116 loss: 0.1370\n",
      "Epoch 63: Batch 42/116 loss: 0.0724\n",
      "Epoch 63: Batch 43/116 loss: 0.0724\n",
      "Epoch 63: Batch 44/116 loss: 0.0827\n",
      "Epoch 63: Batch 45/116 loss: 0.0630\n",
      "Epoch 63: Batch 46/116 loss: 0.0664\n",
      "Epoch 63: Batch 47/116 loss: 0.0617\n",
      "Epoch 63: Batch 48/116 loss: 0.0711\n",
      "Epoch 63: Batch 49/116 loss: 0.0726\n",
      "Epoch 63: Batch 50/116 loss: 0.0801\n",
      "Epoch 63: Batch 51/116 loss: 0.0559\n",
      "Epoch 63: Batch 52/116 loss: 0.0631\n",
      "Epoch 63: Batch 53/116 loss: 0.0611\n",
      "Epoch 63: Batch 54/116 loss: 0.0576\n",
      "Epoch 63: Batch 55/116 loss: 0.0854\n",
      "Epoch 63: Batch 56/116 loss: 0.0546\n",
      "Epoch 63: Batch 57/116 loss: 0.0554\n",
      "Epoch 63: Batch 58/116 loss: 0.0416\n",
      "Epoch 63: Batch 59/116 loss: 0.0566\n",
      "Epoch 63: Batch 60/116 loss: 0.0774\n",
      "Epoch 63: Batch 61/116 loss: 0.0502\n",
      "Epoch 63: Batch 62/116 loss: 0.0677\n",
      "Epoch 63: Batch 63/116 loss: 0.0626\n",
      "Epoch 63: Batch 64/116 loss: 0.0738\n",
      "Epoch 63: Batch 65/116 loss: 0.0524\n",
      "Epoch 63: Batch 66/116 loss: 0.0637\n",
      "Epoch 63: Batch 67/116 loss: 0.0584\n",
      "Epoch 63: Batch 68/116 loss: 0.0627\n",
      "Epoch 63: Batch 69/116 loss: 0.0591\n",
      "Epoch 63: Batch 70/116 loss: 0.0398\n",
      "Epoch 63: Batch 71/116 loss: 0.0820\n",
      "Epoch 63: Batch 72/116 loss: 0.0706\n",
      "Epoch 63: Batch 73/116 loss: 0.0354\n",
      "Epoch 63: Batch 74/116 loss: 0.0549\n",
      "Epoch 63: Batch 75/116 loss: 0.0592\n",
      "Epoch 63: Batch 76/116 loss: 0.0460\n",
      "Epoch 63: Batch 77/116 loss: 0.1108\n",
      "Epoch 63: Batch 78/116 loss: 0.0504\n",
      "Epoch 63: Batch 79/116 loss: 0.0425\n",
      "Epoch 63: Batch 80/116 loss: 0.0546\n",
      "Epoch 63: Batch 81/116 loss: 0.0673\n",
      "Epoch 63: Batch 82/116 loss: 0.0529\n",
      "Epoch 63: Batch 83/116 loss: 0.0593\n",
      "Epoch 63: Batch 84/116 loss: 0.0751\n",
      "Epoch 63: Batch 85/116 loss: 0.0575\n",
      "Epoch 63: Batch 86/116 loss: 0.0506\n",
      "Epoch 63: Batch 87/116 loss: 0.0518\n",
      "Epoch 63: Batch 88/116 loss: 0.0443\n",
      "Epoch 63: Batch 89/116 loss: 0.0596\n",
      "Epoch 63: Batch 90/116 loss: 0.0521\n",
      "Epoch 63: Batch 91/116 loss: 0.0542\n",
      "Epoch 63: Batch 92/116 loss: 0.0799\n",
      "Epoch 63: Batch 93/116 loss: 0.0763\n",
      "Epoch 63: Batch 94/116 loss: 0.0603\n",
      "Epoch 63: Batch 95/116 loss: 0.0673\n",
      "Epoch 63: Batch 96/116 loss: 0.0532\n",
      "Epoch 63: Batch 97/116 loss: 0.0631\n",
      "Epoch 63: Batch 98/116 loss: 0.0616\n",
      "Epoch 63: Batch 99/116 loss: 0.0484\n",
      "Epoch 63: Batch 100/116 loss: 0.0648\n",
      "Epoch 63: Batch 101/116 loss: 0.0702\n",
      "Epoch 63: Batch 102/116 loss: 0.0630\n",
      "Epoch 63: Batch 103/116 loss: 0.1351\n",
      "Epoch 63: Batch 104/116 loss: 0.0702\n",
      "Epoch 63: Batch 105/116 loss: 0.0764\n",
      "Epoch 63: Batch 106/116 loss: 0.0675\n",
      "Epoch 63: Batch 107/116 loss: 0.0641\n",
      "Epoch 63: Batch 108/116 loss: 0.0633\n",
      "Epoch 63: Batch 109/116 loss: 0.0714\n",
      "Epoch 63: Batch 110/116 loss: 0.0564\n",
      "Epoch 63: Batch 111/116 loss: 0.0700\n",
      "Epoch 63: Batch 112/116 loss: 0.0520\n",
      "Epoch 63: Batch 113/116 loss: 0.0459\n",
      "Epoch 63: Batch 114/116 loss: 0.0733\n",
      "Epoch 63: Batch 115/116 loss: 0.0410\n",
      "Epoch 63: Batch 116/116 loss: 0.0512\n",
      "Epoch 63 train loss: 0.0628 valid loss: 0.0719\n",
      "performance reducing: 1\n",
      "Epoch 64: Batch 1/116 loss: 0.0508\n",
      "Epoch 64: Batch 2/116 loss: 0.0646\n",
      "Epoch 64: Batch 3/116 loss: 0.0400\n",
      "Epoch 64: Batch 4/116 loss: 0.0666\n",
      "Epoch 64: Batch 5/116 loss: 0.0439\n",
      "Epoch 64: Batch 6/116 loss: 0.0815\n",
      "Epoch 64: Batch 7/116 loss: 0.0378\n",
      "Epoch 64: Batch 8/116 loss: 0.0665\n",
      "Epoch 64: Batch 9/116 loss: 0.0508\n",
      "Epoch 64: Batch 10/116 loss: 0.0456\n",
      "Epoch 64: Batch 11/116 loss: 0.0411\n",
      "Epoch 64: Batch 12/116 loss: 0.0656\n",
      "Epoch 64: Batch 13/116 loss: 0.1093\n",
      "Epoch 64: Batch 14/116 loss: 0.0698\n",
      "Epoch 64: Batch 15/116 loss: 0.0778\n",
      "Epoch 64: Batch 16/116 loss: 0.0859\n",
      "Epoch 64: Batch 17/116 loss: 0.0595\n",
      "Epoch 64: Batch 18/116 loss: 0.0495\n",
      "Epoch 64: Batch 19/116 loss: 0.0842\n",
      "Epoch 64: Batch 20/116 loss: 0.0658\n",
      "Epoch 64: Batch 21/116 loss: 0.0551\n",
      "Epoch 64: Batch 22/116 loss: 0.0778\n",
      "Epoch 64: Batch 23/116 loss: 0.0659\n",
      "Epoch 64: Batch 24/116 loss: 0.0688\n",
      "Epoch 64: Batch 25/116 loss: 0.0693\n",
      "Epoch 64: Batch 26/116 loss: 0.0583\n",
      "Epoch 64: Batch 27/116 loss: 0.0542\n",
      "Epoch 64: Batch 28/116 loss: 0.0519\n",
      "Epoch 64: Batch 29/116 loss: 0.0619\n",
      "Epoch 64: Batch 30/116 loss: 0.0601\n",
      "Epoch 64: Batch 31/116 loss: 0.0628\n",
      "Epoch 64: Batch 32/116 loss: 0.0530\n",
      "Epoch 64: Batch 33/116 loss: 0.0435\n",
      "Epoch 64: Batch 34/116 loss: 0.0832\n",
      "Epoch 64: Batch 35/116 loss: 0.0683\n",
      "Epoch 64: Batch 36/116 loss: 0.0590\n",
      "Epoch 64: Batch 37/116 loss: 0.0667\n",
      "Epoch 64: Batch 38/116 loss: 0.0699\n",
      "Epoch 64: Batch 39/116 loss: 0.0636\n",
      "Epoch 64: Batch 40/116 loss: 0.0577\n",
      "Epoch 64: Batch 41/116 loss: 0.0730\n",
      "Epoch 64: Batch 42/116 loss: 0.0554\n",
      "Epoch 64: Batch 43/116 loss: 0.0677\n",
      "Epoch 64: Batch 44/116 loss: 0.0666\n",
      "Epoch 64: Batch 45/116 loss: 0.0732\n",
      "Epoch 64: Batch 46/116 loss: 0.0568\n",
      "Epoch 64: Batch 47/116 loss: 0.0571\n",
      "Epoch 64: Batch 48/116 loss: 0.0585\n",
      "Epoch 64: Batch 49/116 loss: 0.0535\n",
      "Epoch 64: Batch 50/116 loss: 0.0518\n",
      "Epoch 64: Batch 51/116 loss: 0.0828\n",
      "Epoch 64: Batch 52/116 loss: 0.0730\n",
      "Epoch 64: Batch 53/116 loss: 0.0639\n",
      "Epoch 64: Batch 54/116 loss: 0.0638\n",
      "Epoch 64: Batch 55/116 loss: 0.0561\n",
      "Epoch 64: Batch 56/116 loss: 0.0494\n",
      "Epoch 64: Batch 57/116 loss: 0.0454\n",
      "Epoch 64: Batch 58/116 loss: 0.0575\n",
      "Epoch 64: Batch 59/116 loss: 0.0661\n",
      "Epoch 64: Batch 60/116 loss: 0.0596\n",
      "Epoch 64: Batch 61/116 loss: 0.0599\n",
      "Epoch 64: Batch 62/116 loss: 0.0597\n",
      "Epoch 64: Batch 63/116 loss: 0.0564\n",
      "Epoch 64: Batch 64/116 loss: 0.0405\n",
      "Epoch 64: Batch 65/116 loss: 0.0493\n",
      "Epoch 64: Batch 66/116 loss: 0.0633\n",
      "Epoch 64: Batch 67/116 loss: 0.0570\n",
      "Epoch 64: Batch 68/116 loss: 0.0827\n",
      "Epoch 64: Batch 69/116 loss: 0.0597\n",
      "Epoch 64: Batch 70/116 loss: 0.0522\n",
      "Epoch 64: Batch 71/116 loss: 0.0586\n",
      "Epoch 64: Batch 72/116 loss: 0.0562\n",
      "Epoch 64: Batch 73/116 loss: 0.0923\n",
      "Epoch 64: Batch 74/116 loss: 0.0504\n",
      "Epoch 64: Batch 75/116 loss: 0.0747\n",
      "Epoch 64: Batch 76/116 loss: 0.0868\n",
      "Epoch 64: Batch 77/116 loss: 0.0382\n",
      "Epoch 64: Batch 78/116 loss: 0.0693\n",
      "Epoch 64: Batch 79/116 loss: 0.0705\n",
      "Epoch 64: Batch 80/116 loss: 0.0569\n",
      "Epoch 64: Batch 81/116 loss: 0.0692\n",
      "Epoch 64: Batch 82/116 loss: 0.0642\n",
      "Epoch 64: Batch 83/116 loss: 0.0543\n",
      "Epoch 64: Batch 84/116 loss: 0.0404\n",
      "Epoch 64: Batch 85/116 loss: 0.0717\n",
      "Epoch 64: Batch 86/116 loss: 0.0357\n",
      "Epoch 64: Batch 87/116 loss: 0.0861\n",
      "Epoch 64: Batch 88/116 loss: 0.0666\n",
      "Epoch 64: Batch 89/116 loss: 0.0473\n",
      "Epoch 64: Batch 90/116 loss: 0.0716\n",
      "Epoch 64: Batch 91/116 loss: 0.0652\n",
      "Epoch 64: Batch 92/116 loss: 0.0472\n",
      "Epoch 64: Batch 93/116 loss: 0.0523\n",
      "Epoch 64: Batch 94/116 loss: 0.0745\n",
      "Epoch 64: Batch 95/116 loss: 0.0681\n",
      "Epoch 64: Batch 96/116 loss: 0.0518\n",
      "Epoch 64: Batch 97/116 loss: 0.0640\n",
      "Epoch 64: Batch 98/116 loss: 0.0587\n",
      "Epoch 64: Batch 99/116 loss: 0.0703\n",
      "Epoch 64: Batch 100/116 loss: 0.0468\n",
      "Epoch 64: Batch 101/116 loss: 0.0577\n",
      "Epoch 64: Batch 102/116 loss: 0.0503\n",
      "Epoch 64: Batch 103/116 loss: 0.0536\n",
      "Epoch 64: Batch 104/116 loss: 0.0596\n",
      "Epoch 64: Batch 105/116 loss: 0.0658\n",
      "Epoch 64: Batch 106/116 loss: 0.0693\n",
      "Epoch 64: Batch 107/116 loss: 0.0514\n",
      "Epoch 64: Batch 108/116 loss: 0.0823\n",
      "Epoch 64: Batch 109/116 loss: 0.0581\n",
      "Epoch 64: Batch 110/116 loss: 0.0543\n",
      "Epoch 64: Batch 111/116 loss: 0.0551\n",
      "Epoch 64: Batch 112/116 loss: 0.0520\n",
      "Epoch 64: Batch 113/116 loss: 0.0574\n",
      "Epoch 64: Batch 114/116 loss: 0.0517\n",
      "Epoch 64: Batch 115/116 loss: 0.0775\n",
      "Epoch 64: Batch 116/116 loss: 0.0655\n",
      "Epoch 64 train loss: 0.0616 valid loss: 0.0627\n",
      "Epoch 65: Batch 1/116 loss: 0.0731\n",
      "Epoch 65: Batch 2/116 loss: 0.0549\n",
      "Epoch 65: Batch 3/116 loss: 0.0627\n",
      "Epoch 65: Batch 4/116 loss: 0.0493\n",
      "Epoch 65: Batch 5/116 loss: 0.0400\n",
      "Epoch 65: Batch 6/116 loss: 0.1216\n",
      "Epoch 65: Batch 7/116 loss: 0.0583\n",
      "Epoch 65: Batch 8/116 loss: 0.0590\n",
      "Epoch 65: Batch 9/116 loss: 0.0625\n",
      "Epoch 65: Batch 10/116 loss: 0.0617\n",
      "Epoch 65: Batch 11/116 loss: 0.0616\n",
      "Epoch 65: Batch 12/116 loss: 0.0732\n",
      "Epoch 65: Batch 13/116 loss: 0.0540\n",
      "Epoch 65: Batch 14/116 loss: 0.0374\n",
      "Epoch 65: Batch 15/116 loss: 0.0753\n",
      "Epoch 65: Batch 16/116 loss: 0.0672\n",
      "Epoch 65: Batch 17/116 loss: 0.0398\n",
      "Epoch 65: Batch 18/116 loss: 0.0692\n",
      "Epoch 65: Batch 19/116 loss: 0.0675\n",
      "Epoch 65: Batch 20/116 loss: 0.0778\n",
      "Epoch 65: Batch 21/116 loss: 0.0626\n",
      "Epoch 65: Batch 22/116 loss: 0.0566\n",
      "Epoch 65: Batch 23/116 loss: 0.0798\n",
      "Epoch 65: Batch 24/116 loss: 0.0869\n",
      "Epoch 65: Batch 25/116 loss: 0.0862\n",
      "Epoch 65: Batch 26/116 loss: 0.0544\n",
      "Epoch 65: Batch 27/116 loss: 0.0663\n",
      "Epoch 65: Batch 28/116 loss: 0.0706\n",
      "Epoch 65: Batch 29/116 loss: 0.0714\n",
      "Epoch 65: Batch 30/116 loss: 0.0759\n",
      "Epoch 65: Batch 31/116 loss: 0.0715\n",
      "Epoch 65: Batch 32/116 loss: 0.0511\n",
      "Epoch 65: Batch 33/116 loss: 0.0413\n",
      "Epoch 65: Batch 34/116 loss: 0.0718\n",
      "Epoch 65: Batch 35/116 loss: 0.0569\n",
      "Epoch 65: Batch 36/116 loss: 0.0513\n",
      "Epoch 65: Batch 37/116 loss: 0.0578\n",
      "Epoch 65: Batch 38/116 loss: 0.0569\n",
      "Epoch 65: Batch 39/116 loss: 0.0586\n",
      "Epoch 65: Batch 40/116 loss: 0.0473\n",
      "Epoch 65: Batch 41/116 loss: 0.0583\n",
      "Epoch 65: Batch 42/116 loss: 0.0538\n",
      "Epoch 65: Batch 43/116 loss: 0.0564\n",
      "Epoch 65: Batch 44/116 loss: 0.0537\n",
      "Epoch 65: Batch 45/116 loss: 0.0496\n",
      "Epoch 65: Batch 46/116 loss: 0.0379\n",
      "Epoch 65: Batch 47/116 loss: 0.0400\n",
      "Epoch 65: Batch 48/116 loss: 0.0462\n",
      "Epoch 65: Batch 49/116 loss: 0.0561\n",
      "Epoch 65: Batch 50/116 loss: 0.0676\n",
      "Epoch 65: Batch 51/116 loss: 0.0807\n",
      "Epoch 65: Batch 52/116 loss: 0.0445\n",
      "Epoch 65: Batch 53/116 loss: 0.0622\n",
      "Epoch 65: Batch 54/116 loss: 0.0672\n",
      "Epoch 65: Batch 55/116 loss: 0.0586\n",
      "Epoch 65: Batch 56/116 loss: 0.0588\n",
      "Epoch 65: Batch 57/116 loss: 0.0648\n",
      "Epoch 65: Batch 58/116 loss: 0.0641\n",
      "Epoch 65: Batch 59/116 loss: 0.0501\n",
      "Epoch 65: Batch 60/116 loss: 0.0950\n",
      "Epoch 65: Batch 61/116 loss: 0.0962\n",
      "Epoch 65: Batch 62/116 loss: 0.0466\n",
      "Epoch 65: Batch 63/116 loss: 0.0759\n",
      "Epoch 65: Batch 64/116 loss: 0.0475\n",
      "Epoch 65: Batch 65/116 loss: 0.0592\n",
      "Epoch 65: Batch 66/116 loss: 0.0380\n",
      "Epoch 65: Batch 67/116 loss: 0.0506\n",
      "Epoch 65: Batch 68/116 loss: 0.0973\n",
      "Epoch 65: Batch 69/116 loss: 0.0609\n",
      "Epoch 65: Batch 70/116 loss: 0.0570\n",
      "Epoch 65: Batch 71/116 loss: 0.0646\n",
      "Epoch 65: Batch 72/116 loss: 0.0591\n",
      "Epoch 65: Batch 73/116 loss: 0.0781\n",
      "Epoch 65: Batch 74/116 loss: 0.0566\n",
      "Epoch 65: Batch 75/116 loss: 0.0526\n",
      "Epoch 65: Batch 76/116 loss: 0.0408\n",
      "Epoch 65: Batch 77/116 loss: 0.0572\n",
      "Epoch 65: Batch 78/116 loss: 0.0441\n",
      "Epoch 65: Batch 79/116 loss: 0.0685\n",
      "Epoch 65: Batch 80/116 loss: 0.0639\n",
      "Epoch 65: Batch 81/116 loss: 0.0974\n",
      "Epoch 65: Batch 82/116 loss: 0.0701\n",
      "Epoch 65: Batch 83/116 loss: 0.0611\n",
      "Epoch 65: Batch 84/116 loss: 0.0701\n",
      "Epoch 65: Batch 85/116 loss: 0.0697\n",
      "Epoch 65: Batch 86/116 loss: 0.0677\n",
      "Epoch 65: Batch 87/116 loss: 0.0808\n",
      "Epoch 65: Batch 88/116 loss: 0.0711\n",
      "Epoch 65: Batch 89/116 loss: 0.0609\n",
      "Epoch 65: Batch 90/116 loss: 0.0503\n",
      "Epoch 65: Batch 91/116 loss: 0.0507\n",
      "Epoch 65: Batch 92/116 loss: 0.0768\n",
      "Epoch 65: Batch 93/116 loss: 0.0558\n",
      "Epoch 65: Batch 94/116 loss: 0.0822\n",
      "Epoch 65: Batch 95/116 loss: 0.0602\n",
      "Epoch 65: Batch 96/116 loss: 0.0591\n",
      "Epoch 65: Batch 97/116 loss: 0.0451\n",
      "Epoch 65: Batch 98/116 loss: 0.0435\n",
      "Epoch 65: Batch 99/116 loss: 0.0811\n",
      "Epoch 65: Batch 100/116 loss: 0.0527\n",
      "Epoch 65: Batch 101/116 loss: 0.0719\n",
      "Epoch 65: Batch 102/116 loss: 0.0892\n",
      "Epoch 65: Batch 103/116 loss: 0.0502\n",
      "Epoch 65: Batch 104/116 loss: 0.0464\n",
      "Epoch 65: Batch 105/116 loss: 0.0692\n",
      "Epoch 65: Batch 106/116 loss: 0.0655\n",
      "Epoch 65: Batch 107/116 loss: 0.0676\n",
      "Epoch 65: Batch 108/116 loss: 0.0576\n",
      "Epoch 65: Batch 109/116 loss: 0.0601\n",
      "Epoch 65: Batch 110/116 loss: 0.0501\n",
      "Epoch 65: Batch 111/116 loss: 0.0490\n",
      "Epoch 65: Batch 112/116 loss: 0.0451\n",
      "Epoch 65: Batch 113/116 loss: 0.0621\n",
      "Epoch 65: Batch 114/116 loss: 0.0706\n",
      "Epoch 65: Batch 115/116 loss: 0.0536\n",
      "Epoch 65: Batch 116/116 loss: 0.0881\n",
      "Epoch 65 train loss: 0.0623 valid loss: 0.0857\n",
      "performance reducing: 1\n",
      "Epoch 66: Batch 1/116 loss: 0.0586\n",
      "Epoch 66: Batch 2/116 loss: 0.0599\n",
      "Epoch 66: Batch 3/116 loss: 0.0728\n",
      "Epoch 66: Batch 4/116 loss: 0.0472\n",
      "Epoch 66: Batch 5/116 loss: 0.0679\n",
      "Epoch 66: Batch 6/116 loss: 0.0850\n",
      "Epoch 66: Batch 7/116 loss: 0.0603\n",
      "Epoch 66: Batch 8/116 loss: 0.0675\n",
      "Epoch 66: Batch 9/116 loss: 0.0527\n",
      "Epoch 66: Batch 10/116 loss: 0.0504\n",
      "Epoch 66: Batch 11/116 loss: 0.0518\n",
      "Epoch 66: Batch 12/116 loss: 0.0592\n",
      "Epoch 66: Batch 13/116 loss: 0.0404\n",
      "Epoch 66: Batch 14/116 loss: 0.0711\n",
      "Epoch 66: Batch 15/116 loss: 0.0709\n",
      "Epoch 66: Batch 16/116 loss: 0.0509\n",
      "Epoch 66: Batch 17/116 loss: 0.0460\n",
      "Epoch 66: Batch 18/116 loss: 0.0704\n",
      "Epoch 66: Batch 19/116 loss: 0.0719\n",
      "Epoch 66: Batch 20/116 loss: 0.0465\n",
      "Epoch 66: Batch 21/116 loss: 0.0420\n",
      "Epoch 66: Batch 22/116 loss: 0.0647\n",
      "Epoch 66: Batch 23/116 loss: 0.0425\n",
      "Epoch 66: Batch 24/116 loss: 0.0654\n",
      "Epoch 66: Batch 25/116 loss: 0.0728\n",
      "Epoch 66: Batch 26/116 loss: 0.0376\n",
      "Epoch 66: Batch 27/116 loss: 0.0522\n",
      "Epoch 66: Batch 28/116 loss: 0.0673\n",
      "Epoch 66: Batch 29/116 loss: 0.0407\n",
      "Epoch 66: Batch 30/116 loss: 0.0569\n",
      "Epoch 66: Batch 31/116 loss: 0.0558\n",
      "Epoch 66: Batch 32/116 loss: 0.0721\n",
      "Epoch 66: Batch 33/116 loss: 0.0558\n",
      "Epoch 66: Batch 34/116 loss: 0.0525\n",
      "Epoch 66: Batch 35/116 loss: 0.1071\n",
      "Epoch 66: Batch 36/116 loss: 0.0753\n",
      "Epoch 66: Batch 37/116 loss: 0.0657\n",
      "Epoch 66: Batch 38/116 loss: 0.0467\n",
      "Epoch 66: Batch 39/116 loss: 0.0544\n",
      "Epoch 66: Batch 40/116 loss: 0.0841\n",
      "Epoch 66: Batch 41/116 loss: 0.0453\n",
      "Epoch 66: Batch 42/116 loss: 0.0486\n",
      "Epoch 66: Batch 43/116 loss: 0.0865\n",
      "Epoch 66: Batch 44/116 loss: 0.0769\n",
      "Epoch 66: Batch 45/116 loss: 0.0759\n",
      "Epoch 66: Batch 46/116 loss: 0.0617\n",
      "Epoch 66: Batch 47/116 loss: 0.0965\n",
      "Epoch 66: Batch 48/116 loss: 0.0617\n",
      "Epoch 66: Batch 49/116 loss: 0.0590\n",
      "Epoch 66: Batch 50/116 loss: 0.0473\n",
      "Epoch 66: Batch 51/116 loss: 0.0710\n",
      "Epoch 66: Batch 52/116 loss: 0.0343\n",
      "Epoch 66: Batch 53/116 loss: 0.0610\n",
      "Epoch 66: Batch 54/116 loss: 0.0903\n",
      "Epoch 66: Batch 55/116 loss: 0.0600\n",
      "Epoch 66: Batch 56/116 loss: 0.0492\n",
      "Epoch 66: Batch 57/116 loss: 0.0529\n",
      "Epoch 66: Batch 58/116 loss: 0.0734\n",
      "Epoch 66: Batch 59/116 loss: 0.0618\n",
      "Epoch 66: Batch 60/116 loss: 0.0811\n",
      "Epoch 66: Batch 61/116 loss: 0.0513\n",
      "Epoch 66: Batch 62/116 loss: 0.0694\n",
      "Epoch 66: Batch 63/116 loss: 0.0622\n",
      "Epoch 66: Batch 64/116 loss: 0.0567\n",
      "Epoch 66: Batch 65/116 loss: 0.0759\n",
      "Epoch 66: Batch 66/116 loss: 0.0441\n",
      "Epoch 66: Batch 67/116 loss: 0.0459\n",
      "Epoch 66: Batch 68/116 loss: 0.0509\n",
      "Epoch 66: Batch 69/116 loss: 0.0677\n",
      "Epoch 66: Batch 70/116 loss: 0.0508\n",
      "Epoch 66: Batch 71/116 loss: 0.0622\n",
      "Epoch 66: Batch 72/116 loss: 0.0811\n",
      "Epoch 66: Batch 73/116 loss: 0.0459\n",
      "Epoch 66: Batch 74/116 loss: 0.0534\n",
      "Epoch 66: Batch 75/116 loss: 0.0473\n",
      "Epoch 66: Batch 76/116 loss: 0.0611\n",
      "Epoch 66: Batch 77/116 loss: 0.1059\n",
      "Epoch 66: Batch 78/116 loss: 0.0701\n",
      "Epoch 66: Batch 79/116 loss: 0.0553\n",
      "Epoch 66: Batch 80/116 loss: 0.0735\n",
      "Epoch 66: Batch 81/116 loss: 0.0583\n",
      "Epoch 66: Batch 82/116 loss: 0.0633\n",
      "Epoch 66: Batch 83/116 loss: 0.0538\n",
      "Epoch 66: Batch 84/116 loss: 0.0490\n",
      "Epoch 66: Batch 85/116 loss: 0.0637\n",
      "Epoch 66: Batch 86/116 loss: 0.0531\n",
      "Epoch 66: Batch 87/116 loss: 0.1509\n",
      "Epoch 66: Batch 88/116 loss: 0.0436\n",
      "Epoch 66: Batch 89/116 loss: 0.0809\n",
      "Epoch 66: Batch 90/116 loss: 0.0532\n",
      "Epoch 66: Batch 91/116 loss: 0.0584\n",
      "Epoch 66: Batch 92/116 loss: 0.0602\n",
      "Epoch 66: Batch 93/116 loss: 0.0783\n",
      "Epoch 66: Batch 94/116 loss: 0.0568\n",
      "Epoch 66: Batch 95/116 loss: 0.0591\n",
      "Epoch 66: Batch 96/116 loss: 0.0509\n",
      "Epoch 66: Batch 97/116 loss: 0.0459\n",
      "Epoch 66: Batch 98/116 loss: 0.0972\n",
      "Epoch 66: Batch 99/116 loss: 0.0515\n",
      "Epoch 66: Batch 100/116 loss: 0.0609\n",
      "Epoch 66: Batch 101/116 loss: 0.0672\n",
      "Epoch 66: Batch 102/116 loss: 0.0777\n",
      "Epoch 66: Batch 103/116 loss: 0.0506\n",
      "Epoch 66: Batch 104/116 loss: 0.0682\n",
      "Epoch 66: Batch 105/116 loss: 0.0672\n",
      "Epoch 66: Batch 106/116 loss: 0.0463\n",
      "Epoch 66: Batch 107/116 loss: 0.0754\n",
      "Epoch 66: Batch 108/116 loss: 0.0516\n",
      "Epoch 66: Batch 109/116 loss: 0.0989\n",
      "Epoch 66: Batch 110/116 loss: 0.0505\n",
      "Epoch 66: Batch 111/116 loss: 0.0665\n",
      "Epoch 66: Batch 112/116 loss: 0.0644\n",
      "Epoch 66: Batch 113/116 loss: 0.0622\n",
      "Epoch 66: Batch 114/116 loss: 0.0550\n",
      "Epoch 66: Batch 115/116 loss: 0.1150\n",
      "Epoch 66: Batch 116/116 loss: 0.0466\n",
      "Epoch 66 train loss: 0.0628 valid loss: 0.0629\n",
      "performance reducing: 2\n",
      "Epoch 67: Batch 1/116 loss: 0.0480\n",
      "Epoch 67: Batch 2/116 loss: 0.0784\n",
      "Epoch 67: Batch 3/116 loss: 0.0691\n",
      "Epoch 67: Batch 4/116 loss: 0.0560\n",
      "Epoch 67: Batch 5/116 loss: 0.0591\n",
      "Epoch 67: Batch 6/116 loss: 0.0581\n",
      "Epoch 67: Batch 7/116 loss: 0.0554\n",
      "Epoch 67: Batch 8/116 loss: 0.0448\n",
      "Epoch 67: Batch 9/116 loss: 0.0723\n",
      "Epoch 67: Batch 10/116 loss: 0.0496\n",
      "Epoch 67: Batch 11/116 loss: 0.0539\n",
      "Epoch 67: Batch 12/116 loss: 0.0539\n",
      "Epoch 67: Batch 13/116 loss: 0.0457\n",
      "Epoch 67: Batch 14/116 loss: 0.0646\n",
      "Epoch 67: Batch 15/116 loss: 0.0905\n",
      "Epoch 67: Batch 16/116 loss: 0.0470\n",
      "Epoch 67: Batch 17/116 loss: 0.0435\n",
      "Epoch 67: Batch 18/116 loss: 0.0637\n",
      "Epoch 67: Batch 19/116 loss: 0.0774\n",
      "Epoch 67: Batch 20/116 loss: 0.0589\n",
      "Epoch 67: Batch 21/116 loss: 0.0752\n",
      "Epoch 67: Batch 22/116 loss: 0.0605\n",
      "Epoch 67: Batch 23/116 loss: 0.0583\n",
      "Epoch 67: Batch 24/116 loss: 0.0521\n",
      "Epoch 67: Batch 25/116 loss: 0.0640\n",
      "Epoch 67: Batch 26/116 loss: 0.0833\n",
      "Epoch 67: Batch 27/116 loss: 0.0635\n",
      "Epoch 67: Batch 28/116 loss: 0.0494\n",
      "Epoch 67: Batch 29/116 loss: 0.0851\n",
      "Epoch 67: Batch 30/116 loss: 0.0623\n",
      "Epoch 67: Batch 31/116 loss: 0.0500\n",
      "Epoch 67: Batch 32/116 loss: 0.0604\n",
      "Epoch 67: Batch 33/116 loss: 0.0655\n",
      "Epoch 67: Batch 34/116 loss: 0.0395\n",
      "Epoch 67: Batch 35/116 loss: 0.0725\n",
      "Epoch 67: Batch 36/116 loss: 0.0602\n",
      "Epoch 67: Batch 37/116 loss: 0.0720\n",
      "Epoch 67: Batch 38/116 loss: 0.0602\n",
      "Epoch 67: Batch 39/116 loss: 0.0722\n",
      "Epoch 67: Batch 40/116 loss: 0.0454\n",
      "Epoch 67: Batch 41/116 loss: 0.0518\n",
      "Epoch 67: Batch 42/116 loss: 0.0698\n",
      "Epoch 67: Batch 43/116 loss: 0.0757\n",
      "Epoch 67: Batch 44/116 loss: 0.0430\n",
      "Epoch 67: Batch 45/116 loss: 0.0641\n",
      "Epoch 67: Batch 46/116 loss: 0.0694\n",
      "Epoch 67: Batch 47/116 loss: 0.0656\n",
      "Epoch 67: Batch 48/116 loss: 0.0490\n",
      "Epoch 67: Batch 49/116 loss: 0.0649\n",
      "Epoch 67: Batch 50/116 loss: 0.0449\n",
      "Epoch 67: Batch 51/116 loss: 0.0441\n",
      "Epoch 67: Batch 52/116 loss: 0.0556\n",
      "Epoch 67: Batch 53/116 loss: 0.0517\n",
      "Epoch 67: Batch 54/116 loss: 0.0427\n",
      "Epoch 67: Batch 55/116 loss: 0.0561\n",
      "Epoch 67: Batch 56/116 loss: 0.0700\n",
      "Epoch 67: Batch 57/116 loss: 0.0500\n",
      "Epoch 67: Batch 58/116 loss: 0.0539\n",
      "Epoch 67: Batch 59/116 loss: 0.0600\n",
      "Epoch 67: Batch 60/116 loss: 0.0683\n",
      "Epoch 67: Batch 61/116 loss: 0.0737\n",
      "Epoch 67: Batch 62/116 loss: 0.0687\n",
      "Epoch 67: Batch 63/116 loss: 0.0388\n",
      "Epoch 67: Batch 64/116 loss: 0.0901\n",
      "Epoch 67: Batch 65/116 loss: 0.0543\n",
      "Epoch 67: Batch 66/116 loss: 0.0621\n",
      "Epoch 67: Batch 67/116 loss: 0.0712\n",
      "Epoch 67: Batch 68/116 loss: 0.0587\n",
      "Epoch 67: Batch 69/116 loss: 0.0476\n",
      "Epoch 67: Batch 70/116 loss: 0.0670\n",
      "Epoch 67: Batch 71/116 loss: 0.0600\n",
      "Epoch 67: Batch 72/116 loss: 0.0558\n",
      "Epoch 67: Batch 73/116 loss: 0.0421\n",
      "Epoch 67: Batch 74/116 loss: 0.0473\n",
      "Epoch 67: Batch 75/116 loss: 0.0662\n",
      "Epoch 67: Batch 76/116 loss: 0.0466\n",
      "Epoch 67: Batch 77/116 loss: 0.0776\n",
      "Epoch 67: Batch 78/116 loss: 0.0917\n",
      "Epoch 67: Batch 79/116 loss: 0.0510\n",
      "Epoch 67: Batch 80/116 loss: 0.0610\n",
      "Epoch 67: Batch 81/116 loss: 0.0718\n",
      "Epoch 67: Batch 82/116 loss: 0.0453\n",
      "Epoch 67: Batch 83/116 loss: 0.0658\n",
      "Epoch 67: Batch 84/116 loss: 0.0605\n",
      "Epoch 67: Batch 85/116 loss: 0.0509\n",
      "Epoch 67: Batch 86/116 loss: 0.1076\n",
      "Epoch 67: Batch 87/116 loss: 0.0778\n",
      "Epoch 67: Batch 88/116 loss: 0.0526\n",
      "Epoch 67: Batch 89/116 loss: 0.0628\n",
      "Epoch 67: Batch 90/116 loss: 0.0691\n",
      "Epoch 67: Batch 91/116 loss: 0.0542\n",
      "Epoch 67: Batch 92/116 loss: 0.0510\n",
      "Epoch 67: Batch 93/116 loss: 0.0583\n",
      "Epoch 67: Batch 94/116 loss: 0.0492\n",
      "Epoch 67: Batch 95/116 loss: 0.0503\n",
      "Epoch 67: Batch 96/116 loss: 0.0491\n",
      "Epoch 67: Batch 97/116 loss: 0.0641\n",
      "Epoch 67: Batch 98/116 loss: 0.0540\n",
      "Epoch 67: Batch 99/116 loss: 0.0559\n",
      "Epoch 67: Batch 100/116 loss: 0.0805\n",
      "Epoch 67: Batch 101/116 loss: 0.0736\n",
      "Epoch 67: Batch 102/116 loss: 0.0927\n",
      "Epoch 67: Batch 103/116 loss: 0.0501\n",
      "Epoch 67: Batch 104/116 loss: 0.0651\n",
      "Epoch 67: Batch 105/116 loss: 0.0763\n",
      "Epoch 67: Batch 106/116 loss: 0.0635\n",
      "Epoch 67: Batch 107/116 loss: 0.0639\n",
      "Epoch 67: Batch 108/116 loss: 0.0884\n",
      "Epoch 67: Batch 109/116 loss: 0.0774\n",
      "Epoch 67: Batch 110/116 loss: 0.0712\n",
      "Epoch 67: Batch 111/116 loss: 0.0646\n",
      "Epoch 67: Batch 112/116 loss: 0.0637\n",
      "Epoch 67: Batch 113/116 loss: 0.0604\n",
      "Epoch 67: Batch 114/116 loss: 0.0566\n",
      "Epoch 67: Batch 115/116 loss: 0.0662\n",
      "Epoch 67: Batch 116/116 loss: 0.0789\n",
      "Epoch 67 train loss: 0.0618 valid loss: 0.0648\n",
      "performance reducing: 3\n",
      "Epoch 68: Batch 1/116 loss: 0.0588\n",
      "Epoch 68: Batch 2/116 loss: 0.0609\n",
      "Epoch 68: Batch 3/116 loss: 0.0597\n",
      "Epoch 68: Batch 4/116 loss: 0.0658\n",
      "Epoch 68: Batch 5/116 loss: 0.0532\n",
      "Epoch 68: Batch 6/116 loss: 0.0506\n",
      "Epoch 68: Batch 7/116 loss: 0.0573\n",
      "Epoch 68: Batch 8/116 loss: 0.0432\n",
      "Epoch 68: Batch 9/116 loss: 0.0703\n",
      "Epoch 68: Batch 10/116 loss: 0.0565\n",
      "Epoch 68: Batch 11/116 loss: 0.0601\n",
      "Epoch 68: Batch 12/116 loss: 0.0548\n",
      "Epoch 68: Batch 13/116 loss: 0.0552\n",
      "Epoch 68: Batch 14/116 loss: 0.0559\n",
      "Epoch 68: Batch 15/116 loss: 0.0452\n",
      "Epoch 68: Batch 16/116 loss: 0.0571\n",
      "Epoch 68: Batch 17/116 loss: 0.0894\n",
      "Epoch 68: Batch 18/116 loss: 0.0462\n",
      "Epoch 68: Batch 19/116 loss: 0.0560\n",
      "Epoch 68: Batch 20/116 loss: 0.1440\n",
      "Epoch 68: Batch 21/116 loss: 0.0571\n",
      "Epoch 68: Batch 22/116 loss: 0.0642\n",
      "Epoch 68: Batch 23/116 loss: 0.0655\n",
      "Epoch 68: Batch 24/116 loss: 0.1020\n",
      "Epoch 68: Batch 25/116 loss: 0.0547\n",
      "Epoch 68: Batch 26/116 loss: 0.0868\n",
      "Epoch 68: Batch 27/116 loss: 0.0658\n",
      "Epoch 68: Batch 28/116 loss: 0.0752\n",
      "Epoch 68: Batch 29/116 loss: 0.0585\n",
      "Epoch 68: Batch 30/116 loss: 0.0699\n",
      "Epoch 68: Batch 31/116 loss: 0.0618\n",
      "Epoch 68: Batch 32/116 loss: 0.0655\n",
      "Epoch 68: Batch 33/116 loss: 0.0663\n",
      "Epoch 68: Batch 34/116 loss: 0.0781\n",
      "Epoch 68: Batch 35/116 loss: 0.0436\n",
      "Epoch 68: Batch 36/116 loss: 0.0704\n",
      "Epoch 68: Batch 37/116 loss: 0.0591\n",
      "Epoch 68: Batch 38/116 loss: 0.0798\n",
      "Epoch 68: Batch 39/116 loss: 0.0531\n",
      "Epoch 68: Batch 40/116 loss: 0.0491\n",
      "Epoch 68: Batch 41/116 loss: 0.0763\n",
      "Epoch 68: Batch 42/116 loss: 0.0676\n",
      "Epoch 68: Batch 43/116 loss: 0.0573\n",
      "Epoch 68: Batch 44/116 loss: 0.0494\n",
      "Epoch 68: Batch 45/116 loss: 0.0435\n",
      "Epoch 68: Batch 46/116 loss: 0.0864\n",
      "Epoch 68: Batch 47/116 loss: 0.0677\n",
      "Epoch 68: Batch 48/116 loss: 0.0493\n",
      "Epoch 68: Batch 49/116 loss: 0.0526\n",
      "Epoch 68: Batch 50/116 loss: 0.0556\n",
      "Epoch 68: Batch 51/116 loss: 0.0624\n",
      "Epoch 68: Batch 52/116 loss: 0.0685\n",
      "Epoch 68: Batch 53/116 loss: 0.0563\n",
      "Epoch 68: Batch 54/116 loss: 0.0651\n",
      "Epoch 68: Batch 55/116 loss: 0.0367\n",
      "Epoch 68: Batch 56/116 loss: 0.0450\n",
      "Epoch 68: Batch 57/116 loss: 0.0455\n",
      "Epoch 68: Batch 58/116 loss: 0.0672\n",
      "Epoch 68: Batch 59/116 loss: 0.0420\n",
      "Epoch 68: Batch 60/116 loss: 0.0676\n",
      "Epoch 68: Batch 61/116 loss: 0.0614\n",
      "Epoch 68: Batch 62/116 loss: 0.0526\n",
      "Epoch 68: Batch 63/116 loss: 0.0629\n",
      "Epoch 68: Batch 64/116 loss: 0.0629\n",
      "Epoch 68: Batch 65/116 loss: 0.0576\n",
      "Epoch 68: Batch 66/116 loss: 0.0734\n",
      "Epoch 68: Batch 67/116 loss: 0.0505\n",
      "Epoch 68: Batch 68/116 loss: 0.0442\n",
      "Epoch 68: Batch 69/116 loss: 0.0576\n",
      "Epoch 68: Batch 70/116 loss: 0.0577\n",
      "Epoch 68: Batch 71/116 loss: 0.0580\n",
      "Epoch 68: Batch 72/116 loss: 0.0789\n",
      "Epoch 68: Batch 73/116 loss: 0.1303\n",
      "Epoch 68: Batch 74/116 loss: 0.0596\n",
      "Epoch 68: Batch 75/116 loss: 0.0741\n",
      "Epoch 68: Batch 76/116 loss: 0.0532\n",
      "Epoch 68: Batch 77/116 loss: 0.0498\n",
      "Epoch 68: Batch 78/116 loss: 0.0512\n",
      "Epoch 68: Batch 79/116 loss: 0.0645\n",
      "Epoch 68: Batch 80/116 loss: 0.0665\n",
      "Epoch 68: Batch 81/116 loss: 0.0592\n",
      "Epoch 68: Batch 82/116 loss: 0.0548\n",
      "Epoch 68: Batch 83/116 loss: 0.0558\n",
      "Epoch 68: Batch 84/116 loss: 0.0618\n",
      "Epoch 68: Batch 85/116 loss: 0.0500\n",
      "Epoch 68: Batch 86/116 loss: 0.0783\n",
      "Epoch 68: Batch 87/116 loss: 0.0740\n",
      "Epoch 68: Batch 88/116 loss: 0.0546\n",
      "Epoch 68: Batch 89/116 loss: 0.0485\n",
      "Epoch 68: Batch 90/116 loss: 0.0477\n",
      "Epoch 68: Batch 91/116 loss: 0.0613\n",
      "Epoch 68: Batch 92/116 loss: 0.0692\n",
      "Epoch 68: Batch 93/116 loss: 0.1063\n",
      "Epoch 68: Batch 94/116 loss: 0.0714\n",
      "Epoch 68: Batch 95/116 loss: 0.0890\n",
      "Epoch 68: Batch 96/116 loss: 0.0788\n",
      "Epoch 68: Batch 97/116 loss: 0.0677\n",
      "Epoch 68: Batch 98/116 loss: 0.0450\n",
      "Epoch 68: Batch 99/116 loss: 0.0490\n",
      "Epoch 68: Batch 100/116 loss: 0.0503\n",
      "Epoch 68: Batch 101/116 loss: 0.0660\n",
      "Epoch 68: Batch 102/116 loss: 0.0477\n",
      "Epoch 68: Batch 103/116 loss: 0.0864\n",
      "Epoch 68: Batch 104/116 loss: 0.0664\n",
      "Epoch 68: Batch 105/116 loss: 0.0564\n",
      "Epoch 68: Batch 106/116 loss: 0.0487\n",
      "Epoch 68: Batch 107/116 loss: 0.1241\n",
      "Epoch 68: Batch 108/116 loss: 0.0914\n",
      "Epoch 68: Batch 109/116 loss: 0.1196\n",
      "Epoch 68: Batch 110/116 loss: 0.0677\n",
      "Epoch 68: Batch 111/116 loss: 0.0815\n",
      "Epoch 68: Batch 112/116 loss: 0.0955\n",
      "Epoch 68: Batch 113/116 loss: 0.0737\n",
      "Epoch 68: Batch 114/116 loss: 0.0945\n",
      "Epoch 68: Batch 115/116 loss: 0.0819\n",
      "Epoch 68: Batch 116/116 loss: 0.0546\n",
      "Epoch 68 train loss: 0.0649 valid loss: 0.0777\n",
      "performance reducing: 4\n",
      "Epoch 69: Batch 1/116 loss: 0.0522\n",
      "Epoch 69: Batch 2/116 loss: 0.0630\n",
      "Epoch 69: Batch 3/116 loss: 0.0488\n",
      "Epoch 69: Batch 4/116 loss: 0.0641\n",
      "Epoch 69: Batch 5/116 loss: 0.0441\n",
      "Epoch 69: Batch 6/116 loss: 0.0609\n",
      "Epoch 69: Batch 7/116 loss: 0.0675\n",
      "Epoch 69: Batch 8/116 loss: 0.0456\n",
      "Epoch 69: Batch 9/116 loss: 0.0481\n",
      "Epoch 69: Batch 10/116 loss: 0.0750\n",
      "Epoch 69: Batch 11/116 loss: 0.0852\n",
      "Epoch 69: Batch 12/116 loss: 0.0520\n",
      "Epoch 69: Batch 13/116 loss: 0.0627\n",
      "Epoch 69: Batch 14/116 loss: 0.0597\n",
      "Epoch 69: Batch 15/116 loss: 0.0708\n",
      "Epoch 69: Batch 16/116 loss: 0.0643\n",
      "Epoch 69: Batch 17/116 loss: 0.0505\n",
      "Epoch 69: Batch 18/116 loss: 0.0763\n",
      "Epoch 69: Batch 19/116 loss: 0.0559\n",
      "Epoch 69: Batch 20/116 loss: 0.0527\n",
      "Epoch 69: Batch 21/116 loss: 0.0686\n",
      "Epoch 69: Batch 22/116 loss: 0.0485\n",
      "Epoch 69: Batch 23/116 loss: 0.0597\n",
      "Epoch 69: Batch 24/116 loss: 0.0501\n",
      "Epoch 69: Batch 25/116 loss: 0.1020\n",
      "Epoch 69: Batch 26/116 loss: 0.0485\n",
      "Epoch 69: Batch 27/116 loss: 0.0726\n",
      "Epoch 69: Batch 28/116 loss: 0.0635\n",
      "Epoch 69: Batch 29/116 loss: 0.0562\n",
      "Epoch 69: Batch 30/116 loss: 0.0719\n",
      "Epoch 69: Batch 31/116 loss: 0.0726\n",
      "Epoch 69: Batch 32/116 loss: 0.0524\n",
      "Epoch 69: Batch 33/116 loss: 0.0603\n",
      "Epoch 69: Batch 34/116 loss: 0.0495\n",
      "Epoch 69: Batch 35/116 loss: 0.0471\n",
      "Epoch 69: Batch 36/116 loss: 0.0580\n",
      "Epoch 69: Batch 37/116 loss: 0.0671\n",
      "Epoch 69: Batch 38/116 loss: 0.0626\n",
      "Epoch 69: Batch 39/116 loss: 0.0737\n",
      "Epoch 69: Batch 40/116 loss: 0.0866\n",
      "Epoch 69: Batch 41/116 loss: 0.0379\n",
      "Epoch 69: Batch 42/116 loss: 0.0727\n",
      "Epoch 69: Batch 43/116 loss: 0.0376\n",
      "Epoch 69: Batch 44/116 loss: 0.0525\n",
      "Epoch 69: Batch 45/116 loss: 0.0619\n",
      "Epoch 69: Batch 46/116 loss: 0.0557\n",
      "Epoch 69: Batch 47/116 loss: 0.0804\n",
      "Epoch 69: Batch 48/116 loss: 0.0530\n",
      "Epoch 69: Batch 49/116 loss: 0.0607\n",
      "Epoch 69: Batch 50/116 loss: 0.0469\n",
      "Epoch 69: Batch 51/116 loss: 0.0658\n",
      "Epoch 69: Batch 52/116 loss: 0.0632\n",
      "Epoch 69: Batch 53/116 loss: 0.0474\n",
      "Epoch 69: Batch 54/116 loss: 0.0504\n",
      "Epoch 69: Batch 55/116 loss: 0.0553\n",
      "Epoch 69: Batch 56/116 loss: 0.0608\n",
      "Epoch 69: Batch 57/116 loss: 0.0439\n",
      "Epoch 69: Batch 58/116 loss: 0.0530\n",
      "Epoch 69: Batch 59/116 loss: 0.0794\n",
      "Epoch 69: Batch 60/116 loss: 0.0963\n",
      "Epoch 69: Batch 61/116 loss: 0.0617\n",
      "Epoch 69: Batch 62/116 loss: 0.0504\n",
      "Epoch 69: Batch 63/116 loss: 0.0729\n",
      "Epoch 69: Batch 64/116 loss: 0.0828\n",
      "Epoch 69: Batch 65/116 loss: 0.0537\n",
      "Epoch 69: Batch 66/116 loss: 0.0471\n",
      "Epoch 69: Batch 67/116 loss: 0.0475\n",
      "Epoch 69: Batch 68/116 loss: 0.0695\n",
      "Epoch 69: Batch 69/116 loss: 0.0651\n",
      "Epoch 69: Batch 70/116 loss: 0.0628\n",
      "Epoch 69: Batch 71/116 loss: 0.0756\n",
      "Epoch 69: Batch 72/116 loss: 0.0564\n",
      "Epoch 69: Batch 73/116 loss: 0.0515\n",
      "Epoch 69: Batch 74/116 loss: 0.0651\n",
      "Epoch 69: Batch 75/116 loss: 0.0930\n",
      "Epoch 69: Batch 76/116 loss: 0.0622\n",
      "Epoch 69: Batch 77/116 loss: 0.0825\n",
      "Epoch 69: Batch 78/116 loss: 0.0823\n",
      "Epoch 69: Batch 79/116 loss: 0.0478\n",
      "Epoch 69: Batch 80/116 loss: 0.0666\n",
      "Epoch 69: Batch 81/116 loss: 0.0753\n",
      "Epoch 69: Batch 82/116 loss: 0.0469\n",
      "Epoch 69: Batch 83/116 loss: 0.0709\n",
      "Epoch 69: Batch 84/116 loss: 0.0516\n",
      "Epoch 69: Batch 85/116 loss: 0.0857\n",
      "Epoch 69: Batch 86/116 loss: 0.1007\n",
      "Epoch 69: Batch 87/116 loss: 0.0626\n",
      "Epoch 69: Batch 88/116 loss: 0.0767\n",
      "Epoch 69: Batch 89/116 loss: 0.0472\n",
      "Epoch 69: Batch 90/116 loss: 0.0495\n",
      "Epoch 69: Batch 91/116 loss: 0.0641\n",
      "Epoch 69: Batch 92/116 loss: 0.0566\n",
      "Epoch 69: Batch 93/116 loss: 0.0615\n",
      "Epoch 69: Batch 94/116 loss: 0.0765\n",
      "Epoch 69: Batch 95/116 loss: 0.0899\n",
      "Epoch 69: Batch 96/116 loss: 0.0582\n",
      "Epoch 69: Batch 97/116 loss: 0.0651\n",
      "Epoch 69: Batch 98/116 loss: 0.0779\n",
      "Epoch 69: Batch 99/116 loss: 0.0646\n",
      "Epoch 69: Batch 100/116 loss: 0.0555\n",
      "Epoch 69: Batch 101/116 loss: 0.0643\n",
      "Epoch 69: Batch 102/116 loss: 0.0597\n",
      "Epoch 69: Batch 103/116 loss: 0.0328\n",
      "Epoch 69: Batch 104/116 loss: 0.0577\n",
      "Epoch 69: Batch 105/116 loss: 0.0479\n",
      "Epoch 69: Batch 106/116 loss: 0.0874\n",
      "Epoch 69: Batch 107/116 loss: 0.0327\n",
      "Epoch 69: Batch 108/116 loss: 0.0558\n",
      "Epoch 69: Batch 109/116 loss: 0.0505\n",
      "Epoch 69: Batch 110/116 loss: 0.0836\n",
      "Epoch 69: Batch 111/116 loss: 0.0603\n",
      "Epoch 69: Batch 112/116 loss: 0.0616\n",
      "Epoch 69: Batch 113/116 loss: 0.0529\n",
      "Epoch 69: Batch 114/116 loss: 0.0435\n",
      "Epoch 69: Batch 115/116 loss: 0.0450\n",
      "Epoch 69: Batch 116/116 loss: 0.0551\n",
      "Epoch 69 train loss: 0.0618 valid loss: 0.0624\n",
      "Epoch 70: Batch 1/116 loss: 0.0435\n",
      "Epoch 70: Batch 2/116 loss: 0.0600\n",
      "Epoch 70: Batch 3/116 loss: 0.0444\n",
      "Epoch 70: Batch 4/116 loss: 0.0515\n",
      "Epoch 70: Batch 5/116 loss: 0.0698\n",
      "Epoch 70: Batch 6/116 loss: 0.1315\n",
      "Epoch 70: Batch 7/116 loss: 0.0675\n",
      "Epoch 70: Batch 8/116 loss: 0.0681\n",
      "Epoch 70: Batch 9/116 loss: 0.0626\n",
      "Epoch 70: Batch 10/116 loss: 0.0763\n",
      "Epoch 70: Batch 11/116 loss: 0.0657\n",
      "Epoch 70: Batch 12/116 loss: 0.0513\n",
      "Epoch 70: Batch 13/116 loss: 0.0419\n",
      "Epoch 70: Batch 14/116 loss: 0.0603\n",
      "Epoch 70: Batch 15/116 loss: 0.0791\n",
      "Epoch 70: Batch 16/116 loss: 0.0515\n",
      "Epoch 70: Batch 17/116 loss: 0.0663\n",
      "Epoch 70: Batch 18/116 loss: 0.0673\n",
      "Epoch 70: Batch 19/116 loss: 0.0592\n",
      "Epoch 70: Batch 20/116 loss: 0.0824\n",
      "Epoch 70: Batch 21/116 loss: 0.0684\n",
      "Epoch 70: Batch 22/116 loss: 0.0586\n",
      "Epoch 70: Batch 23/116 loss: 0.0729\n",
      "Epoch 70: Batch 24/116 loss: 0.0574\n",
      "Epoch 70: Batch 25/116 loss: 0.0618\n",
      "Epoch 70: Batch 26/116 loss: 0.0736\n",
      "Epoch 70: Batch 27/116 loss: 0.0525\n",
      "Epoch 70: Batch 28/116 loss: 0.0754\n",
      "Epoch 70: Batch 29/116 loss: 0.0496\n",
      "Epoch 70: Batch 30/116 loss: 0.0769\n",
      "Epoch 70: Batch 31/116 loss: 0.0720\n",
      "Epoch 70: Batch 32/116 loss: 0.0782\n",
      "Epoch 70: Batch 33/116 loss: 0.0560\n",
      "Epoch 70: Batch 34/116 loss: 0.0531\n",
      "Epoch 70: Batch 35/116 loss: 0.0608\n",
      "Epoch 70: Batch 36/116 loss: 0.0805\n",
      "Epoch 70: Batch 37/116 loss: 0.0537\n",
      "Epoch 70: Batch 38/116 loss: 0.0480\n",
      "Epoch 70: Batch 39/116 loss: 0.0652\n",
      "Epoch 70: Batch 40/116 loss: 0.0596\n",
      "Epoch 70: Batch 41/116 loss: 0.0445\n",
      "Epoch 70: Batch 42/116 loss: 0.0775\n",
      "Epoch 70: Batch 43/116 loss: 0.0517\n",
      "Epoch 70: Batch 44/116 loss: 0.0654\n",
      "Epoch 70: Batch 45/116 loss: 0.0538\n",
      "Epoch 70: Batch 46/116 loss: 0.0533\n",
      "Epoch 70: Batch 47/116 loss: 0.0646\n",
      "Epoch 70: Batch 48/116 loss: 0.0453\n",
      "Epoch 70: Batch 49/116 loss: 0.0556\n",
      "Epoch 70: Batch 50/116 loss: 0.0557\n",
      "Epoch 70: Batch 51/116 loss: 0.0459\n",
      "Epoch 70: Batch 52/116 loss: 0.0585\n",
      "Epoch 70: Batch 53/116 loss: 0.0555\n",
      "Epoch 70: Batch 54/116 loss: 0.0714\n",
      "Epoch 70: Batch 55/116 loss: 0.0483\n",
      "Epoch 70: Batch 56/116 loss: 0.0444\n",
      "Epoch 70: Batch 57/116 loss: 0.0563\n",
      "Epoch 70: Batch 58/116 loss: 0.0423\n",
      "Epoch 70: Batch 59/116 loss: 0.0523\n",
      "Epoch 70: Batch 60/116 loss: 0.0794\n",
      "Epoch 70: Batch 61/116 loss: 0.0663\n",
      "Epoch 70: Batch 62/116 loss: 0.0567\n",
      "Epoch 70: Batch 63/116 loss: 0.0578\n",
      "Epoch 70: Batch 64/116 loss: 0.0545\n",
      "Epoch 70: Batch 65/116 loss: 0.0623\n",
      "Epoch 70: Batch 66/116 loss: 0.0414\n",
      "Epoch 70: Batch 67/116 loss: 0.0559\n",
      "Epoch 70: Batch 68/116 loss: 0.0534\n",
      "Epoch 70: Batch 69/116 loss: 0.0368\n",
      "Epoch 70: Batch 70/116 loss: 0.0672\n",
      "Epoch 70: Batch 71/116 loss: 0.0564\n",
      "Epoch 70: Batch 72/116 loss: 0.0839\n",
      "Epoch 70: Batch 73/116 loss: 0.0574\n",
      "Epoch 70: Batch 74/116 loss: 0.0695\n",
      "Epoch 70: Batch 75/116 loss: 0.0473\n",
      "Epoch 70: Batch 76/116 loss: 0.0683\n",
      "Epoch 70: Batch 77/116 loss: 0.0963\n",
      "Epoch 70: Batch 78/116 loss: 0.0447\n",
      "Epoch 70: Batch 79/116 loss: 0.0609\n",
      "Epoch 70: Batch 80/116 loss: 0.0715\n",
      "Epoch 70: Batch 81/116 loss: 0.0486\n",
      "Epoch 70: Batch 82/116 loss: 0.0560\n",
      "Epoch 70: Batch 83/116 loss: 0.0458\n",
      "Epoch 70: Batch 84/116 loss: 0.0537\n",
      "Epoch 70: Batch 85/116 loss: 0.0481\n",
      "Epoch 70: Batch 86/116 loss: 0.0620\n",
      "Epoch 70: Batch 87/116 loss: 0.0413\n",
      "Epoch 70: Batch 88/116 loss: 0.0452\n",
      "Epoch 70: Batch 89/116 loss: 0.0681\n",
      "Epoch 70: Batch 90/116 loss: 0.0695\n",
      "Epoch 70: Batch 91/116 loss: 0.0761\n",
      "Epoch 70: Batch 92/116 loss: 0.0625\n",
      "Epoch 70: Batch 93/116 loss: 0.0475\n",
      "Epoch 70: Batch 94/116 loss: 0.0514\n",
      "Epoch 70: Batch 95/116 loss: 0.0772\n",
      "Epoch 70: Batch 96/116 loss: 0.0624\n",
      "Epoch 70: Batch 97/116 loss: 0.0726\n",
      "Epoch 70: Batch 98/116 loss: 0.0791\n",
      "Epoch 70: Batch 99/116 loss: 0.0519\n",
      "Epoch 70: Batch 100/116 loss: 0.0638\n",
      "Epoch 70: Batch 101/116 loss: 0.0582\n",
      "Epoch 70: Batch 102/116 loss: 0.0536\n",
      "Epoch 70: Batch 103/116 loss: 0.0517\n",
      "Epoch 70: Batch 104/116 loss: 0.0611\n",
      "Epoch 70: Batch 105/116 loss: 0.0847\n",
      "Epoch 70: Batch 106/116 loss: 0.0665\n",
      "Epoch 70: Batch 107/116 loss: 0.0573\n",
      "Epoch 70: Batch 108/116 loss: 0.0400\n",
      "Epoch 70: Batch 109/116 loss: 0.0500\n",
      "Epoch 70: Batch 110/116 loss: 0.0599\n",
      "Epoch 70: Batch 111/116 loss: 0.0812\n",
      "Epoch 70: Batch 112/116 loss: 0.0869\n",
      "Epoch 70: Batch 113/116 loss: 0.0541\n",
      "Epoch 70: Batch 114/116 loss: 0.0530\n",
      "Epoch 70: Batch 115/116 loss: 0.0524\n",
      "Epoch 70: Batch 116/116 loss: 0.0647\n",
      "Epoch 70 train loss: 0.0609 valid loss: 0.0659\n",
      "performance reducing: 1\n",
      "Epoch 71: Batch 1/116 loss: 0.0507\n",
      "Epoch 71: Batch 2/116 loss: 0.0555\n",
      "Epoch 71: Batch 3/116 loss: 0.0386\n",
      "Epoch 71: Batch 4/116 loss: 0.0438\n",
      "Epoch 71: Batch 5/116 loss: 0.0714\n",
      "Epoch 71: Batch 6/116 loss: 0.0577\n",
      "Epoch 71: Batch 7/116 loss: 0.0586\n",
      "Epoch 71: Batch 8/116 loss: 0.0435\n",
      "Epoch 71: Batch 9/116 loss: 0.0673\n",
      "Epoch 71: Batch 10/116 loss: 0.0600\n",
      "Epoch 71: Batch 11/116 loss: 0.0564\n",
      "Epoch 71: Batch 12/116 loss: 0.0685\n",
      "Epoch 71: Batch 13/116 loss: 0.0391\n",
      "Epoch 71: Batch 14/116 loss: 0.0689\n",
      "Epoch 71: Batch 15/116 loss: 0.0528\n",
      "Epoch 71: Batch 16/116 loss: 0.0448\n",
      "Epoch 71: Batch 17/116 loss: 0.0730\n",
      "Epoch 71: Batch 18/116 loss: 0.0460\n",
      "Epoch 71: Batch 19/116 loss: 0.0473\n",
      "Epoch 71: Batch 20/116 loss: 0.0571\n",
      "Epoch 71: Batch 21/116 loss: 0.0662\n",
      "Epoch 71: Batch 22/116 loss: 0.0782\n",
      "Epoch 71: Batch 23/116 loss: 0.0658\n",
      "Epoch 71: Batch 24/116 loss: 0.0731\n",
      "Epoch 71: Batch 25/116 loss: 0.0419\n",
      "Epoch 71: Batch 26/116 loss: 0.0596\n",
      "Epoch 71: Batch 27/116 loss: 0.0426\n",
      "Epoch 71: Batch 28/116 loss: 0.0471\n",
      "Epoch 71: Batch 29/116 loss: 0.0408\n",
      "Epoch 71: Batch 30/116 loss: 0.0452\n",
      "Epoch 71: Batch 31/116 loss: 0.0484\n",
      "Epoch 71: Batch 32/116 loss: 0.0562\n",
      "Epoch 71: Batch 33/116 loss: 0.0304\n",
      "Epoch 71: Batch 34/116 loss: 0.0593\n",
      "Epoch 71: Batch 35/116 loss: 0.0548\n",
      "Epoch 71: Batch 36/116 loss: 0.1018\n",
      "Epoch 71: Batch 37/116 loss: 0.0431\n",
      "Epoch 71: Batch 38/116 loss: 0.0528\n",
      "Epoch 71: Batch 39/116 loss: 0.0583\n",
      "Epoch 71: Batch 40/116 loss: 0.0414\n",
      "Epoch 71: Batch 41/116 loss: 0.0576\n",
      "Epoch 71: Batch 42/116 loss: 0.0591\n",
      "Epoch 71: Batch 43/116 loss: 0.0598\n",
      "Epoch 71: Batch 44/116 loss: 0.0630\n",
      "Epoch 71: Batch 45/116 loss: 0.0659\n",
      "Epoch 71: Batch 46/116 loss: 0.0715\n",
      "Epoch 71: Batch 47/116 loss: 0.0500\n",
      "Epoch 71: Batch 48/116 loss: 0.0618\n",
      "Epoch 71: Batch 49/116 loss: 0.0453\n",
      "Epoch 71: Batch 50/116 loss: 0.0574\n",
      "Epoch 71: Batch 51/116 loss: 0.0533\n",
      "Epoch 71: Batch 52/116 loss: 0.0367\n",
      "Epoch 71: Batch 53/116 loss: 0.0594\n",
      "Epoch 71: Batch 54/116 loss: 0.0626\n",
      "Epoch 71: Batch 55/116 loss: 0.0496\n",
      "Epoch 71: Batch 56/116 loss: 0.0433\n",
      "Epoch 71: Batch 57/116 loss: 0.0562\n",
      "Epoch 71: Batch 58/116 loss: 0.0891\n",
      "Epoch 71: Batch 59/116 loss: 0.0639\n",
      "Epoch 71: Batch 60/116 loss: 0.0521\n",
      "Epoch 71: Batch 61/116 loss: 0.0679\n",
      "Epoch 71: Batch 62/116 loss: 0.0591\n",
      "Epoch 71: Batch 63/116 loss: 0.1109\n",
      "Epoch 71: Batch 64/116 loss: 0.0633\n",
      "Epoch 71: Batch 65/116 loss: 0.0478\n",
      "Epoch 71: Batch 66/116 loss: 0.0572\n",
      "Epoch 71: Batch 67/116 loss: 0.0577\n",
      "Epoch 71: Batch 68/116 loss: 0.0699\n",
      "Epoch 71: Batch 69/116 loss: 0.0512\n",
      "Epoch 71: Batch 70/116 loss: 0.0433\n",
      "Epoch 71: Batch 71/116 loss: 0.0497\n",
      "Epoch 71: Batch 72/116 loss: 0.0460\n",
      "Epoch 71: Batch 73/116 loss: 0.0552\n",
      "Epoch 71: Batch 74/116 loss: 0.0925\n",
      "Epoch 71: Batch 75/116 loss: 0.0624\n",
      "Epoch 71: Batch 76/116 loss: 0.0628\n",
      "Epoch 71: Batch 77/116 loss: 0.0730\n",
      "Epoch 71: Batch 78/116 loss: 0.0427\n",
      "Epoch 71: Batch 79/116 loss: 0.0760\n",
      "Epoch 71: Batch 80/116 loss: 0.0785\n",
      "Epoch 71: Batch 81/116 loss: 0.0432\n",
      "Epoch 71: Batch 82/116 loss: 0.0545\n",
      "Epoch 71: Batch 83/116 loss: 0.0628\n",
      "Epoch 71: Batch 84/116 loss: 0.0817\n",
      "Epoch 71: Batch 85/116 loss: 0.0530\n",
      "Epoch 71: Batch 86/116 loss: 0.0669\n",
      "Epoch 71: Batch 87/116 loss: 0.0372\n",
      "Epoch 71: Batch 88/116 loss: 0.0483\n",
      "Epoch 71: Batch 89/116 loss: 0.0511\n",
      "Epoch 71: Batch 90/116 loss: 0.0506\n",
      "Epoch 71: Batch 91/116 loss: 0.0553\n",
      "Epoch 71: Batch 92/116 loss: 0.0788\n",
      "Epoch 71: Batch 93/116 loss: 0.0450\n",
      "Epoch 71: Batch 94/116 loss: 0.0419\n",
      "Epoch 71: Batch 95/116 loss: 0.0569\n",
      "Epoch 71: Batch 96/116 loss: 0.0355\n",
      "Epoch 71: Batch 97/116 loss: 0.0375\n",
      "Epoch 71: Batch 98/116 loss: 0.0685\n",
      "Epoch 71: Batch 99/116 loss: 0.0763\n",
      "Epoch 71: Batch 100/116 loss: 0.0594\n",
      "Epoch 71: Batch 101/116 loss: 0.0580\n",
      "Epoch 71: Batch 102/116 loss: 0.0844\n",
      "Epoch 71: Batch 103/116 loss: 0.0408\n",
      "Epoch 71: Batch 104/116 loss: 0.0441\n",
      "Epoch 71: Batch 105/116 loss: 0.0832\n",
      "Epoch 71: Batch 106/116 loss: 0.0606\n",
      "Epoch 71: Batch 107/116 loss: 0.0972\n",
      "Epoch 71: Batch 108/116 loss: 0.0716\n",
      "Epoch 71: Batch 109/116 loss: 0.1005\n",
      "Epoch 71: Batch 110/116 loss: 0.0677\n",
      "Epoch 71: Batch 111/116 loss: 0.0636\n",
      "Epoch 71: Batch 112/116 loss: 0.0834\n",
      "Epoch 71: Batch 113/116 loss: 0.0594\n",
      "Epoch 71: Batch 114/116 loss: 0.0884\n",
      "Epoch 71: Batch 115/116 loss: 0.0860\n",
      "Epoch 71: Batch 116/116 loss: 0.0525\n",
      "Epoch 71 train loss: 0.0593 valid loss: 0.0699\n",
      "performance reducing: 2\n",
      "Epoch 72: Batch 1/116 loss: 0.0592\n",
      "Epoch 72: Batch 2/116 loss: 0.0711\n",
      "Epoch 72: Batch 3/116 loss: 0.0496\n",
      "Epoch 72: Batch 4/116 loss: 0.0622\n",
      "Epoch 72: Batch 5/116 loss: 0.0573\n",
      "Epoch 72: Batch 6/116 loss: 0.0398\n",
      "Epoch 72: Batch 7/116 loss: 0.0666\n",
      "Epoch 72: Batch 8/116 loss: 0.0523\n",
      "Epoch 72: Batch 9/116 loss: 0.0606\n",
      "Epoch 72: Batch 10/116 loss: 0.0325\n",
      "Epoch 72: Batch 11/116 loss: 0.0700\n",
      "Epoch 72: Batch 12/116 loss: 0.0453\n",
      "Epoch 72: Batch 13/116 loss: 0.0868\n",
      "Epoch 72: Batch 14/116 loss: 0.0527\n",
      "Epoch 72: Batch 15/116 loss: 0.0614\n",
      "Epoch 72: Batch 16/116 loss: 0.0664\n",
      "Epoch 72: Batch 17/116 loss: 0.0556\n",
      "Epoch 72: Batch 18/116 loss: 0.0496\n",
      "Epoch 72: Batch 19/116 loss: 0.0500\n",
      "Epoch 72: Batch 20/116 loss: 0.0619\n",
      "Epoch 72: Batch 21/116 loss: 0.0650\n",
      "Epoch 72: Batch 22/116 loss: 0.0504\n",
      "Epoch 72: Batch 23/116 loss: 0.0576\n",
      "Epoch 72: Batch 24/116 loss: 0.0666\n",
      "Epoch 72: Batch 25/116 loss: 0.0385\n",
      "Epoch 72: Batch 26/116 loss: 0.0536\n",
      "Epoch 72: Batch 27/116 loss: 0.0583\n",
      "Epoch 72: Batch 28/116 loss: 0.0390\n",
      "Epoch 72: Batch 29/116 loss: 0.0820\n",
      "Epoch 72: Batch 30/116 loss: 0.0596\n",
      "Epoch 72: Batch 31/116 loss: 0.0524\n",
      "Epoch 72: Batch 32/116 loss: 0.0893\n",
      "Epoch 72: Batch 33/116 loss: 0.0571\n",
      "Epoch 72: Batch 34/116 loss: 0.0538\n",
      "Epoch 72: Batch 35/116 loss: 0.0479\n",
      "Epoch 72: Batch 36/116 loss: 0.0709\n",
      "Epoch 72: Batch 37/116 loss: 0.0514\n",
      "Epoch 72: Batch 38/116 loss: 0.0541\n",
      "Epoch 72: Batch 39/116 loss: 0.0721\n",
      "Epoch 72: Batch 40/116 loss: 0.0751\n",
      "Epoch 72: Batch 41/116 loss: 0.0767\n",
      "Epoch 72: Batch 42/116 loss: 0.0603\n",
      "Epoch 72: Batch 43/116 loss: 0.0618\n",
      "Epoch 72: Batch 44/116 loss: 0.0481\n",
      "Epoch 72: Batch 45/116 loss: 0.0783\n",
      "Epoch 72: Batch 46/116 loss: 0.0625\n",
      "Epoch 72: Batch 47/116 loss: 0.0474\n",
      "Epoch 72: Batch 48/116 loss: 0.0671\n",
      "Epoch 72: Batch 49/116 loss: 0.0487\n",
      "Epoch 72: Batch 50/116 loss: 0.0629\n",
      "Epoch 72: Batch 51/116 loss: 0.0602\n",
      "Epoch 72: Batch 52/116 loss: 0.0739\n",
      "Epoch 72: Batch 53/116 loss: 0.0591\n",
      "Epoch 72: Batch 54/116 loss: 0.0514\n",
      "Epoch 72: Batch 55/116 loss: 0.0420\n",
      "Epoch 72: Batch 56/116 loss: 0.0788\n",
      "Epoch 72: Batch 57/116 loss: 0.0714\n",
      "Epoch 72: Batch 58/116 loss: 0.0783\n",
      "Epoch 72: Batch 59/116 loss: 0.0731\n",
      "Epoch 72: Batch 60/116 loss: 0.0803\n",
      "Epoch 72: Batch 61/116 loss: 0.0624\n",
      "Epoch 72: Batch 62/116 loss: 0.0788\n",
      "Epoch 72: Batch 63/116 loss: 0.0595\n",
      "Epoch 72: Batch 64/116 loss: 0.0405\n",
      "Epoch 72: Batch 65/116 loss: 0.0518\n",
      "Epoch 72: Batch 66/116 loss: 0.0597\n",
      "Epoch 72: Batch 67/116 loss: 0.0920\n",
      "Epoch 72: Batch 68/116 loss: 0.0476\n",
      "Epoch 72: Batch 69/116 loss: 0.0457\n",
      "Epoch 72: Batch 70/116 loss: 0.0672\n",
      "Epoch 72: Batch 71/116 loss: 0.0697\n",
      "Epoch 72: Batch 72/116 loss: 0.0732\n",
      "Epoch 72: Batch 73/116 loss: 0.0508\n",
      "Epoch 72: Batch 74/116 loss: 0.0689\n",
      "Epoch 72: Batch 75/116 loss: 0.0593\n",
      "Epoch 72: Batch 76/116 loss: 0.0434\n",
      "Epoch 72: Batch 77/116 loss: 0.0522\n",
      "Epoch 72: Batch 78/116 loss: 0.0572\n",
      "Epoch 72: Batch 79/116 loss: 0.0583\n",
      "Epoch 72: Batch 80/116 loss: 0.0441\n",
      "Epoch 72: Batch 81/116 loss: 0.0585\n",
      "Epoch 72: Batch 82/116 loss: 0.0425\n",
      "Epoch 72: Batch 83/116 loss: 0.0441\n",
      "Epoch 72: Batch 84/116 loss: 0.0785\n",
      "Epoch 72: Batch 85/116 loss: 0.0542\n",
      "Epoch 72: Batch 86/116 loss: 0.0852\n",
      "Epoch 72: Batch 87/116 loss: 0.0502\n",
      "Epoch 72: Batch 88/116 loss: 0.0630\n",
      "Epoch 72: Batch 89/116 loss: 0.0740\n",
      "Epoch 72: Batch 90/116 loss: 0.0620\n",
      "Epoch 72: Batch 91/116 loss: 0.0502\n",
      "Epoch 72: Batch 92/116 loss: 0.0568\n",
      "Epoch 72: Batch 93/116 loss: 0.0556\n",
      "Epoch 72: Batch 94/116 loss: 0.0652\n",
      "Epoch 72: Batch 95/116 loss: 0.0589\n",
      "Epoch 72: Batch 96/116 loss: 0.0711\n",
      "Epoch 72: Batch 97/116 loss: 0.0744\n",
      "Epoch 72: Batch 98/116 loss: 0.0494\n",
      "Epoch 72: Batch 99/116 loss: 0.0511\n",
      "Epoch 72: Batch 100/116 loss: 0.0614\n",
      "Epoch 72: Batch 101/116 loss: 0.0516\n",
      "Epoch 72: Batch 102/116 loss: 0.0437\n",
      "Epoch 72: Batch 103/116 loss: 0.0323\n",
      "Epoch 72: Batch 104/116 loss: 0.0444\n",
      "Epoch 72: Batch 105/116 loss: 0.0798\n",
      "Epoch 72: Batch 106/116 loss: 0.0744\n",
      "Epoch 72: Batch 107/116 loss: 0.0678\n",
      "Epoch 72: Batch 108/116 loss: 0.0493\n",
      "Epoch 72: Batch 109/116 loss: 0.0649\n",
      "Epoch 72: Batch 110/116 loss: 0.0697\n",
      "Epoch 72: Batch 111/116 loss: 0.0723\n",
      "Epoch 72: Batch 112/116 loss: 0.0730\n",
      "Epoch 72: Batch 113/116 loss: 0.0403\n",
      "Epoch 72: Batch 114/116 loss: 0.0554\n",
      "Epoch 72: Batch 115/116 loss: 0.0589\n",
      "Epoch 72: Batch 116/116 loss: 0.0523\n",
      "Epoch 72 train loss: 0.0599 valid loss: 0.0602\n",
      "Epoch 73: Batch 1/116 loss: 0.0433\n",
      "Epoch 73: Batch 2/116 loss: 0.0920\n",
      "Epoch 73: Batch 3/116 loss: 0.0601\n",
      "Epoch 73: Batch 4/116 loss: 0.0478\n",
      "Epoch 73: Batch 5/116 loss: 0.0557\n",
      "Epoch 73: Batch 6/116 loss: 0.0514\n",
      "Epoch 73: Batch 7/116 loss: 0.0454\n",
      "Epoch 73: Batch 8/116 loss: 0.1000\n",
      "Epoch 73: Batch 9/116 loss: 0.0579\n",
      "Epoch 73: Batch 10/116 loss: 0.0681\n",
      "Epoch 73: Batch 11/116 loss: 0.0373\n",
      "Epoch 73: Batch 12/116 loss: 0.0745\n",
      "Epoch 73: Batch 13/116 loss: 0.0574\n",
      "Epoch 73: Batch 14/116 loss: 0.0496\n",
      "Epoch 73: Batch 15/116 loss: 0.0520\n",
      "Epoch 73: Batch 16/116 loss: 0.0611\n",
      "Epoch 73: Batch 17/116 loss: 0.0622\n",
      "Epoch 73: Batch 18/116 loss: 0.0739\n",
      "Epoch 73: Batch 19/116 loss: 0.0784\n",
      "Epoch 73: Batch 20/116 loss: 0.0679\n",
      "Epoch 73: Batch 21/116 loss: 0.0796\n",
      "Epoch 73: Batch 22/116 loss: 0.0504\n",
      "Epoch 73: Batch 23/116 loss: 0.0622\n",
      "Epoch 73: Batch 24/116 loss: 0.0490\n",
      "Epoch 73: Batch 25/116 loss: 0.0671\n",
      "Epoch 73: Batch 26/116 loss: 0.0506\n",
      "Epoch 73: Batch 27/116 loss: 0.0538\n",
      "Epoch 73: Batch 28/116 loss: 0.0420\n",
      "Epoch 73: Batch 29/116 loss: 0.0576\n",
      "Epoch 73: Batch 30/116 loss: 0.0533\n",
      "Epoch 73: Batch 31/116 loss: 0.0610\n",
      "Epoch 73: Batch 32/116 loss: 0.0638\n",
      "Epoch 73: Batch 33/116 loss: 0.0699\n",
      "Epoch 73: Batch 34/116 loss: 0.0603\n",
      "Epoch 73: Batch 35/116 loss: 0.0519\n",
      "Epoch 73: Batch 36/116 loss: 0.0534\n",
      "Epoch 73: Batch 37/116 loss: 0.0338\n",
      "Epoch 73: Batch 38/116 loss: 0.0587\n",
      "Epoch 73: Batch 39/116 loss: 0.0351\n",
      "Epoch 73: Batch 40/116 loss: 0.0642\n",
      "Epoch 73: Batch 41/116 loss: 0.0466\n",
      "Epoch 73: Batch 42/116 loss: 0.0570\n",
      "Epoch 73: Batch 43/116 loss: 0.0621\n",
      "Epoch 73: Batch 44/116 loss: 0.0421\n",
      "Epoch 73: Batch 45/116 loss: 0.0611\n",
      "Epoch 73: Batch 46/116 loss: 0.0518\n",
      "Epoch 73: Batch 47/116 loss: 0.0515\n",
      "Epoch 73: Batch 48/116 loss: 0.0564\n",
      "Epoch 73: Batch 49/116 loss: 0.0657\n",
      "Epoch 73: Batch 50/116 loss: 0.0645\n",
      "Epoch 73: Batch 51/116 loss: 0.0653\n",
      "Epoch 73: Batch 52/116 loss: 0.0489\n",
      "Epoch 73: Batch 53/116 loss: 0.0449\n",
      "Epoch 73: Batch 54/116 loss: 0.0619\n",
      "Epoch 73: Batch 55/116 loss: 0.0474\n",
      "Epoch 73: Batch 56/116 loss: 0.0786\n",
      "Epoch 73: Batch 57/116 loss: 0.0506\n",
      "Epoch 73: Batch 58/116 loss: 0.0531\n",
      "Epoch 73: Batch 59/116 loss: 0.0651\n",
      "Epoch 73: Batch 60/116 loss: 0.0614\n",
      "Epoch 73: Batch 61/116 loss: 0.0633\n",
      "Epoch 73: Batch 62/116 loss: 0.0516\n",
      "Epoch 73: Batch 63/116 loss: 0.0587\n",
      "Epoch 73: Batch 64/116 loss: 0.0586\n",
      "Epoch 73: Batch 65/116 loss: 0.0541\n",
      "Epoch 73: Batch 66/116 loss: 0.0719\n",
      "Epoch 73: Batch 67/116 loss: 0.0555\n",
      "Epoch 73: Batch 68/116 loss: 0.0478\n",
      "Epoch 73: Batch 69/116 loss: 0.0545\n",
      "Epoch 73: Batch 70/116 loss: 0.0686\n",
      "Epoch 73: Batch 71/116 loss: 0.0483\n",
      "Epoch 73: Batch 72/116 loss: 0.0459\n",
      "Epoch 73: Batch 73/116 loss: 0.0767\n",
      "Epoch 73: Batch 74/116 loss: 0.0475\n",
      "Epoch 73: Batch 75/116 loss: 0.0440\n",
      "Epoch 73: Batch 76/116 loss: 0.0645\n",
      "Epoch 73: Batch 77/116 loss: 0.0447\n",
      "Epoch 73: Batch 78/116 loss: 0.1035\n",
      "Epoch 73: Batch 79/116 loss: 0.0686\n",
      "Epoch 73: Batch 80/116 loss: 0.0548\n",
      "Epoch 73: Batch 81/116 loss: 0.0758\n",
      "Epoch 73: Batch 82/116 loss: 0.0526\n",
      "Epoch 73: Batch 83/116 loss: 0.0408\n",
      "Epoch 73: Batch 84/116 loss: 0.0961\n",
      "Epoch 73: Batch 85/116 loss: 0.0812\n",
      "Epoch 73: Batch 86/116 loss: 0.0820\n",
      "Epoch 73: Batch 87/116 loss: 0.0675\n",
      "Epoch 73: Batch 88/116 loss: 0.0530\n",
      "Epoch 73: Batch 89/116 loss: 0.0471\n",
      "Epoch 73: Batch 90/116 loss: 0.0536\n",
      "Epoch 73: Batch 91/116 loss: 0.0664\n",
      "Epoch 73: Batch 92/116 loss: 0.0420\n",
      "Epoch 73: Batch 93/116 loss: 0.0591\n",
      "Epoch 73: Batch 94/116 loss: 0.0387\n",
      "Epoch 73: Batch 95/116 loss: 0.0569\n",
      "Epoch 73: Batch 96/116 loss: 0.0630\n",
      "Epoch 73: Batch 97/116 loss: 0.0504\n",
      "Epoch 73: Batch 98/116 loss: 0.0540\n",
      "Epoch 73: Batch 99/116 loss: 0.0744\n",
      "Epoch 73: Batch 100/116 loss: 0.0595\n",
      "Epoch 73: Batch 101/116 loss: 0.0739\n",
      "Epoch 73: Batch 102/116 loss: 0.0483\n",
      "Epoch 73: Batch 103/116 loss: 0.0577\n",
      "Epoch 73: Batch 104/116 loss: 0.0506\n",
      "Epoch 73: Batch 105/116 loss: 0.0594\n",
      "Epoch 73: Batch 106/116 loss: 0.0674\n",
      "Epoch 73: Batch 107/116 loss: 0.0422\n",
      "Epoch 73: Batch 108/116 loss: 0.0865\n",
      "Epoch 73: Batch 109/116 loss: 0.0489\n",
      "Epoch 73: Batch 110/116 loss: 0.0586\n",
      "Epoch 73: Batch 111/116 loss: 0.0572\n",
      "Epoch 73: Batch 112/116 loss: 0.0689\n",
      "Epoch 73: Batch 113/116 loss: 0.0706\n",
      "Epoch 73: Batch 114/116 loss: 0.0593\n",
      "Epoch 73: Batch 115/116 loss: 0.0601\n",
      "Epoch 73: Batch 116/116 loss: 0.0506\n",
      "Epoch 73 train loss: 0.0590 valid loss: 0.0650\n",
      "performance reducing: 1\n",
      "Epoch 74: Batch 1/116 loss: 0.0532\n",
      "Epoch 74: Batch 2/116 loss: 0.0886\n",
      "Epoch 74: Batch 3/116 loss: 0.0630\n",
      "Epoch 74: Batch 4/116 loss: 0.0480\n",
      "Epoch 74: Batch 5/116 loss: 0.0561\n",
      "Epoch 74: Batch 6/116 loss: 0.0405\n",
      "Epoch 74: Batch 7/116 loss: 0.0572\n",
      "Epoch 74: Batch 8/116 loss: 0.0704\n",
      "Epoch 74: Batch 9/116 loss: 0.0497\n",
      "Epoch 74: Batch 10/116 loss: 0.0587\n",
      "Epoch 74: Batch 11/116 loss: 0.0491\n",
      "Epoch 74: Batch 12/116 loss: 0.0609\n",
      "Epoch 74: Batch 13/116 loss: 0.0537\n",
      "Epoch 74: Batch 14/116 loss: 0.0509\n",
      "Epoch 74: Batch 15/116 loss: 0.0649\n",
      "Epoch 74: Batch 16/116 loss: 0.0755\n",
      "Epoch 74: Batch 17/116 loss: 0.0556\n",
      "Epoch 74: Batch 18/116 loss: 0.0488\n",
      "Epoch 74: Batch 19/116 loss: 0.0663\n",
      "Epoch 74: Batch 20/116 loss: 0.0481\n",
      "Epoch 74: Batch 21/116 loss: 0.0495\n",
      "Epoch 74: Batch 22/116 loss: 0.0823\n",
      "Epoch 74: Batch 23/116 loss: 0.0600\n",
      "Epoch 74: Batch 24/116 loss: 0.0687\n",
      "Epoch 74: Batch 25/116 loss: 0.0786\n",
      "Epoch 74: Batch 26/116 loss: 0.0487\n",
      "Epoch 74: Batch 27/116 loss: 0.0898\n",
      "Epoch 74: Batch 28/116 loss: 0.0718\n",
      "Epoch 74: Batch 29/116 loss: 0.0648\n",
      "Epoch 74: Batch 30/116 loss: 0.0773\n",
      "Epoch 74: Batch 31/116 loss: 0.0540\n",
      "Epoch 74: Batch 32/116 loss: 0.0511\n",
      "Epoch 74: Batch 33/116 loss: 0.0495\n",
      "Epoch 74: Batch 34/116 loss: 0.0572\n",
      "Epoch 74: Batch 35/116 loss: 0.0700\n",
      "Epoch 74: Batch 36/116 loss: 0.0582\n",
      "Epoch 74: Batch 37/116 loss: 0.0492\n",
      "Epoch 74: Batch 38/116 loss: 0.0512\n",
      "Epoch 74: Batch 39/116 loss: 0.0354\n",
      "Epoch 74: Batch 40/116 loss: 0.0395\n",
      "Epoch 74: Batch 41/116 loss: 0.0907\n",
      "Epoch 74: Batch 42/116 loss: 0.0475\n",
      "Epoch 74: Batch 43/116 loss: 0.0648\n",
      "Epoch 74: Batch 44/116 loss: 0.0459\n",
      "Epoch 74: Batch 45/116 loss: 0.1056\n",
      "Epoch 74: Batch 46/116 loss: 0.0467\n",
      "Epoch 74: Batch 47/116 loss: 0.0631\n",
      "Epoch 74: Batch 48/116 loss: 0.0471\n",
      "Epoch 74: Batch 49/116 loss: 0.0411\n",
      "Epoch 74: Batch 50/116 loss: 0.0451\n",
      "Epoch 74: Batch 51/116 loss: 0.0477\n",
      "Epoch 74: Batch 52/116 loss: 0.0451\n",
      "Epoch 74: Batch 53/116 loss: 0.0625\n",
      "Epoch 74: Batch 54/116 loss: 0.0698\n",
      "Epoch 74: Batch 55/116 loss: 0.0451\n",
      "Epoch 74: Batch 56/116 loss: 0.0607\n",
      "Epoch 74: Batch 57/116 loss: 0.0460\n",
      "Epoch 74: Batch 58/116 loss: 0.0588\n",
      "Epoch 74: Batch 59/116 loss: 0.0496\n",
      "Epoch 74: Batch 60/116 loss: 0.0639\n",
      "Epoch 74: Batch 61/116 loss: 0.0490\n",
      "Epoch 74: Batch 62/116 loss: 0.0560\n",
      "Epoch 74: Batch 63/116 loss: 0.0437\n",
      "Epoch 74: Batch 64/116 loss: 0.0572\n",
      "Epoch 74: Batch 65/116 loss: 0.0618\n",
      "Epoch 74: Batch 66/116 loss: 0.0535\n",
      "Epoch 74: Batch 67/116 loss: 0.0550\n",
      "Epoch 74: Batch 68/116 loss: 0.0716\n",
      "Epoch 74: Batch 69/116 loss: 0.0718\n",
      "Epoch 74: Batch 70/116 loss: 0.0572\n",
      "Epoch 74: Batch 71/116 loss: 0.0594\n",
      "Epoch 74: Batch 72/116 loss: 0.0644\n",
      "Epoch 74: Batch 73/116 loss: 0.0573\n",
      "Epoch 74: Batch 74/116 loss: 0.0504\n",
      "Epoch 74: Batch 75/116 loss: 0.0576\n",
      "Epoch 74: Batch 76/116 loss: 0.0522\n",
      "Epoch 74: Batch 77/116 loss: 0.0551\n",
      "Epoch 74: Batch 78/116 loss: 0.0691\n",
      "Epoch 74: Batch 79/116 loss: 0.0629\n",
      "Epoch 74: Batch 80/116 loss: 0.0873\n",
      "Epoch 74: Batch 81/116 loss: 0.0465\n",
      "Epoch 74: Batch 82/116 loss: 0.0538\n",
      "Epoch 74: Batch 83/116 loss: 0.0668\n",
      "Epoch 74: Batch 84/116 loss: 0.0501\n",
      "Epoch 74: Batch 85/116 loss: 0.0633\n",
      "Epoch 74: Batch 86/116 loss: 0.0669\n",
      "Epoch 74: Batch 87/116 loss: 0.0892\n",
      "Epoch 74: Batch 88/116 loss: 0.0435\n",
      "Epoch 74: Batch 89/116 loss: 0.0528\n",
      "Epoch 74: Batch 90/116 loss: 0.0421\n",
      "Epoch 74: Batch 91/116 loss: 0.0478\n",
      "Epoch 74: Batch 92/116 loss: 0.0651\n",
      "Epoch 74: Batch 93/116 loss: 0.0569\n",
      "Epoch 74: Batch 94/116 loss: 0.0470\n",
      "Epoch 74: Batch 95/116 loss: 0.0687\n",
      "Epoch 74: Batch 96/116 loss: 0.0464\n",
      "Epoch 74: Batch 97/116 loss: 0.0648\n",
      "Epoch 74: Batch 98/116 loss: 0.0810\n",
      "Epoch 74: Batch 99/116 loss: 0.0742\n",
      "Epoch 74: Batch 100/116 loss: 0.1193\n",
      "Epoch 74: Batch 101/116 loss: 0.0720\n",
      "Epoch 74: Batch 102/116 loss: 0.0773\n",
      "Epoch 74: Batch 103/116 loss: 0.0620\n",
      "Epoch 74: Batch 104/116 loss: 0.0637\n",
      "Epoch 74: Batch 105/116 loss: 0.0769\n",
      "Epoch 74: Batch 106/116 loss: 0.0674\n",
      "Epoch 74: Batch 107/116 loss: 0.0672\n",
      "Epoch 74: Batch 108/116 loss: 0.0499\n",
      "Epoch 74: Batch 109/116 loss: 0.0468\n",
      "Epoch 74: Batch 110/116 loss: 0.0368\n",
      "Epoch 74: Batch 111/116 loss: 0.0500\n",
      "Epoch 74: Batch 112/116 loss: 0.0521\n",
      "Epoch 74: Batch 113/116 loss: 0.0501\n",
      "Epoch 74: Batch 114/116 loss: 0.0586\n",
      "Epoch 74: Batch 115/116 loss: 0.0506\n",
      "Epoch 74: Batch 116/116 loss: 0.0308\n",
      "Epoch 74 train loss: 0.0592 valid loss: 0.0708\n",
      "performance reducing: 2\n",
      "Epoch 75: Batch 1/116 loss: 0.0611\n",
      "Epoch 75: Batch 2/116 loss: 0.0594\n",
      "Epoch 75: Batch 3/116 loss: 0.0732\n",
      "Epoch 75: Batch 4/116 loss: 0.0682\n",
      "Epoch 75: Batch 5/116 loss: 0.0632\n",
      "Epoch 75: Batch 6/116 loss: 0.0662\n",
      "Epoch 75: Batch 7/116 loss: 0.0520\n",
      "Epoch 75: Batch 8/116 loss: 0.0668\n",
      "Epoch 75: Batch 9/116 loss: 0.0463\n",
      "Epoch 75: Batch 10/116 loss: 0.0508\n",
      "Epoch 75: Batch 11/116 loss: 0.0618\n",
      "Epoch 75: Batch 12/116 loss: 0.0646\n",
      "Epoch 75: Batch 13/116 loss: 0.0745\n",
      "Epoch 75: Batch 14/116 loss: 0.0590\n",
      "Epoch 75: Batch 15/116 loss: 0.0470\n",
      "Epoch 75: Batch 16/116 loss: 0.1046\n",
      "Epoch 75: Batch 17/116 loss: 0.0498\n",
      "Epoch 75: Batch 18/116 loss: 0.0701\n",
      "Epoch 75: Batch 19/116 loss: 0.0885\n",
      "Epoch 75: Batch 20/116 loss: 0.0647\n",
      "Epoch 75: Batch 21/116 loss: 0.0620\n",
      "Epoch 75: Batch 22/116 loss: 0.0585\n",
      "Epoch 75: Batch 23/116 loss: 0.0553\n",
      "Epoch 75: Batch 24/116 loss: 0.0526\n",
      "Epoch 75: Batch 25/116 loss: 0.0593\n",
      "Epoch 75: Batch 26/116 loss: 0.0518\n",
      "Epoch 75: Batch 27/116 loss: 0.0570\n",
      "Epoch 75: Batch 28/116 loss: 0.0534\n",
      "Epoch 75: Batch 29/116 loss: 0.0612\n",
      "Epoch 75: Batch 30/116 loss: 0.0473\n",
      "Epoch 75: Batch 31/116 loss: 0.0598\n",
      "Epoch 75: Batch 32/116 loss: 0.0648\n",
      "Epoch 75: Batch 33/116 loss: 0.0582\n",
      "Epoch 75: Batch 34/116 loss: 0.0697\n",
      "Epoch 75: Batch 35/116 loss: 0.0568\n",
      "Epoch 75: Batch 36/116 loss: 0.0642\n",
      "Epoch 75: Batch 37/116 loss: 0.0601\n",
      "Epoch 75: Batch 38/116 loss: 0.0568\n",
      "Epoch 75: Batch 39/116 loss: 0.0565\n",
      "Epoch 75: Batch 40/116 loss: 0.0621\n",
      "Epoch 75: Batch 41/116 loss: 0.0474\n",
      "Epoch 75: Batch 42/116 loss: 0.0451\n",
      "Epoch 75: Batch 43/116 loss: 0.0360\n",
      "Epoch 75: Batch 44/116 loss: 0.0508\n",
      "Epoch 75: Batch 45/116 loss: 0.0415\n",
      "Epoch 75: Batch 46/116 loss: 0.0464\n",
      "Epoch 75: Batch 47/116 loss: 0.0695\n",
      "Epoch 75: Batch 48/116 loss: 0.0485\n",
      "Epoch 75: Batch 49/116 loss: 0.0712\n",
      "Epoch 75: Batch 50/116 loss: 0.0749\n",
      "Epoch 75: Batch 51/116 loss: 0.0593\n",
      "Epoch 75: Batch 52/116 loss: 0.0603\n",
      "Epoch 75: Batch 53/116 loss: 0.0522\n",
      "Epoch 75: Batch 54/116 loss: 0.0640\n",
      "Epoch 75: Batch 55/116 loss: 0.0491\n",
      "Epoch 75: Batch 56/116 loss: 0.1005\n",
      "Epoch 75: Batch 57/116 loss: 0.0499\n",
      "Epoch 75: Batch 58/116 loss: 0.0527\n",
      "Epoch 75: Batch 59/116 loss: 0.0433\n",
      "Epoch 75: Batch 60/116 loss: 0.0792\n",
      "Epoch 75: Batch 61/116 loss: 0.0425\n",
      "Epoch 75: Batch 62/116 loss: 0.0392\n",
      "Epoch 75: Batch 63/116 loss: 0.0695\n",
      "Epoch 75: Batch 64/116 loss: 0.0594\n",
      "Epoch 75: Batch 65/116 loss: 0.0736\n",
      "Epoch 75: Batch 66/116 loss: 0.0637\n",
      "Epoch 75: Batch 67/116 loss: 0.0546\n",
      "Epoch 75: Batch 68/116 loss: 0.0723\n",
      "Epoch 75: Batch 69/116 loss: 0.0430\n",
      "Epoch 75: Batch 70/116 loss: 0.1018\n",
      "Epoch 75: Batch 71/116 loss: 0.0446\n",
      "Epoch 75: Batch 72/116 loss: 0.0710\n",
      "Epoch 75: Batch 73/116 loss: 0.0675\n",
      "Epoch 75: Batch 74/116 loss: 0.0685\n",
      "Epoch 75: Batch 75/116 loss: 0.0621\n",
      "Epoch 75: Batch 76/116 loss: 0.0438\n",
      "Epoch 75: Batch 77/116 loss: 0.0630\n",
      "Epoch 75: Batch 78/116 loss: 0.0735\n",
      "Epoch 75: Batch 79/116 loss: 0.0625\n",
      "Epoch 75: Batch 80/116 loss: 0.0531\n",
      "Epoch 75: Batch 81/116 loss: 0.0604\n",
      "Epoch 75: Batch 82/116 loss: 0.0578\n",
      "Epoch 75: Batch 83/116 loss: 0.0472\n",
      "Epoch 75: Batch 84/116 loss: 0.0781\n",
      "Epoch 75: Batch 85/116 loss: 0.0496\n",
      "Epoch 75: Batch 86/116 loss: 0.0823\n",
      "Epoch 75: Batch 87/116 loss: 0.0490\n",
      "Epoch 75: Batch 88/116 loss: 0.0554\n",
      "Epoch 75: Batch 89/116 loss: 0.0624\n",
      "Epoch 75: Batch 90/116 loss: 0.0674\n",
      "Epoch 75: Batch 91/116 loss: 0.0606\n",
      "Epoch 75: Batch 92/116 loss: 0.0443\n",
      "Epoch 75: Batch 93/116 loss: 0.0411\n",
      "Epoch 75: Batch 94/116 loss: 0.0416\n",
      "Epoch 75: Batch 95/116 loss: 0.0662\n",
      "Epoch 75: Batch 96/116 loss: 0.0526\n",
      "Epoch 75: Batch 97/116 loss: 0.0689\n",
      "Epoch 75: Batch 98/116 loss: 0.0731\n",
      "Epoch 75: Batch 99/116 loss: 0.0570\n",
      "Epoch 75: Batch 100/116 loss: 0.0351\n",
      "Epoch 75: Batch 101/116 loss: 0.0444\n",
      "Epoch 75: Batch 102/116 loss: 0.0756\n",
      "Epoch 75: Batch 103/116 loss: 0.0475\n",
      "Epoch 75: Batch 104/116 loss: 0.0551\n",
      "Epoch 75: Batch 105/116 loss: 0.0473\n",
      "Epoch 75: Batch 106/116 loss: 0.0465\n",
      "Epoch 75: Batch 107/116 loss: 0.0645\n",
      "Epoch 75: Batch 108/116 loss: 0.0480\n",
      "Epoch 75: Batch 109/116 loss: 0.0606\n",
      "Epoch 75: Batch 110/116 loss: 0.0554\n",
      "Epoch 75: Batch 111/116 loss: 0.0530\n",
      "Epoch 75: Batch 112/116 loss: 0.0410\n",
      "Epoch 75: Batch 113/116 loss: 0.0507\n",
      "Epoch 75: Batch 114/116 loss: 0.0507\n",
      "Epoch 75: Batch 115/116 loss: 0.0509\n",
      "Epoch 75: Batch 116/116 loss: 0.0410\n",
      "Epoch 75 train loss: 0.0587 valid loss: 0.0590\n",
      "Epoch 76: Batch 1/116 loss: 0.0510\n",
      "Epoch 76: Batch 2/116 loss: 0.0531\n",
      "Epoch 76: Batch 3/116 loss: 0.0581\n",
      "Epoch 76: Batch 4/116 loss: 0.0525\n",
      "Epoch 76: Batch 5/116 loss: 0.0769\n",
      "Epoch 76: Batch 6/116 loss: 0.0544\n",
      "Epoch 76: Batch 7/116 loss: 0.0392\n",
      "Epoch 76: Batch 8/116 loss: 0.0655\n",
      "Epoch 76: Batch 9/116 loss: 0.0739\n",
      "Epoch 76: Batch 10/116 loss: 0.0777\n",
      "Epoch 76: Batch 11/116 loss: 0.0584\n",
      "Epoch 76: Batch 12/116 loss: 0.0502\n",
      "Epoch 76: Batch 13/116 loss: 0.0629\n",
      "Epoch 76: Batch 14/116 loss: 0.0448\n",
      "Epoch 76: Batch 15/116 loss: 0.0551\n",
      "Epoch 76: Batch 16/116 loss: 0.0484\n",
      "Epoch 76: Batch 17/116 loss: 0.0594\n",
      "Epoch 76: Batch 18/116 loss: 0.0962\n",
      "Epoch 76: Batch 19/116 loss: 0.0662\n",
      "Epoch 76: Batch 20/116 loss: 0.0664\n",
      "Epoch 76: Batch 21/116 loss: 0.0835\n",
      "Epoch 76: Batch 22/116 loss: 0.0560\n",
      "Epoch 76: Batch 23/116 loss: 0.0656\n",
      "Epoch 76: Batch 24/116 loss: 0.0369\n",
      "Epoch 76: Batch 25/116 loss: 0.0519\n",
      "Epoch 76: Batch 26/116 loss: 0.0650\n",
      "Epoch 76: Batch 27/116 loss: 0.0455\n",
      "Epoch 76: Batch 28/116 loss: 0.0571\n",
      "Epoch 76: Batch 29/116 loss: 0.0499\n",
      "Epoch 76: Batch 30/116 loss: 0.0542\n",
      "Epoch 76: Batch 31/116 loss: 0.0768\n",
      "Epoch 76: Batch 32/116 loss: 0.0551\n",
      "Epoch 76: Batch 33/116 loss: 0.0636\n",
      "Epoch 76: Batch 34/116 loss: 0.0682\n",
      "Epoch 76: Batch 35/116 loss: 0.0507\n",
      "Epoch 76: Batch 36/116 loss: 0.0686\n",
      "Epoch 76: Batch 37/116 loss: 0.0687\n",
      "Epoch 76: Batch 38/116 loss: 0.0568\n",
      "Epoch 76: Batch 39/116 loss: 0.0543\n",
      "Epoch 76: Batch 40/116 loss: 0.0554\n",
      "Epoch 76: Batch 41/116 loss: 0.0529\n",
      "Epoch 76: Batch 42/116 loss: 0.0603\n",
      "Epoch 76: Batch 43/116 loss: 0.0557\n",
      "Epoch 76: Batch 44/116 loss: 0.0546\n",
      "Epoch 76: Batch 45/116 loss: 0.0531\n",
      "Epoch 76: Batch 46/116 loss: 0.0923\n",
      "Epoch 76: Batch 47/116 loss: 0.0650\n",
      "Epoch 76: Batch 48/116 loss: 0.0604\n",
      "Epoch 76: Batch 49/116 loss: 0.1194\n",
      "Epoch 76: Batch 50/116 loss: 0.0760\n",
      "Epoch 76: Batch 51/116 loss: 0.0694\n",
      "Epoch 76: Batch 52/116 loss: 0.0547\n",
      "Epoch 76: Batch 53/116 loss: 0.0544\n",
      "Epoch 76: Batch 54/116 loss: 0.0720\n",
      "Epoch 76: Batch 55/116 loss: 0.0631\n",
      "Epoch 76: Batch 56/116 loss: 0.0559\n",
      "Epoch 76: Batch 57/116 loss: 0.0351\n",
      "Epoch 76: Batch 58/116 loss: 0.0640\n",
      "Epoch 76: Batch 59/116 loss: 0.0694\n",
      "Epoch 76: Batch 60/116 loss: 0.0686\n",
      "Epoch 76: Batch 61/116 loss: 0.0383\n",
      "Epoch 76: Batch 62/116 loss: 0.0612\n",
      "Epoch 76: Batch 63/116 loss: 0.0759\n",
      "Epoch 76: Batch 64/116 loss: 0.0621\n",
      "Epoch 76: Batch 65/116 loss: 0.0465\n",
      "Epoch 76: Batch 66/116 loss: 0.0623\n",
      "Epoch 76: Batch 67/116 loss: 0.0728\n",
      "Epoch 76: Batch 68/116 loss: 0.0488\n",
      "Epoch 76: Batch 69/116 loss: 0.0409\n",
      "Epoch 76: Batch 70/116 loss: 0.0482\n",
      "Epoch 76: Batch 71/116 loss: 0.0479\n",
      "Epoch 76: Batch 72/116 loss: 0.0532\n",
      "Epoch 76: Batch 73/116 loss: 0.0554\n",
      "Epoch 76: Batch 74/116 loss: 0.0521\n",
      "Epoch 76: Batch 75/116 loss: 0.0560\n",
      "Epoch 76: Batch 76/116 loss: 0.0521\n",
      "Epoch 76: Batch 77/116 loss: 0.0452\n",
      "Epoch 76: Batch 78/116 loss: 0.0295\n",
      "Epoch 76: Batch 79/116 loss: 0.0711\n",
      "Epoch 76: Batch 80/116 loss: 0.0417\n",
      "Epoch 76: Batch 81/116 loss: 0.0441\n",
      "Epoch 76: Batch 82/116 loss: 0.0651\n",
      "Epoch 76: Batch 83/116 loss: 0.0459\n",
      "Epoch 76: Batch 84/116 loss: 0.0615\n",
      "Epoch 76: Batch 85/116 loss: 0.0682\n",
      "Epoch 76: Batch 86/116 loss: 0.0641\n",
      "Epoch 76: Batch 87/116 loss: 0.0573\n",
      "Epoch 76: Batch 88/116 loss: 0.0586\n",
      "Epoch 76: Batch 89/116 loss: 0.0571\n",
      "Epoch 76: Batch 90/116 loss: 0.0398\n",
      "Epoch 76: Batch 91/116 loss: 0.0394\n",
      "Epoch 76: Batch 92/116 loss: 0.0494\n",
      "Epoch 76: Batch 93/116 loss: 0.0412\n",
      "Epoch 76: Batch 94/116 loss: 0.0658\n",
      "Epoch 76: Batch 95/116 loss: 0.0621\n",
      "Epoch 76: Batch 96/116 loss: 0.0523\n",
      "Epoch 76: Batch 97/116 loss: 0.0516\n",
      "Epoch 76: Batch 98/116 loss: 0.0538\n",
      "Epoch 76: Batch 99/116 loss: 0.0676\n",
      "Epoch 76: Batch 100/116 loss: 0.0657\n",
      "Epoch 76: Batch 101/116 loss: 0.0412\n",
      "Epoch 76: Batch 102/116 loss: 0.0706\n",
      "Epoch 76: Batch 103/116 loss: 0.0847\n",
      "Epoch 76: Batch 104/116 loss: 0.0445\n",
      "Epoch 76: Batch 105/116 loss: 0.0678\n",
      "Epoch 76: Batch 106/116 loss: 0.0641\n",
      "Epoch 76: Batch 107/116 loss: 0.0654\n",
      "Epoch 76: Batch 108/116 loss: 0.0654\n",
      "Epoch 76: Batch 109/116 loss: 0.0671\n",
      "Epoch 76: Batch 110/116 loss: 0.0553\n",
      "Epoch 76: Batch 111/116 loss: 0.0496\n",
      "Epoch 76: Batch 112/116 loss: 0.0713\n",
      "Epoch 76: Batch 113/116 loss: 0.0472\n",
      "Epoch 76: Batch 114/116 loss: 0.0379\n",
      "Epoch 76: Batch 115/116 loss: 0.0822\n",
      "Epoch 76: Batch 116/116 loss: 0.0603\n",
      "Epoch 76 train loss: 0.0589 valid loss: 0.0638\n",
      "performance reducing: 1\n",
      "Epoch 77: Batch 1/116 loss: 0.0610\n",
      "Epoch 77: Batch 2/116 loss: 0.0425\n",
      "Epoch 77: Batch 3/116 loss: 0.0579\n",
      "Epoch 77: Batch 4/116 loss: 0.0693\n",
      "Epoch 77: Batch 5/116 loss: 0.0389\n",
      "Epoch 77: Batch 6/116 loss: 0.0380\n",
      "Epoch 77: Batch 7/116 loss: 0.0488\n",
      "Epoch 77: Batch 8/116 loss: 0.0639\n",
      "Epoch 77: Batch 9/116 loss: 0.0494\n",
      "Epoch 77: Batch 10/116 loss: 0.0803\n",
      "Epoch 77: Batch 11/116 loss: 0.0573\n",
      "Epoch 77: Batch 12/116 loss: 0.0708\n",
      "Epoch 77: Batch 13/116 loss: 0.0598\n",
      "Epoch 77: Batch 14/116 loss: 0.0401\n",
      "Epoch 77: Batch 15/116 loss: 0.0692\n",
      "Epoch 77: Batch 16/116 loss: 0.0808\n",
      "Epoch 77: Batch 17/116 loss: 0.0995\n",
      "Epoch 77: Batch 18/116 loss: 0.0735\n",
      "Epoch 77: Batch 19/116 loss: 0.0510\n",
      "Epoch 77: Batch 20/116 loss: 0.0621\n",
      "Epoch 77: Batch 21/116 loss: 0.0505\n",
      "Epoch 77: Batch 22/116 loss: 0.1703\n",
      "Epoch 77: Batch 23/116 loss: 0.0593\n",
      "Epoch 77: Batch 24/116 loss: 0.0957\n",
      "Epoch 77: Batch 25/116 loss: 0.0789\n",
      "Epoch 77: Batch 26/116 loss: 0.0694\n",
      "Epoch 77: Batch 27/116 loss: 0.0624\n",
      "Epoch 77: Batch 28/116 loss: 0.0664\n",
      "Epoch 77: Batch 29/116 loss: 0.0641\n",
      "Epoch 77: Batch 30/116 loss: 0.0423\n",
      "Epoch 77: Batch 31/116 loss: 0.0424\n",
      "Epoch 77: Batch 32/116 loss: 0.0780\n",
      "Epoch 77: Batch 33/116 loss: 0.0709\n",
      "Epoch 77: Batch 34/116 loss: 0.0609\n",
      "Epoch 77: Batch 35/116 loss: 0.0799\n",
      "Epoch 77: Batch 36/116 loss: 0.0499\n",
      "Epoch 77: Batch 37/116 loss: 0.0653\n",
      "Epoch 77: Batch 38/116 loss: 0.0557\n",
      "Epoch 77: Batch 39/116 loss: 0.0722\n",
      "Epoch 77: Batch 40/116 loss: 0.0665\n",
      "Epoch 77: Batch 41/116 loss: 0.0381\n",
      "Epoch 77: Batch 42/116 loss: 0.0761\n",
      "Epoch 77: Batch 43/116 loss: 0.0463\n",
      "Epoch 77: Batch 44/116 loss: 0.0717\n",
      "Epoch 77: Batch 45/116 loss: 0.0591\n",
      "Epoch 77: Batch 46/116 loss: 0.0630\n",
      "Epoch 77: Batch 47/116 loss: 0.0567\n",
      "Epoch 77: Batch 48/116 loss: 0.0472\n",
      "Epoch 77: Batch 49/116 loss: 0.0640\n",
      "Epoch 77: Batch 50/116 loss: 0.0535\n",
      "Epoch 77: Batch 51/116 loss: 0.0461\n",
      "Epoch 77: Batch 52/116 loss: 0.0724\n",
      "Epoch 77: Batch 53/116 loss: 0.0770\n",
      "Epoch 77: Batch 54/116 loss: 0.0484\n",
      "Epoch 77: Batch 55/116 loss: 0.0563\n",
      "Epoch 77: Batch 56/116 loss: 0.0829\n",
      "Epoch 77: Batch 57/116 loss: 0.0479\n",
      "Epoch 77: Batch 58/116 loss: 0.0492\n",
      "Epoch 77: Batch 59/116 loss: 0.0457\n",
      "Epoch 77: Batch 60/116 loss: 0.0766\n",
      "Epoch 77: Batch 61/116 loss: 0.0611\n",
      "Epoch 77: Batch 62/116 loss: 0.0540\n",
      "Epoch 77: Batch 63/116 loss: 0.0578\n",
      "Epoch 77: Batch 64/116 loss: 0.0439\n",
      "Epoch 77: Batch 65/116 loss: 0.0571\n",
      "Epoch 77: Batch 66/116 loss: 0.0583\n",
      "Epoch 77: Batch 67/116 loss: 0.0747\n",
      "Epoch 77: Batch 68/116 loss: 0.0565\n",
      "Epoch 77: Batch 69/116 loss: 0.0491\n",
      "Epoch 77: Batch 70/116 loss: 0.0518\n",
      "Epoch 77: Batch 71/116 loss: 0.0569\n",
      "Epoch 77: Batch 72/116 loss: 0.0532\n",
      "Epoch 77: Batch 73/116 loss: 0.0714\n",
      "Epoch 77: Batch 74/116 loss: 0.0597\n",
      "Epoch 77: Batch 75/116 loss: 0.0709\n",
      "Epoch 77: Batch 76/116 loss: 0.0647\n",
      "Epoch 77: Batch 77/116 loss: 0.0407\n",
      "Epoch 77: Batch 78/116 loss: 0.0526\n",
      "Epoch 77: Batch 79/116 loss: 0.0348\n",
      "Epoch 77: Batch 80/116 loss: 0.0569\n",
      "Epoch 77: Batch 81/116 loss: 0.0455\n",
      "Epoch 77: Batch 82/116 loss: 0.0546\n",
      "Epoch 77: Batch 83/116 loss: 0.0591\n",
      "Epoch 77: Batch 84/116 loss: 0.0669\n",
      "Epoch 77: Batch 85/116 loss: 0.0590\n",
      "Epoch 77: Batch 86/116 loss: 0.0429\n",
      "Epoch 77: Batch 87/116 loss: 0.0428\n",
      "Epoch 77: Batch 88/116 loss: 0.0639\n",
      "Epoch 77: Batch 89/116 loss: 0.0539\n",
      "Epoch 77: Batch 90/116 loss: 0.0491\n",
      "Epoch 77: Batch 91/116 loss: 0.0583\n",
      "Epoch 77: Batch 92/116 loss: 0.0400\n",
      "Epoch 77: Batch 93/116 loss: 0.0701\n",
      "Epoch 77: Batch 94/116 loss: 0.0540\n",
      "Epoch 77: Batch 95/116 loss: 0.0539\n",
      "Epoch 77: Batch 96/116 loss: 0.0576\n",
      "Epoch 77: Batch 97/116 loss: 0.0580\n",
      "Epoch 77: Batch 98/116 loss: 0.0377\n",
      "Epoch 77: Batch 99/116 loss: 0.0560\n",
      "Epoch 77: Batch 100/116 loss: 0.0624\n",
      "Epoch 77: Batch 101/116 loss: 0.0466\n",
      "Epoch 77: Batch 102/116 loss: 0.0564\n",
      "Epoch 77: Batch 103/116 loss: 0.0642\n",
      "Epoch 77: Batch 104/116 loss: 0.0668\n",
      "Epoch 77: Batch 105/116 loss: 0.0393\n",
      "Epoch 77: Batch 106/116 loss: 0.0695\n",
      "Epoch 77: Batch 107/116 loss: 0.1205\n",
      "Epoch 77: Batch 108/116 loss: 0.0699\n",
      "Epoch 77: Batch 109/116 loss: 0.0587\n",
      "Epoch 77: Batch 110/116 loss: 0.0549\n",
      "Epoch 77: Batch 111/116 loss: 0.0615\n",
      "Epoch 77: Batch 112/116 loss: 0.0660\n",
      "Epoch 77: Batch 113/116 loss: 0.0561\n",
      "Epoch 77: Batch 114/116 loss: 0.0733\n",
      "Epoch 77: Batch 115/116 loss: 0.0654\n",
      "Epoch 77: Batch 116/116 loss: 0.0656\n",
      "Epoch 77 train loss: 0.0607 valid loss: 0.0735\n",
      "performance reducing: 2\n",
      "Epoch 78: Batch 1/116 loss: 0.0677\n",
      "Epoch 78: Batch 2/116 loss: 0.0662\n",
      "Epoch 78: Batch 3/116 loss: 0.0652\n",
      "Epoch 78: Batch 4/116 loss: 0.0547\n",
      "Epoch 78: Batch 5/116 loss: 0.0722\n",
      "Epoch 78: Batch 6/116 loss: 0.0566\n",
      "Epoch 78: Batch 7/116 loss: 0.0588\n",
      "Epoch 78: Batch 8/116 loss: 0.0614\n",
      "Epoch 78: Batch 9/116 loss: 0.0568\n",
      "Epoch 78: Batch 10/116 loss: 0.0467\n",
      "Epoch 78: Batch 11/116 loss: 0.0378\n",
      "Epoch 78: Batch 12/116 loss: 0.0468\n",
      "Epoch 78: Batch 13/116 loss: 0.0491\n",
      "Epoch 78: Batch 14/116 loss: 0.0537\n",
      "Epoch 78: Batch 15/116 loss: 0.0701\n",
      "Epoch 78: Batch 16/116 loss: 0.0418\n",
      "Epoch 78: Batch 17/116 loss: 0.0695\n",
      "Epoch 78: Batch 18/116 loss: 0.0532\n",
      "Epoch 78: Batch 19/116 loss: 0.0618\n",
      "Epoch 78: Batch 20/116 loss: 0.0908\n",
      "Epoch 78: Batch 21/116 loss: 0.0797\n",
      "Epoch 78: Batch 22/116 loss: 0.0789\n",
      "Epoch 78: Batch 23/116 loss: 0.0613\n",
      "Epoch 78: Batch 24/116 loss: 0.0532\n",
      "Epoch 78: Batch 25/116 loss: 0.0581\n",
      "Epoch 78: Batch 26/116 loss: 0.0415\n",
      "Epoch 78: Batch 27/116 loss: 0.0725\n",
      "Epoch 78: Batch 28/116 loss: 0.0391\n",
      "Epoch 78: Batch 29/116 loss: 0.0522\n",
      "Epoch 78: Batch 30/116 loss: 0.0534\n",
      "Epoch 78: Batch 31/116 loss: 0.0465\n",
      "Epoch 78: Batch 32/116 loss: 0.0417\n",
      "Epoch 78: Batch 33/116 loss: 0.0630\n",
      "Epoch 78: Batch 34/116 loss: 0.0745\n",
      "Epoch 78: Batch 35/116 loss: 0.0539\n",
      "Epoch 78: Batch 36/116 loss: 0.0920\n",
      "Epoch 78: Batch 37/116 loss: 0.0604\n",
      "Epoch 78: Batch 38/116 loss: 0.0420\n",
      "Epoch 78: Batch 39/116 loss: 0.0567\n",
      "Epoch 78: Batch 40/116 loss: 0.0451\n",
      "Epoch 78: Batch 41/116 loss: 0.0495\n",
      "Epoch 78: Batch 42/116 loss: 0.0464\n",
      "Epoch 78: Batch 43/116 loss: 0.0444\n",
      "Epoch 78: Batch 44/116 loss: 0.0402\n",
      "Epoch 78: Batch 45/116 loss: 0.0506\n",
      "Epoch 78: Batch 46/116 loss: 0.0484\n",
      "Epoch 78: Batch 47/116 loss: 0.0716\n",
      "Epoch 78: Batch 48/116 loss: 0.0488\n",
      "Epoch 78: Batch 49/116 loss: 0.0505\n",
      "Epoch 78: Batch 50/116 loss: 0.0557\n",
      "Epoch 78: Batch 51/116 loss: 0.0561\n",
      "Epoch 78: Batch 52/116 loss: 0.0445\n",
      "Epoch 78: Batch 53/116 loss: 0.0629\n",
      "Epoch 78: Batch 54/116 loss: 0.0522\n",
      "Epoch 78: Batch 55/116 loss: 0.0451\n",
      "Epoch 78: Batch 56/116 loss: 0.0694\n",
      "Epoch 78: Batch 57/116 loss: 0.0554\n",
      "Epoch 78: Batch 58/116 loss: 0.0422\n",
      "Epoch 78: Batch 59/116 loss: 0.0611\n",
      "Epoch 78: Batch 60/116 loss: 0.0495\n",
      "Epoch 78: Batch 61/116 loss: 0.0445\n",
      "Epoch 78: Batch 62/116 loss: 0.0821\n",
      "Epoch 78: Batch 63/116 loss: 0.0523\n",
      "Epoch 78: Batch 64/116 loss: 0.0488\n",
      "Epoch 78: Batch 65/116 loss: 0.0534\n",
      "Epoch 78: Batch 66/116 loss: 0.0577\n",
      "Epoch 78: Batch 67/116 loss: 0.0528\n",
      "Epoch 78: Batch 68/116 loss: 0.0647\n",
      "Epoch 78: Batch 69/116 loss: 0.0480\n",
      "Epoch 78: Batch 70/116 loss: 0.0548\n",
      "Epoch 78: Batch 71/116 loss: 0.1058\n",
      "Epoch 78: Batch 72/116 loss: 0.0606\n",
      "Epoch 78: Batch 73/116 loss: 0.0692\n",
      "Epoch 78: Batch 74/116 loss: 0.0597\n",
      "Epoch 78: Batch 75/116 loss: 0.0562\n",
      "Epoch 78: Batch 76/116 loss: 0.0542\n",
      "Epoch 78: Batch 77/116 loss: 0.1057\n",
      "Epoch 78: Batch 78/116 loss: 0.0667\n",
      "Epoch 78: Batch 79/116 loss: 0.0635\n",
      "Epoch 78: Batch 80/116 loss: 0.0493\n",
      "Epoch 78: Batch 81/116 loss: 0.0499\n",
      "Epoch 78: Batch 82/116 loss: 0.0567\n",
      "Epoch 78: Batch 83/116 loss: 0.0565\n",
      "Epoch 78: Batch 84/116 loss: 0.0578\n",
      "Epoch 78: Batch 85/116 loss: 0.0639\n",
      "Epoch 78: Batch 86/116 loss: 0.0990\n",
      "Epoch 78: Batch 87/116 loss: 0.0616\n",
      "Epoch 78: Batch 88/116 loss: 0.0473\n",
      "Epoch 78: Batch 89/116 loss: 0.0563\n",
      "Epoch 78: Batch 90/116 loss: 0.0475\n",
      "Epoch 78: Batch 91/116 loss: 0.0594\n",
      "Epoch 78: Batch 92/116 loss: 0.0462\n",
      "Epoch 78: Batch 93/116 loss: 0.0710\n",
      "Epoch 78: Batch 94/116 loss: 0.0797\n",
      "Epoch 78: Batch 95/116 loss: 0.0405\n",
      "Epoch 78: Batch 96/116 loss: 0.0643\n",
      "Epoch 78: Batch 97/116 loss: 0.0419\n",
      "Epoch 78: Batch 98/116 loss: 0.0426\n",
      "Epoch 78: Batch 99/116 loss: 0.0700\n",
      "Epoch 78: Batch 100/116 loss: 0.0556\n",
      "Epoch 78: Batch 101/116 loss: 0.0576\n",
      "Epoch 78: Batch 102/116 loss: 0.0894\n",
      "Epoch 78: Batch 103/116 loss: 0.0521\n",
      "Epoch 78: Batch 104/116 loss: 0.0425\n",
      "Epoch 78: Batch 105/116 loss: 0.0852\n",
      "Epoch 78: Batch 106/116 loss: 0.0602\n",
      "Epoch 78: Batch 107/116 loss: 0.0467\n",
      "Epoch 78: Batch 108/116 loss: 0.0539\n",
      "Epoch 78: Batch 109/116 loss: 0.0597\n",
      "Epoch 78: Batch 110/116 loss: 0.0438\n",
      "Epoch 78: Batch 111/116 loss: 0.0886\n",
      "Epoch 78: Batch 112/116 loss: 0.0653\n",
      "Epoch 78: Batch 113/116 loss: 0.0622\n",
      "Epoch 78: Batch 114/116 loss: 0.0650\n",
      "Epoch 78: Batch 115/116 loss: 0.0634\n",
      "Epoch 78: Batch 116/116 loss: 0.0760\n",
      "Epoch 78 train loss: 0.0588 valid loss: 0.0907\n",
      "performance reducing: 3\n",
      "Epoch 79: Batch 1/116 loss: 0.0460\n",
      "Epoch 79: Batch 2/116 loss: 0.0565\n",
      "Epoch 79: Batch 3/116 loss: 0.0399\n",
      "Epoch 79: Batch 4/116 loss: 0.0718\n",
      "Epoch 79: Batch 5/116 loss: 0.0393\n",
      "Epoch 79: Batch 6/116 loss: 0.0637\n",
      "Epoch 79: Batch 7/116 loss: 0.0571\n",
      "Epoch 79: Batch 8/116 loss: 0.0402\n",
      "Epoch 79: Batch 9/116 loss: 0.0539\n",
      "Epoch 79: Batch 10/116 loss: 0.0771\n",
      "Epoch 79: Batch 11/116 loss: 0.0728\n",
      "Epoch 79: Batch 12/116 loss: 0.0518\n",
      "Epoch 79: Batch 13/116 loss: 0.0519\n",
      "Epoch 79: Batch 14/116 loss: 0.0609\n",
      "Epoch 79: Batch 15/116 loss: 0.0696\n",
      "Epoch 79: Batch 16/116 loss: 0.0587\n",
      "Epoch 79: Batch 17/116 loss: 0.0463\n",
      "Epoch 79: Batch 18/116 loss: 0.0713\n",
      "Epoch 79: Batch 19/116 loss: 0.0562\n",
      "Epoch 79: Batch 20/116 loss: 0.0565\n",
      "Epoch 79: Batch 21/116 loss: 0.0627\n",
      "Epoch 79: Batch 22/116 loss: 0.0531\n",
      "Epoch 79: Batch 23/116 loss: 0.0568\n",
      "Epoch 79: Batch 24/116 loss: 0.0434\n",
      "Epoch 79: Batch 25/116 loss: 0.0942\n",
      "Epoch 79: Batch 26/116 loss: 0.0416\n",
      "Epoch 79: Batch 27/116 loss: 0.0708\n",
      "Epoch 79: Batch 28/116 loss: 0.0575\n",
      "Epoch 79: Batch 29/116 loss: 0.0601\n",
      "Epoch 79: Batch 30/116 loss: 0.0711\n",
      "Epoch 79: Batch 31/116 loss: 0.0621\n",
      "Epoch 79: Batch 32/116 loss: 0.0501\n",
      "Epoch 79: Batch 33/116 loss: 0.0496\n",
      "Epoch 79: Batch 34/116 loss: 0.0484\n",
      "Epoch 79: Batch 35/116 loss: 0.0410\n",
      "Epoch 79: Batch 36/116 loss: 0.0566\n",
      "Epoch 79: Batch 37/116 loss: 0.0619\n",
      "Epoch 79: Batch 38/116 loss: 0.0529\n",
      "Epoch 79: Batch 39/116 loss: 0.0719\n",
      "Epoch 79: Batch 40/116 loss: 0.0808\n",
      "Epoch 79: Batch 41/116 loss: 0.0436\n",
      "Epoch 79: Batch 42/116 loss: 0.0705\n",
      "Epoch 79: Batch 43/116 loss: 0.0364\n",
      "Epoch 79: Batch 44/116 loss: 0.0555\n",
      "Epoch 79: Batch 45/116 loss: 0.0675\n",
      "Epoch 79: Batch 46/116 loss: 0.0586\n",
      "Epoch 79: Batch 47/116 loss: 0.0654\n",
      "Epoch 79: Batch 48/116 loss: 0.0467\n",
      "Epoch 79: Batch 49/116 loss: 0.0548\n",
      "Epoch 79: Batch 50/116 loss: 0.0456\n",
      "Epoch 79: Batch 51/116 loss: 0.0620\n",
      "Epoch 79: Batch 52/116 loss: 0.0626\n",
      "Epoch 79: Batch 53/116 loss: 0.0489\n",
      "Epoch 79: Batch 54/116 loss: 0.0492\n",
      "Epoch 79: Batch 55/116 loss: 0.0536\n",
      "Epoch 79: Batch 56/116 loss: 0.0565\n",
      "Epoch 79: Batch 57/116 loss: 0.0485\n",
      "Epoch 79: Batch 58/116 loss: 0.0534\n",
      "Epoch 79: Batch 59/116 loss: 0.0765\n",
      "Epoch 79: Batch 60/116 loss: 0.0870\n",
      "Epoch 79: Batch 61/116 loss: 0.0576\n",
      "Epoch 79: Batch 62/116 loss: 0.0476\n",
      "Epoch 79: Batch 63/116 loss: 0.0651\n",
      "Epoch 79: Batch 64/116 loss: 0.0911\n",
      "Epoch 79: Batch 65/116 loss: 0.0535\n",
      "Epoch 79: Batch 66/116 loss: 0.0468\n",
      "Epoch 79: Batch 67/116 loss: 0.0406\n",
      "Epoch 79: Batch 68/116 loss: 0.0715\n",
      "Epoch 79: Batch 69/116 loss: 0.0642\n",
      "Epoch 79: Batch 70/116 loss: 0.0578\n",
      "Epoch 79: Batch 71/116 loss: 0.0657\n",
      "Epoch 79: Batch 72/116 loss: 0.0604\n",
      "Epoch 79: Batch 73/116 loss: 0.0488\n",
      "Epoch 79: Batch 74/116 loss: 0.0634\n",
      "Epoch 79: Batch 75/116 loss: 0.1039\n",
      "Epoch 79: Batch 76/116 loss: 0.0611\n",
      "Epoch 79: Batch 77/116 loss: 0.0591\n",
      "Epoch 79: Batch 78/116 loss: 0.0731\n",
      "Epoch 79: Batch 79/116 loss: 0.0446\n",
      "Epoch 79: Batch 80/116 loss: 0.0708\n",
      "Epoch 79: Batch 81/116 loss: 0.0675\n",
      "Epoch 79: Batch 82/116 loss: 0.0418\n",
      "Epoch 79: Batch 83/116 loss: 0.0664\n",
      "Epoch 79: Batch 84/116 loss: 0.0441\n",
      "Epoch 79: Batch 85/116 loss: 0.0871\n",
      "Epoch 79: Batch 86/116 loss: 0.0965\n",
      "Epoch 79: Batch 87/116 loss: 0.0628\n",
      "Epoch 79: Batch 88/116 loss: 0.0791\n",
      "Epoch 79: Batch 89/116 loss: 0.0502\n",
      "Epoch 79: Batch 90/116 loss: 0.0568\n",
      "Epoch 79: Batch 91/116 loss: 0.0607\n",
      "Epoch 79: Batch 92/116 loss: 0.0583\n",
      "Epoch 79: Batch 93/116 loss: 0.0655\n",
      "Epoch 79: Batch 94/116 loss: 0.0790\n",
      "Epoch 79: Batch 95/116 loss: 0.0803\n",
      "Epoch 79: Batch 96/116 loss: 0.0562\n",
      "Epoch 79: Batch 97/116 loss: 0.0616\n",
      "Epoch 79: Batch 98/116 loss: 0.0735\n",
      "Epoch 79: Batch 99/116 loss: 0.0598\n",
      "Epoch 79: Batch 100/116 loss: 0.0513\n",
      "Epoch 79: Batch 101/116 loss: 0.0546\n",
      "Epoch 79: Batch 102/116 loss: 0.0489\n",
      "Epoch 79: Batch 103/116 loss: 0.0357\n",
      "Epoch 79: Batch 104/116 loss: 0.0586\n",
      "Epoch 79: Batch 105/116 loss: 0.0448\n",
      "Epoch 79: Batch 106/116 loss: 0.0678\n",
      "Epoch 79: Batch 107/116 loss: 0.0306\n",
      "Epoch 79: Batch 108/116 loss: 0.0514\n",
      "Epoch 79: Batch 109/116 loss: 0.0445\n",
      "Epoch 79: Batch 110/116 loss: 0.0786\n",
      "Epoch 79: Batch 111/116 loss: 0.0574\n",
      "Epoch 79: Batch 112/116 loss: 0.0628\n",
      "Epoch 79: Batch 113/116 loss: 0.0350\n",
      "Epoch 79: Batch 114/116 loss: 0.0437\n",
      "Epoch 79: Batch 115/116 loss: 0.0416\n",
      "Epoch 79: Batch 116/116 loss: 0.0479\n",
      "Epoch 79 train loss: 0.0588 valid loss: 0.0654\n",
      "performance reducing: 4\n",
      "Epoch 80: Batch 1/116 loss: 0.0430\n",
      "Epoch 80: Batch 2/116 loss: 0.0416\n",
      "Epoch 80: Batch 3/116 loss: 0.0481\n",
      "Epoch 80: Batch 4/116 loss: 0.0855\n",
      "Epoch 80: Batch 5/116 loss: 0.0350\n",
      "Epoch 80: Batch 6/116 loss: 0.0541\n",
      "Epoch 80: Batch 7/116 loss: 0.0479\n",
      "Epoch 80: Batch 8/116 loss: 0.0565\n",
      "Epoch 80: Batch 9/116 loss: 0.0721\n",
      "Epoch 80: Batch 10/116 loss: 0.0550\n",
      "Epoch 80: Batch 11/116 loss: 0.0428\n",
      "Epoch 80: Batch 12/116 loss: 0.0633\n",
      "Epoch 80: Batch 13/116 loss: 0.0825\n",
      "Epoch 80: Batch 14/116 loss: 0.0489\n",
      "Epoch 80: Batch 15/116 loss: 0.0735\n",
      "Epoch 80: Batch 16/116 loss: 0.0529\n",
      "Epoch 80: Batch 17/116 loss: 0.0471\n",
      "Epoch 80: Batch 18/116 loss: 0.0650\n",
      "Epoch 80: Batch 19/116 loss: 0.0345\n",
      "Epoch 80: Batch 20/116 loss: 0.0644\n",
      "Epoch 80: Batch 21/116 loss: 0.0417\n",
      "Epoch 80: Batch 22/116 loss: 0.0635\n",
      "Epoch 80: Batch 23/116 loss: 0.0576\n",
      "Epoch 80: Batch 24/116 loss: 0.0466\n",
      "Epoch 80: Batch 25/116 loss: 0.0863\n",
      "Epoch 80: Batch 26/116 loss: 0.0559\n",
      "Epoch 80: Batch 27/116 loss: 0.0628\n",
      "Epoch 80: Batch 28/116 loss: 0.0554\n",
      "Epoch 80: Batch 29/116 loss: 0.0549\n",
      "Epoch 80: Batch 30/116 loss: 0.0541\n",
      "Epoch 80: Batch 31/116 loss: 0.0568\n",
      "Epoch 80: Batch 32/116 loss: 0.0589\n",
      "Epoch 80: Batch 33/116 loss: 0.0575\n",
      "Epoch 80: Batch 34/116 loss: 0.0527\n",
      "Epoch 80: Batch 35/116 loss: 0.0526\n",
      "Epoch 80: Batch 36/116 loss: 0.0670\n",
      "Epoch 80: Batch 37/116 loss: 0.0524\n",
      "Epoch 80: Batch 38/116 loss: 0.1042\n",
      "Epoch 80: Batch 39/116 loss: 0.0566\n",
      "Epoch 80: Batch 40/116 loss: 0.0339\n",
      "Epoch 80: Batch 41/116 loss: 0.0459\n",
      "Epoch 80: Batch 42/116 loss: 0.1000\n",
      "Epoch 80: Batch 43/116 loss: 0.0556\n",
      "Epoch 80: Batch 44/116 loss: 0.0562\n",
      "Epoch 80: Batch 45/116 loss: 0.0359\n",
      "Epoch 80: Batch 46/116 loss: 0.0574\n",
      "Epoch 80: Batch 47/116 loss: 0.0562\n",
      "Epoch 80: Batch 48/116 loss: 0.0528\n",
      "Epoch 80: Batch 49/116 loss: 0.0581\n",
      "Epoch 80: Batch 50/116 loss: 0.0654\n",
      "Epoch 80: Batch 51/116 loss: 0.0468\n",
      "Epoch 80: Batch 52/116 loss: 0.0378\n",
      "Epoch 80: Batch 53/116 loss: 0.0888\n",
      "Epoch 80: Batch 54/116 loss: 0.0520\n",
      "Epoch 80: Batch 55/116 loss: 0.0525\n",
      "Epoch 80: Batch 56/116 loss: 0.0457\n",
      "Epoch 80: Batch 57/116 loss: 0.0541\n",
      "Epoch 80: Batch 58/116 loss: 0.0465\n",
      "Epoch 80: Batch 59/116 loss: 0.0401\n",
      "Epoch 80: Batch 60/116 loss: 0.0932\n",
      "Epoch 80: Batch 61/116 loss: 0.0652\n",
      "Epoch 80: Batch 62/116 loss: 0.0459\n",
      "Epoch 80: Batch 63/116 loss: 0.0802\n",
      "Epoch 80: Batch 64/116 loss: 0.0521\n",
      "Epoch 80: Batch 65/116 loss: 0.0389\n",
      "Epoch 80: Batch 66/116 loss: 0.0799\n",
      "Epoch 80: Batch 67/116 loss: 0.0637\n",
      "Epoch 80: Batch 68/116 loss: 0.0639\n",
      "Epoch 80: Batch 69/116 loss: 0.0561\n",
      "Epoch 80: Batch 70/116 loss: 0.0486\n",
      "Epoch 80: Batch 71/116 loss: 0.0512\n",
      "Epoch 80: Batch 72/116 loss: 0.0940\n",
      "Epoch 80: Batch 73/116 loss: 0.0932\n",
      "Epoch 80: Batch 74/116 loss: 0.0572\n",
      "Epoch 80: Batch 75/116 loss: 0.0628\n",
      "Epoch 80: Batch 76/116 loss: 0.0429\n",
      "Epoch 80: Batch 77/116 loss: 0.0781\n",
      "Epoch 80: Batch 78/116 loss: 0.0618\n",
      "Epoch 80: Batch 79/116 loss: 0.0511\n",
      "Epoch 80: Batch 80/116 loss: 0.0574\n",
      "Epoch 80: Batch 81/116 loss: 0.0524\n",
      "Epoch 80: Batch 82/116 loss: 0.0476\n",
      "Epoch 80: Batch 83/116 loss: 0.0663\n",
      "Epoch 80: Batch 84/116 loss: 0.0445\n",
      "Epoch 80: Batch 85/116 loss: 0.0373\n",
      "Epoch 80: Batch 86/116 loss: 0.0522\n",
      "Epoch 80: Batch 87/116 loss: 0.0549\n",
      "Epoch 80: Batch 88/116 loss: 0.0532\n",
      "Epoch 80: Batch 89/116 loss: 0.0465\n",
      "Epoch 80: Batch 90/116 loss: 0.0425\n",
      "Epoch 80: Batch 91/116 loss: 0.0720\n",
      "Epoch 80: Batch 92/116 loss: 0.0507\n",
      "Epoch 80: Batch 93/116 loss: 0.0670\n",
      "Epoch 80: Batch 94/116 loss: 0.0714\n",
      "Epoch 80: Batch 95/116 loss: 0.0551\n",
      "Epoch 80: Batch 96/116 loss: 0.0676\n",
      "Epoch 80: Batch 97/116 loss: 0.0627\n",
      "Epoch 80: Batch 98/116 loss: 0.0540\n",
      "Epoch 80: Batch 99/116 loss: 0.0539\n",
      "Epoch 80: Batch 100/116 loss: 0.0713\n",
      "Epoch 80: Batch 101/116 loss: 0.0698\n",
      "Epoch 80: Batch 102/116 loss: 0.0611\n",
      "Epoch 80: Batch 103/116 loss: 0.1172\n",
      "Epoch 80: Batch 104/116 loss: 0.0484\n",
      "Epoch 80: Batch 105/116 loss: 0.0838\n",
      "Epoch 80: Batch 106/116 loss: 0.1066\n",
      "Epoch 80: Batch 107/116 loss: 0.0756\n",
      "Epoch 80: Batch 108/116 loss: 0.0614\n",
      "Epoch 80: Batch 109/116 loss: 0.0762\n",
      "Epoch 80: Batch 110/116 loss: 0.0572\n",
      "Epoch 80: Batch 111/116 loss: 0.0824\n",
      "Epoch 80: Batch 112/116 loss: 0.0518\n",
      "Epoch 80: Batch 113/116 loss: 0.0648\n",
      "Epoch 80: Batch 114/116 loss: 0.0798\n",
      "Epoch 80: Batch 115/116 loss: 0.0611\n",
      "Epoch 80: Batch 116/116 loss: 0.0432\n",
      "Epoch 80 train loss: 0.0598 valid loss: 0.0603\n",
      "performance reducing: 5\n",
      "Epoch 81: Batch 1/116 loss: 0.0384\n",
      "Epoch 81: Batch 2/116 loss: 0.0468\n",
      "Epoch 81: Batch 3/116 loss: 0.0517\n",
      "Epoch 81: Batch 4/116 loss: 0.0513\n",
      "Epoch 81: Batch 5/116 loss: 0.0584\n",
      "Epoch 81: Batch 6/116 loss: 0.0543\n",
      "Epoch 81: Batch 7/116 loss: 0.0610\n",
      "Epoch 81: Batch 8/116 loss: 0.0465\n",
      "Epoch 81: Batch 9/116 loss: 0.0675\n",
      "Epoch 81: Batch 10/116 loss: 0.0543\n",
      "Epoch 81: Batch 11/116 loss: 0.0626\n",
      "Epoch 81: Batch 12/116 loss: 0.0628\n",
      "Epoch 81: Batch 13/116 loss: 0.0504\n",
      "Epoch 81: Batch 14/116 loss: 0.0500\n",
      "Epoch 81: Batch 15/116 loss: 0.0648\n",
      "Epoch 81: Batch 16/116 loss: 0.0734\n",
      "Epoch 81: Batch 17/116 loss: 0.0671\n",
      "Epoch 81: Batch 18/116 loss: 0.0549\n",
      "Epoch 81: Batch 19/116 loss: 0.0700\n",
      "Epoch 81: Batch 20/116 loss: 0.0398\n",
      "Epoch 81: Batch 21/116 loss: 0.0780\n",
      "Epoch 81: Batch 22/116 loss: 0.0498\n",
      "Epoch 81: Batch 23/116 loss: 0.0616\n",
      "Epoch 81: Batch 24/116 loss: 0.0554\n",
      "Epoch 81: Batch 25/116 loss: 0.0736\n",
      "Epoch 81: Batch 26/116 loss: 0.0495\n",
      "Epoch 81: Batch 27/116 loss: 0.0477\n",
      "Epoch 81: Batch 28/116 loss: 0.0897\n",
      "Epoch 81: Batch 29/116 loss: 0.0584\n",
      "Epoch 81: Batch 30/116 loss: 0.0548\n",
      "Epoch 81: Batch 31/116 loss: 0.0645\n",
      "Epoch 81: Batch 32/116 loss: 0.0455\n",
      "Epoch 81: Batch 33/116 loss: 0.0547\n",
      "Epoch 81: Batch 34/116 loss: 0.0597\n",
      "Epoch 81: Batch 35/116 loss: 0.0753\n",
      "Epoch 81: Batch 36/116 loss: 0.0599\n",
      "Epoch 81: Batch 37/116 loss: 0.0613\n",
      "Epoch 81: Batch 38/116 loss: 0.0416\n",
      "Epoch 81: Batch 39/116 loss: 0.0421\n",
      "Epoch 81: Batch 40/116 loss: 0.0679\n",
      "Epoch 81: Batch 41/116 loss: 0.0580\n",
      "Epoch 81: Batch 42/116 loss: 0.0599\n",
      "Epoch 81: Batch 43/116 loss: 0.0401\n",
      "Epoch 81: Batch 44/116 loss: 0.0428\n",
      "Epoch 81: Batch 45/116 loss: 0.0535\n",
      "Epoch 81: Batch 46/116 loss: 0.0441\n",
      "Epoch 81: Batch 47/116 loss: 0.0430\n",
      "Epoch 81: Batch 48/116 loss: 0.0443\n",
      "Epoch 81: Batch 49/116 loss: 0.0660\n",
      "Epoch 81: Batch 50/116 loss: 0.0460\n",
      "Epoch 81: Batch 51/116 loss: 0.0507\n",
      "Epoch 81: Batch 52/116 loss: 0.0603\n",
      "Epoch 81: Batch 53/116 loss: 0.0581\n",
      "Epoch 81: Batch 54/116 loss: 0.0476\n",
      "Epoch 81: Batch 55/116 loss: 0.0534\n",
      "Epoch 81: Batch 56/116 loss: 0.0583\n",
      "Epoch 81: Batch 57/116 loss: 0.0536\n",
      "Epoch 81: Batch 58/116 loss: 0.0573\n",
      "Epoch 81: Batch 59/116 loss: 0.0628\n",
      "Epoch 81: Batch 60/116 loss: 0.0729\n",
      "Epoch 81: Batch 61/116 loss: 0.0534\n",
      "Epoch 81: Batch 62/116 loss: 0.0655\n",
      "Epoch 81: Batch 63/116 loss: 0.0528\n",
      "Epoch 81: Batch 64/116 loss: 0.0626\n",
      "Epoch 81: Batch 65/116 loss: 0.0814\n",
      "Epoch 81: Batch 66/116 loss: 0.0425\n",
      "Epoch 81: Batch 67/116 loss: 0.0490\n",
      "Epoch 81: Batch 68/116 loss: 0.0405\n",
      "Epoch 81: Batch 69/116 loss: 0.1037\n",
      "Epoch 81: Batch 70/116 loss: 0.0714\n",
      "Epoch 81: Batch 71/116 loss: 0.0662\n",
      "Epoch 81: Batch 72/116 loss: 0.0673\n",
      "Epoch 81: Batch 73/116 loss: 0.0517\n",
      "Epoch 81: Batch 74/116 loss: 0.0707\n",
      "Epoch 81: Batch 75/116 loss: 0.0581\n",
      "Epoch 81: Batch 76/116 loss: 0.0457\n",
      "Epoch 81: Batch 77/116 loss: 0.0487\n",
      "Epoch 81: Batch 78/116 loss: 0.0488\n",
      "Epoch 81: Batch 79/116 loss: 0.0571\n",
      "Epoch 81: Batch 80/116 loss: 0.0406\n",
      "Epoch 81: Batch 81/116 loss: 0.0508\n",
      "Epoch 81: Batch 82/116 loss: 0.0579\n",
      "Epoch 81: Batch 83/116 loss: 0.1179\n",
      "Epoch 81: Batch 84/116 loss: 0.0517\n",
      "Epoch 81: Batch 85/116 loss: 0.0659\n",
      "Epoch 81: Batch 86/116 loss: 0.0527\n",
      "Epoch 81: Batch 87/116 loss: 0.0725\n",
      "Epoch 81: Batch 88/116 loss: 0.0604\n",
      "Epoch 81: Batch 89/116 loss: 0.0467\n",
      "Epoch 81: Batch 90/116 loss: 0.0396\n",
      "Epoch 81: Batch 91/116 loss: 0.0887\n",
      "Epoch 81: Batch 92/116 loss: 0.0639\n",
      "Epoch 81: Batch 93/116 loss: 0.0596\n",
      "Epoch 81: Batch 94/116 loss: 0.0616\n",
      "Epoch 81: Batch 95/116 loss: 0.0626\n",
      "Epoch 81: Batch 96/116 loss: 0.0676\n",
      "Epoch 81: Batch 97/116 loss: 0.0451\n",
      "Epoch 81: Batch 98/116 loss: 0.0368\n",
      "Epoch 81: Batch 99/116 loss: 0.0688\n",
      "Epoch 81: Batch 100/116 loss: 0.0595\n",
      "Epoch 81: Batch 101/116 loss: 0.0649\n",
      "Epoch 81: Batch 102/116 loss: 0.0506\n",
      "Epoch 81: Batch 103/116 loss: 0.0468\n",
      "Epoch 81: Batch 104/116 loss: 0.0576\n",
      "Epoch 81: Batch 105/116 loss: 0.0436\n",
      "Epoch 81: Batch 106/116 loss: 0.0533\n",
      "Epoch 81: Batch 107/116 loss: 0.0593\n",
      "Epoch 81: Batch 108/116 loss: 0.0712\n",
      "Epoch 81: Batch 109/116 loss: 0.0401\n",
      "Epoch 81: Batch 110/116 loss: 0.0770\n",
      "Epoch 81: Batch 111/116 loss: 0.0471\n",
      "Epoch 81: Batch 112/116 loss: 0.0468\n",
      "Epoch 81: Batch 113/116 loss: 0.0691\n",
      "Epoch 81: Batch 114/116 loss: 0.0540\n",
      "Epoch 81: Batch 115/116 loss: 0.0637\n",
      "Epoch 81: Batch 116/116 loss: 0.0563\n",
      "Epoch 81 train loss: 0.0577 valid loss: 0.0606\n",
      "performance reducing: 6\n",
      "Epoch 82: Batch 1/116 loss: 0.0705\n",
      "Epoch 82: Batch 2/116 loss: 0.0480\n",
      "Epoch 82: Batch 3/116 loss: 0.0604\n",
      "Epoch 82: Batch 4/116 loss: 0.0444\n",
      "Epoch 82: Batch 5/116 loss: 0.0641\n",
      "Epoch 82: Batch 6/116 loss: 0.0581\n",
      "Epoch 82: Batch 7/116 loss: 0.0565\n",
      "Epoch 82: Batch 8/116 loss: 0.0587\n",
      "Epoch 82: Batch 9/116 loss: 0.0554\n",
      "Epoch 82: Batch 10/116 loss: 0.0491\n",
      "Epoch 82: Batch 11/116 loss: 0.0678\n",
      "Epoch 82: Batch 12/116 loss: 0.0576\n",
      "Epoch 82: Batch 13/116 loss: 0.0578\n",
      "Epoch 82: Batch 14/116 loss: 0.0528\n",
      "Epoch 82: Batch 15/116 loss: 0.0403\n",
      "Epoch 82: Batch 16/116 loss: 0.0508\n",
      "Epoch 82: Batch 17/116 loss: 0.0551\n",
      "Epoch 82: Batch 18/116 loss: 0.0549\n",
      "Epoch 82: Batch 19/116 loss: 0.0750\n",
      "Epoch 82: Batch 20/116 loss: 0.0781\n",
      "Epoch 82: Batch 21/116 loss: 0.0735\n",
      "Epoch 82: Batch 22/116 loss: 0.0383\n",
      "Epoch 82: Batch 23/116 loss: 0.0613\n",
      "Epoch 82: Batch 24/116 loss: 0.0427\n",
      "Epoch 82: Batch 25/116 loss: 0.0644\n",
      "Epoch 82: Batch 26/116 loss: 0.0633\n",
      "Epoch 82: Batch 27/116 loss: 0.0600\n",
      "Epoch 82: Batch 28/116 loss: 0.0665\n",
      "Epoch 82: Batch 29/116 loss: 0.0576\n",
      "Epoch 82: Batch 30/116 loss: 0.0429\n",
      "Epoch 82: Batch 31/116 loss: 0.0532\n",
      "Epoch 82: Batch 32/116 loss: 0.0633\n",
      "Epoch 82: Batch 33/116 loss: 0.0701\n",
      "Epoch 82: Batch 34/116 loss: 0.0588\n",
      "Epoch 82: Batch 35/116 loss: 0.0705\n",
      "Epoch 82: Batch 36/116 loss: 0.0615\n",
      "Epoch 82: Batch 37/116 loss: 0.0489\n",
      "Epoch 82: Batch 38/116 loss: 0.0362\n",
      "Epoch 82: Batch 39/116 loss: 0.0838\n",
      "Epoch 82: Batch 40/116 loss: 0.1111\n",
      "Epoch 82: Batch 41/116 loss: 0.0632\n",
      "Epoch 82: Batch 42/116 loss: 0.0653\n",
      "Epoch 82: Batch 43/116 loss: 0.0577\n",
      "Epoch 82: Batch 44/116 loss: 0.0545\n",
      "Epoch 82: Batch 45/116 loss: 0.0624\n",
      "Epoch 82: Batch 46/116 loss: 0.0506\n",
      "Epoch 82: Batch 47/116 loss: 0.0694\n",
      "Epoch 82: Batch 48/116 loss: 0.0346\n",
      "Epoch 82: Batch 49/116 loss: 0.0689\n",
      "Epoch 82: Batch 50/116 loss: 0.0647\n",
      "Epoch 82: Batch 51/116 loss: 0.0592\n",
      "Epoch 82: Batch 52/116 loss: 0.0450\n",
      "Epoch 82: Batch 53/116 loss: 0.0507\n",
      "Epoch 82: Batch 54/116 loss: 0.0346\n",
      "Epoch 82: Batch 55/116 loss: 0.0488\n",
      "Epoch 82: Batch 56/116 loss: 0.0537\n",
      "Epoch 82: Batch 57/116 loss: 0.0431\n",
      "Epoch 82: Batch 58/116 loss: 0.0616\n",
      "Epoch 82: Batch 59/116 loss: 0.0571\n",
      "Epoch 82: Batch 60/116 loss: 0.0425\n",
      "Epoch 82: Batch 61/116 loss: 0.0423\n",
      "Epoch 82: Batch 62/116 loss: 0.0553\n",
      "Epoch 82: Batch 63/116 loss: 0.0494\n",
      "Epoch 82: Batch 64/116 loss: 0.0751\n",
      "Epoch 82: Batch 65/116 loss: 0.0696\n",
      "Epoch 82: Batch 66/116 loss: 0.0502\n",
      "Epoch 82: Batch 67/116 loss: 0.0640\n",
      "Epoch 82: Batch 68/116 loss: 0.0617\n",
      "Epoch 82: Batch 69/116 loss: 0.0626\n",
      "Epoch 82: Batch 70/116 loss: 0.0651\n",
      "Epoch 82: Batch 71/116 loss: 0.0584\n",
      "Epoch 82: Batch 72/116 loss: 0.0829\n",
      "Epoch 82: Batch 73/116 loss: 0.0733\n",
      "Epoch 82: Batch 74/116 loss: 0.0694\n",
      "Epoch 82: Batch 75/116 loss: 0.0912\n",
      "Epoch 82: Batch 76/116 loss: 0.0727\n",
      "Epoch 82: Batch 77/116 loss: 0.0665\n",
      "Epoch 82: Batch 78/116 loss: 0.0550\n",
      "Epoch 82: Batch 79/116 loss: 0.0492\n",
      "Epoch 82: Batch 80/116 loss: 0.0431\n",
      "Epoch 82: Batch 81/116 loss: 0.0379\n",
      "Epoch 82: Batch 82/116 loss: 0.0372\n",
      "Epoch 82: Batch 83/116 loss: 0.0297\n",
      "Epoch 82: Batch 84/116 loss: 0.0690\n",
      "Epoch 82: Batch 85/116 loss: 0.0565\n",
      "Epoch 82: Batch 86/116 loss: 0.0587\n",
      "Epoch 82: Batch 87/116 loss: 0.0415\n",
      "Epoch 82: Batch 88/116 loss: 0.0368\n",
      "Epoch 82: Batch 89/116 loss: 0.0657\n",
      "Epoch 82: Batch 90/116 loss: 0.0554\n",
      "Epoch 82: Batch 91/116 loss: 0.0518\n",
      "Epoch 82: Batch 92/116 loss: 0.0521\n",
      "Epoch 82: Batch 93/116 loss: 0.0497\n",
      "Epoch 82: Batch 94/116 loss: 0.0629\n",
      "Epoch 82: Batch 95/116 loss: 0.0716\n",
      "Epoch 82: Batch 96/116 loss: 0.0670\n",
      "Epoch 82: Batch 97/116 loss: 0.0851\n",
      "Epoch 82: Batch 98/116 loss: 0.0397\n",
      "Epoch 82: Batch 99/116 loss: 0.0792\n",
      "Epoch 82: Batch 100/116 loss: 0.0511\n",
      "Epoch 82: Batch 101/116 loss: 0.0475\n",
      "Epoch 82: Batch 102/116 loss: 0.0565\n",
      "Epoch 82: Batch 103/116 loss: 0.0427\n",
      "Epoch 82: Batch 104/116 loss: 0.0519\n",
      "Epoch 82: Batch 105/116 loss: 0.0506\n",
      "Epoch 82: Batch 106/116 loss: 0.0406\n",
      "Epoch 82: Batch 107/116 loss: 0.0512\n",
      "Epoch 82: Batch 108/116 loss: 0.0464\n",
      "Epoch 82: Batch 109/116 loss: 0.0511\n",
      "Epoch 82: Batch 110/116 loss: 0.0548\n",
      "Epoch 82: Batch 111/116 loss: 0.0461\n",
      "Epoch 82: Batch 112/116 loss: 0.0587\n",
      "Epoch 82: Batch 113/116 loss: 0.0606\n",
      "Epoch 82: Batch 114/116 loss: 0.0498\n",
      "Epoch 82: Batch 115/116 loss: 0.0601\n",
      "Epoch 82: Batch 116/116 loss: 0.0300\n",
      "Epoch 82 train loss: 0.0571 valid loss: 0.0562\n",
      "Epoch 83: Batch 1/116 loss: 0.0483\n",
      "Epoch 83: Batch 2/116 loss: 0.0659\n",
      "Epoch 83: Batch 3/116 loss: 0.0416\n",
      "Epoch 83: Batch 4/116 loss: 0.0650\n",
      "Epoch 83: Batch 5/116 loss: 0.0477\n",
      "Epoch 83: Batch 6/116 loss: 0.0664\n",
      "Epoch 83: Batch 7/116 loss: 0.0547\n",
      "Epoch 83: Batch 8/116 loss: 0.0526\n",
      "Epoch 83: Batch 9/116 loss: 0.0575\n",
      "Epoch 83: Batch 10/116 loss: 0.0382\n",
      "Epoch 83: Batch 11/116 loss: 0.0423\n",
      "Epoch 83: Batch 12/116 loss: 0.0547\n",
      "Epoch 83: Batch 13/116 loss: 0.0416\n",
      "Epoch 83: Batch 14/116 loss: 0.0724\n",
      "Epoch 83: Batch 15/116 loss: 0.0501\n",
      "Epoch 83: Batch 16/116 loss: 0.0540\n",
      "Epoch 83: Batch 17/116 loss: 0.0592\n",
      "Epoch 83: Batch 18/116 loss: 0.0494\n",
      "Epoch 83: Batch 19/116 loss: 0.0490\n",
      "Epoch 83: Batch 20/116 loss: 0.0398\n",
      "Epoch 83: Batch 21/116 loss: 0.0423\n",
      "Epoch 83: Batch 22/116 loss: 0.0572\n",
      "Epoch 83: Batch 23/116 loss: 0.0653\n",
      "Epoch 83: Batch 24/116 loss: 0.0537\n",
      "Epoch 83: Batch 25/116 loss: 0.0760\n",
      "Epoch 83: Batch 26/116 loss: 0.0393\n",
      "Epoch 83: Batch 27/116 loss: 0.0648\n",
      "Epoch 83: Batch 28/116 loss: 0.0698\n",
      "Epoch 83: Batch 29/116 loss: 0.0616\n",
      "Epoch 83: Batch 30/116 loss: 0.0668\n",
      "Epoch 83: Batch 31/116 loss: 0.0416\n",
      "Epoch 83: Batch 32/116 loss: 0.0983\n",
      "Epoch 83: Batch 33/116 loss: 0.0523\n",
      "Epoch 83: Batch 34/116 loss: 0.0487\n",
      "Epoch 83: Batch 35/116 loss: 0.0461\n",
      "Epoch 83: Batch 36/116 loss: 0.0451\n",
      "Epoch 83: Batch 37/116 loss: 0.0456\n",
      "Epoch 83: Batch 38/116 loss: 0.0488\n",
      "Epoch 83: Batch 39/116 loss: 0.0364\n",
      "Epoch 83: Batch 40/116 loss: 0.0678\n",
      "Epoch 83: Batch 41/116 loss: 0.0800\n",
      "Epoch 83: Batch 42/116 loss: 0.0525\n",
      "Epoch 83: Batch 43/116 loss: 0.0589\n",
      "Epoch 83: Batch 44/116 loss: 0.0932\n",
      "Epoch 83: Batch 45/116 loss: 0.0817\n",
      "Epoch 83: Batch 46/116 loss: 0.0440\n",
      "Epoch 83: Batch 47/116 loss: 0.0629\n",
      "Epoch 83: Batch 48/116 loss: 0.0722\n",
      "Epoch 83: Batch 49/116 loss: 0.0547\n",
      "Epoch 83: Batch 50/116 loss: 0.0511\n",
      "Epoch 83: Batch 51/116 loss: 0.0624\n",
      "Epoch 83: Batch 52/116 loss: 0.0494\n",
      "Epoch 83: Batch 53/116 loss: 0.0722\n",
      "Epoch 83: Batch 54/116 loss: 0.0432\n",
      "Epoch 83: Batch 55/116 loss: 0.0800\n",
      "Epoch 83: Batch 56/116 loss: 0.0694\n",
      "Epoch 83: Batch 57/116 loss: 0.0749\n",
      "Epoch 83: Batch 58/116 loss: 0.0497\n",
      "Epoch 83: Batch 59/116 loss: 0.0581\n",
      "Epoch 83: Batch 60/116 loss: 0.0565\n",
      "Epoch 83: Batch 61/116 loss: 0.0648\n",
      "Epoch 83: Batch 62/116 loss: 0.0692\n",
      "Epoch 83: Batch 63/116 loss: 0.0522\n",
      "Epoch 83: Batch 64/116 loss: 0.0460\n",
      "Epoch 83: Batch 65/116 loss: 0.0493\n",
      "Epoch 83: Batch 66/116 loss: 0.0695\n",
      "Epoch 83: Batch 67/116 loss: 0.0695\n",
      "Epoch 83: Batch 68/116 loss: 0.0629\n",
      "Epoch 83: Batch 69/116 loss: 0.0622\n",
      "Epoch 83: Batch 70/116 loss: 0.0508\n",
      "Epoch 83: Batch 71/116 loss: 0.0540\n",
      "Epoch 83: Batch 72/116 loss: 0.0553\n",
      "Epoch 83: Batch 73/116 loss: 0.0482\n",
      "Epoch 83: Batch 74/116 loss: 0.0415\n",
      "Epoch 83: Batch 75/116 loss: 0.0371\n",
      "Epoch 83: Batch 76/116 loss: 0.0486\n",
      "Epoch 83: Batch 77/116 loss: 0.0450\n",
      "Epoch 83: Batch 78/116 loss: 0.0606\n",
      "Epoch 83: Batch 79/116 loss: 0.0581\n",
      "Epoch 83: Batch 80/116 loss: 0.0507\n",
      "Epoch 83: Batch 81/116 loss: 0.0493\n",
      "Epoch 83: Batch 82/116 loss: 0.0602\n",
      "Epoch 83: Batch 83/116 loss: 0.0722\n",
      "Epoch 83: Batch 84/116 loss: 0.0459\n",
      "Epoch 83: Batch 85/116 loss: 0.0546\n",
      "Epoch 83: Batch 86/116 loss: 0.0432\n",
      "Epoch 83: Batch 87/116 loss: 0.0713\n",
      "Epoch 83: Batch 88/116 loss: 0.0435\n",
      "Epoch 83: Batch 89/116 loss: 0.0695\n",
      "Epoch 83: Batch 90/116 loss: 0.0599\n",
      "Epoch 83: Batch 91/116 loss: 0.0565\n",
      "Epoch 83: Batch 92/116 loss: 0.0611\n",
      "Epoch 83: Batch 93/116 loss: 0.0521\n",
      "Epoch 83: Batch 94/116 loss: 0.0594\n",
      "Epoch 83: Batch 95/116 loss: 0.0988\n",
      "Epoch 83: Batch 96/116 loss: 0.0575\n",
      "Epoch 83: Batch 97/116 loss: 0.0756\n",
      "Epoch 83: Batch 98/116 loss: 0.0600\n",
      "Epoch 83: Batch 99/116 loss: 0.0742\n",
      "Epoch 83: Batch 100/116 loss: 0.0592\n",
      "Epoch 83: Batch 101/116 loss: 0.0307\n",
      "Epoch 83: Batch 102/116 loss: 0.0625\n",
      "Epoch 83: Batch 103/116 loss: 0.0751\n",
      "Epoch 83: Batch 104/116 loss: 0.0465\n",
      "Epoch 83: Batch 105/116 loss: 0.0835\n",
      "Epoch 83: Batch 106/116 loss: 0.0743\n",
      "Epoch 83: Batch 107/116 loss: 0.0614\n",
      "Epoch 83: Batch 108/116 loss: 0.0669\n",
      "Epoch 83: Batch 109/116 loss: 0.0686\n",
      "Epoch 83: Batch 110/116 loss: 0.0498\n",
      "Epoch 83: Batch 111/116 loss: 0.0581\n",
      "Epoch 83: Batch 112/116 loss: 0.0703\n",
      "Epoch 83: Batch 113/116 loss: 0.0431\n",
      "Epoch 83: Batch 114/116 loss: 0.0496\n",
      "Epoch 83: Batch 115/116 loss: 0.0460\n",
      "Epoch 83: Batch 116/116 loss: 0.0615\n",
      "Epoch 83 train loss: 0.0579 valid loss: 0.0579\n",
      "performance reducing: 1\n",
      "Epoch 84: Batch 1/116 loss: 0.0528\n",
      "Epoch 84: Batch 2/116 loss: 0.0536\n",
      "Epoch 84: Batch 3/116 loss: 0.0602\n",
      "Epoch 84: Batch 4/116 loss: 0.0303\n",
      "Epoch 84: Batch 5/116 loss: 0.0721\n",
      "Epoch 84: Batch 6/116 loss: 0.0456\n",
      "Epoch 84: Batch 7/116 loss: 0.0563\n",
      "Epoch 84: Batch 8/116 loss: 0.0455\n",
      "Epoch 84: Batch 9/116 loss: 0.0762\n",
      "Epoch 84: Batch 10/116 loss: 0.0502\n",
      "Epoch 84: Batch 11/116 loss: 0.0597\n",
      "Epoch 84: Batch 12/116 loss: 0.0719\n",
      "Epoch 84: Batch 13/116 loss: 0.0500\n",
      "Epoch 84: Batch 14/116 loss: 0.0498\n",
      "Epoch 84: Batch 15/116 loss: 0.0579\n",
      "Epoch 84: Batch 16/116 loss: 0.0384\n",
      "Epoch 84: Batch 17/116 loss: 0.0541\n",
      "Epoch 84: Batch 18/116 loss: 0.0805\n",
      "Epoch 84: Batch 19/116 loss: 0.0536\n",
      "Epoch 84: Batch 20/116 loss: 0.0701\n",
      "Epoch 84: Batch 21/116 loss: 0.0750\n",
      "Epoch 84: Batch 22/116 loss: 0.0613\n",
      "Epoch 84: Batch 23/116 loss: 0.0460\n",
      "Epoch 84: Batch 24/116 loss: 0.0591\n",
      "Epoch 84: Batch 25/116 loss: 0.0520\n",
      "Epoch 84: Batch 26/116 loss: 0.0714\n",
      "Epoch 84: Batch 27/116 loss: 0.0530\n",
      "Epoch 84: Batch 28/116 loss: 0.0543\n",
      "Epoch 84: Batch 29/116 loss: 0.0383\n",
      "Epoch 84: Batch 30/116 loss: 0.0544\n",
      "Epoch 84: Batch 31/116 loss: 0.0352\n",
      "Epoch 84: Batch 32/116 loss: 0.0390\n",
      "Epoch 84: Batch 33/116 loss: 0.0752\n",
      "Epoch 84: Batch 34/116 loss: 0.0520\n",
      "Epoch 84: Batch 35/116 loss: 0.0524\n",
      "Epoch 84: Batch 36/116 loss: 0.0837\n",
      "Epoch 84: Batch 37/116 loss: 0.0621\n",
      "Epoch 84: Batch 38/116 loss: 0.0446\n",
      "Epoch 84: Batch 39/116 loss: 0.0470\n",
      "Epoch 84: Batch 40/116 loss: 0.0340\n",
      "Epoch 84: Batch 41/116 loss: 0.0473\n",
      "Epoch 84: Batch 42/116 loss: 0.0416\n",
      "Epoch 84: Batch 43/116 loss: 0.0436\n",
      "Epoch 84: Batch 44/116 loss: 0.0485\n",
      "Epoch 84: Batch 45/116 loss: 0.0463\n",
      "Epoch 84: Batch 46/116 loss: 0.0367\n",
      "Epoch 84: Batch 47/116 loss: 0.0702\n",
      "Epoch 84: Batch 48/116 loss: 0.0611\n",
      "Epoch 84: Batch 49/116 loss: 0.0432\n",
      "Epoch 84: Batch 50/116 loss: 0.0649\n",
      "Epoch 84: Batch 51/116 loss: 0.0559\n",
      "Epoch 84: Batch 52/116 loss: 0.0680\n",
      "Epoch 84: Batch 53/116 loss: 0.0621\n",
      "Epoch 84: Batch 54/116 loss: 0.0470\n",
      "Epoch 84: Batch 55/116 loss: 0.0643\n",
      "Epoch 84: Batch 56/116 loss: 0.0712\n",
      "Epoch 84: Batch 57/116 loss: 0.0438\n",
      "Epoch 84: Batch 58/116 loss: 0.0458\n",
      "Epoch 84: Batch 59/116 loss: 0.0570\n",
      "Epoch 84: Batch 60/116 loss: 0.0481\n",
      "Epoch 84: Batch 61/116 loss: 0.0410\n",
      "Epoch 84: Batch 62/116 loss: 0.0868\n",
      "Epoch 84: Batch 63/116 loss: 0.0564\n",
      "Epoch 84: Batch 64/116 loss: 0.0868\n",
      "Epoch 84: Batch 65/116 loss: 0.0537\n",
      "Epoch 84: Batch 66/116 loss: 0.0415\n",
      "Epoch 84: Batch 67/116 loss: 0.0535\n",
      "Epoch 84: Batch 68/116 loss: 0.0460\n",
      "Epoch 84: Batch 69/116 loss: 0.0605\n",
      "Epoch 84: Batch 70/116 loss: 0.0543\n",
      "Epoch 84: Batch 71/116 loss: 0.0567\n",
      "Epoch 84: Batch 72/116 loss: 0.0465\n",
      "Epoch 84: Batch 73/116 loss: 0.0565\n",
      "Epoch 84: Batch 74/116 loss: 0.0544\n",
      "Epoch 84: Batch 75/116 loss: 0.0639\n",
      "Epoch 84: Batch 76/116 loss: 0.0491\n",
      "Epoch 84: Batch 77/116 loss: 0.0525\n",
      "Epoch 84: Batch 78/116 loss: 0.0558\n",
      "Epoch 84: Batch 79/116 loss: 0.0770\n",
      "Epoch 84: Batch 80/116 loss: 0.0876\n",
      "Epoch 84: Batch 81/116 loss: 0.0732\n",
      "Epoch 84: Batch 82/116 loss: 0.0504\n",
      "Epoch 84: Batch 83/116 loss: 0.0683\n",
      "Epoch 84: Batch 84/116 loss: 0.0461\n",
      "Epoch 84: Batch 85/116 loss: 0.0709\n",
      "Epoch 84: Batch 86/116 loss: 0.0414\n",
      "Epoch 84: Batch 87/116 loss: 0.0389\n",
      "Epoch 84: Batch 88/116 loss: 0.0556\n",
      "Epoch 84: Batch 89/116 loss: 0.0575\n",
      "Epoch 84: Batch 90/116 loss: 0.0821\n",
      "Epoch 84: Batch 91/116 loss: 0.0488\n",
      "Epoch 84: Batch 92/116 loss: 0.0560\n",
      "Epoch 84: Batch 93/116 loss: 0.0585\n",
      "Epoch 84: Batch 94/116 loss: 0.0534\n",
      "Epoch 84: Batch 95/116 loss: 0.0511\n",
      "Epoch 84: Batch 96/116 loss: 0.0476\n",
      "Epoch 84: Batch 97/116 loss: 0.0509\n",
      "Epoch 84: Batch 98/116 loss: 0.0570\n",
      "Epoch 84: Batch 99/116 loss: 0.0498\n",
      "Epoch 84: Batch 100/116 loss: 0.0540\n",
      "Epoch 84: Batch 101/116 loss: 0.0422\n",
      "Epoch 84: Batch 102/116 loss: 0.0788\n",
      "Epoch 84: Batch 103/116 loss: 0.0486\n",
      "Epoch 84: Batch 104/116 loss: 0.0538\n",
      "Epoch 84: Batch 105/116 loss: 0.0391\n",
      "Epoch 84: Batch 106/116 loss: 0.0439\n",
      "Epoch 84: Batch 107/116 loss: 0.0553\n",
      "Epoch 84: Batch 108/116 loss: 0.0383\n",
      "Epoch 84: Batch 109/116 loss: 0.0696\n",
      "Epoch 84: Batch 110/116 loss: 0.0606\n",
      "Epoch 84: Batch 111/116 loss: 0.0375\n",
      "Epoch 84: Batch 112/116 loss: 0.0658\n",
      "Epoch 84: Batch 113/116 loss: 0.0710\n",
      "Epoch 84: Batch 114/116 loss: 0.0498\n",
      "Epoch 84: Batch 115/116 loss: 0.0903\n",
      "Epoch 84: Batch 116/116 loss: 0.0449\n",
      "Epoch 84 train loss: 0.0557 valid loss: 0.0576\n",
      "performance reducing: 2\n",
      "Epoch 85: Batch 1/116 loss: 0.0438\n",
      "Epoch 85: Batch 2/116 loss: 0.0500\n",
      "Epoch 85: Batch 3/116 loss: 0.0557\n",
      "Epoch 85: Batch 4/116 loss: 0.0708\n",
      "Epoch 85: Batch 5/116 loss: 0.0599\n",
      "Epoch 85: Batch 6/116 loss: 0.0593\n",
      "Epoch 85: Batch 7/116 loss: 0.0774\n",
      "Epoch 85: Batch 8/116 loss: 0.0683\n",
      "Epoch 85: Batch 9/116 loss: 0.0524\n",
      "Epoch 85: Batch 10/116 loss: 0.0733\n",
      "Epoch 85: Batch 11/116 loss: 0.0470\n",
      "Epoch 85: Batch 12/116 loss: 0.0441\n",
      "Epoch 85: Batch 13/116 loss: 0.0722\n",
      "Epoch 85: Batch 14/116 loss: 0.0574\n",
      "Epoch 85: Batch 15/116 loss: 0.0338\n",
      "Epoch 85: Batch 16/116 loss: 0.0446\n",
      "Epoch 85: Batch 17/116 loss: 0.0322\n",
      "Epoch 85: Batch 18/116 loss: 0.0545\n",
      "Epoch 85: Batch 19/116 loss: 0.0729\n",
      "Epoch 85: Batch 20/116 loss: 0.1014\n",
      "Epoch 85: Batch 21/116 loss: 0.0639\n",
      "Epoch 85: Batch 22/116 loss: 0.0814\n",
      "Epoch 85: Batch 23/116 loss: 0.0753\n",
      "Epoch 85: Batch 24/116 loss: 0.0541\n",
      "Epoch 85: Batch 25/116 loss: 0.0696\n",
      "Epoch 85: Batch 26/116 loss: 0.0392\n",
      "Epoch 85: Batch 27/116 loss: 0.0653\n",
      "Epoch 85: Batch 28/116 loss: 0.0488\n",
      "Epoch 85: Batch 29/116 loss: 0.0500\n",
      "Epoch 85: Batch 30/116 loss: 0.0611\n",
      "Epoch 85: Batch 31/116 loss: 0.0472\n",
      "Epoch 85: Batch 32/116 loss: 0.0445\n",
      "Epoch 85: Batch 33/116 loss: 0.0395\n",
      "Epoch 85: Batch 34/116 loss: 0.0468\n",
      "Epoch 85: Batch 35/116 loss: 0.0669\n",
      "Epoch 85: Batch 36/116 loss: 0.0598\n",
      "Epoch 85: Batch 37/116 loss: 0.0489\n",
      "Epoch 85: Batch 38/116 loss: 0.0433\n",
      "Epoch 85: Batch 39/116 loss: 0.0354\n",
      "Epoch 85: Batch 40/116 loss: 0.0672\n",
      "Epoch 85: Batch 41/116 loss: 0.0501\n",
      "Epoch 85: Batch 42/116 loss: 0.0494\n",
      "Epoch 85: Batch 43/116 loss: 0.0506\n",
      "Epoch 85: Batch 44/116 loss: 0.0735\n",
      "Epoch 85: Batch 45/116 loss: 0.0606\n",
      "Epoch 85: Batch 46/116 loss: 0.0716\n",
      "Epoch 85: Batch 47/116 loss: 0.0552\n",
      "Epoch 85: Batch 48/116 loss: 0.0864\n",
      "Epoch 85: Batch 49/116 loss: 0.0654\n",
      "Epoch 85: Batch 50/116 loss: 0.0695\n",
      "Epoch 85: Batch 51/116 loss: 0.0505\n",
      "Epoch 85: Batch 52/116 loss: 0.0461\n",
      "Epoch 85: Batch 53/116 loss: 0.0504\n",
      "Epoch 85: Batch 54/116 loss: 0.0402\n",
      "Epoch 85: Batch 55/116 loss: 0.0539\n",
      "Epoch 85: Batch 56/116 loss: 0.0654\n",
      "Epoch 85: Batch 57/116 loss: 0.0466\n",
      "Epoch 85: Batch 58/116 loss: 0.0553\n",
      "Epoch 85: Batch 59/116 loss: 0.0580\n",
      "Epoch 85: Batch 60/116 loss: 0.0719\n",
      "Epoch 85: Batch 61/116 loss: 0.0502\n",
      "Epoch 85: Batch 62/116 loss: 0.0470\n",
      "Epoch 85: Batch 63/116 loss: 0.0502\n",
      "Epoch 85: Batch 64/116 loss: 0.0719\n",
      "Epoch 85: Batch 65/116 loss: 0.0450\n",
      "Epoch 85: Batch 66/116 loss: 0.0476\n",
      "Epoch 85: Batch 67/116 loss: 0.0927\n",
      "Epoch 85: Batch 68/116 loss: 0.0536\n",
      "Epoch 85: Batch 69/116 loss: 0.0631\n",
      "Epoch 85: Batch 70/116 loss: 0.0546\n",
      "Epoch 85: Batch 71/116 loss: 0.0394\n",
      "Epoch 85: Batch 72/116 loss: 0.0449\n",
      "Epoch 85: Batch 73/116 loss: 0.0630\n",
      "Epoch 85: Batch 74/116 loss: 0.0571\n",
      "Epoch 85: Batch 75/116 loss: 0.0362\n",
      "Epoch 85: Batch 76/116 loss: 0.0388\n",
      "Epoch 85: Batch 77/116 loss: 0.0894\n",
      "Epoch 85: Batch 78/116 loss: 0.0661\n",
      "Epoch 85: Batch 79/116 loss: 0.0436\n",
      "Epoch 85: Batch 80/116 loss: 0.0749\n",
      "Epoch 85: Batch 81/116 loss: 0.0357\n",
      "Epoch 85: Batch 82/116 loss: 0.0338\n",
      "Epoch 85: Batch 83/116 loss: 0.0587\n",
      "Epoch 85: Batch 84/116 loss: 0.0487\n",
      "Epoch 85: Batch 85/116 loss: 0.0631\n",
      "Epoch 85: Batch 86/116 loss: 0.0456\n",
      "Epoch 85: Batch 87/116 loss: 0.0448\n",
      "Epoch 85: Batch 88/116 loss: 0.0463\n",
      "Epoch 85: Batch 89/116 loss: 0.0521\n",
      "Epoch 85: Batch 90/116 loss: 0.0480\n",
      "Epoch 85: Batch 91/116 loss: 0.0520\n",
      "Epoch 85: Batch 92/116 loss: 0.0789\n",
      "Epoch 85: Batch 93/116 loss: 0.0552\n",
      "Epoch 85: Batch 94/116 loss: 0.0702\n",
      "Epoch 85: Batch 95/116 loss: 0.0582\n",
      "Epoch 85: Batch 96/116 loss: 0.0394\n",
      "Epoch 85: Batch 97/116 loss: 0.1005\n",
      "Epoch 85: Batch 98/116 loss: 0.0623\n",
      "Epoch 85: Batch 99/116 loss: 0.0686\n",
      "Epoch 85: Batch 100/116 loss: 0.0500\n",
      "Epoch 85: Batch 101/116 loss: 0.0747\n",
      "Epoch 85: Batch 102/116 loss: 0.0806\n",
      "Epoch 85: Batch 103/116 loss: 0.0349\n",
      "Epoch 85: Batch 104/116 loss: 0.0521\n",
      "Epoch 85: Batch 105/116 loss: 0.0663\n",
      "Epoch 85: Batch 106/116 loss: 0.0543\n",
      "Epoch 85: Batch 107/116 loss: 0.0634\n",
      "Epoch 85: Batch 108/116 loss: 0.0372\n",
      "Epoch 85: Batch 109/116 loss: 0.0725\n",
      "Epoch 85: Batch 110/116 loss: 0.0654\n",
      "Epoch 85: Batch 111/116 loss: 0.0468\n",
      "Epoch 85: Batch 112/116 loss: 0.0624\n",
      "Epoch 85: Batch 113/116 loss: 0.0767\n",
      "Epoch 85: Batch 114/116 loss: 0.0420\n",
      "Epoch 85: Batch 115/116 loss: 0.0640\n",
      "Epoch 85: Batch 116/116 loss: 0.0580\n",
      "Epoch 85 train loss: 0.0573 valid loss: 0.0874\n",
      "performance reducing: 3\n",
      "Epoch 86: Batch 1/116 loss: 0.0509\n",
      "Epoch 86: Batch 2/116 loss: 0.0402\n",
      "Epoch 86: Batch 3/116 loss: 0.0518\n",
      "Epoch 86: Batch 4/116 loss: 0.0568\n",
      "Epoch 86: Batch 5/116 loss: 0.0697\n",
      "Epoch 86: Batch 6/116 loss: 0.0647\n",
      "Epoch 86: Batch 7/116 loss: 0.0664\n",
      "Epoch 86: Batch 8/116 loss: 0.0843\n",
      "Epoch 86: Batch 9/116 loss: 0.0527\n",
      "Epoch 86: Batch 10/116 loss: 0.0868\n",
      "Epoch 86: Batch 11/116 loss: 0.0706\n",
      "Epoch 86: Batch 12/116 loss: 0.0521\n",
      "Epoch 86: Batch 13/116 loss: 0.0464\n",
      "Epoch 86: Batch 14/116 loss: 0.0570\n",
      "Epoch 86: Batch 15/116 loss: 0.0629\n",
      "Epoch 86: Batch 16/116 loss: 0.0517\n",
      "Epoch 86: Batch 17/116 loss: 0.0459\n",
      "Epoch 86: Batch 18/116 loss: 0.0612\n",
      "Epoch 86: Batch 19/116 loss: 0.0521\n",
      "Epoch 86: Batch 20/116 loss: 0.0626\n",
      "Epoch 86: Batch 21/116 loss: 0.0331\n",
      "Epoch 86: Batch 22/116 loss: 0.0480\n",
      "Epoch 86: Batch 23/116 loss: 0.0527\n",
      "Epoch 86: Batch 24/116 loss: 0.0576\n",
      "Epoch 86: Batch 25/116 loss: 0.0361\n",
      "Epoch 86: Batch 26/116 loss: 0.0524\n",
      "Epoch 86: Batch 27/116 loss: 0.0586\n",
      "Epoch 86: Batch 28/116 loss: 0.0754\n",
      "Epoch 86: Batch 29/116 loss: 0.0571\n",
      "Epoch 86: Batch 30/116 loss: 0.0605\n",
      "Epoch 86: Batch 31/116 loss: 0.0540\n",
      "Epoch 86: Batch 32/116 loss: 0.0516\n",
      "Epoch 86: Batch 33/116 loss: 0.0698\n",
      "Epoch 86: Batch 34/116 loss: 0.0503\n",
      "Epoch 86: Batch 35/116 loss: 0.0415\n",
      "Epoch 86: Batch 36/116 loss: 0.0395\n",
      "Epoch 86: Batch 37/116 loss: 0.0556\n",
      "Epoch 86: Batch 38/116 loss: 0.0401\n",
      "Epoch 86: Batch 39/116 loss: 0.0639\n",
      "Epoch 86: Batch 40/116 loss: 0.0522\n",
      "Epoch 86: Batch 41/116 loss: 0.0495\n",
      "Epoch 86: Batch 42/116 loss: 0.0797\n",
      "Epoch 86: Batch 43/116 loss: 0.0659\n",
      "Epoch 86: Batch 44/116 loss: 0.0629\n",
      "Epoch 86: Batch 45/116 loss: 0.0593\n",
      "Epoch 86: Batch 46/116 loss: 0.0640\n",
      "Epoch 86: Batch 47/116 loss: 0.0514\n",
      "Epoch 86: Batch 48/116 loss: 0.0787\n",
      "Epoch 86: Batch 49/116 loss: 0.0434\n",
      "Epoch 86: Batch 50/116 loss: 0.0605\n",
      "Epoch 86: Batch 51/116 loss: 0.0446\n",
      "Epoch 86: Batch 52/116 loss: 0.0765\n",
      "Epoch 86: Batch 53/116 loss: 0.0491\n",
      "Epoch 86: Batch 54/116 loss: 0.0553\n",
      "Epoch 86: Batch 55/116 loss: 0.0368\n",
      "Epoch 86: Batch 56/116 loss: 0.0753\n",
      "Epoch 86: Batch 57/116 loss: 0.0497\n",
      "Epoch 86: Batch 58/116 loss: 0.0532\n",
      "Epoch 86: Batch 59/116 loss: 0.0679\n",
      "Epoch 86: Batch 60/116 loss: 0.0417\n",
      "Epoch 86: Batch 61/116 loss: 0.0692\n",
      "Epoch 86: Batch 62/116 loss: 0.0423\n",
      "Epoch 86: Batch 63/116 loss: 0.0809\n",
      "Epoch 86: Batch 64/116 loss: 0.0489\n",
      "Epoch 86: Batch 65/116 loss: 0.0333\n",
      "Epoch 86: Batch 66/116 loss: 0.0556\n",
      "Epoch 86: Batch 67/116 loss: 0.0628\n",
      "Epoch 86: Batch 68/116 loss: 0.0505\n",
      "Epoch 86: Batch 69/116 loss: 0.0628\n",
      "Epoch 86: Batch 70/116 loss: 0.0570\n",
      "Epoch 86: Batch 71/116 loss: 0.0408\n",
      "Epoch 86: Batch 72/116 loss: 0.0438\n",
      "Epoch 86: Batch 73/116 loss: 0.0848\n",
      "Epoch 86: Batch 74/116 loss: 0.0538\n",
      "Epoch 86: Batch 75/116 loss: 0.0627\n",
      "Epoch 86: Batch 76/116 loss: 0.0560\n",
      "Epoch 86: Batch 77/116 loss: 0.0498\n",
      "Epoch 86: Batch 78/116 loss: 0.0718\n",
      "Epoch 86: Batch 79/116 loss: 0.0536\n",
      "Epoch 86: Batch 80/116 loss: 0.0589\n",
      "Epoch 86: Batch 81/116 loss: 0.0578\n",
      "Epoch 86: Batch 82/116 loss: 0.0541\n",
      "Epoch 86: Batch 83/116 loss: 0.0676\n",
      "Epoch 86: Batch 84/116 loss: 0.0442\n",
      "Epoch 86: Batch 85/116 loss: 0.0427\n",
      "Epoch 86: Batch 86/116 loss: 0.0653\n",
      "Epoch 86: Batch 87/116 loss: 0.0494\n",
      "Epoch 86: Batch 88/116 loss: 0.0619\n",
      "Epoch 86: Batch 89/116 loss: 0.0638\n",
      "Epoch 86: Batch 90/116 loss: 0.0594\n",
      "Epoch 86: Batch 91/116 loss: 0.0702\n",
      "Epoch 86: Batch 92/116 loss: 0.0593\n",
      "Epoch 86: Batch 93/116 loss: 0.0471\n",
      "Epoch 86: Batch 94/116 loss: 0.0430\n",
      "Epoch 86: Batch 95/116 loss: 0.0500\n",
      "Epoch 86: Batch 96/116 loss: 0.0652\n",
      "Epoch 86: Batch 97/116 loss: 0.0737\n",
      "Epoch 86: Batch 98/116 loss: 0.0472\n",
      "Epoch 86: Batch 99/116 loss: 0.0424\n",
      "Epoch 86: Batch 100/116 loss: 0.0565\n",
      "Epoch 86: Batch 101/116 loss: 0.0692\n",
      "Epoch 86: Batch 102/116 loss: 0.0688\n",
      "Epoch 86: Batch 103/116 loss: 0.0359\n",
      "Epoch 86: Batch 104/116 loss: 0.0579\n",
      "Epoch 86: Batch 105/116 loss: 0.0479\n",
      "Epoch 86: Batch 106/116 loss: 0.0502\n",
      "Epoch 86: Batch 107/116 loss: 0.0623\n",
      "Epoch 86: Batch 108/116 loss: 0.0414\n",
      "Epoch 86: Batch 109/116 loss: 0.0549\n",
      "Epoch 86: Batch 110/116 loss: 0.0546\n",
      "Epoch 86: Batch 111/116 loss: 0.0608\n",
      "Epoch 86: Batch 112/116 loss: 0.0645\n",
      "Epoch 86: Batch 113/116 loss: 0.0580\n",
      "Epoch 86: Batch 114/116 loss: 0.0380\n",
      "Epoch 86: Batch 115/116 loss: 0.0556\n",
      "Epoch 86: Batch 116/116 loss: 0.0424\n",
      "Epoch 86 train loss: 0.0562 valid loss: 0.0615\n",
      "performance reducing: 4\n",
      "Epoch 87: Batch 1/116 loss: 0.0539\n",
      "Epoch 87: Batch 2/116 loss: 0.0546\n",
      "Epoch 87: Batch 3/116 loss: 0.0585\n",
      "Epoch 87: Batch 4/116 loss: 0.0468\n",
      "Epoch 87: Batch 5/116 loss: 0.0536\n",
      "Epoch 87: Batch 6/116 loss: 0.0620\n",
      "Epoch 87: Batch 7/116 loss: 0.0388\n",
      "Epoch 87: Batch 8/116 loss: 0.0835\n",
      "Epoch 87: Batch 9/116 loss: 0.0389\n",
      "Epoch 87: Batch 10/116 loss: 0.0867\n",
      "Epoch 87: Batch 11/116 loss: 0.1107\n",
      "Epoch 87: Batch 12/116 loss: 0.0534\n",
      "Epoch 87: Batch 13/116 loss: 0.0632\n",
      "Epoch 87: Batch 14/116 loss: 0.0707\n",
      "Epoch 87: Batch 15/116 loss: 0.0602\n",
      "Epoch 87: Batch 16/116 loss: 0.0493\n",
      "Epoch 87: Batch 17/116 loss: 0.0767\n",
      "Epoch 87: Batch 18/116 loss: 0.0562\n",
      "Epoch 87: Batch 19/116 loss: 0.0783\n",
      "Epoch 87: Batch 20/116 loss: 0.0816\n",
      "Epoch 87: Batch 21/116 loss: 0.0603\n",
      "Epoch 87: Batch 22/116 loss: 0.0437\n",
      "Epoch 87: Batch 23/116 loss: 0.0512\n",
      "Epoch 87: Batch 24/116 loss: 0.0606\n",
      "Epoch 87: Batch 25/116 loss: 0.0654\n",
      "Epoch 87: Batch 26/116 loss: 0.0478\n",
      "Epoch 87: Batch 27/116 loss: 0.0550\n",
      "Epoch 87: Batch 28/116 loss: 0.0577\n",
      "Epoch 87: Batch 29/116 loss: 0.0716\n",
      "Epoch 87: Batch 30/116 loss: 0.0514\n",
      "Epoch 87: Batch 31/116 loss: 0.0511\n",
      "Epoch 87: Batch 32/116 loss: 0.0440\n",
      "Epoch 87: Batch 33/116 loss: 0.0436\n",
      "Epoch 87: Batch 34/116 loss: 0.0426\n",
      "Epoch 87: Batch 35/116 loss: 0.0672\n",
      "Epoch 87: Batch 36/116 loss: 0.0638\n",
      "Epoch 87: Batch 37/116 loss: 0.0611\n",
      "Epoch 87: Batch 38/116 loss: 0.0524\n",
      "Epoch 87: Batch 39/116 loss: 0.0450\n",
      "Epoch 87: Batch 40/116 loss: 0.0629\n",
      "Epoch 87: Batch 41/116 loss: 0.0532\n",
      "Epoch 87: Batch 42/116 loss: 0.0564\n",
      "Epoch 87: Batch 43/116 loss: 0.0564\n",
      "Epoch 87: Batch 44/116 loss: 0.0547\n",
      "Epoch 87: Batch 45/116 loss: 0.0526\n",
      "Epoch 87: Batch 46/116 loss: 0.0627\n",
      "Epoch 87: Batch 47/116 loss: 0.0410\n",
      "Epoch 87: Batch 48/116 loss: 0.0609\n",
      "Epoch 87: Batch 49/116 loss: 0.0558\n",
      "Epoch 87: Batch 50/116 loss: 0.0540\n",
      "Epoch 87: Batch 51/116 loss: 0.0483\n",
      "Epoch 87: Batch 52/116 loss: 0.0473\n",
      "Epoch 87: Batch 53/116 loss: 0.0424\n",
      "Epoch 87: Batch 54/116 loss: 0.0629\n",
      "Epoch 87: Batch 55/116 loss: 0.0697\n",
      "Epoch 87: Batch 56/116 loss: 0.0535\n",
      "Epoch 87: Batch 57/116 loss: 0.0574\n",
      "Epoch 87: Batch 58/116 loss: 0.0374\n",
      "Epoch 87: Batch 59/116 loss: 0.0500\n",
      "Epoch 87: Batch 60/116 loss: 0.0410\n",
      "Epoch 87: Batch 61/116 loss: 0.0591\n",
      "Epoch 87: Batch 62/116 loss: 0.0320\n",
      "Epoch 87: Batch 63/116 loss: 0.0523\n",
      "Epoch 87: Batch 64/116 loss: 0.0557\n",
      "Epoch 87: Batch 65/116 loss: 0.0513\n",
      "Epoch 87: Batch 66/116 loss: 0.0538\n",
      "Epoch 87: Batch 67/116 loss: 0.0586\n",
      "Epoch 87: Batch 68/116 loss: 0.0515\n",
      "Epoch 87: Batch 69/116 loss: 0.0656\n",
      "Epoch 87: Batch 70/116 loss: 0.0537\n",
      "Epoch 87: Batch 71/116 loss: 0.0539\n",
      "Epoch 87: Batch 72/116 loss: 0.0578\n",
      "Epoch 87: Batch 73/116 loss: 0.0747\n",
      "Epoch 87: Batch 74/116 loss: 0.0607\n",
      "Epoch 87: Batch 75/116 loss: 0.0398\n",
      "Epoch 87: Batch 76/116 loss: 0.0511\n",
      "Epoch 87: Batch 77/116 loss: 0.0610\n",
      "Epoch 87: Batch 78/116 loss: 0.0516\n",
      "Epoch 87: Batch 79/116 loss: 0.0864\n",
      "Epoch 87: Batch 80/116 loss: 0.0358\n",
      "Epoch 87: Batch 81/116 loss: 0.0597\n",
      "Epoch 87: Batch 82/116 loss: 0.0594\n",
      "Epoch 87: Batch 83/116 loss: 0.0639\n",
      "Epoch 87: Batch 84/116 loss: 0.0572\n",
      "Epoch 87: Batch 85/116 loss: 0.0397\n",
      "Epoch 87: Batch 86/116 loss: 0.0553\n",
      "Epoch 87: Batch 87/116 loss: 0.0461\n",
      "Epoch 87: Batch 88/116 loss: 0.0446\n",
      "Epoch 87: Batch 89/116 loss: 0.0450\n",
      "Epoch 87: Batch 90/116 loss: 0.0432\n",
      "Epoch 87: Batch 91/116 loss: 0.0866\n",
      "Epoch 87: Batch 92/116 loss: 0.0720\n",
      "Epoch 87: Batch 93/116 loss: 0.0564\n",
      "Epoch 87: Batch 94/116 loss: 0.0469\n",
      "Epoch 87: Batch 95/116 loss: 0.0473\n",
      "Epoch 87: Batch 96/116 loss: 0.0843\n",
      "Epoch 87: Batch 97/116 loss: 0.0430\n",
      "Epoch 87: Batch 98/116 loss: 0.0525\n",
      "Epoch 87: Batch 99/116 loss: 0.0398\n",
      "Epoch 87: Batch 100/116 loss: 0.0398\n",
      "Epoch 87: Batch 101/116 loss: 0.0596\n",
      "Epoch 87: Batch 102/116 loss: 0.0526\n",
      "Epoch 87: Batch 103/116 loss: 0.0648\n",
      "Epoch 87: Batch 104/116 loss: 0.0476\n",
      "Epoch 87: Batch 105/116 loss: 0.0522\n",
      "Epoch 87: Batch 106/116 loss: 0.0452\n",
      "Epoch 87: Batch 107/116 loss: 0.0575\n",
      "Epoch 87: Batch 108/116 loss: 0.0683\n",
      "Epoch 87: Batch 109/116 loss: 0.0555\n",
      "Epoch 87: Batch 110/116 loss: 0.0410\n",
      "Epoch 87: Batch 111/116 loss: 0.0426\n",
      "Epoch 87: Batch 112/116 loss: 0.0705\n",
      "Epoch 87: Batch 113/116 loss: 0.0664\n",
      "Epoch 87: Batch 114/116 loss: 0.0495\n",
      "Epoch 87: Batch 115/116 loss: 0.0492\n",
      "Epoch 87: Batch 116/116 loss: 0.0442\n",
      "Epoch 87 train loss: 0.0559 valid loss: 0.0572\n",
      "performance reducing: 5\n",
      "Epoch 88: Batch 1/116 loss: 0.0568\n",
      "Epoch 88: Batch 2/116 loss: 0.0439\n",
      "Epoch 88: Batch 3/116 loss: 0.0317\n",
      "Epoch 88: Batch 4/116 loss: 0.0484\n",
      "Epoch 88: Batch 5/116 loss: 0.0376\n",
      "Epoch 88: Batch 6/116 loss: 0.0539\n",
      "Epoch 88: Batch 7/116 loss: 0.0559\n",
      "Epoch 88: Batch 8/116 loss: 0.0550\n",
      "Epoch 88: Batch 9/116 loss: 0.0581\n",
      "Epoch 88: Batch 10/116 loss: 0.0645\n",
      "Epoch 88: Batch 11/116 loss: 0.0415\n",
      "Epoch 88: Batch 12/116 loss: 0.0393\n",
      "Epoch 88: Batch 13/116 loss: 0.0477\n",
      "Epoch 88: Batch 14/116 loss: 0.0608\n",
      "Epoch 88: Batch 15/116 loss: 0.0311\n",
      "Epoch 88: Batch 16/116 loss: 0.0685\n",
      "Epoch 88: Batch 17/116 loss: 0.0377\n",
      "Epoch 88: Batch 18/116 loss: 0.0501\n",
      "Epoch 88: Batch 19/116 loss: 0.0575\n",
      "Epoch 88: Batch 20/116 loss: 0.0493\n",
      "Epoch 88: Batch 21/116 loss: 0.0590\n",
      "Epoch 88: Batch 22/116 loss: 0.0565\n",
      "Epoch 88: Batch 23/116 loss: 0.0528\n",
      "Epoch 88: Batch 24/116 loss: 0.0551\n",
      "Epoch 88: Batch 25/116 loss: 0.0549\n",
      "Epoch 88: Batch 26/116 loss: 0.0469\n",
      "Epoch 88: Batch 27/116 loss: 0.0997\n",
      "Epoch 88: Batch 28/116 loss: 0.0951\n",
      "Epoch 88: Batch 29/116 loss: 0.0773\n",
      "Epoch 88: Batch 30/116 loss: 0.0837\n",
      "Epoch 88: Batch 31/116 loss: 0.0528\n",
      "Epoch 88: Batch 32/116 loss: 0.0492\n",
      "Epoch 88: Batch 33/116 loss: 0.0559\n",
      "Epoch 88: Batch 34/116 loss: 0.0441\n",
      "Epoch 88: Batch 35/116 loss: 0.0557\n",
      "Epoch 88: Batch 36/116 loss: 0.0484\n",
      "Epoch 88: Batch 37/116 loss: 0.0546\n",
      "Epoch 88: Batch 38/116 loss: 0.0666\n",
      "Epoch 88: Batch 39/116 loss: 0.0623\n",
      "Epoch 88: Batch 40/116 loss: 0.0584\n",
      "Epoch 88: Batch 41/116 loss: 0.0515\n",
      "Epoch 88: Batch 42/116 loss: 0.0452\n",
      "Epoch 88: Batch 43/116 loss: 0.0397\n",
      "Epoch 88: Batch 44/116 loss: 0.0686\n",
      "Epoch 88: Batch 45/116 loss: 0.0411\n",
      "Epoch 88: Batch 46/116 loss: 0.0672\n",
      "Epoch 88: Batch 47/116 loss: 0.0583\n",
      "Epoch 88: Batch 48/116 loss: 0.0951\n",
      "Epoch 88: Batch 49/116 loss: 0.0598\n",
      "Epoch 88: Batch 50/116 loss: 0.0571\n",
      "Epoch 88: Batch 51/116 loss: 0.0749\n",
      "Epoch 88: Batch 52/116 loss: 0.0548\n",
      "Epoch 88: Batch 53/116 loss: 0.0627\n",
      "Epoch 88: Batch 54/116 loss: 0.0383\n",
      "Epoch 88: Batch 55/116 loss: 0.0560\n",
      "Epoch 88: Batch 56/116 loss: 0.0539\n",
      "Epoch 88: Batch 57/116 loss: 0.0595\n",
      "Epoch 88: Batch 58/116 loss: 0.0629\n",
      "Epoch 88: Batch 59/116 loss: 0.0636\n",
      "Epoch 88: Batch 60/116 loss: 0.0454\n",
      "Epoch 88: Batch 61/116 loss: 0.0498\n",
      "Epoch 88: Batch 62/116 loss: 0.0366\n",
      "Epoch 88: Batch 63/116 loss: 0.0460\n",
      "Epoch 88: Batch 64/116 loss: 0.0564\n",
      "Epoch 88: Batch 65/116 loss: 0.0506\n",
      "Epoch 88: Batch 66/116 loss: 0.0708\n",
      "Epoch 88: Batch 67/116 loss: 0.0420\n",
      "Epoch 88: Batch 68/116 loss: 0.0472\n",
      "Epoch 88: Batch 69/116 loss: 0.0421\n",
      "Epoch 88: Batch 70/116 loss: 0.0422\n",
      "Epoch 88: Batch 71/116 loss: 0.0672\n",
      "Epoch 88: Batch 72/116 loss: 0.0872\n",
      "Epoch 88: Batch 73/116 loss: 0.0652\n",
      "Epoch 88: Batch 74/116 loss: 0.0585\n",
      "Epoch 88: Batch 75/116 loss: 0.0599\n",
      "Epoch 88: Batch 76/116 loss: 0.0560\n",
      "Epoch 88: Batch 77/116 loss: 0.0434\n",
      "Epoch 88: Batch 78/116 loss: 0.0357\n",
      "Epoch 88: Batch 79/116 loss: 0.0511\n",
      "Epoch 88: Batch 80/116 loss: 0.0705\n",
      "Epoch 88: Batch 81/116 loss: 0.0400\n",
      "Epoch 88: Batch 82/116 loss: 0.0423\n",
      "Epoch 88: Batch 83/116 loss: 0.1025\n",
      "Epoch 88: Batch 84/116 loss: 0.0595\n",
      "Epoch 88: Batch 85/116 loss: 0.0543\n",
      "Epoch 88: Batch 86/116 loss: 0.0719\n",
      "Epoch 88: Batch 87/116 loss: 0.0615\n",
      "Epoch 88: Batch 88/116 loss: 0.0587\n",
      "Epoch 88: Batch 89/116 loss: 0.0407\n",
      "Epoch 88: Batch 90/116 loss: 0.0649\n",
      "Epoch 88: Batch 91/116 loss: 0.0419\n",
      "Epoch 88: Batch 92/116 loss: 0.0430\n",
      "Epoch 88: Batch 93/116 loss: 0.0364\n",
      "Epoch 88: Batch 94/116 loss: 0.0674\n",
      "Epoch 88: Batch 95/116 loss: 0.0392\n",
      "Epoch 88: Batch 96/116 loss: 0.0481\n",
      "Epoch 88: Batch 97/116 loss: 0.0644\n",
      "Epoch 88: Batch 98/116 loss: 0.0858\n",
      "Epoch 88: Batch 99/116 loss: 0.0479\n",
      "Epoch 88: Batch 100/116 loss: 0.0457\n",
      "Epoch 88: Batch 101/116 loss: 0.0532\n",
      "Epoch 88: Batch 102/116 loss: 0.0526\n",
      "Epoch 88: Batch 103/116 loss: 0.0613\n",
      "Epoch 88: Batch 104/116 loss: 0.0471\n",
      "Epoch 88: Batch 105/116 loss: 0.0357\n",
      "Epoch 88: Batch 106/116 loss: 0.0556\n",
      "Epoch 88: Batch 107/116 loss: 0.0833\n",
      "Epoch 88: Batch 108/116 loss: 0.0373\n",
      "Epoch 88: Batch 109/116 loss: 0.0389\n",
      "Epoch 88: Batch 110/116 loss: 0.0658\n",
      "Epoch 88: Batch 111/116 loss: 0.0416\n",
      "Epoch 88: Batch 112/116 loss: 0.0626\n",
      "Epoch 88: Batch 113/116 loss: 0.0523\n",
      "Epoch 88: Batch 114/116 loss: 0.0410\n",
      "Epoch 88: Batch 115/116 loss: 0.0643\n",
      "Epoch 88: Batch 116/116 loss: 0.0382\n",
      "Epoch 88 train loss: 0.0552 valid loss: 0.0610\n",
      "performance reducing: 6\n",
      "Epoch 89: Batch 1/116 loss: 0.0353\n",
      "Epoch 89: Batch 2/116 loss: 0.0485\n",
      "Epoch 89: Batch 3/116 loss: 0.0488\n",
      "Epoch 89: Batch 4/116 loss: 0.0947\n",
      "Epoch 89: Batch 5/116 loss: 0.0551\n",
      "Epoch 89: Batch 6/116 loss: 0.0417\n",
      "Epoch 89: Batch 7/116 loss: 0.0789\n",
      "Epoch 89: Batch 8/116 loss: 0.0445\n",
      "Epoch 89: Batch 9/116 loss: 0.0776\n",
      "Epoch 89: Batch 10/116 loss: 0.0552\n",
      "Epoch 89: Batch 11/116 loss: 0.0760\n",
      "Epoch 89: Batch 12/116 loss: 0.0597\n",
      "Epoch 89: Batch 13/116 loss: 0.0711\n",
      "Epoch 89: Batch 14/116 loss: 0.0672\n",
      "Epoch 89: Batch 15/116 loss: 0.0362\n",
      "Epoch 89: Batch 16/116 loss: 0.0642\n",
      "Epoch 89: Batch 17/116 loss: 0.0372\n",
      "Epoch 89: Batch 18/116 loss: 0.0485\n",
      "Epoch 89: Batch 19/116 loss: 0.0699\n",
      "Epoch 89: Batch 20/116 loss: 0.0727\n",
      "Epoch 89: Batch 21/116 loss: 0.0548\n",
      "Epoch 89: Batch 22/116 loss: 0.0566\n",
      "Epoch 89: Batch 23/116 loss: 0.0428\n",
      "Epoch 89: Batch 24/116 loss: 0.0395\n",
      "Epoch 89: Batch 25/116 loss: 0.0442\n",
      "Epoch 89: Batch 26/116 loss: 0.0536\n",
      "Epoch 89: Batch 27/116 loss: 0.0535\n",
      "Epoch 89: Batch 28/116 loss: 0.0551\n",
      "Epoch 89: Batch 29/116 loss: 0.0500\n",
      "Epoch 89: Batch 30/116 loss: 0.0695\n",
      "Epoch 89: Batch 31/116 loss: 0.0566\n",
      "Epoch 89: Batch 32/116 loss: 0.0587\n",
      "Epoch 89: Batch 33/116 loss: 0.0510\n",
      "Epoch 89: Batch 34/116 loss: 0.0501\n",
      "Epoch 89: Batch 35/116 loss: 0.0549\n",
      "Epoch 89: Batch 36/116 loss: 0.0575\n",
      "Epoch 89: Batch 37/116 loss: 0.0639\n",
      "Epoch 89: Batch 38/116 loss: 0.0625\n",
      "Epoch 89: Batch 39/116 loss: 0.0387\n",
      "Epoch 89: Batch 40/116 loss: 0.0497\n",
      "Epoch 89: Batch 41/116 loss: 0.0644\n",
      "Epoch 89: Batch 42/116 loss: 0.0556\n",
      "Epoch 89: Batch 43/116 loss: 0.0547\n",
      "Epoch 89: Batch 44/116 loss: 0.0744\n",
      "Epoch 89: Batch 45/116 loss: 0.0284\n",
      "Epoch 89: Batch 46/116 loss: 0.0568\n",
      "Epoch 89: Batch 47/116 loss: 0.0449\n",
      "Epoch 89: Batch 48/116 loss: 0.0386\n",
      "Epoch 89: Batch 49/116 loss: 0.0512\n",
      "Epoch 89: Batch 50/116 loss: 0.0507\n",
      "Epoch 89: Batch 51/116 loss: 0.0717\n",
      "Epoch 89: Batch 52/116 loss: 0.0517\n",
      "Epoch 89: Batch 53/116 loss: 0.0766\n",
      "Epoch 89: Batch 54/116 loss: 0.0600\n",
      "Epoch 89: Batch 55/116 loss: 0.0496\n",
      "Epoch 89: Batch 56/116 loss: 0.0786\n",
      "Epoch 89: Batch 57/116 loss: 0.0488\n",
      "Epoch 89: Batch 58/116 loss: 0.0509\n",
      "Epoch 89: Batch 59/116 loss: 0.0523\n",
      "Epoch 89: Batch 60/116 loss: 0.0688\n",
      "Epoch 89: Batch 61/116 loss: 0.0818\n",
      "Epoch 89: Batch 62/116 loss: 0.0469\n",
      "Epoch 89: Batch 63/116 loss: 0.0563\n",
      "Epoch 89: Batch 64/116 loss: 0.0480\n",
      "Epoch 89: Batch 65/116 loss: 0.0703\n",
      "Epoch 89: Batch 66/116 loss: 0.0368\n",
      "Epoch 89: Batch 67/116 loss: 0.0452\n",
      "Epoch 89: Batch 68/116 loss: 0.0444\n",
      "Epoch 89: Batch 69/116 loss: 0.0439\n",
      "Epoch 89: Batch 70/116 loss: 0.0584\n",
      "Epoch 89: Batch 71/116 loss: 0.0679\n",
      "Epoch 89: Batch 72/116 loss: 0.0673\n",
      "Epoch 89: Batch 73/116 loss: 0.0628\n",
      "Epoch 89: Batch 74/116 loss: 0.0558\n",
      "Epoch 89: Batch 75/116 loss: 0.0348\n",
      "Epoch 89: Batch 76/116 loss: 0.0571\n",
      "Epoch 89: Batch 77/116 loss: 0.0664\n",
      "Epoch 89: Batch 78/116 loss: 0.0460\n",
      "Epoch 89: Batch 79/116 loss: 0.0624\n",
      "Epoch 89: Batch 80/116 loss: 0.0304\n",
      "Epoch 89: Batch 81/116 loss: 0.0620\n",
      "Epoch 89: Batch 82/116 loss: 0.0632\n",
      "Epoch 89: Batch 83/116 loss: 0.0592\n",
      "Epoch 89: Batch 84/116 loss: 0.0478\n",
      "Epoch 89: Batch 85/116 loss: 0.0344\n",
      "Epoch 89: Batch 86/116 loss: 0.0377\n",
      "Epoch 89: Batch 87/116 loss: 0.0542\n",
      "Epoch 89: Batch 88/116 loss: 0.0721\n",
      "Epoch 89: Batch 89/116 loss: 0.0543\n",
      "Epoch 89: Batch 90/116 loss: 0.0383\n",
      "Epoch 89: Batch 91/116 loss: 0.0433\n",
      "Epoch 89: Batch 92/116 loss: 0.0541\n",
      "Epoch 89: Batch 93/116 loss: 0.0576\n",
      "Epoch 89: Batch 94/116 loss: 0.0357\n",
      "Epoch 89: Batch 95/116 loss: 0.0487\n",
      "Epoch 89: Batch 96/116 loss: 0.0773\n",
      "Epoch 89: Batch 97/116 loss: 0.0527\n",
      "Epoch 89: Batch 98/116 loss: 0.0375\n",
      "Epoch 89: Batch 99/116 loss: 0.0528\n",
      "Epoch 89: Batch 100/116 loss: 0.0538\n",
      "Epoch 89: Batch 101/116 loss: 0.0487\n",
      "Epoch 89: Batch 102/116 loss: 0.0575\n",
      "Epoch 89: Batch 103/116 loss: 0.0523\n",
      "Epoch 89: Batch 104/116 loss: 0.0426\n",
      "Epoch 89: Batch 105/116 loss: 0.0715\n",
      "Epoch 89: Batch 106/116 loss: 0.0370\n",
      "Epoch 89: Batch 107/116 loss: 0.0487\n",
      "Epoch 89: Batch 108/116 loss: 0.0476\n",
      "Epoch 89: Batch 109/116 loss: 0.0513\n",
      "Epoch 89: Batch 110/116 loss: 0.0714\n",
      "Epoch 89: Batch 111/116 loss: 0.0558\n",
      "Epoch 89: Batch 112/116 loss: 0.0460\n",
      "Epoch 89: Batch 113/116 loss: 0.0646\n",
      "Epoch 89: Batch 114/116 loss: 0.0522\n",
      "Epoch 89: Batch 115/116 loss: 0.0358\n",
      "Epoch 89: Batch 116/116 loss: 0.0477\n",
      "Epoch 89 train loss: 0.0545 valid loss: 0.0626\n",
      "performance reducing: 7\n",
      "Epoch 90: Batch 1/116 loss: 0.0408\n",
      "Epoch 90: Batch 2/116 loss: 0.0713\n",
      "Epoch 90: Batch 3/116 loss: 0.0326\n",
      "Epoch 90: Batch 4/116 loss: 0.0315\n",
      "Epoch 90: Batch 5/116 loss: 0.0599\n",
      "Epoch 90: Batch 6/116 loss: 0.0460\n",
      "Epoch 90: Batch 7/116 loss: 0.0502\n",
      "Epoch 90: Batch 8/116 loss: 0.0406\n",
      "Epoch 90: Batch 9/116 loss: 0.0345\n",
      "Epoch 90: Batch 10/116 loss: 0.0371\n",
      "Epoch 90: Batch 11/116 loss: 0.0567\n",
      "Epoch 90: Batch 12/116 loss: 0.0580\n",
      "Epoch 90: Batch 13/116 loss: 0.0423\n",
      "Epoch 90: Batch 14/116 loss: 0.0564\n",
      "Epoch 90: Batch 15/116 loss: 0.0512\n",
      "Epoch 90: Batch 16/116 loss: 0.0613\n",
      "Epoch 90: Batch 17/116 loss: 0.0583\n",
      "Epoch 90: Batch 18/116 loss: 0.0545\n",
      "Epoch 90: Batch 19/116 loss: 0.0387\n",
      "Epoch 90: Batch 20/116 loss: 0.0859\n",
      "Epoch 90: Batch 21/116 loss: 0.0570\n",
      "Epoch 90: Batch 22/116 loss: 0.0489\n",
      "Epoch 90: Batch 23/116 loss: 0.0902\n",
      "Epoch 90: Batch 24/116 loss: 0.0427\n",
      "Epoch 90: Batch 25/116 loss: 0.0415\n",
      "Epoch 90: Batch 26/116 loss: 0.0627\n",
      "Epoch 90: Batch 27/116 loss: 0.0680\n",
      "Epoch 90: Batch 28/116 loss: 0.0377\n",
      "Epoch 90: Batch 29/116 loss: 0.0840\n",
      "Epoch 90: Batch 30/116 loss: 0.0669\n",
      "Epoch 90: Batch 31/116 loss: 0.0642\n",
      "Epoch 90: Batch 32/116 loss: 0.0540\n",
      "Epoch 90: Batch 33/116 loss: 0.0510\n",
      "Epoch 90: Batch 34/116 loss: 0.0611\n",
      "Epoch 90: Batch 35/116 loss: 0.0429\n",
      "Epoch 90: Batch 36/116 loss: 0.0538\n",
      "Epoch 90: Batch 37/116 loss: 0.0605\n",
      "Epoch 90: Batch 38/116 loss: 0.0770\n",
      "Epoch 90: Batch 39/116 loss: 0.0428\n",
      "Epoch 90: Batch 40/116 loss: 0.0821\n",
      "Epoch 90: Batch 41/116 loss: 0.0380\n",
      "Epoch 90: Batch 42/116 loss: 0.0612\n",
      "Epoch 90: Batch 43/116 loss: 0.0608\n",
      "Epoch 90: Batch 44/116 loss: 0.0626\n",
      "Epoch 90: Batch 45/116 loss: 0.0625\n",
      "Epoch 90: Batch 46/116 loss: 0.0582\n",
      "Epoch 90: Batch 47/116 loss: 0.0578\n",
      "Epoch 90: Batch 48/116 loss: 0.0536\n",
      "Epoch 90: Batch 49/116 loss: 0.0466\n",
      "Epoch 90: Batch 50/116 loss: 0.0459\n",
      "Epoch 90: Batch 51/116 loss: 0.0341\n",
      "Epoch 90: Batch 52/116 loss: 0.0553\n",
      "Epoch 90: Batch 53/116 loss: 0.0773\n",
      "Epoch 90: Batch 54/116 loss: 0.0460\n",
      "Epoch 90: Batch 55/116 loss: 0.0491\n",
      "Epoch 90: Batch 56/116 loss: 0.0580\n",
      "Epoch 90: Batch 57/116 loss: 0.0505\n",
      "Epoch 90: Batch 58/116 loss: 0.0634\n",
      "Epoch 90: Batch 59/116 loss: 0.0489\n",
      "Epoch 90: Batch 60/116 loss: 0.0764\n",
      "Epoch 90: Batch 61/116 loss: 0.0670\n",
      "Epoch 90: Batch 62/116 loss: 0.0428\n",
      "Epoch 90: Batch 63/116 loss: 0.0566\n",
      "Epoch 90: Batch 64/116 loss: 0.0530\n",
      "Epoch 90: Batch 65/116 loss: 0.0418\n",
      "Epoch 90: Batch 66/116 loss: 0.0420\n",
      "Epoch 90: Batch 67/116 loss: 0.0465\n",
      "Epoch 90: Batch 68/116 loss: 0.0501\n",
      "Epoch 90: Batch 69/116 loss: 0.0475\n",
      "Epoch 90: Batch 70/116 loss: 0.0681\n",
      "Epoch 90: Batch 71/116 loss: 0.0479\n",
      "Epoch 90: Batch 72/116 loss: 0.0542\n",
      "Epoch 90: Batch 73/116 loss: 0.0443\n",
      "Epoch 90: Batch 74/116 loss: 0.0302\n",
      "Epoch 90: Batch 75/116 loss: 0.0646\n",
      "Epoch 90: Batch 76/116 loss: 0.0538\n",
      "Epoch 90: Batch 77/116 loss: 0.0518\n",
      "Epoch 90: Batch 78/116 loss: 0.0516\n",
      "Epoch 90: Batch 79/116 loss: 0.0528\n",
      "Epoch 90: Batch 80/116 loss: 0.0850\n",
      "Epoch 90: Batch 81/116 loss: 0.0450\n",
      "Epoch 90: Batch 82/116 loss: 0.0719\n",
      "Epoch 90: Batch 83/116 loss: 0.0432\n",
      "Epoch 90: Batch 84/116 loss: 0.0516\n",
      "Epoch 90: Batch 85/116 loss: 0.0393\n",
      "Epoch 90: Batch 86/116 loss: 0.0745\n",
      "Epoch 90: Batch 87/116 loss: 0.0749\n",
      "Epoch 90: Batch 88/116 loss: 0.0719\n",
      "Epoch 90: Batch 89/116 loss: 0.0552\n",
      "Epoch 90: Batch 90/116 loss: 0.0406\n",
      "Epoch 90: Batch 91/116 loss: 0.0907\n",
      "Epoch 90: Batch 92/116 loss: 0.0586\n",
      "Epoch 90: Batch 93/116 loss: 0.0534\n",
      "Epoch 90: Batch 94/116 loss: 0.0500\n",
      "Epoch 90: Batch 95/116 loss: 0.0350\n",
      "Epoch 90: Batch 96/116 loss: 0.0753\n",
      "Epoch 90: Batch 97/116 loss: 0.0590\n",
      "Epoch 90: Batch 98/116 loss: 0.0444\n",
      "Epoch 90: Batch 99/116 loss: 0.0848\n",
      "Epoch 90: Batch 100/116 loss: 0.0420\n",
      "Epoch 90: Batch 101/116 loss: 0.0595\n",
      "Epoch 90: Batch 102/116 loss: 0.0746\n",
      "Epoch 90: Batch 103/116 loss: 0.0558\n",
      "Epoch 90: Batch 104/116 loss: 0.0454\n",
      "Epoch 90: Batch 105/116 loss: 0.0681\n",
      "Epoch 90: Batch 106/116 loss: 0.0627\n",
      "Epoch 90: Batch 107/116 loss: 0.0612\n",
      "Epoch 90: Batch 108/116 loss: 0.0512\n",
      "Epoch 90: Batch 109/116 loss: 0.0432\n",
      "Epoch 90: Batch 110/116 loss: 0.0469\n",
      "Epoch 90: Batch 111/116 loss: 0.0464\n",
      "Epoch 90: Batch 112/116 loss: 0.0494\n",
      "Epoch 90: Batch 113/116 loss: 0.0561\n",
      "Epoch 90: Batch 114/116 loss: 0.0460\n",
      "Epoch 90: Batch 115/116 loss: 0.0419\n",
      "Epoch 90: Batch 116/116 loss: 0.0765\n",
      "Epoch 90 train loss: 0.0552 valid loss: 0.0663\n",
      "performance reducing: 8\n",
      "Epoch 91: Batch 1/116 loss: 0.0483\n",
      "Epoch 91: Batch 2/116 loss: 0.0524\n",
      "Epoch 91: Batch 3/116 loss: 0.0580\n",
      "Epoch 91: Batch 4/116 loss: 0.0422\n",
      "Epoch 91: Batch 5/116 loss: 0.0527\n",
      "Epoch 91: Batch 6/116 loss: 0.0411\n",
      "Epoch 91: Batch 7/116 loss: 0.0495\n",
      "Epoch 91: Batch 8/116 loss: 0.0676\n",
      "Epoch 91: Batch 9/116 loss: 0.0489\n",
      "Epoch 91: Batch 10/116 loss: 0.0394\n",
      "Epoch 91: Batch 11/116 loss: 0.0635\n",
      "Epoch 91: Batch 12/116 loss: 0.0580\n",
      "Epoch 91: Batch 13/116 loss: 0.0543\n",
      "Epoch 91: Batch 14/116 loss: 0.0719\n",
      "Epoch 91: Batch 15/116 loss: 0.0477\n",
      "Epoch 91: Batch 16/116 loss: 0.0497\n",
      "Epoch 91: Batch 17/116 loss: 0.0423\n",
      "Epoch 91: Batch 18/116 loss: 0.0583\n",
      "Epoch 91: Batch 19/116 loss: 0.0459\n",
      "Epoch 91: Batch 20/116 loss: 0.0406\n",
      "Epoch 91: Batch 21/116 loss: 0.0584\n",
      "Epoch 91: Batch 22/116 loss: 0.0494\n",
      "Epoch 91: Batch 23/116 loss: 0.0408\n",
      "Epoch 91: Batch 24/116 loss: 0.0673\n",
      "Epoch 91: Batch 25/116 loss: 0.0494\n",
      "Epoch 91: Batch 26/116 loss: 0.0555\n",
      "Epoch 91: Batch 27/116 loss: 0.0447\n",
      "Epoch 91: Batch 28/116 loss: 0.0519\n",
      "Epoch 91: Batch 29/116 loss: 0.0637\n",
      "Epoch 91: Batch 30/116 loss: 0.0443\n",
      "Epoch 91: Batch 31/116 loss: 0.0445\n",
      "Epoch 91: Batch 32/116 loss: 0.0523\n",
      "Epoch 91: Batch 33/116 loss: 0.0982\n",
      "Epoch 91: Batch 34/116 loss: 0.0628\n",
      "Epoch 91: Batch 35/116 loss: 0.0513\n",
      "Epoch 91: Batch 36/116 loss: 0.0435\n",
      "Epoch 91: Batch 37/116 loss: 0.0535\n",
      "Epoch 91: Batch 38/116 loss: 0.0456\n",
      "Epoch 91: Batch 39/116 loss: 0.0469\n",
      "Epoch 91: Batch 40/116 loss: 0.0516\n",
      "Epoch 91: Batch 41/116 loss: 0.0575\n",
      "Epoch 91: Batch 42/116 loss: 0.0527\n",
      "Epoch 91: Batch 43/116 loss: 0.0549\n",
      "Epoch 91: Batch 44/116 loss: 0.0442\n",
      "Epoch 91: Batch 45/116 loss: 0.0702\n",
      "Epoch 91: Batch 46/116 loss: 0.0468\n",
      "Epoch 91: Batch 47/116 loss: 0.0537\n",
      "Epoch 91: Batch 48/116 loss: 0.0298\n",
      "Epoch 91: Batch 49/116 loss: 0.0642\n",
      "Epoch 91: Batch 50/116 loss: 0.0447\n",
      "Epoch 91: Batch 51/116 loss: 0.0531\n",
      "Epoch 91: Batch 52/116 loss: 0.0453\n",
      "Epoch 91: Batch 53/116 loss: 0.0687\n",
      "Epoch 91: Batch 54/116 loss: 0.0635\n",
      "Epoch 91: Batch 55/116 loss: 0.0576\n",
      "Epoch 91: Batch 56/116 loss: 0.0764\n",
      "Epoch 91: Batch 57/116 loss: 0.0495\n",
      "Epoch 91: Batch 58/116 loss: 0.0714\n",
      "Epoch 91: Batch 59/116 loss: 0.0489\n",
      "Epoch 91: Batch 60/116 loss: 0.0750\n",
      "Epoch 91: Batch 61/116 loss: 0.0336\n",
      "Epoch 91: Batch 62/116 loss: 0.0626\n",
      "Epoch 91: Batch 63/116 loss: 0.0549\n",
      "Epoch 91: Batch 64/116 loss: 0.0376\n",
      "Epoch 91: Batch 65/116 loss: 0.0472\n",
      "Epoch 91: Batch 66/116 loss: 0.0371\n",
      "Epoch 91: Batch 67/116 loss: 0.0524\n",
      "Epoch 91: Batch 68/116 loss: 0.0637\n",
      "Epoch 91: Batch 69/116 loss: 0.0578\n",
      "Epoch 91: Batch 70/116 loss: 0.0724\n",
      "Epoch 91: Batch 71/116 loss: 0.0567\n",
      "Epoch 91: Batch 72/116 loss: 0.0590\n",
      "Epoch 91: Batch 73/116 loss: 0.0382\n",
      "Epoch 91: Batch 74/116 loss: 0.1199\n",
      "Epoch 91: Batch 75/116 loss: 0.0476\n",
      "Epoch 91: Batch 76/116 loss: 0.0732\n",
      "Epoch 91: Batch 77/116 loss: 0.0548\n",
      "Epoch 91: Batch 78/116 loss: 0.0581\n",
      "Epoch 91: Batch 79/116 loss: 0.0473\n",
      "Epoch 91: Batch 80/116 loss: 0.0822\n",
      "Epoch 91: Batch 81/116 loss: 0.0696\n",
      "Epoch 91: Batch 82/116 loss: 0.0662\n",
      "Epoch 91: Batch 83/116 loss: 0.0488\n",
      "Epoch 91: Batch 84/116 loss: 0.0565\n",
      "Epoch 91: Batch 85/116 loss: 0.0505\n",
      "Epoch 91: Batch 86/116 loss: 0.0464\n",
      "Epoch 91: Batch 87/116 loss: 0.0451\n",
      "Epoch 91: Batch 88/116 loss: 0.0604\n",
      "Epoch 91: Batch 89/116 loss: 0.0672\n",
      "Epoch 91: Batch 90/116 loss: 0.0484\n",
      "Epoch 91: Batch 91/116 loss: 0.0607\n",
      "Epoch 91: Batch 92/116 loss: 0.0567\n",
      "Epoch 91: Batch 93/116 loss: 0.0406\n",
      "Epoch 91: Batch 94/116 loss: 0.0465\n",
      "Epoch 91: Batch 95/116 loss: 0.0401\n",
      "Epoch 91: Batch 96/116 loss: 0.0510\n",
      "Epoch 91: Batch 97/116 loss: 0.0520\n",
      "Epoch 91: Batch 98/116 loss: 0.0506\n",
      "Epoch 91: Batch 99/116 loss: 0.0536\n",
      "Epoch 91: Batch 100/116 loss: 0.0475\n",
      "Epoch 91: Batch 101/116 loss: 0.0556\n",
      "Epoch 91: Batch 102/116 loss: 0.0377\n",
      "Epoch 91: Batch 103/116 loss: 0.0644\n",
      "Epoch 91: Batch 104/116 loss: 0.0602\n",
      "Epoch 91: Batch 105/116 loss: 0.0646\n",
      "Epoch 91: Batch 106/116 loss: 0.0662\n",
      "Epoch 91: Batch 107/116 loss: 0.0549\n",
      "Epoch 91: Batch 108/116 loss: 0.0640\n",
      "Epoch 91: Batch 109/116 loss: 0.0548\n",
      "Epoch 91: Batch 110/116 loss: 0.0326\n",
      "Epoch 91: Batch 111/116 loss: 0.0662\n",
      "Epoch 91: Batch 112/116 loss: 0.0652\n",
      "Epoch 91: Batch 113/116 loss: 0.0539\n",
      "Epoch 91: Batch 114/116 loss: 0.0735\n",
      "Epoch 91: Batch 115/116 loss: 0.0593\n",
      "Epoch 91: Batch 116/116 loss: 0.0317\n",
      "Epoch 91 train loss: 0.0548 valid loss: 0.0546\n",
      "Epoch 92: Batch 1/116 loss: 0.0510\n",
      "Epoch 92: Batch 2/116 loss: 0.0845\n",
      "Epoch 92: Batch 3/116 loss: 0.0452\n",
      "Epoch 92: Batch 4/116 loss: 0.0444\n",
      "Epoch 92: Batch 5/116 loss: 0.0507\n",
      "Epoch 92: Batch 6/116 loss: 0.0700\n",
      "Epoch 92: Batch 7/116 loss: 0.0593\n",
      "Epoch 92: Batch 8/116 loss: 0.0471\n",
      "Epoch 92: Batch 9/116 loss: 0.0513\n",
      "Epoch 92: Batch 10/116 loss: 0.0512\n",
      "Epoch 92: Batch 11/116 loss: 0.0568\n",
      "Epoch 92: Batch 12/116 loss: 0.0606\n",
      "Epoch 92: Batch 13/116 loss: 0.0482\n",
      "Epoch 92: Batch 14/116 loss: 0.0911\n",
      "Epoch 92: Batch 15/116 loss: 0.0420\n",
      "Epoch 92: Batch 16/116 loss: 0.0470\n",
      "Epoch 92: Batch 17/116 loss: 0.0640\n",
      "Epoch 92: Batch 18/116 loss: 0.0743\n",
      "Epoch 92: Batch 19/116 loss: 0.0527\n",
      "Epoch 92: Batch 20/116 loss: 0.0727\n",
      "Epoch 92: Batch 21/116 loss: 0.0683\n",
      "Epoch 92: Batch 22/116 loss: 0.0684\n",
      "Epoch 92: Batch 23/116 loss: 0.0486\n",
      "Epoch 92: Batch 24/116 loss: 0.0504\n",
      "Epoch 92: Batch 25/116 loss: 0.0457\n",
      "Epoch 92: Batch 26/116 loss: 0.0633\n",
      "Epoch 92: Batch 27/116 loss: 0.0675\n",
      "Epoch 92: Batch 28/116 loss: 0.0271\n",
      "Epoch 92: Batch 29/116 loss: 0.0520\n",
      "Epoch 92: Batch 30/116 loss: 0.0463\n",
      "Epoch 92: Batch 31/116 loss: 0.0398\n",
      "Epoch 92: Batch 32/116 loss: 0.0404\n",
      "Epoch 92: Batch 33/116 loss: 0.0840\n",
      "Epoch 92: Batch 34/116 loss: 0.0576\n",
      "Epoch 92: Batch 35/116 loss: 0.0413\n",
      "Epoch 92: Batch 36/116 loss: 0.0436\n",
      "Epoch 92: Batch 37/116 loss: 0.0705\n",
      "Epoch 92: Batch 38/116 loss: 0.0391\n",
      "Epoch 92: Batch 39/116 loss: 0.0463\n",
      "Epoch 92: Batch 40/116 loss: 0.0517\n",
      "Epoch 92: Batch 41/116 loss: 0.0433\n",
      "Epoch 92: Batch 42/116 loss: 0.0809\n",
      "Epoch 92: Batch 43/116 loss: 0.0479\n",
      "Epoch 92: Batch 44/116 loss: 0.0395\n",
      "Epoch 92: Batch 45/116 loss: 0.0620\n",
      "Epoch 92: Batch 46/116 loss: 0.0530\n",
      "Epoch 92: Batch 47/116 loss: 0.0330\n",
      "Epoch 92: Batch 48/116 loss: 0.0562\n",
      "Epoch 92: Batch 49/116 loss: 0.0722\n",
      "Epoch 92: Batch 50/116 loss: 0.0539\n",
      "Epoch 92: Batch 51/116 loss: 0.0504\n",
      "Epoch 92: Batch 52/116 loss: 0.0605\n",
      "Epoch 92: Batch 53/116 loss: 0.0572\n",
      "Epoch 92: Batch 54/116 loss: 0.0491\n",
      "Epoch 92: Batch 55/116 loss: 0.0436\n",
      "Epoch 92: Batch 56/116 loss: 0.0338\n",
      "Epoch 92: Batch 57/116 loss: 0.0711\n",
      "Epoch 92: Batch 58/116 loss: 0.0499\n",
      "Epoch 92: Batch 59/116 loss: 0.0482\n",
      "Epoch 92: Batch 60/116 loss: 0.0513\n",
      "Epoch 92: Batch 61/116 loss: 0.0427\n",
      "Epoch 92: Batch 62/116 loss: 0.0393\n",
      "Epoch 92: Batch 63/116 loss: 0.0408\n",
      "Epoch 92: Batch 64/116 loss: 0.0555\n",
      "Epoch 92: Batch 65/116 loss: 0.0510\n",
      "Epoch 92: Batch 66/116 loss: 0.0517\n",
      "Epoch 92: Batch 67/116 loss: 0.0516\n",
      "Epoch 92: Batch 68/116 loss: 0.0618\n",
      "Epoch 92: Batch 69/116 loss: 0.0497\n",
      "Epoch 92: Batch 70/116 loss: 0.0579\n",
      "Epoch 92: Batch 71/116 loss: 0.0552\n",
      "Epoch 92: Batch 72/116 loss: 0.0443\n",
      "Epoch 92: Batch 73/116 loss: 0.0528\n",
      "Epoch 92: Batch 74/116 loss: 0.0730\n",
      "Epoch 92: Batch 75/116 loss: 0.0663\n",
      "Epoch 92: Batch 76/116 loss: 0.0657\n",
      "Epoch 92: Batch 77/116 loss: 0.0530\n",
      "Epoch 92: Batch 78/116 loss: 0.0469\n",
      "Epoch 92: Batch 79/116 loss: 0.0369\n",
      "Epoch 92: Batch 80/116 loss: 0.0521\n",
      "Epoch 92: Batch 81/116 loss: 0.0373\n",
      "Epoch 92: Batch 82/116 loss: 0.0608\n",
      "Epoch 92: Batch 83/116 loss: 0.0579\n",
      "Epoch 92: Batch 84/116 loss: 0.0693\n",
      "Epoch 92: Batch 85/116 loss: 0.0589\n",
      "Epoch 92: Batch 86/116 loss: 0.0807\n",
      "Epoch 92: Batch 87/116 loss: 0.0470\n",
      "Epoch 92: Batch 88/116 loss: 0.0568\n",
      "Epoch 92: Batch 89/116 loss: 0.0260\n",
      "Epoch 92: Batch 90/116 loss: 0.0466\n",
      "Epoch 92: Batch 91/116 loss: 0.0475\n",
      "Epoch 92: Batch 92/116 loss: 0.0504\n",
      "Epoch 92: Batch 93/116 loss: 0.0358\n",
      "Epoch 92: Batch 94/116 loss: 0.0499\n",
      "Epoch 92: Batch 95/116 loss: 0.0359\n",
      "Epoch 92: Batch 96/116 loss: 0.0585\n",
      "Epoch 92: Batch 97/116 loss: 0.0321\n",
      "Epoch 92: Batch 98/116 loss: 0.0547\n",
      "Epoch 92: Batch 99/116 loss: 0.0680\n",
      "Epoch 92: Batch 100/116 loss: 0.0478\n",
      "Epoch 92: Batch 101/116 loss: 0.0541\n",
      "Epoch 92: Batch 102/116 loss: 0.0476\n",
      "Epoch 92: Batch 103/116 loss: 0.0617\n",
      "Epoch 92: Batch 104/116 loss: 0.0416\n",
      "Epoch 92: Batch 105/116 loss: 0.0481\n",
      "Epoch 92: Batch 106/116 loss: 0.0459\n",
      "Epoch 92: Batch 107/116 loss: 0.0491\n",
      "Epoch 92: Batch 108/116 loss: 0.0306\n",
      "Epoch 92: Batch 109/116 loss: 0.0558\n",
      "Epoch 92: Batch 110/116 loss: 0.0558\n",
      "Epoch 92: Batch 111/116 loss: 0.0599\n",
      "Epoch 92: Batch 112/116 loss: 0.0462\n",
      "Epoch 92: Batch 113/116 loss: 0.0329\n",
      "Epoch 92: Batch 114/116 loss: 0.0749\n",
      "Epoch 92: Batch 115/116 loss: 0.0670\n",
      "Epoch 92: Batch 116/116 loss: 0.0640\n",
      "Epoch 92 train loss: 0.0533 valid loss: 0.0598\n",
      "performance reducing: 1\n",
      "Epoch 93: Batch 1/116 loss: 0.0501\n",
      "Epoch 93: Batch 2/116 loss: 0.0443\n",
      "Epoch 93: Batch 3/116 loss: 0.0560\n",
      "Epoch 93: Batch 4/116 loss: 0.0561\n",
      "Epoch 93: Batch 5/116 loss: 0.0654\n",
      "Epoch 93: Batch 6/116 loss: 0.0488\n",
      "Epoch 93: Batch 7/116 loss: 0.0668\n",
      "Epoch 93: Batch 8/116 loss: 0.0424\n",
      "Epoch 93: Batch 9/116 loss: 0.0598\n",
      "Epoch 93: Batch 10/116 loss: 0.0758\n",
      "Epoch 93: Batch 11/116 loss: 0.0612\n",
      "Epoch 93: Batch 12/116 loss: 0.0471\n",
      "Epoch 93: Batch 13/116 loss: 0.0534\n",
      "Epoch 93: Batch 14/116 loss: 0.0614\n",
      "Epoch 93: Batch 15/116 loss: 0.0702\n",
      "Epoch 93: Batch 16/116 loss: 0.0794\n",
      "Epoch 93: Batch 17/116 loss: 0.0576\n",
      "Epoch 93: Batch 18/116 loss: 0.0511\n",
      "Epoch 93: Batch 19/116 loss: 0.0391\n",
      "Epoch 93: Batch 20/116 loss: 0.0500\n",
      "Epoch 93: Batch 21/116 loss: 0.0519\n",
      "Epoch 93: Batch 22/116 loss: 0.0749\n",
      "Epoch 93: Batch 23/116 loss: 0.0699\n",
      "Epoch 93: Batch 24/116 loss: 0.0361\n",
      "Epoch 93: Batch 25/116 loss: 0.0397\n",
      "Epoch 93: Batch 26/116 loss: 0.0364\n",
      "Epoch 93: Batch 27/116 loss: 0.0568\n",
      "Epoch 93: Batch 28/116 loss: 0.0499\n",
      "Epoch 93: Batch 29/116 loss: 0.0402\n",
      "Epoch 93: Batch 30/116 loss: 0.0445\n",
      "Epoch 93: Batch 31/116 loss: 0.0528\n",
      "Epoch 93: Batch 32/116 loss: 0.0364\n",
      "Epoch 93: Batch 33/116 loss: 0.0422\n",
      "Epoch 93: Batch 34/116 loss: 0.0428\n",
      "Epoch 93: Batch 35/116 loss: 0.0514\n",
      "Epoch 93: Batch 36/116 loss: 0.0435\n",
      "Epoch 93: Batch 37/116 loss: 0.0373\n",
      "Epoch 93: Batch 38/116 loss: 0.0450\n",
      "Epoch 93: Batch 39/116 loss: 0.0566\n",
      "Epoch 93: Batch 40/116 loss: 0.0649\n",
      "Epoch 93: Batch 41/116 loss: 0.0465\n",
      "Epoch 93: Batch 42/116 loss: 0.0374\n",
      "Epoch 93: Batch 43/116 loss: 0.0451\n",
      "Epoch 93: Batch 44/116 loss: 0.0414\n",
      "Epoch 93: Batch 45/116 loss: 0.0573\n",
      "Epoch 93: Batch 46/116 loss: 0.0423\n",
      "Epoch 93: Batch 47/116 loss: 0.0668\n",
      "Epoch 93: Batch 48/116 loss: 0.0548\n",
      "Epoch 93: Batch 49/116 loss: 0.0688\n",
      "Epoch 93: Batch 50/116 loss: 0.0328\n",
      "Epoch 93: Batch 51/116 loss: 0.0504\n",
      "Epoch 93: Batch 52/116 loss: 0.0536\n",
      "Epoch 93: Batch 53/116 loss: 0.0517\n",
      "Epoch 93: Batch 54/116 loss: 0.0472\n",
      "Epoch 93: Batch 55/116 loss: 0.0444\n",
      "Epoch 93: Batch 56/116 loss: 0.0724\n",
      "Epoch 93: Batch 57/116 loss: 0.0496\n",
      "Epoch 93: Batch 58/116 loss: 0.0379\n",
      "Epoch 93: Batch 59/116 loss: 0.0605\n",
      "Epoch 93: Batch 60/116 loss: 0.0430\n",
      "Epoch 93: Batch 61/116 loss: 0.0491\n",
      "Epoch 93: Batch 62/116 loss: 0.0477\n",
      "Epoch 93: Batch 63/116 loss: 0.0530\n",
      "Epoch 93: Batch 64/116 loss: 0.0670\n",
      "Epoch 93: Batch 65/116 loss: 0.0437\n",
      "Epoch 93: Batch 66/116 loss: 0.0451\n",
      "Epoch 93: Batch 67/116 loss: 0.0409\n",
      "Epoch 93: Batch 68/116 loss: 0.0457\n",
      "Epoch 93: Batch 69/116 loss: 0.0528\n",
      "Epoch 93: Batch 70/116 loss: 0.0791\n",
      "Epoch 93: Batch 71/116 loss: 0.1062\n",
      "Epoch 93: Batch 72/116 loss: 0.0831\n",
      "Epoch 93: Batch 73/116 loss: 0.0887\n",
      "Epoch 93: Batch 74/116 loss: 0.0689\n",
      "Epoch 93: Batch 75/116 loss: 0.0392\n",
      "Epoch 93: Batch 76/116 loss: 0.0596\n",
      "Epoch 93: Batch 77/116 loss: 0.0368\n",
      "Epoch 93: Batch 78/116 loss: 0.0590\n",
      "Epoch 93: Batch 79/116 loss: 0.0535\n",
      "Epoch 93: Batch 80/116 loss: 0.0377\n",
      "Epoch 93: Batch 81/116 loss: 0.0556\n",
      "Epoch 93: Batch 82/116 loss: 0.0518\n",
      "Epoch 93: Batch 83/116 loss: 0.0532\n",
      "Epoch 93: Batch 84/116 loss: 0.0675\n",
      "Epoch 93: Batch 85/116 loss: 0.0637\n",
      "Epoch 93: Batch 86/116 loss: 0.0313\n",
      "Epoch 93: Batch 87/116 loss: 0.0413\n",
      "Epoch 93: Batch 88/116 loss: 0.0521\n",
      "Epoch 93: Batch 89/116 loss: 0.0368\n",
      "Epoch 93: Batch 90/116 loss: 0.0482\n",
      "Epoch 93: Batch 91/116 loss: 0.0630\n",
      "Epoch 93: Batch 92/116 loss: 0.0401\n",
      "Epoch 93: Batch 93/116 loss: 0.0676\n",
      "Epoch 93: Batch 94/116 loss: 0.0652\n",
      "Epoch 93: Batch 95/116 loss: 0.0445\n",
      "Epoch 93: Batch 96/116 loss: 0.0762\n",
      "Epoch 93: Batch 97/116 loss: 0.0572\n",
      "Epoch 93: Batch 98/116 loss: 0.0683\n",
      "Epoch 93: Batch 99/116 loss: 0.0705\n",
      "Epoch 93: Batch 100/116 loss: 0.0468\n",
      "Epoch 93: Batch 101/116 loss: 0.0653\n",
      "Epoch 93: Batch 102/116 loss: 0.0527\n",
      "Epoch 93: Batch 103/116 loss: 0.0512\n",
      "Epoch 93: Batch 104/116 loss: 0.0545\n",
      "Epoch 93: Batch 105/116 loss: 0.0665\n",
      "Epoch 93: Batch 106/116 loss: 0.0526\n",
      "Epoch 93: Batch 107/116 loss: 0.0591\n",
      "Epoch 93: Batch 108/116 loss: 0.0689\n",
      "Epoch 93: Batch 109/116 loss: 0.0566\n",
      "Epoch 93: Batch 110/116 loss: 0.0342\n",
      "Epoch 93: Batch 111/116 loss: 0.0764\n",
      "Epoch 93: Batch 112/116 loss: 0.0437\n",
      "Epoch 93: Batch 113/116 loss: 0.0800\n",
      "Epoch 93: Batch 114/116 loss: 0.0382\n",
      "Epoch 93: Batch 115/116 loss: 0.0448\n",
      "Epoch 93: Batch 116/116 loss: 0.0310\n",
      "Epoch 93 train loss: 0.0538 valid loss: 0.0572\n",
      "performance reducing: 2\n",
      "Epoch 94: Batch 1/116 loss: 0.0514\n",
      "Epoch 94: Batch 2/116 loss: 0.0539\n",
      "Epoch 94: Batch 3/116 loss: 0.0478\n",
      "Epoch 94: Batch 4/116 loss: 0.0503\n",
      "Epoch 94: Batch 5/116 loss: 0.0755\n",
      "Epoch 94: Batch 6/116 loss: 0.0598\n",
      "Epoch 94: Batch 7/116 loss: 0.0582\n",
      "Epoch 94: Batch 8/116 loss: 0.0741\n",
      "Epoch 94: Batch 9/116 loss: 0.0347\n",
      "Epoch 94: Batch 10/116 loss: 0.0386\n",
      "Epoch 94: Batch 11/116 loss: 0.0501\n",
      "Epoch 94: Batch 12/116 loss: 0.0659\n",
      "Epoch 94: Batch 13/116 loss: 0.0638\n",
      "Epoch 94: Batch 14/116 loss: 0.0532\n",
      "Epoch 94: Batch 15/116 loss: 0.0506\n",
      "Epoch 94: Batch 16/116 loss: 0.0661\n",
      "Epoch 94: Batch 17/116 loss: 0.0350\n",
      "Epoch 94: Batch 18/116 loss: 0.0486\n",
      "Epoch 94: Batch 19/116 loss: 0.0518\n",
      "Epoch 94: Batch 20/116 loss: 0.0311\n",
      "Epoch 94: Batch 21/116 loss: 0.0452\n",
      "Epoch 94: Batch 22/116 loss: 0.0553\n",
      "Epoch 94: Batch 23/116 loss: 0.0354\n",
      "Epoch 94: Batch 24/116 loss: 0.0583\n",
      "Epoch 94: Batch 25/116 loss: 0.0488\n",
      "Epoch 94: Batch 26/116 loss: 0.0422\n",
      "Epoch 94: Batch 27/116 loss: 0.0486\n",
      "Epoch 94: Batch 28/116 loss: 0.0608\n",
      "Epoch 94: Batch 29/116 loss: 0.0567\n",
      "Epoch 94: Batch 30/116 loss: 0.0478\n",
      "Epoch 94: Batch 31/116 loss: 0.0514\n",
      "Epoch 94: Batch 32/116 loss: 0.0394\n",
      "Epoch 94: Batch 33/116 loss: 0.0342\n",
      "Epoch 94: Batch 34/116 loss: 0.0779\n",
      "Epoch 94: Batch 35/116 loss: 0.0519\n",
      "Epoch 94: Batch 36/116 loss: 0.0550\n",
      "Epoch 94: Batch 37/116 loss: 0.0536\n",
      "Epoch 94: Batch 38/116 loss: 0.0579\n",
      "Epoch 94: Batch 39/116 loss: 0.0457\n",
      "Epoch 94: Batch 40/116 loss: 0.0466\n",
      "Epoch 94: Batch 41/116 loss: 0.0576\n",
      "Epoch 94: Batch 42/116 loss: 0.0494\n",
      "Epoch 94: Batch 43/116 loss: 0.0634\n",
      "Epoch 94: Batch 44/116 loss: 0.0642\n",
      "Epoch 94: Batch 45/116 loss: 0.0402\n",
      "Epoch 94: Batch 46/116 loss: 0.0915\n",
      "Epoch 94: Batch 47/116 loss: 0.0518\n",
      "Epoch 94: Batch 48/116 loss: 0.0591\n",
      "Epoch 94: Batch 49/116 loss: 0.0634\n",
      "Epoch 94: Batch 50/116 loss: 0.0356\n",
      "Epoch 94: Batch 51/116 loss: 0.0673\n",
      "Epoch 94: Batch 52/116 loss: 0.0502\n",
      "Epoch 94: Batch 53/116 loss: 0.0462\n",
      "Epoch 94: Batch 54/116 loss: 0.0445\n",
      "Epoch 94: Batch 55/116 loss: 0.0634\n",
      "Epoch 94: Batch 56/116 loss: 0.0467\n",
      "Epoch 94: Batch 57/116 loss: 0.0656\n",
      "Epoch 94: Batch 58/116 loss: 0.0647\n",
      "Epoch 94: Batch 59/116 loss: 0.0457\n",
      "Epoch 94: Batch 60/116 loss: 0.0812\n",
      "Epoch 94: Batch 61/116 loss: 0.0465\n",
      "Epoch 94: Batch 62/116 loss: 0.0514\n",
      "Epoch 94: Batch 63/116 loss: 0.0439\n",
      "Epoch 94: Batch 64/116 loss: 0.0398\n",
      "Epoch 94: Batch 65/116 loss: 0.0538\n",
      "Epoch 94: Batch 66/116 loss: 0.0472\n",
      "Epoch 94: Batch 67/116 loss: 0.0563\n",
      "Epoch 94: Batch 68/116 loss: 0.0740\n",
      "Epoch 94: Batch 69/116 loss: 0.0499\n",
      "Epoch 94: Batch 70/116 loss: 0.0523\n",
      "Epoch 94: Batch 71/116 loss: 0.0347\n",
      "Epoch 94: Batch 72/116 loss: 0.0579\n",
      "Epoch 94: Batch 73/116 loss: 0.0342\n",
      "Epoch 94: Batch 74/116 loss: 0.0604\n",
      "Epoch 94: Batch 75/116 loss: 0.0712\n",
      "Epoch 94: Batch 76/116 loss: 0.0725\n",
      "Epoch 94: Batch 77/116 loss: 0.0711\n",
      "Epoch 94: Batch 78/116 loss: 0.0654\n",
      "Epoch 94: Batch 79/116 loss: 0.0750\n",
      "Epoch 94: Batch 80/116 loss: 0.0414\n",
      "Epoch 94: Batch 81/116 loss: 0.0383\n",
      "Epoch 94: Batch 82/116 loss: 0.0728\n",
      "Epoch 94: Batch 83/116 loss: 0.0717\n",
      "Epoch 94: Batch 84/116 loss: 0.0800\n",
      "Epoch 94: Batch 85/116 loss: 0.0490\n",
      "Epoch 94: Batch 86/116 loss: 0.0571\n",
      "Epoch 94: Batch 87/116 loss: 0.0565\n",
      "Epoch 94: Batch 88/116 loss: 0.0369\n",
      "Epoch 94: Batch 89/116 loss: 0.0685\n",
      "Epoch 94: Batch 90/116 loss: 0.0431\n",
      "Epoch 94: Batch 91/116 loss: 0.0622\n",
      "Epoch 94: Batch 92/116 loss: 0.0510\n",
      "Epoch 94: Batch 93/116 loss: 0.0680\n",
      "Epoch 94: Batch 94/116 loss: 0.0488\n",
      "Epoch 94: Batch 95/116 loss: 0.0568\n",
      "Epoch 94: Batch 96/116 loss: 0.0463\n",
      "Epoch 94: Batch 97/116 loss: 0.0387\n",
      "Epoch 94: Batch 98/116 loss: 0.0676\n",
      "Epoch 94: Batch 99/116 loss: 0.0312\n",
      "Epoch 94: Batch 100/116 loss: 0.0516\n",
      "Epoch 94: Batch 101/116 loss: 0.0659\n",
      "Epoch 94: Batch 102/116 loss: 0.0650\n",
      "Epoch 94: Batch 103/116 loss: 0.0535\n",
      "Epoch 94: Batch 104/116 loss: 0.0754\n",
      "Epoch 94: Batch 105/116 loss: 0.0505\n",
      "Epoch 94: Batch 106/116 loss: 0.0466\n",
      "Epoch 94: Batch 107/116 loss: 0.0515\n",
      "Epoch 94: Batch 108/116 loss: 0.0372\n",
      "Epoch 94: Batch 109/116 loss: 0.0385\n",
      "Epoch 94: Batch 110/116 loss: 0.0484\n",
      "Epoch 94: Batch 111/116 loss: 0.0412\n",
      "Epoch 94: Batch 112/116 loss: 0.0300\n",
      "Epoch 94: Batch 113/116 loss: 0.0463\n",
      "Epoch 94: Batch 114/116 loss: 0.0587\n",
      "Epoch 94: Batch 115/116 loss: 0.0511\n",
      "Epoch 94: Batch 116/116 loss: 0.0421\n",
      "Epoch 94 train loss: 0.0536 valid loss: 0.0653\n",
      "performance reducing: 3\n",
      "Epoch 95: Batch 1/116 loss: 0.0748\n",
      "Epoch 95: Batch 2/116 loss: 0.0592\n",
      "Epoch 95: Batch 3/116 loss: 0.0459\n",
      "Epoch 95: Batch 4/116 loss: 0.0525\n",
      "Epoch 95: Batch 5/116 loss: 0.0412\n",
      "Epoch 95: Batch 6/116 loss: 0.0520\n",
      "Epoch 95: Batch 7/116 loss: 0.0648\n",
      "Epoch 95: Batch 8/116 loss: 0.0422\n",
      "Epoch 95: Batch 9/116 loss: 0.0507\n",
      "Epoch 95: Batch 10/116 loss: 0.0584\n",
      "Epoch 95: Batch 11/116 loss: 0.0359\n",
      "Epoch 95: Batch 12/116 loss: 0.0594\n",
      "Epoch 95: Batch 13/116 loss: 0.0580\n",
      "Epoch 95: Batch 14/116 loss: 0.0448\n",
      "Epoch 95: Batch 15/116 loss: 0.0313\n",
      "Epoch 95: Batch 16/116 loss: 0.0546\n",
      "Epoch 95: Batch 17/116 loss: 0.0537\n",
      "Epoch 95: Batch 18/116 loss: 0.0619\n",
      "Epoch 95: Batch 19/116 loss: 0.0650\n",
      "Epoch 95: Batch 20/116 loss: 0.0515\n",
      "Epoch 95: Batch 21/116 loss: 0.0632\n",
      "Epoch 95: Batch 22/116 loss: 0.0504\n",
      "Epoch 95: Batch 23/116 loss: 0.0600\n",
      "Epoch 95: Batch 24/116 loss: 0.0306\n",
      "Epoch 95: Batch 25/116 loss: 0.0350\n",
      "Epoch 95: Batch 26/116 loss: 0.0572\n",
      "Epoch 95: Batch 27/116 loss: 0.0369\n",
      "Epoch 95: Batch 28/116 loss: 0.0437\n",
      "Epoch 95: Batch 29/116 loss: 0.0703\n",
      "Epoch 95: Batch 30/116 loss: 0.0467\n",
      "Epoch 95: Batch 31/116 loss: 0.0517\n",
      "Epoch 95: Batch 32/116 loss: 0.0606\n",
      "Epoch 95: Batch 33/116 loss: 0.0624\n",
      "Epoch 95: Batch 34/116 loss: 0.0579\n",
      "Epoch 95: Batch 35/116 loss: 0.0537\n",
      "Epoch 95: Batch 36/116 loss: 0.0589\n",
      "Epoch 95: Batch 37/116 loss: 0.0464\n",
      "Epoch 95: Batch 38/116 loss: 0.0537\n",
      "Epoch 95: Batch 39/116 loss: 0.0509\n",
      "Epoch 95: Batch 40/116 loss: 0.0724\n",
      "Epoch 95: Batch 41/116 loss: 0.0451\n",
      "Epoch 95: Batch 42/116 loss: 0.0381\n",
      "Epoch 95: Batch 43/116 loss: 0.0562\n",
      "Epoch 95: Batch 44/116 loss: 0.0423\n",
      "Epoch 95: Batch 45/116 loss: 0.0498\n",
      "Epoch 95: Batch 46/116 loss: 0.0497\n",
      "Epoch 95: Batch 47/116 loss: 0.0515\n",
      "Epoch 95: Batch 48/116 loss: 0.0361\n",
      "Epoch 95: Batch 49/116 loss: 0.0636\n",
      "Epoch 95: Batch 50/116 loss: 0.0437\n",
      "Epoch 95: Batch 51/116 loss: 0.0645\n",
      "Epoch 95: Batch 52/116 loss: 0.0520\n",
      "Epoch 95: Batch 53/116 loss: 0.0436\n",
      "Epoch 95: Batch 54/116 loss: 0.0502\n",
      "Epoch 95: Batch 55/116 loss: 0.0447\n",
      "Epoch 95: Batch 56/116 loss: 0.0611\n",
      "Epoch 95: Batch 57/116 loss: 0.0460\n",
      "Epoch 95: Batch 58/116 loss: 0.0472\n",
      "Epoch 95: Batch 59/116 loss: 0.0543\n",
      "Epoch 95: Batch 60/116 loss: 0.0466\n",
      "Epoch 95: Batch 61/116 loss: 0.0490\n",
      "Epoch 95: Batch 62/116 loss: 0.0517\n",
      "Epoch 95: Batch 63/116 loss: 0.0453\n",
      "Epoch 95: Batch 64/116 loss: 0.0688\n",
      "Epoch 95: Batch 65/116 loss: 0.0552\n",
      "Epoch 95: Batch 66/116 loss: 0.0544\n",
      "Epoch 95: Batch 67/116 loss: 0.0577\n",
      "Epoch 95: Batch 68/116 loss: 0.0701\n",
      "Epoch 95: Batch 69/116 loss: 0.0678\n",
      "Epoch 95: Batch 70/116 loss: 0.0482\n",
      "Epoch 95: Batch 71/116 loss: 0.0403\n",
      "Epoch 95: Batch 72/116 loss: 0.0472\n",
      "Epoch 95: Batch 73/116 loss: 0.0524\n",
      "Epoch 95: Batch 74/116 loss: 0.0567\n",
      "Epoch 95: Batch 75/116 loss: 0.0571\n",
      "Epoch 95: Batch 76/116 loss: 0.0506\n",
      "Epoch 95: Batch 77/116 loss: 0.0646\n",
      "Epoch 95: Batch 78/116 loss: 0.0533\n",
      "Epoch 95: Batch 79/116 loss: 0.0534\n",
      "Epoch 95: Batch 80/116 loss: 0.0666\n",
      "Epoch 95: Batch 81/116 loss: 0.0556\n",
      "Epoch 95: Batch 82/116 loss: 0.0645\n",
      "Epoch 95: Batch 83/116 loss: 0.0612\n",
      "Epoch 95: Batch 84/116 loss: 0.0560\n",
      "Epoch 95: Batch 85/116 loss: 0.0572\n",
      "Epoch 95: Batch 86/116 loss: 0.0658\n",
      "Epoch 95: Batch 87/116 loss: 0.0713\n",
      "Epoch 95: Batch 88/116 loss: 0.0476\n",
      "Epoch 95: Batch 89/116 loss: 0.0485\n",
      "Epoch 95: Batch 90/116 loss: 0.0380\n",
      "Epoch 95: Batch 91/116 loss: 0.0519\n",
      "Epoch 95: Batch 92/116 loss: 0.0382\n",
      "Epoch 95: Batch 93/116 loss: 0.0623\n",
      "Epoch 95: Batch 94/116 loss: 0.0559\n",
      "Epoch 95: Batch 95/116 loss: 0.0639\n",
      "Epoch 95: Batch 96/116 loss: 0.0427\n",
      "Epoch 95: Batch 97/116 loss: 0.0461\n",
      "Epoch 95: Batch 98/116 loss: 0.0519\n",
      "Epoch 95: Batch 99/116 loss: 0.0536\n",
      "Epoch 95: Batch 100/116 loss: 0.0734\n",
      "Epoch 95: Batch 101/116 loss: 0.0408\n",
      "Epoch 95: Batch 102/116 loss: 0.0640\n",
      "Epoch 95: Batch 103/116 loss: 0.1021\n",
      "Epoch 95: Batch 104/116 loss: 0.0374\n",
      "Epoch 95: Batch 105/116 loss: 0.0485\n",
      "Epoch 95: Batch 106/116 loss: 0.0529\n",
      "Epoch 95: Batch 107/116 loss: 0.0644\n",
      "Epoch 95: Batch 108/116 loss: 0.0531\n",
      "Epoch 95: Batch 109/116 loss: 0.0564\n",
      "Epoch 95: Batch 110/116 loss: 0.0574\n",
      "Epoch 95: Batch 111/116 loss: 0.0543\n",
      "Epoch 95: Batch 112/116 loss: 0.0688\n",
      "Epoch 95: Batch 113/116 loss: 0.0553\n",
      "Epoch 95: Batch 114/116 loss: 0.0568\n",
      "Epoch 95: Batch 115/116 loss: 0.0451\n",
      "Epoch 95: Batch 116/116 loss: 0.0426\n",
      "Epoch 95 train loss: 0.0536 valid loss: 0.0604\n",
      "performance reducing: 4\n",
      "Epoch 96: Batch 1/116 loss: 0.0615\n",
      "Epoch 96: Batch 2/116 loss: 0.0519\n",
      "Epoch 96: Batch 3/116 loss: 0.0468\n",
      "Epoch 96: Batch 4/116 loss: 0.0616\n",
      "Epoch 96: Batch 5/116 loss: 0.0651\n",
      "Epoch 96: Batch 6/116 loss: 0.0470\n",
      "Epoch 96: Batch 7/116 loss: 0.0241\n",
      "Epoch 96: Batch 8/116 loss: 0.0784\n",
      "Epoch 96: Batch 9/116 loss: 0.0372\n",
      "Epoch 96: Batch 10/116 loss: 0.0506\n",
      "Epoch 96: Batch 11/116 loss: 0.0480\n",
      "Epoch 96: Batch 12/116 loss: 0.0398\n",
      "Epoch 96: Batch 13/116 loss: 0.0571\n",
      "Epoch 96: Batch 14/116 loss: 0.0393\n",
      "Epoch 96: Batch 15/116 loss: 0.0728\n",
      "Epoch 96: Batch 16/116 loss: 0.0631\n",
      "Epoch 96: Batch 17/116 loss: 0.0577\n",
      "Epoch 96: Batch 18/116 loss: 0.0389\n",
      "Epoch 96: Batch 19/116 loss: 0.0610\n",
      "Epoch 96: Batch 20/116 loss: 0.0715\n",
      "Epoch 96: Batch 21/116 loss: 0.0626\n",
      "Epoch 96: Batch 22/116 loss: 0.0710\n",
      "Epoch 96: Batch 23/116 loss: 0.0607\n",
      "Epoch 96: Batch 24/116 loss: 0.0495\n",
      "Epoch 96: Batch 25/116 loss: 0.0509\n",
      "Epoch 96: Batch 26/116 loss: 0.0573\n",
      "Epoch 96: Batch 27/116 loss: 0.0609\n",
      "Epoch 96: Batch 28/116 loss: 0.0666\n",
      "Epoch 96: Batch 29/116 loss: 0.0574\n",
      "Epoch 96: Batch 30/116 loss: 0.0431\n",
      "Epoch 96: Batch 31/116 loss: 0.0477\n",
      "Epoch 96: Batch 32/116 loss: 0.0725\n",
      "Epoch 96: Batch 33/116 loss: 0.0641\n",
      "Epoch 96: Batch 34/116 loss: 0.0430\n",
      "Epoch 96: Batch 35/116 loss: 0.0473\n",
      "Epoch 96: Batch 36/116 loss: 0.0600\n",
      "Epoch 96: Batch 37/116 loss: 0.0418\n",
      "Epoch 96: Batch 38/116 loss: 0.0664\n",
      "Epoch 96: Batch 39/116 loss: 0.0554\n",
      "Epoch 96: Batch 40/116 loss: 0.0465\n",
      "Epoch 96: Batch 41/116 loss: 0.0774\n",
      "Epoch 96: Batch 42/116 loss: 0.0629\n",
      "Epoch 96: Batch 43/116 loss: 0.0591\n",
      "Epoch 96: Batch 44/116 loss: 0.0529\n",
      "Epoch 96: Batch 45/116 loss: 0.0695\n",
      "Epoch 96: Batch 46/116 loss: 0.0364\n",
      "Epoch 96: Batch 47/116 loss: 0.0363\n",
      "Epoch 96: Batch 48/116 loss: 0.0493\n",
      "Epoch 96: Batch 49/116 loss: 0.0515\n",
      "Epoch 96: Batch 50/116 loss: 0.0432\n",
      "Epoch 96: Batch 51/116 loss: 0.0433\n",
      "Epoch 96: Batch 52/116 loss: 0.0411\n",
      "Epoch 96: Batch 53/116 loss: 0.0351\n",
      "Epoch 96: Batch 54/116 loss: 0.0624\n",
      "Epoch 96: Batch 55/116 loss: 0.0455\n",
      "Epoch 96: Batch 56/116 loss: 0.0542\n",
      "Epoch 96: Batch 57/116 loss: 0.0411\n",
      "Epoch 96: Batch 58/116 loss: 0.0570\n",
      "Epoch 96: Batch 59/116 loss: 0.0420\n",
      "Epoch 96: Batch 60/116 loss: 0.0363\n",
      "Epoch 96: Batch 61/116 loss: 0.0454\n",
      "Epoch 96: Batch 62/116 loss: 0.0662\n",
      "Epoch 96: Batch 63/116 loss: 0.0361\n",
      "Epoch 96: Batch 64/116 loss: 0.0843\n",
      "Epoch 96: Batch 65/116 loss: 0.0546\n",
      "Epoch 96: Batch 66/116 loss: 0.0454\n",
      "Epoch 96: Batch 67/116 loss: 0.0708\n",
      "Epoch 96: Batch 68/116 loss: 0.0468\n",
      "Epoch 96: Batch 69/116 loss: 0.0531\n",
      "Epoch 96: Batch 70/116 loss: 0.0654\n",
      "Epoch 96: Batch 71/116 loss: 0.0574\n",
      "Epoch 96: Batch 72/116 loss: 0.0431\n",
      "Epoch 96: Batch 73/116 loss: 0.0489\n",
      "Epoch 96: Batch 74/116 loss: 0.0438\n",
      "Epoch 96: Batch 75/116 loss: 0.0377\n",
      "Epoch 96: Batch 76/116 loss: 0.0559\n",
      "Epoch 96: Batch 77/116 loss: 0.0455\n",
      "Epoch 96: Batch 78/116 loss: 0.0724\n",
      "Epoch 96: Batch 79/116 loss: 0.0437\n",
      "Epoch 96: Batch 80/116 loss: 0.0568\n",
      "Epoch 96: Batch 81/116 loss: 0.0345\n",
      "Epoch 96: Batch 82/116 loss: 0.0467\n",
      "Epoch 96: Batch 83/116 loss: 0.0469\n",
      "Epoch 96: Batch 84/116 loss: 0.0811\n",
      "Epoch 96: Batch 85/116 loss: 0.0595\n",
      "Epoch 96: Batch 86/116 loss: 0.0583\n",
      "Epoch 96: Batch 87/116 loss: 0.0424\n",
      "Epoch 96: Batch 88/116 loss: 0.0465\n",
      "Epoch 96: Batch 89/116 loss: 0.0414\n",
      "Epoch 96: Batch 90/116 loss: 0.0731\n",
      "Epoch 96: Batch 91/116 loss: 0.0529\n",
      "Epoch 96: Batch 92/116 loss: 0.0643\n",
      "Epoch 96: Batch 93/116 loss: 0.0624\n",
      "Epoch 96: Batch 94/116 loss: 0.0714\n",
      "Epoch 96: Batch 95/116 loss: 0.0636\n",
      "Epoch 96: Batch 96/116 loss: 0.0358\n",
      "Epoch 96: Batch 97/116 loss: 0.0536\n",
      "Epoch 96: Batch 98/116 loss: 0.0429\n",
      "Epoch 96: Batch 99/116 loss: 0.0656\n",
      "Epoch 96: Batch 100/116 loss: 0.0379\n",
      "Epoch 96: Batch 101/116 loss: 0.0472\n",
      "Epoch 96: Batch 102/116 loss: 0.0866\n",
      "Epoch 96: Batch 103/116 loss: 0.0558\n",
      "Epoch 96: Batch 104/116 loss: 0.0441\n",
      "Epoch 96: Batch 105/116 loss: 0.0661\n",
      "Epoch 96: Batch 106/116 loss: 0.0531\n",
      "Epoch 96: Batch 107/116 loss: 0.0367\n",
      "Epoch 96: Batch 108/116 loss: 0.0572\n",
      "Epoch 96: Batch 109/116 loss: 0.0560\n",
      "Epoch 96: Batch 110/116 loss: 0.0505\n",
      "Epoch 96: Batch 111/116 loss: 0.0393\n",
      "Epoch 96: Batch 112/116 loss: 0.0339\n",
      "Epoch 96: Batch 113/116 loss: 0.0561\n",
      "Epoch 96: Batch 114/116 loss: 0.0455\n",
      "Epoch 96: Batch 115/116 loss: 0.0551\n",
      "Epoch 96: Batch 116/116 loss: 0.0395\n",
      "Epoch 96 train loss: 0.0532 valid loss: 0.0696\n",
      "performance reducing: 5\n",
      "Epoch 97: Batch 1/116 loss: 0.0725\n",
      "Epoch 97: Batch 2/116 loss: 0.0630\n",
      "Epoch 97: Batch 3/116 loss: 0.0460\n",
      "Epoch 97: Batch 4/116 loss: 0.0383\n",
      "Epoch 97: Batch 5/116 loss: 0.0616\n",
      "Epoch 97: Batch 6/116 loss: 0.0418\n",
      "Epoch 97: Batch 7/116 loss: 0.0546\n",
      "Epoch 97: Batch 8/116 loss: 0.0497\n",
      "Epoch 97: Batch 9/116 loss: 0.0510\n",
      "Epoch 97: Batch 10/116 loss: 0.0479\n",
      "Epoch 97: Batch 11/116 loss: 0.0585\n",
      "Epoch 97: Batch 12/116 loss: 0.0776\n",
      "Epoch 97: Batch 13/116 loss: 0.0340\n",
      "Epoch 97: Batch 14/116 loss: 0.0569\n",
      "Epoch 97: Batch 15/116 loss: 0.0398\n",
      "Epoch 97: Batch 16/116 loss: 0.0612\n",
      "Epoch 97: Batch 17/116 loss: 0.0432\n",
      "Epoch 97: Batch 18/116 loss: 0.0338\n",
      "Epoch 97: Batch 19/116 loss: 0.0670\n",
      "Epoch 97: Batch 20/116 loss: 0.0469\n",
      "Epoch 97: Batch 21/116 loss: 0.0462\n",
      "Epoch 97: Batch 22/116 loss: 0.0450\n",
      "Epoch 97: Batch 23/116 loss: 0.0355\n",
      "Epoch 97: Batch 24/116 loss: 0.0580\n",
      "Epoch 97: Batch 25/116 loss: 0.0570\n",
      "Epoch 97: Batch 26/116 loss: 0.0377\n",
      "Epoch 97: Batch 27/116 loss: 0.0448\n",
      "Epoch 97: Batch 28/116 loss: 0.0495\n",
      "Epoch 97: Batch 29/116 loss: 0.0745\n",
      "Epoch 97: Batch 30/116 loss: 0.0507\n",
      "Epoch 97: Batch 31/116 loss: 0.0537\n",
      "Epoch 97: Batch 32/116 loss: 0.0621\n",
      "Epoch 97: Batch 33/116 loss: 0.0569\n",
      "Epoch 97: Batch 34/116 loss: 0.0621\n",
      "Epoch 97: Batch 35/116 loss: 0.0707\n",
      "Epoch 97: Batch 36/116 loss: 0.0356\n",
      "Epoch 97: Batch 37/116 loss: 0.0611\n",
      "Epoch 97: Batch 38/116 loss: 0.0544\n",
      "Epoch 97: Batch 39/116 loss: 0.0539\n",
      "Epoch 97: Batch 40/116 loss: 0.0488\n",
      "Epoch 97: Batch 41/116 loss: 0.0486\n",
      "Epoch 97: Batch 42/116 loss: 0.0524\n",
      "Epoch 97: Batch 43/116 loss: 0.0719\n",
      "Epoch 97: Batch 44/116 loss: 0.0492\n",
      "Epoch 97: Batch 45/116 loss: 0.0495\n",
      "Epoch 97: Batch 46/116 loss: 0.0683\n",
      "Epoch 97: Batch 47/116 loss: 0.0474\n",
      "Epoch 97: Batch 48/116 loss: 0.0624\n",
      "Epoch 97: Batch 49/116 loss: 0.0471\n",
      "Epoch 97: Batch 50/116 loss: 0.0924\n",
      "Epoch 97: Batch 51/116 loss: 0.0358\n",
      "Epoch 97: Batch 52/116 loss: 0.0569\n",
      "Epoch 97: Batch 53/116 loss: 0.0735\n",
      "Epoch 97: Batch 54/116 loss: 0.0556\n",
      "Epoch 97: Batch 55/116 loss: 0.0466\n",
      "Epoch 97: Batch 56/116 loss: 0.0391\n",
      "Epoch 97: Batch 57/116 loss: 0.0489\n",
      "Epoch 97: Batch 58/116 loss: 0.0331\n",
      "Epoch 97: Batch 59/116 loss: 0.0597\n",
      "Epoch 97: Batch 60/116 loss: 0.0539\n",
      "Epoch 97: Batch 61/116 loss: 0.0537\n",
      "Epoch 97: Batch 62/116 loss: 0.0544\n",
      "Epoch 97: Batch 63/116 loss: 0.0660\n",
      "Epoch 97: Batch 64/116 loss: 0.0696\n",
      "Epoch 97: Batch 65/116 loss: 0.0607\n",
      "Epoch 97: Batch 66/116 loss: 0.0439\n",
      "Epoch 97: Batch 67/116 loss: 0.0664\n",
      "Epoch 97: Batch 68/116 loss: 0.0646\n",
      "Epoch 97: Batch 69/116 loss: 0.0281\n",
      "Epoch 97: Batch 70/116 loss: 0.0534\n",
      "Epoch 97: Batch 71/116 loss: 0.0785\n",
      "Epoch 97: Batch 72/116 loss: 0.0330\n",
      "Epoch 97: Batch 73/116 loss: 0.0487\n",
      "Epoch 97: Batch 74/116 loss: 0.0484\n",
      "Epoch 97: Batch 75/116 loss: 0.0376\n",
      "Epoch 97: Batch 76/116 loss: 0.0749\n",
      "Epoch 97: Batch 77/116 loss: 0.0432\n",
      "Epoch 97: Batch 78/116 loss: 0.0636\n",
      "Epoch 97: Batch 79/116 loss: 0.0444\n",
      "Epoch 97: Batch 80/116 loss: 0.0678\n",
      "Epoch 97: Batch 81/116 loss: 0.0320\n",
      "Epoch 97: Batch 82/116 loss: 0.0513\n",
      "Epoch 97: Batch 83/116 loss: 0.0533\n",
      "Epoch 97: Batch 84/116 loss: 0.0364\n",
      "Epoch 97: Batch 85/116 loss: 0.0609\n",
      "Epoch 97: Batch 86/116 loss: 0.0722\n",
      "Epoch 97: Batch 87/116 loss: 0.0478\n",
      "Epoch 97: Batch 88/116 loss: 0.0360\n",
      "Epoch 97: Batch 89/116 loss: 0.0544\n",
      "Epoch 97: Batch 90/116 loss: 0.0586\n",
      "Epoch 97: Batch 91/116 loss: 0.0771\n",
      "Epoch 97: Batch 92/116 loss: 0.0567\n",
      "Epoch 97: Batch 93/116 loss: 0.0595\n",
      "Epoch 97: Batch 94/116 loss: 0.0518\n",
      "Epoch 97: Batch 95/116 loss: 0.0619\n",
      "Epoch 97: Batch 96/116 loss: 0.0700\n",
      "Epoch 97: Batch 97/116 loss: 0.0583\n",
      "Epoch 97: Batch 98/116 loss: 0.0444\n",
      "Epoch 97: Batch 99/116 loss: 0.0518\n",
      "Epoch 97: Batch 100/116 loss: 0.0393\n",
      "Epoch 97: Batch 101/116 loss: 0.0773\n",
      "Epoch 97: Batch 102/116 loss: 0.0476\n",
      "Epoch 97: Batch 103/116 loss: 0.0353\n",
      "Epoch 97: Batch 104/116 loss: 0.0765\n",
      "Epoch 97: Batch 105/116 loss: 0.0459\n",
      "Epoch 97: Batch 106/116 loss: 0.0599\n",
      "Epoch 97: Batch 107/116 loss: 0.0435\n",
      "Epoch 97: Batch 108/116 loss: 0.0351\n",
      "Epoch 97: Batch 109/116 loss: 0.0358\n",
      "Epoch 97: Batch 110/116 loss: 0.0648\n",
      "Epoch 97: Batch 111/116 loss: 0.0709\n",
      "Epoch 97: Batch 112/116 loss: 0.0591\n",
      "Epoch 97: Batch 113/116 loss: 0.0494\n",
      "Epoch 97: Batch 114/116 loss: 0.0693\n",
      "Epoch 97: Batch 115/116 loss: 0.0398\n",
      "Epoch 97: Batch 116/116 loss: 0.0363\n",
      "Epoch 97 train loss: 0.0536 valid loss: 0.0551\n",
      "performance reducing: 6\n",
      "Epoch 98: Batch 1/116 loss: 0.0465\n",
      "Epoch 98: Batch 2/116 loss: 0.0523\n",
      "Epoch 98: Batch 3/116 loss: 0.0579\n",
      "Epoch 98: Batch 4/116 loss: 0.0394\n",
      "Epoch 98: Batch 5/116 loss: 0.0620\n",
      "Epoch 98: Batch 6/116 loss: 0.0565\n",
      "Epoch 98: Batch 7/116 loss: 0.0804\n",
      "Epoch 98: Batch 8/116 loss: 0.0447\n",
      "Epoch 98: Batch 9/116 loss: 0.0306\n",
      "Epoch 98: Batch 10/116 loss: 0.0402\n",
      "Epoch 98: Batch 11/116 loss: 0.0398\n",
      "Epoch 98: Batch 12/116 loss: 0.0249\n",
      "Epoch 98: Batch 13/116 loss: 0.0371\n",
      "Epoch 98: Batch 14/116 loss: 0.0451\n",
      "Epoch 98: Batch 15/116 loss: 0.0666\n",
      "Epoch 98: Batch 16/116 loss: 0.0489\n",
      "Epoch 98: Batch 17/116 loss: 0.0688\n",
      "Epoch 98: Batch 18/116 loss: 0.0508\n",
      "Epoch 98: Batch 19/116 loss: 0.0480\n",
      "Epoch 98: Batch 20/116 loss: 0.0639\n",
      "Epoch 98: Batch 21/116 loss: 0.0660\n",
      "Epoch 98: Batch 22/116 loss: 0.0657\n",
      "Epoch 98: Batch 23/116 loss: 0.0361\n",
      "Epoch 98: Batch 24/116 loss: 0.0696\n",
      "Epoch 98: Batch 25/116 loss: 0.0404\n",
      "Epoch 98: Batch 26/116 loss: 0.0542\n",
      "Epoch 98: Batch 27/116 loss: 0.0600\n",
      "Epoch 98: Batch 28/116 loss: 0.0521\n",
      "Epoch 98: Batch 29/116 loss: 0.0477\n",
      "Epoch 98: Batch 30/116 loss: 0.0331\n",
      "Epoch 98: Batch 31/116 loss: 0.0717\n",
      "Epoch 98: Batch 32/116 loss: 0.0574\n",
      "Epoch 98: Batch 33/116 loss: 0.0657\n",
      "Epoch 98: Batch 34/116 loss: 0.0605\n",
      "Epoch 98: Batch 35/116 loss: 0.0568\n",
      "Epoch 98: Batch 36/116 loss: 0.0486\n",
      "Epoch 98: Batch 37/116 loss: 0.0471\n",
      "Epoch 98: Batch 38/116 loss: 0.0414\n",
      "Epoch 98: Batch 39/116 loss: 0.0451\n",
      "Epoch 98: Batch 40/116 loss: 0.0444\n",
      "Epoch 98: Batch 41/116 loss: 0.0549\n",
      "Epoch 98: Batch 42/116 loss: 0.0492\n",
      "Epoch 98: Batch 43/116 loss: 0.0547\n",
      "Epoch 98: Batch 44/116 loss: 0.0632\n",
      "Epoch 98: Batch 45/116 loss: 0.0500\n",
      "Epoch 98: Batch 46/116 loss: 0.0807\n",
      "Epoch 98: Batch 47/116 loss: 0.0504\n",
      "Epoch 98: Batch 48/116 loss: 0.0568\n",
      "Epoch 98: Batch 49/116 loss: 0.0380\n",
      "Epoch 98: Batch 50/116 loss: 0.0505\n",
      "Epoch 98: Batch 51/116 loss: 0.0655\n",
      "Epoch 98: Batch 52/116 loss: 0.0646\n",
      "Epoch 98: Batch 53/116 loss: 0.0582\n",
      "Epoch 98: Batch 54/116 loss: 0.0450\n",
      "Epoch 98: Batch 55/116 loss: 0.0577\n",
      "Epoch 98: Batch 56/116 loss: 0.0520\n",
      "Epoch 98: Batch 57/116 loss: 0.0535\n",
      "Epoch 98: Batch 58/116 loss: 0.0516\n",
      "Epoch 98: Batch 59/116 loss: 0.0394\n",
      "Epoch 98: Batch 60/116 loss: 0.0319\n",
      "Epoch 98: Batch 61/116 loss: 0.0520\n",
      "Epoch 98: Batch 62/116 loss: 0.0332\n",
      "Epoch 98: Batch 63/116 loss: 0.0420\n",
      "Epoch 98: Batch 64/116 loss: 0.0540\n",
      "Epoch 98: Batch 65/116 loss: 0.0299\n",
      "Epoch 98: Batch 66/116 loss: 0.0500\n",
      "Epoch 98: Batch 67/116 loss: 0.0388\n",
      "Epoch 98: Batch 68/116 loss: 0.0526\n",
      "Epoch 98: Batch 69/116 loss: 0.0681\n",
      "Epoch 98: Batch 70/116 loss: 0.0475\n",
      "Epoch 98: Batch 71/116 loss: 0.0517\n",
      "Epoch 98: Batch 72/116 loss: 0.0483\n",
      "Epoch 98: Batch 73/116 loss: 0.0590\n",
      "Epoch 98: Batch 74/116 loss: 0.0610\n",
      "Epoch 98: Batch 75/116 loss: 0.0678\n",
      "Epoch 98: Batch 76/116 loss: 0.0500\n",
      "Epoch 98: Batch 77/116 loss: 0.0684\n",
      "Epoch 98: Batch 78/116 loss: 0.0476\n",
      "Epoch 98: Batch 79/116 loss: 0.0439\n",
      "Epoch 98: Batch 80/116 loss: 0.0508\n",
      "Epoch 98: Batch 81/116 loss: 0.0879\n",
      "Epoch 98: Batch 82/116 loss: 0.0605\n",
      "Epoch 98: Batch 83/116 loss: 0.0406\n",
      "Epoch 98: Batch 84/116 loss: 0.0522\n",
      "Epoch 98: Batch 85/116 loss: 0.0445\n",
      "Epoch 98: Batch 86/116 loss: 0.0496\n",
      "Epoch 98: Batch 87/116 loss: 0.0393\n",
      "Epoch 98: Batch 88/116 loss: 0.1061\n",
      "Epoch 98: Batch 89/116 loss: 0.0762\n",
      "Epoch 98: Batch 90/116 loss: 0.0851\n",
      "Epoch 98: Batch 91/116 loss: 0.1047\n",
      "Epoch 98: Batch 92/116 loss: 0.0691\n",
      "Epoch 98: Batch 93/116 loss: 0.0732\n",
      "Epoch 98: Batch 94/116 loss: 0.0982\n",
      "Epoch 98: Batch 95/116 loss: 0.0597\n",
      "Epoch 98: Batch 96/116 loss: 0.0648\n",
      "Epoch 98: Batch 97/116 loss: 0.0682\n",
      "Epoch 98: Batch 98/116 loss: 0.0854\n",
      "Epoch 98: Batch 99/116 loss: 0.0604\n",
      "Epoch 98: Batch 100/116 loss: 0.0609\n",
      "Epoch 98: Batch 101/116 loss: 0.0731\n",
      "Epoch 98: Batch 102/116 loss: 0.0499\n",
      "Epoch 98: Batch 103/116 loss: 0.0372\n",
      "Epoch 98: Batch 104/116 loss: 0.0569\n",
      "Epoch 98: Batch 105/116 loss: 0.0558\n",
      "Epoch 98: Batch 106/116 loss: 0.0570\n",
      "Epoch 98: Batch 107/116 loss: 0.0921\n",
      "Epoch 98: Batch 108/116 loss: 0.0559\n",
      "Epoch 98: Batch 109/116 loss: 0.0521\n",
      "Epoch 98: Batch 110/116 loss: 0.0484\n",
      "Epoch 98: Batch 111/116 loss: 0.0595\n",
      "Epoch 98: Batch 112/116 loss: 0.0710\n",
      "Epoch 98: Batch 113/116 loss: 0.0870\n",
      "Epoch 98: Batch 114/116 loss: 0.0531\n",
      "Epoch 98: Batch 115/116 loss: 0.0581\n",
      "Epoch 98: Batch 116/116 loss: 0.0632\n",
      "Epoch 98 train loss: 0.0561 valid loss: 0.0867\n",
      "performance reducing: 7\n",
      "Epoch 99: Batch 1/116 loss: 0.0408\n",
      "Epoch 99: Batch 2/116 loss: 0.0648\n",
      "Epoch 99: Batch 3/116 loss: 0.0446\n",
      "Epoch 99: Batch 4/116 loss: 0.1300\n",
      "Epoch 99: Batch 5/116 loss: 0.0598\n",
      "Epoch 99: Batch 6/116 loss: 0.0621\n",
      "Epoch 99: Batch 7/116 loss: 0.0525\n",
      "Epoch 99: Batch 8/116 loss: 0.0662\n",
      "Epoch 99: Batch 9/116 loss: 0.0542\n",
      "Epoch 99: Batch 10/116 loss: 0.0562\n",
      "Epoch 99: Batch 11/116 loss: 0.0496\n",
      "Epoch 99: Batch 12/116 loss: 0.0557\n",
      "Epoch 99: Batch 13/116 loss: 0.0613\n",
      "Epoch 99: Batch 14/116 loss: 0.0586\n",
      "Epoch 99: Batch 15/116 loss: 0.0539\n",
      "Epoch 99: Batch 16/116 loss: 0.0460\n",
      "Epoch 99: Batch 17/116 loss: 0.0581\n",
      "Epoch 99: Batch 18/116 loss: 0.0413\n",
      "Epoch 99: Batch 19/116 loss: 0.0316\n",
      "Epoch 99: Batch 20/116 loss: 0.0610\n",
      "Epoch 99: Batch 21/116 loss: 0.0496\n",
      "Epoch 99: Batch 22/116 loss: 0.0573\n",
      "Epoch 99: Batch 23/116 loss: 0.0607\n",
      "Epoch 99: Batch 24/116 loss: 0.0613\n",
      "Epoch 99: Batch 25/116 loss: 0.0576\n",
      "Epoch 99: Batch 26/116 loss: 0.0528\n",
      "Epoch 99: Batch 27/116 loss: 0.0414\n",
      "Epoch 99: Batch 28/116 loss: 0.0421\n",
      "Epoch 99: Batch 29/116 loss: 0.0423\n",
      "Epoch 99: Batch 30/116 loss: 0.0740\n",
      "Epoch 99: Batch 31/116 loss: 0.0763\n",
      "Epoch 99: Batch 32/116 loss: 0.0482\n",
      "Epoch 99: Batch 33/116 loss: 0.0430\n",
      "Epoch 99: Batch 34/116 loss: 0.0589\n",
      "Epoch 99: Batch 35/116 loss: 0.0369\n",
      "Epoch 99: Batch 36/116 loss: 0.0424\n",
      "Epoch 99: Batch 37/116 loss: 0.0662\n",
      "Epoch 99: Batch 38/116 loss: 0.0571\n",
      "Epoch 99: Batch 39/116 loss: 0.0485\n",
      "Epoch 99: Batch 40/116 loss: 0.0493\n",
      "Epoch 99: Batch 41/116 loss: 0.0545\n",
      "Epoch 99: Batch 42/116 loss: 0.0529\n",
      "Epoch 99: Batch 43/116 loss: 0.0486\n",
      "Epoch 99: Batch 44/116 loss: 0.0525\n",
      "Epoch 99: Batch 45/116 loss: 0.0647\n",
      "Epoch 99: Batch 46/116 loss: 0.0373\n",
      "Epoch 99: Batch 47/116 loss: 0.0557\n",
      "Epoch 99: Batch 48/116 loss: 0.0553\n",
      "Epoch 99: Batch 49/116 loss: 0.0464\n",
      "Epoch 99: Batch 50/116 loss: 0.0399\n",
      "Epoch 99: Batch 51/116 loss: 0.0556\n",
      "Epoch 99: Batch 52/116 loss: 0.0481\n",
      "Epoch 99: Batch 53/116 loss: 0.0525\n",
      "Epoch 99: Batch 54/116 loss: 0.0537\n",
      "Epoch 99: Batch 55/116 loss: 0.0704\n",
      "Epoch 99: Batch 56/116 loss: 0.0442\n",
      "Epoch 99: Batch 57/116 loss: 0.0670\n",
      "Epoch 99: Batch 58/116 loss: 0.0534\n",
      "Epoch 99: Batch 59/116 loss: 0.0536\n",
      "Epoch 99: Batch 60/116 loss: 0.0679\n",
      "Epoch 99: Batch 61/116 loss: 0.0692\n",
      "Epoch 99: Batch 62/116 loss: 0.0567\n",
      "Epoch 99: Batch 63/116 loss: 0.0408\n",
      "Epoch 99: Batch 64/116 loss: 0.0520\n",
      "Epoch 99: Batch 65/116 loss: 0.0487\n",
      "Epoch 99: Batch 66/116 loss: 0.0450\n",
      "Epoch 99: Batch 67/116 loss: 0.0457\n",
      "Epoch 99: Batch 68/116 loss: 0.0315\n",
      "Epoch 99: Batch 69/116 loss: 0.0624\n",
      "Epoch 99: Batch 70/116 loss: 0.0623\n",
      "Epoch 99: Batch 71/116 loss: 0.0395\n",
      "Epoch 99: Batch 72/116 loss: 0.0523\n",
      "Epoch 99: Batch 73/116 loss: 0.0446\n",
      "Epoch 99: Batch 74/116 loss: 0.0454\n",
      "Epoch 99: Batch 75/116 loss: 0.0620\n",
      "Epoch 99: Batch 76/116 loss: 0.0417\n",
      "Epoch 99: Batch 77/116 loss: 0.0558\n",
      "Epoch 99: Batch 78/116 loss: 0.0486\n",
      "Epoch 99: Batch 79/116 loss: 0.0770\n",
      "Epoch 99: Batch 80/116 loss: 0.0394\n",
      "Epoch 99: Batch 81/116 loss: 0.0773\n",
      "Epoch 99: Batch 82/116 loss: 0.0528\n",
      "Epoch 99: Batch 83/116 loss: 0.0449\n",
      "Epoch 99: Batch 84/116 loss: 0.0528\n",
      "Epoch 99: Batch 85/116 loss: 0.0409\n",
      "Epoch 99: Batch 86/116 loss: 0.0364\n",
      "Epoch 99: Batch 87/116 loss: 0.0439\n",
      "Epoch 99: Batch 88/116 loss: 0.0507\n",
      "Epoch 99: Batch 89/116 loss: 0.0657\n",
      "Epoch 99: Batch 90/116 loss: 0.0508\n",
      "Epoch 99: Batch 91/116 loss: 0.0386\n",
      "Epoch 99: Batch 92/116 loss: 0.0521\n",
      "Epoch 99: Batch 93/116 loss: 0.0431\n",
      "Epoch 99: Batch 94/116 loss: 0.0529\n",
      "Epoch 99: Batch 95/116 loss: 0.0543\n",
      "Epoch 99: Batch 96/116 loss: 0.0553\n",
      "Epoch 99: Batch 97/116 loss: 0.0425\n",
      "Epoch 99: Batch 98/116 loss: 0.0777\n",
      "Epoch 99: Batch 99/116 loss: 0.0600\n",
      "Epoch 99: Batch 100/116 loss: 0.0505\n",
      "Epoch 99: Batch 101/116 loss: 0.0673\n",
      "Epoch 99: Batch 102/116 loss: 0.0496\n",
      "Epoch 99: Batch 103/116 loss: 0.0516\n",
      "Epoch 99: Batch 104/116 loss: 0.0509\n",
      "Epoch 99: Batch 105/116 loss: 0.0368\n",
      "Epoch 99: Batch 106/116 loss: 0.0441\n",
      "Epoch 99: Batch 107/116 loss: 0.0405\n",
      "Epoch 99: Batch 108/116 loss: 0.0507\n",
      "Epoch 99: Batch 109/116 loss: 0.0604\n",
      "Epoch 99: Batch 110/116 loss: 0.0561\n",
      "Epoch 99: Batch 111/116 loss: 0.0307\n",
      "Epoch 99: Batch 112/116 loss: 0.0522\n",
      "Epoch 99: Batch 113/116 loss: 0.0478\n",
      "Epoch 99: Batch 114/116 loss: 0.0699\n",
      "Epoch 99: Batch 115/116 loss: 0.0748\n",
      "Epoch 99: Batch 116/116 loss: 0.0590\n",
      "Epoch 99 train loss: 0.0534 valid loss: 0.0666\n",
      "performance reducing: 8\n",
      "Epoch 100: Batch 1/116 loss: 0.0457\n",
      "Epoch 100: Batch 2/116 loss: 0.0532\n",
      "Epoch 100: Batch 3/116 loss: 0.0590\n",
      "Epoch 100: Batch 4/116 loss: 0.0545\n",
      "Epoch 100: Batch 5/116 loss: 0.0671\n",
      "Epoch 100: Batch 6/116 loss: 0.0241\n",
      "Epoch 100: Batch 7/116 loss: 0.0463\n",
      "Epoch 100: Batch 8/116 loss: 0.0500\n",
      "Epoch 100: Batch 9/116 loss: 0.0469\n",
      "Epoch 100: Batch 10/116 loss: 0.0297\n",
      "Epoch 100: Batch 11/116 loss: 0.0406\n",
      "Epoch 100: Batch 12/116 loss: 0.0335\n",
      "Epoch 100: Batch 13/116 loss: 0.0608\n",
      "Epoch 100: Batch 14/116 loss: 0.0698\n",
      "Epoch 100: Batch 15/116 loss: 0.0376\n",
      "Epoch 100: Batch 16/116 loss: 0.0487\n",
      "Epoch 100: Batch 17/116 loss: 0.0514\n",
      "Epoch 100: Batch 18/116 loss: 0.0495\n",
      "Epoch 100: Batch 19/116 loss: 0.0571\n",
      "Epoch 100: Batch 20/116 loss: 0.0399\n",
      "Epoch 100: Batch 21/116 loss: 0.0692\n",
      "Epoch 100: Batch 22/116 loss: 0.0305\n",
      "Epoch 100: Batch 23/116 loss: 0.0640\n",
      "Epoch 100: Batch 24/116 loss: 0.0548\n",
      "Epoch 100: Batch 25/116 loss: 0.0797\n",
      "Epoch 100: Batch 26/116 loss: 0.0488\n",
      "Epoch 100: Batch 27/116 loss: 0.0431\n",
      "Epoch 100: Batch 28/116 loss: 0.0419\n",
      "Epoch 100: Batch 29/116 loss: 0.0383\n",
      "Epoch 100: Batch 30/116 loss: 0.0627\n",
      "Epoch 100: Batch 31/116 loss: 0.0732\n",
      "Epoch 100: Batch 32/116 loss: 0.0669\n",
      "Epoch 100: Batch 33/116 loss: 0.0646\n",
      "Epoch 100: Batch 34/116 loss: 0.0457\n",
      "Epoch 100: Batch 35/116 loss: 0.0707\n",
      "Epoch 100: Batch 36/116 loss: 0.0519\n",
      "Epoch 100: Batch 37/116 loss: 0.0566\n",
      "Epoch 100: Batch 38/116 loss: 0.0453\n",
      "Epoch 100: Batch 39/116 loss: 0.0588\n",
      "Epoch 100: Batch 40/116 loss: 0.0406\n",
      "Epoch 100: Batch 41/116 loss: 0.0263\n",
      "Epoch 100: Batch 42/116 loss: 0.0619\n",
      "Epoch 100: Batch 43/116 loss: 0.0528\n",
      "Epoch 100: Batch 44/116 loss: 0.0484\n",
      "Epoch 100: Batch 45/116 loss: 0.0418\n",
      "Epoch 100: Batch 46/116 loss: 0.0543\n",
      "Epoch 100: Batch 47/116 loss: 0.0410\n",
      "Epoch 100: Batch 48/116 loss: 0.0583\n",
      "Epoch 100: Batch 49/116 loss: 0.0447\n",
      "Epoch 100: Batch 50/116 loss: 0.0834\n",
      "Epoch 100: Batch 51/116 loss: 0.0559\n",
      "Epoch 100: Batch 52/116 loss: 0.0680\n",
      "Epoch 100: Batch 53/116 loss: 0.0498\n",
      "Epoch 100: Batch 54/116 loss: 0.0477\n",
      "Epoch 100: Batch 55/116 loss: 0.0667\n",
      "Epoch 100: Batch 56/116 loss: 0.0703\n",
      "Epoch 100: Batch 57/116 loss: 0.0529\n",
      "Epoch 100: Batch 58/116 loss: 0.0645\n",
      "Epoch 100: Batch 59/116 loss: 0.0554\n",
      "Epoch 100: Batch 60/116 loss: 0.0410\n",
      "Epoch 100: Batch 61/116 loss: 0.0726\n",
      "Epoch 100: Batch 62/116 loss: 0.0434\n",
      "Epoch 100: Batch 63/116 loss: 0.0393\n",
      "Epoch 100: Batch 64/116 loss: 0.0800\n",
      "Epoch 100: Batch 65/116 loss: 0.0313\n",
      "Epoch 100: Batch 66/116 loss: 0.0710\n",
      "Epoch 100: Batch 67/116 loss: 0.0472\n",
      "Epoch 100: Batch 68/116 loss: 0.0606\n",
      "Epoch 100: Batch 69/116 loss: 0.0715\n",
      "Epoch 100: Batch 70/116 loss: 0.0602\n",
      "Epoch 100: Batch 71/116 loss: 0.0559\n",
      "Epoch 100: Batch 72/116 loss: 0.0589\n",
      "Epoch 100: Batch 73/116 loss: 0.0465\n",
      "Epoch 100: Batch 74/116 loss: 0.0690\n",
      "Epoch 100: Batch 75/116 loss: 0.0553\n",
      "Epoch 100: Batch 76/116 loss: 0.0815\n",
      "Epoch 100: Batch 77/116 loss: 0.0430\n",
      "Epoch 100: Batch 78/116 loss: 0.0768\n",
      "Epoch 100: Batch 79/116 loss: 0.0905\n",
      "Epoch 100: Batch 80/116 loss: 0.1132\n",
      "Epoch 100: Batch 81/116 loss: 0.0709\n",
      "Epoch 100: Batch 82/116 loss: 0.0733\n",
      "Epoch 100: Batch 83/116 loss: 0.0714\n",
      "Epoch 100: Batch 84/116 loss: 0.0540\n",
      "Epoch 100: Batch 85/116 loss: 0.0532\n",
      "Epoch 100: Batch 86/116 loss: 0.0500\n",
      "Epoch 100: Batch 87/116 loss: 0.0606\n",
      "Epoch 100: Batch 88/116 loss: 0.0576\n",
      "Epoch 100: Batch 89/116 loss: 0.0698\n",
      "Epoch 100: Batch 90/116 loss: 0.0457\n",
      "Epoch 100: Batch 91/116 loss: 0.0535\n",
      "Epoch 100: Batch 92/116 loss: 0.0420\n",
      "Epoch 100: Batch 93/116 loss: 0.0446\n",
      "Epoch 100: Batch 94/116 loss: 0.0454\n",
      "Epoch 100: Batch 95/116 loss: 0.0510\n",
      "Epoch 100: Batch 96/116 loss: 0.0552\n",
      "Epoch 100: Batch 97/116 loss: 0.0379\n",
      "Epoch 100: Batch 98/116 loss: 0.0553\n",
      "Epoch 100: Batch 99/116 loss: 0.0542\n",
      "Epoch 100: Batch 100/116 loss: 0.0529\n",
      "Epoch 100: Batch 101/116 loss: 0.0567\n",
      "Epoch 100: Batch 102/116 loss: 0.0486\n",
      "Epoch 100: Batch 103/116 loss: 0.0429\n",
      "Epoch 100: Batch 104/116 loss: 0.0391\n",
      "Epoch 100: Batch 105/116 loss: 0.0597\n",
      "Epoch 100: Batch 106/116 loss: 0.0628\n",
      "Epoch 100: Batch 107/116 loss: 0.0391\n",
      "Epoch 100: Batch 108/116 loss: 0.0456\n",
      "Epoch 100: Batch 109/116 loss: 0.0563\n",
      "Epoch 100: Batch 110/116 loss: 0.0465\n",
      "Epoch 100: Batch 111/116 loss: 0.0363\n",
      "Epoch 100: Batch 112/116 loss: 0.0414\n",
      "Epoch 100: Batch 113/116 loss: 0.0503\n",
      "Epoch 100: Batch 114/116 loss: 0.0763\n",
      "Epoch 100: Batch 115/116 loss: 0.0333\n",
      "Epoch 100: Batch 116/116 loss: 0.0548\n",
      "Epoch 100 train loss: 0.0544 valid loss: 0.0604\n",
      "performance reducing: 9\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lr = 1e-3\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # 后面修改为Dice损失函数看看效果\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/images/*')\n",
    "    train_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/train/masks/*')\n",
    "    \n",
    "    val_image_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/images/*')\n",
    "    val_mask_path = glob('../input/d/chunlei11/covid19xray/covid19-xray/val/masks/*')\n",
    "\n",
    "    train_transforms = Compose([\n",
    "        # todo 先Resize试一下\n",
    "        ToPILImage(),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomRotation(degrees=(0, 180)),\n",
    "#         ColorJitter(), # 这个有问题，但是是什么问题？\n",
    "#         GrayScale(), # 这个貌似也有问题，问题更大\n",
    "        RandomCrop(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    valid_transforms = Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(256),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = load_data(train_image_path, train_mask_path, batch_size=32, drop_last=True,\n",
    "                             transforms=train_transforms)\n",
    "    valid_loader = load_data(val_image_path, val_mask_path, batch_size=32, transforms=valid_transforms)\n",
    "    \n",
    "    continue_train = True\n",
    "    \n",
    "    if not continue_train:\n",
    "        epoch = 50\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, epoch, device=device)\n",
    "    else:\n",
    "        total_epoch = 100\n",
    "        pretrain_params = torch.load('../input/covid-xray-unet/last_model.pth')\n",
    "        model.load_state_dict(pretrain_params['last_model_state_dict'])\n",
    "        optimizer.load_state_dict(pretrain_params['last_optimizer_state_dict'])\n",
    "        current_epoch = pretrain_params['epoch']\n",
    "        model.train()\n",
    "        train(train_loader, valid_loader, model, criterion, optimizer, total_epoch, current_epoch, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9499.914316,
   "end_time": "2022-08-10T16:17:14.526500",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-10T13:38:54.612184",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
